[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v1",
                "updated": "2025-02-08T03:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernndez"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivankovi Nizi"
                    },
                    {
                        "name": "Milko Jaksi"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaqun Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gmez-Albarrn"
                    },
                    {
                        "name": "Jos-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "Jos-Luis Sierra"
                },
                "author": "Jos-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.06787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06787v1",
                "updated": "2025-02-10T18:59:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    35,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Agentic AI for Spatial Reasoning with a Dynamic API"
                },
                "summary": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/"
                },
                "authors": [
                    {
                        "name": "Damiano Marsili"
                    },
                    {
                        "name": "Rohun Agrawal"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gkioxari"
                },
                "author": "Georgia Gkioxari",
                "arxiv_comment": "Project website: https://glab-caltech.github.io/vadar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06786v1",
                "updated": "2025-02-10T18:59:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    10,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:59:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Quantization"
                },
                "summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to $10\\%$ more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to $10\\%$ more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model."
                },
                "authors": [
                    {
                        "name": "Pranav Nair"
                    },
                    {
                        "name": "Puranjay Datta"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06782v1",
                "updated": "2025-02-10T18:58:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    58,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:58:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    58,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT"
                },
                "summary": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video."
                },
                "authors": [
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08413v2",
                "updated": "2025-02-10T18:57:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    57,
                    27,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-14T20:08:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    8,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data"
                },
                "summary": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling."
                },
                "authors": [
                    {
                        "name": "Jiaxing Qiu"
                    },
                    {
                        "name": "Dongliang Guo"
                    },
                    {
                        "name": "Natalie Papini"
                    },
                    {
                        "name": "Noelle Peace"
                    },
                    {
                        "name": "Cheri A. Levinson"
                    },
                    {
                        "name": "Teague R. Henry"
                    }
                ],
                "author_detail": {
                    "name": "Teague R. Henry"
                },
                "author": "Teague R. Henry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06779v1",
                "updated": "2025-02-10T18:56:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    56,
                    14,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:56:14Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    56,
                    14,
                    0,
                    41,
                    0
                ],
                "title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission\n  for Visual Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission\n  for Visual Classification"
                },
                "summary": "Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_comment": "5 pages, 3 figures, Accepted by ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06776v1",
                "updated": "2025-02-10T18:54:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:54:05Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "title": "Towards Internet-Scale Training For Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Internet-Scale Training For Agents"
                },
                "summary": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io."
                },
                "authors": [
                    {
                        "name": "Brandon Trabucco"
                    },
                    {
                        "name": "Gunnar Sigurdsson"
                    },
                    {
                        "name": "Robinson Piramuthu"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Salakhutdinov"
                },
                "author": "Ruslan Salakhutdinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06773v1",
                "updated": "2025-02-10T18:52:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    52,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:52:04Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    52,
                    4,
                    0,
                    41,
                    0
                ],
                "title": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition"
                },
                "summary": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}."
                },
                "authors": [
                    {
                        "name": "Guanghao Ye"
                    },
                    {
                        "name": "Khiem Duc Pham"
                    },
                    {
                        "name": "Xinzhi Zhang"
                    },
                    {
                        "name": "Sivakanth Gopi"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Janardhan Kulkarni"
                    },
                    {
                        "name": "Huseyin A. Inan"
                    }
                ],
                "author_detail": {
                    "name": "Huseyin A. Inan"
                },
                "author": "Huseyin A. Inan",
                "arxiv_comment": "Abstract shortened for arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06772v1",
                "updated": "2025-02-10T18:51:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    51,
                    47,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:51:47Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    51,
                    47,
                    0,
                    41,
                    0
                ],
                "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates"
                },
                "summary": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code: https://github.com/Gen-Verse/ReasonFlux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06768v1",
                "updated": "2025-02-10T18:47:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:47:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in\n  Masked Diffusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train for the Worst, Plan for the Best: Understanding Token Ordering in\n  Masked Diffusions"
                },
                "summary": "In recent years, masked diffusion models (MDMs) have emerged as a promising\nalternative approach for generative modeling over discrete domains. Compared to\nautoregressive models (ARMs), MDMs trade off complexity at training time with\nflexibility at inference time. At training time, they must learn to solve an\nexponentially large number of infilling problems, but at inference time, they\ncan decode tokens in essentially arbitrary order. In this work, we closely\nexamine these two competing effects. On the training front, we theoretically\nand empirically demonstrate that MDMs indeed train on computationally\nintractable subproblems compared to their autoregressive counterparts. On the\ninference front, we show that a suitable strategy for adaptively choosing the\ntoken decoding order significantly enhances the capabilities of MDMs, allowing\nthem to sidestep hard subproblems. On logic puzzles like Sudoku, we show that\nadaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to\n$\\approx 90$%, even outperforming ARMs with $7\\times$ as many parameters and\nthat were explicitly trained via teacher forcing to learn the right order of\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, masked diffusion models (MDMs) have emerged as a promising\nalternative approach for generative modeling over discrete domains. Compared to\nautoregressive models (ARMs), MDMs trade off complexity at training time with\nflexibility at inference time. At training time, they must learn to solve an\nexponentially large number of infilling problems, but at inference time, they\ncan decode tokens in essentially arbitrary order. In this work, we closely\nexamine these two competing effects. On the training front, we theoretically\nand empirically demonstrate that MDMs indeed train on computationally\nintractable subproblems compared to their autoregressive counterparts. On the\ninference front, we show that a suitable strategy for adaptively choosing the\ntoken decoding order significantly enhances the capabilities of MDMs, allowing\nthem to sidestep hard subproblems. On logic puzzles like Sudoku, we show that\nadaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to\n$\\approx 90$%, even outperforming ARMs with $7\\times$ as many parameters and\nthat were explicitly trained via teacher forcing to learn the right order of\ndecoding."
                },
                "authors": [
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Kulin Shah"
                    },
                    {
                        "name": "Vasilis Kontonis"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Sitan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sitan Chen"
                },
                "author": "Sitan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06766v1",
                "updated": "2025-02-10T18:47:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:47:04Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    4,
                    0,
                    41,
                    0
                ],
                "title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs"
                },
                "summary": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common long context\nbenchmarks (LM-Eval, AlpacaEval, and RULER).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common long context\nbenchmarks (LM-Eval, AlpacaEval, and RULER)."
                },
                "authors": [
                    {
                        "name": "Ryan Synk"
                    },
                    {
                        "name": "Monte Hoover"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Josue Melendez Sanchez"
                    },
                    {
                        "name": "Ramani Duraiswami"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "8 pages, 8 figures, 2 tables in main body",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10626v2",
                "updated": "2025-02-10T18:43:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    43,
                    26,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-14T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts"
                },
                "summary": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters."
                },
                "authors": [
                    {
                        "name": "Guorui Zheng"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yuping Zheng"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00365v2",
                "updated": "2025-02-10T18:40:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    40,
                    59,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-30T05:54:04Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    5,
                    54,
                    4,
                    5,
                    335,
                    0
                ],
                "title": "Cross Helicity and the Helium Abundance as an in situ Metric of Solar\n  Wind Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross Helicity and the Helium Abundance as an in situ Metric of Solar\n  Wind Acceleration"
                },
                "summary": "The two-state solar wind paradigm is based on observations showing that slow\nand fast solar wind have distinct properties like helium abundances, kinetic\nsignatures, elemental composition, and charge-state ratios. Nominally, the fast\nwind originates from solar sources that are continuously magnetically open to\nthe heliosphere like coronal holes while the slow wind is from solar sources\nthat are only intermittently open to the heliosphere like helmet streamers and\npseudostreamers. The Alfv\\'enic slow wind is an emerging 3rd class of solar\nwind that challenges the two-state fast/slow paradigm. It has slow wind speeds\nbut is highly Alfv\\'enic, i.e. has a high correlation between velocity and\nmagnetic field fluctuations along with low compressibility typical of Alfv\\'en\nwaves, which is typically observed in fast wind. Its other properties are also\nmore similar to the fast than slow wind. From 28 years of Wind observations at\n1 AU, we derive the solar wind helium abundance ($A_\\mathrm{He}$),\nAlfv\\'enicity ($\\left|\\sigma_c\\right|$), and solar wind speed\n($v_\\mathrm{sw}$). Characterizing vsw as a function of $\\left|\\sigma_c\\right|$\nand $A_\\mathrm{He}$, we show that the maximum solar wind speed for plasma\naccelerated in source regions that are intermittently open is faster than the\nminimum solar wind speed for plasma accelerated in continuously open regions.\nWe infer that the Alfv\\'enic slow wind is likely solar wind originating from\nopen-field regions with speeds below the maximum solar wind speed for plasma\nfrom intermittently open regions. We then discuss possible implications for\nsolar wind acceleration. Finally, we utilize the combination of helium\nabundance and normalized cross helicity to present a novel solar wind\ncategorization scheme that illustrates the transition in observations of solar\nwind at 1 AU from magnetically closed to magnetically open sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The two-state solar wind paradigm is based on observations showing that slow\nand fast solar wind have distinct properties like helium abundances, kinetic\nsignatures, elemental composition, and charge-state ratios. Nominally, the fast\nwind originates from solar sources that are continuously magnetically open to\nthe heliosphere like coronal holes while the slow wind is from solar sources\nthat are only intermittently open to the heliosphere like helmet streamers and\npseudostreamers. The Alfv\\'enic slow wind is an emerging 3rd class of solar\nwind that challenges the two-state fast/slow paradigm. It has slow wind speeds\nbut is highly Alfv\\'enic, i.e. has a high correlation between velocity and\nmagnetic field fluctuations along with low compressibility typical of Alfv\\'en\nwaves, which is typically observed in fast wind. Its other properties are also\nmore similar to the fast than slow wind. From 28 years of Wind observations at\n1 AU, we derive the solar wind helium abundance ($A_\\mathrm{He}$),\nAlfv\\'enicity ($\\left|\\sigma_c\\right|$), and solar wind speed\n($v_\\mathrm{sw}$). Characterizing vsw as a function of $\\left|\\sigma_c\\right|$\nand $A_\\mathrm{He}$, we show that the maximum solar wind speed for plasma\naccelerated in source regions that are intermittently open is faster than the\nminimum solar wind speed for plasma accelerated in continuously open regions.\nWe infer that the Alfv\\'enic slow wind is likely solar wind originating from\nopen-field regions with speeds below the maximum solar wind speed for plasma\nfrom intermittently open regions. We then discuss possible implications for\nsolar wind acceleration. Finally, we utilize the combination of helium\nabundance and normalized cross helicity to present a novel solar wind\ncategorization scheme that illustrates the transition in observations of solar\nwind at 1 AU from magnetically closed to magnetically open sources."
                },
                "authors": [
                    {
                        "name": "B. L. Alterman"
                    },
                    {
                        "name": "R. D'Amicis"
                    }
                ],
                "author_detail": {
                    "name": "R. D'Amicis"
                },
                "author": "R. D'Amicis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17755v2",
                "updated": "2025-02-10T18:39:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    39,
                    13,
                    0,
                    41,
                    0
                ],
                "published": "2024-09-26T11:40:07Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    40,
                    7,
                    3,
                    270,
                    0
                ],
                "title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning"
                },
                "summary": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis."
                },
                "authors": [
                    {
                        "name": "Rimvydas Rubavicius"
                    },
                    {
                        "name": "Peter David Fagan"
                    },
                    {
                        "name": "Alex Lascarides"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "arxiv_comment": "22 pages,4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06758v1",
                "updated": "2025-02-10T18:38:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    20,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:38:20Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    20,
                    0,
                    41,
                    0
                ],
                "title": "Comment on \"Generic machine learning inference on heterogeneous\n  treatment effects in randomized experiments.\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment on \"Generic machine learning inference on heterogeneous\n  treatment effects in randomized experiments.\""
                },
                "summary": "We analyze the split-sample robust inference (SSRI) methodology proposed by\nChernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying\nuncertainty in heterogeneous treatment effect estimation. While SSRI\neffectively accounts for randomness in data splitting, its computational cost\ncan be prohibitive when combined with complex machine learning (ML) models. We\npresent an alternative randomization inference (RI) approach that maintains\nSSRI's generality without requiring repeated data splitting. By leveraging\ncross-fitting and design-based inference, RI achieves valid confidence\nintervals while significantly reducing computational burden. We compare the two\nmethods through simulation, demonstrating that RI retains statistical\nefficiency while being more practical for large-scale applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the split-sample robust inference (SSRI) methodology proposed by\nChernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying\nuncertainty in heterogeneous treatment effect estimation. While SSRI\neffectively accounts for randomness in data splitting, its computational cost\ncan be prohibitive when combined with complex machine learning (ML) models. We\npresent an alternative randomization inference (RI) approach that maintains\nSSRI's generality without requiring repeated data splitting. By leveraging\ncross-fitting and design-based inference, RI achieves valid confidence\nintervals while significantly reducing computational burden. We compare the two\nmethods through simulation, demonstrating that RI retains statistical\nefficiency while being more practical for large-scale applications."
                },
                "authors": [
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Michael Lingzhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lingzhi Li"
                },
                "author": "Michael Lingzhi Li",
                "arxiv_comment": "Comment on arXiv:1712.04802",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06621v2",
                "updated": "2025-02-10T18:35:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    35,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2024-06-07T15:28:31Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    15,
                    28,
                    31,
                    4,
                    159,
                    0
                ],
                "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering"
                },
                "summary": "We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems."
                },
                "authors": [
                    {
                        "name": "Harry Li"
                    },
                    {
                        "name": "Gabriel Appleby"
                    },
                    {
                        "name": "Ashley Suh"
                    }
                ],
                "author_detail": {
                    "name": "Ashley Suh"
                },
                "author": "Ashley Suh",
                "arxiv_doi": "10.1109/VIS55277.2024.00031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/VIS55277.2024.00031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.06621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Open-source code: https://github.com/mit-ll/linkq",
                "arxiv_journal_ref": "H. Li, G. Appleby and A. Suh, \"LinkQ: An LLM-Assisted Visual\n  Interface for Knowledge Graph Question-Answering,\" 2024 IEEE Visualization\n  and Visual Analytics (VIS)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00761v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00761v4",
                "updated": "2025-02-10T18:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    26,
                    14,
                    0,
                    41,
                    0
                ],
                "published": "2024-08-01T17:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    59,
                    12,
                    3,
                    214,
                    0
                ],
                "title": "Tamper-Resistant Safeguards for Open-Weight LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tamper-Resistant Safeguards for Open-Weight LLMs"
                },
                "summary": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs."
                },
                "authors": [
                    {
                        "name": "Rishub Tamirisa"
                    },
                    {
                        "name": "Bhrugu Bharathi"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Rowan Wang"
                    },
                    {
                        "name": "Ron Arel"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Mantas Mazeika"
                    }
                ],
                "author_detail": {
                    "name": "Mantas Mazeika"
                },
                "author": "Mantas Mazeika",
                "arxiv_comment": "Website: https://www.tamper-resistant-safeguards.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00761v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00761v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16347v2",
                "updated": "2025-02-10T18:22:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    22,
                    51,
                    0,
                    41,
                    0
                ],
                "published": "2024-09-24T18:00:00Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    0,
                    0,
                    1,
                    268,
                    0
                ],
                "title": "TRINITY VI: Connection between Galaxy Star Formation Rates and\n  Supermassive Black Hole Accretion Rates from z=0-10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRINITY VI: Connection between Galaxy Star Formation Rates and\n  Supermassive Black Hole Accretion Rates from z=0-10"
                },
                "summary": "We infer supermassive black hole (SMBH) accretion rates and Eddington ratios\nas a function of SMBH/host galaxy mass and redshift with the empirical TRINITY\nmodel of dark matter halo--galaxy--SMBH connection. The galaxy--SMBH mass and\ngrowth rate connection from TRINITY matches galaxy observables from $0<z<13$\nand SMBH observables from $0<z<6.5$. Key findings include: 1) the ratio between\ncosmic SMBH accretion rate and galaxy star formation rate stays constant at\n$\\sim 2\\times 10^{-3}$ from $z=0-4$, and decreases by 2 orders of magnitude\nfrom $z=4-10$; 2) the average SMBH Eddington ratio $\\overline{\\eta}$ increases\ntowards higher redshifts, nearly reaching $\\overline{\\eta}=1$ at $z\\sim 10$; 3)\nat fixed redshift for $z<3$, SMBHs/galaxies with higher masses have lower\n$\\overline{\\eta}$, consistent with AGN downsizing; 4) the average ratio of\nspecific SMBH accretion rate ($\\overline{\\mathrm{SBHAR}}$) to average specific\nstar formation rate ($\\overline{\\mathrm{SSFR}}$) is nearly mass-independent,\nwith a value $\\overline{\\mathrm{SBHAR}}/\\overline{\\mathrm{SSFR}}\\sim 1$, which\ndecreases slightly from $z=10$ to $z=0$; 5) similar to galaxies, SMBHs reach\ntheir peak efficiency to convert baryons into mass when host halos reach\n$10^{12} M_\\odot$; 6) given galaxy and SMBH growth histories from TRINITY, the\nlocal descendants of $1<z<11$ overmassive JWST AGNs will remain outliers from\nthe local SMBH mass--galaxy mass relation. These findings combine to give a\nsimple explanation for massive ($10^9-10^{10}M_\\odot$) quasars at $z>6$: at\nthese redshifts, dark matter halos grow with an $e$-folding time of $\\sim 45$\nMyrs, driving similar growth rates in both galaxies and SMBHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We infer supermassive black hole (SMBH) accretion rates and Eddington ratios\nas a function of SMBH/host galaxy mass and redshift with the empirical TRINITY\nmodel of dark matter halo--galaxy--SMBH connection. The galaxy--SMBH mass and\ngrowth rate connection from TRINITY matches galaxy observables from $0<z<13$\nand SMBH observables from $0<z<6.5$. Key findings include: 1) the ratio between\ncosmic SMBH accretion rate and galaxy star formation rate stays constant at\n$\\sim 2\\times 10^{-3}$ from $z=0-4$, and decreases by 2 orders of magnitude\nfrom $z=4-10$; 2) the average SMBH Eddington ratio $\\overline{\\eta}$ increases\ntowards higher redshifts, nearly reaching $\\overline{\\eta}=1$ at $z\\sim 10$; 3)\nat fixed redshift for $z<3$, SMBHs/galaxies with higher masses have lower\n$\\overline{\\eta}$, consistent with AGN downsizing; 4) the average ratio of\nspecific SMBH accretion rate ($\\overline{\\mathrm{SBHAR}}$) to average specific\nstar formation rate ($\\overline{\\mathrm{SSFR}}$) is nearly mass-independent,\nwith a value $\\overline{\\mathrm{SBHAR}}/\\overline{\\mathrm{SSFR}}\\sim 1$, which\ndecreases slightly from $z=10$ to $z=0$; 5) similar to galaxies, SMBHs reach\ntheir peak efficiency to convert baryons into mass when host halos reach\n$10^{12} M_\\odot$; 6) given galaxy and SMBH growth histories from TRINITY, the\nlocal descendants of $1<z<11$ overmassive JWST AGNs will remain outliers from\nthe local SMBH mass--galaxy mass relation. These findings combine to give a\nsimple explanation for massive ($10^9-10^{10}M_\\odot$) quasars at $z>6$: at\nthese redshifts, dark matter halos grow with an $e$-folding time of $\\sim 45$\nMyrs, driving similar growth rates in both galaxies and SMBHs."
                },
                "authors": [
                    {
                        "name": "Haowen Zhang"
                    },
                    {
                        "name": "Peter Behroozi"
                    },
                    {
                        "name": "Marta Volonteri"
                    },
                    {
                        "name": "Joseph Silk"
                    },
                    {
                        "name": "Xiaohui Fan"
                    },
                    {
                        "name": "James Aird"
                    },
                    {
                        "name": "Jinyi Yang"
                    },
                    {
                        "name": "Feige Wang"
                    },
                    {
                        "name": "Philip F. Hopkins"
                    }
                ],
                "author_detail": {
                    "name": "Philip F. Hopkins"
                },
                "author": "Philip F. Hopkins",
                "arxiv_comment": "14 pages, 19 figures, accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06742v1",
                "updated": "2025-02-10T18:09:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    9,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:09:53Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    9,
                    53,
                    0,
                    41,
                    0
                ],
                "title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Multi-Normalization for Stateless and Scalable LLM Training"
                },
                "summary": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines."
                },
                "authors": [
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06738v1",
                "updated": "2025-02-10T18:07:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    7,
                    9,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:07:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    7,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "Resurrecting saturated LLM benchmarks with adversarial encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resurrecting saturated LLM benchmarks with adversarial encoding"
                },
                "summary": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks."
                },
                "authors": [
                    {
                        "name": "Igor Ivanov"
                    },
                    {
                        "name": "Dmitrii Volkov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Volkov"
                },
                "author": "Dmitrii Volkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06737v1",
                "updated": "2025-02-10T18:03:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    3,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    3,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data"
                },
                "summary": "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM."
                },
                "authors": [
                    {
                        "name": "Thomas Zeng"
                    },
                    {
                        "name": "Shuibai Zhang"
                    },
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Christian Classen"
                    },
                    {
                        "name": "Daewon Chae"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Heeju Kim"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Jackson Kunde"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Jungtaek Kim"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06736v1",
                "updated": "2025-02-10T18:00:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    0,
                    5,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    0,
                    5,
                    0,
                    41,
                    0
                ],
                "title": "Low-power Spike-based Wearable Analytics on RRAM Crossbars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-power Spike-based Wearable Analytics on RRAM Crossbars"
                },
                "summary": "This work introduces a spike-based wearable analytics system utilizing\nSpiking Neural Networks (SNNs) deployed on an In-memory Computing engine based\non RRAM crossbars, which are known for their compactness and energy-efficiency.\nGiven the hardware constraints and noise characteristics of the underlying RRAM\ncrossbars, we propose online adaptation of pre-trained SNNs in real-time using\nDirect Feedback Alignment (DFA) against traditional backpropagation (BP).\nDirect Feedback Alignment (DFA) learning, that allows layer-parallel gradient\ncomputations, acts as a fast, energy & area-efficient method for online\nadaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance\nagainst those adapted using BP. Through extensive simulations using our\nin-house hardware evaluation engine called DFA_Sim, we find that DFA achieves\nupto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x\nreduction in latency compared to BP, while delivering upto 7.55% higher\ninference accuracy on human activity recognition (HAR) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a spike-based wearable analytics system utilizing\nSpiking Neural Networks (SNNs) deployed on an In-memory Computing engine based\non RRAM crossbars, which are known for their compactness and energy-efficiency.\nGiven the hardware constraints and noise characteristics of the underlying RRAM\ncrossbars, we propose online adaptation of pre-trained SNNs in real-time using\nDirect Feedback Alignment (DFA) against traditional backpropagation (BP).\nDirect Feedback Alignment (DFA) learning, that allows layer-parallel gradient\ncomputations, acts as a fast, energy & area-efficient method for online\nadaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance\nagainst those adapted using BP. Through extensive simulations using our\nin-house hardware evaluation engine called DFA_Sim, we find that DFA achieves\nupto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x\nreduction in latency compared to BP, while delivering upto 7.55% higher\ninference accuracy on human activity recognition (HAR) tasks."
                },
                "authors": [
                    {
                        "name": "Abhiroop Bhattacharjee"
                    },
                    {
                        "name": "Jinquan Shi"
                    },
                    {
                        "name": "Wei-Chen Chen"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "arxiv_comment": "Accepted in 2025 IEEE International Symposium on Circuits and Systems\n  (ISCAS)",
                "arxiv_journal_ref": "IEEE International Symposium on Circuits and Systems (ISCAS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06734v1",
                "updated": "2025-02-10T17:58:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    58,
                    22,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:58:22Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    58,
                    22,
                    0,
                    41,
                    0
                ],
                "title": "Seorita-2M: A High-Quality Instruction-based Dataset for General\n  Video Editing by Video Specialists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seorita-2M: A High-Quality Instruction-based Dataset for General\n  Video Editing by Video Specialists"
                },
                "summary": "Recent advancements in video generation have spurred the development of video\nediting techniques, which can be divided into inversion-based and end-to-end\nmethods. However, current video editing methods still suffer from several\nchallenges. Inversion-based methods, though training-free and flexible, are\ntime-consuming during inference, struggle with fine-grained editing\ninstructions, and produce artifacts and jitter. On the other hand, end-to-end\nmethods, which rely on edited video pairs for training, offer faster inference\nspeeds but often produce poor editing results due to a lack of high-quality\ntraining video pairs. In this paper, to close the gap in end-to-end methods, we\nintroduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M\nconsists of approximately 2 millions of video editing pairs. It is built by\ncrafting four high-quality, specialized video editing models, each crafted and\ntrained by our team to achieve state-of-the-art editing results. We also\npropose a filtering pipeline to eliminate poorly edited video pairs.\nFurthermore, we explore common video editing architectures to identify the most\neffective structure based on current pre-trained generative model. Extensive\nexperiments show that our dataset can help to yield remarkably high-quality\nvideo editing results. More details are available at\nhttps://senorita.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video generation have spurred the development of video\nediting techniques, which can be divided into inversion-based and end-to-end\nmethods. However, current video editing methods still suffer from several\nchallenges. Inversion-based methods, though training-free and flexible, are\ntime-consuming during inference, struggle with fine-grained editing\ninstructions, and produce artifacts and jitter. On the other hand, end-to-end\nmethods, which rely on edited video pairs for training, offer faster inference\nspeeds but often produce poor editing results due to a lack of high-quality\ntraining video pairs. In this paper, to close the gap in end-to-end methods, we\nintroduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M\nconsists of approximately 2 millions of video editing pairs. It is built by\ncrafting four high-quality, specialized video editing models, each crafted and\ntrained by our team to achieve state-of-the-art editing results. We also\npropose a filtering pipeline to eliminate poorly edited video pairs.\nFurthermore, we explore common video editing architectures to identify the most\neffective structure based on current pre-trained generative model. Extensive\nexperiments show that our dataset can help to yield remarkably high-quality\nvideo editing results. More details are available at\nhttps://senorita.github.io."
                },
                "authors": [
                    {
                        "name": "Bojia Zi"
                    },
                    {
                        "name": "Penghui Ruan"
                    },
                    {
                        "name": "Marco Chen"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Shaozhe Hao"
                    },
                    {
                        "name": "Shihao Zhao"
                    },
                    {
                        "name": "Youze Huang"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Rong Xiao"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06733v1",
                "updated": "2025-02-10T17:57:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    57,
                    15,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    57,
                    15,
                    0,
                    41,
                    0
                ],
                "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining"
                },
                "summary": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance."
                },
                "authors": [
                    {
                        "name": "Daouda Sow"
                    },
                    {
                        "name": "Herbert Woisetschlger"
                    },
                    {
                        "name": "Saikiran Bulusu"
                    },
                    {
                        "name": "Shiqiang Wang"
                    },
                    {
                        "name": "Hans-Arno Jacobsen"
                    },
                    {
                        "name": "Yingbin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yingbin Liang"
                },
                "author": "Yingbin Liang",
                "arxiv_comment": "Accepted for publication at ICLR 2025. Code base available:\n  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10961v2",
                "updated": "2025-02-10T17:35:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    35,
                    14,
                    0,
                    41,
                    0
                ],
                "published": "2024-07-15T17:57:02Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    57,
                    2,
                    0,
                    197,
                    0
                ],
                "title": "Galaxy cluster matter profiles: I. Self-similarity, mass calibration,\n  and observable-mass relation validation employing cluster mass posteriors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy cluster matter profiles: I. Self-similarity, mass calibration,\n  and observable-mass relation validation employing cluster mass posteriors"
                },
                "summary": "We present a study of the weak lensing inferred matter profiles\n$\\Delta\\Sigma(R)$ of 698 South Pole Telescope thermal Sunyaev-Zel'dovich effect\nselected and MCMF optically confirmed galaxy clusters in the redshift range\n$0.25 <z< 0.94$ that have associated weak gravitational lensing shear profiles\nfrom the Dark Energy Survey. Rescaling these profiles to account for the mass\ndependent size and the redshift dependent density produces average rescaled\nmatter profiles\n$\\Delta\\Sigma(R/R_\\mathrm{200c})/(\\rho_\\mathrm{crit}R_\\mathrm{200c})$ with a\nlower dispersion than the unscaled $\\Delta\\Sigma(R)$ versions, indicating a\nsignificant degree of self-similarity. Galaxy clusters from hydrodynamical\nsimulations also exhibit matter profiles that suggest a high degree of\nself-similarity, with RMS variation among the average rescaled matter profiles\nwith redshift and mass falling by a factor of approximately six and 23,\nrespectively, compared to the unscaled average matter profiles. We employed\nthis regularity in a new Bayesian method for weak lensing mass calibration that\nemploys the so-called cluster mass posterior $P(M_\\mathrm{200c}|\\hat{\\zeta},\n\\hat{\\lambda}, z)$, which describes the individual cluster masses given their\ntSZE and optical observables. We validated the method using realistic mock\ndatasets and present observable-mass relation constraints for the\nSPT$\\times$DES sample. We present new validation tests of the observable-mass\nrelation that indicate the underlying power-law form and scatter are adequate\nto describe the real cluster sample but that also suggest a redshift variation\nin the intrinsic scatter of the $\\lambda$-mass relation may offer a better\ndescription. In addition, the average rescaled matter profiles offer high\nsignal-to-noise ratio constraints on the shape of real cluster matter profiles,\nwhich are in good agreement with available hydrodynamical $\\Lambda$CDM\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of the weak lensing inferred matter profiles\n$\\Delta\\Sigma(R)$ of 698 South Pole Telescope thermal Sunyaev-Zel'dovich effect\nselected and MCMF optically confirmed galaxy clusters in the redshift range\n$0.25 <z< 0.94$ that have associated weak gravitational lensing shear profiles\nfrom the Dark Energy Survey. Rescaling these profiles to account for the mass\ndependent size and the redshift dependent density produces average rescaled\nmatter profiles\n$\\Delta\\Sigma(R/R_\\mathrm{200c})/(\\rho_\\mathrm{crit}R_\\mathrm{200c})$ with a\nlower dispersion than the unscaled $\\Delta\\Sigma(R)$ versions, indicating a\nsignificant degree of self-similarity. Galaxy clusters from hydrodynamical\nsimulations also exhibit matter profiles that suggest a high degree of\nself-similarity, with RMS variation among the average rescaled matter profiles\nwith redshift and mass falling by a factor of approximately six and 23,\nrespectively, compared to the unscaled average matter profiles. We employed\nthis regularity in a new Bayesian method for weak lensing mass calibration that\nemploys the so-called cluster mass posterior $P(M_\\mathrm{200c}|\\hat{\\zeta},\n\\hat{\\lambda}, z)$, which describes the individual cluster masses given their\ntSZE and optical observables. We validated the method using realistic mock\ndatasets and present observable-mass relation constraints for the\nSPT$\\times$DES sample. We present new validation tests of the observable-mass\nrelation that indicate the underlying power-law form and scatter are adequate\nto describe the real cluster sample but that also suggest a redshift variation\nin the intrinsic scatter of the $\\lambda$-mass relation may offer a better\ndescription. In addition, the average rescaled matter profiles offer high\nsignal-to-noise ratio constraints on the shape of real cluster matter profiles,\nwhich are in good agreement with available hydrodynamical $\\Lambda$CDM\nsimulations."
                },
                "authors": [
                    {
                        "name": "A. Singh"
                    },
                    {
                        "name": "J. J. Mohr"
                    },
                    {
                        "name": "C. T. Davies"
                    },
                    {
                        "name": "S. Bocquet"
                    },
                    {
                        "name": "S. Grandis"
                    },
                    {
                        "name": "M. Klein"
                    },
                    {
                        "name": "J. L. Marshall"
                    },
                    {
                        "name": "M. Aguena"
                    },
                    {
                        "name": "S. S. Allam"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "F. Andrade-Oliveira"
                    },
                    {
                        "name": "D. Bacon"
                    },
                    {
                        "name": "S. Bhargava"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "M. Costanzi"
                    },
                    {
                        "name": "L. N. da Costa"
                    },
                    {
                        "name": "M. E. S. Pereira"
                    },
                    {
                        "name": "S. Desai"
                    },
                    {
                        "name": "H. T. Diehl"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "B. Flaugher"
                    },
                    {
                        "name": "J. Frieman"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "R. A. Gruendl"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "D. L. Hollowood"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. J. James"
                    },
                    {
                        "name": "K. Kuehn"
                    },
                    {
                        "name": "M. Lima"
                    },
                    {
                        "name": "J. Mena-Fernndez"
                    },
                    {
                        "name": "F. Menanteau"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Myles"
                    },
                    {
                        "name": "A. Pieres"
                    },
                    {
                        "name": "A. K. Romer"
                    },
                    {
                        "name": "S. Samuroff"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Sanchez Cid"
                    },
                    {
                        "name": "I. Sevilla-Noarbe"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "E. Suchyta"
                    },
                    {
                        "name": "M. E. C. Swanson"
                    },
                    {
                        "name": "G. Tarle"
                    },
                    {
                        "name": "C. To"
                    },
                    {
                        "name": "D. L. Tucker"
                    },
                    {
                        "name": "V. Vikram"
                    },
                    {
                        "name": "N. Weaverdyck"
                    },
                    {
                        "name": "P. Wiseman"
                    }
                ],
                "author_detail": {
                    "name": "P. Wiseman"
                },
                "author": "P. Wiseman",
                "arxiv_comment": "24 pages, 17 figures, Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06703v1",
                "updated": "2025-02-10T17:30:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    30,
                    23,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:30:23Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    30,
                    23,
                    0,
                    41,
                    0
                ],
                "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling"
                },
                "summary": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18727v2",
                "updated": "2025-02-10T17:27:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    27,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-30T20:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    20,
                    7,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks"
                },
                "summary": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms."
                },
                "authors": [
                    {
                        "name": "Mohd. Farhan Israk Soumik"
                    },
                    {
                        "name": "W. K. M. Mithsara"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "arxiv_comment": "Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop\n  on Privacy-Preserving Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06695v1",
                "updated": "2025-02-10T17:18:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    18,
                    54,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:18:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    18,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "FairDropout: Using Example-Tied Dropout to Enhance Generalization of\n  Minority Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairDropout: Using Example-Tied Dropout to Enhance Generalization of\n  Minority Groups"
                },
                "summary": "Deep learning models frequently exploit spurious features in training data to\nachieve low training error, often resulting in poor generalization when faced\nwith shifted testing distributions. To address this issue, various methods from\nimbalanced learning, representation learning, and classifier recalibration have\nbeen proposed to enhance the robustness of deep neural networks against\nspurious correlations. In this paper, we observe that models trained with\nempirical risk minimization tend to generalize well for examples from the\nmajority groups while memorizing instances from minority groups. Building on\nrecent findings that show memorization can be localized to a limited number of\nneurons, we apply example-tied dropout as a method we term FairDropout, aimed\nat redirecting this memorization to specific neurons that we subsequently drop\nout during inference. We empirically evaluate FairDropout using the\nsubpopulation benchmark suite encompassing vision, language, and healthcare\ntasks, demonstrating that it significantly reduces reliance on spurious\ncorrelations, and outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models frequently exploit spurious features in training data to\nachieve low training error, often resulting in poor generalization when faced\nwith shifted testing distributions. To address this issue, various methods from\nimbalanced learning, representation learning, and classifier recalibration have\nbeen proposed to enhance the robustness of deep neural networks against\nspurious correlations. In this paper, we observe that models trained with\nempirical risk minimization tend to generalize well for examples from the\nmajority groups while memorizing instances from minority groups. Building on\nrecent findings that show memorization can be localized to a limited number of\nneurons, we apply example-tied dropout as a method we term FairDropout, aimed\nat redirecting this memorization to specific neurons that we subsequently drop\nout during inference. We empirically evaluate FairDropout using the\nsubpopulation benchmark suite encompassing vision, language, and healthcare\ntasks, demonstrating that it significantly reduces reliance on spurious\ncorrelations, and outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Geraldin Nanfack"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v3",
                "updated": "2025-02-10T17:15:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    15,
                    52,
                    0,
                    41,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Coverage-Guided LLM-Based Test Generation"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06669v1",
                "updated": "2025-02-10T16:54:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    54,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:54:03Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    54,
                    3,
                    0,
                    41,
                    0
                ],
                "title": "Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations"
                },
                "summary": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research."
                },
                "authors": [
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Tailai Peng"
                    },
                    {
                        "name": "Xinran Xie"
                    },
                    {
                        "name": "Dekun Lin"
                    },
                    {
                        "name": "Zhe Cui"
                    },
                    {
                        "name": "Zheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Chen"
                },
                "author": "Zheng Chen",
                "arxiv_comment": "to be published in ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06666v1",
                "updated": "2025-02-10T16:52:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    52,
                    39,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:52:39Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    52,
                    39,
                    0,
                    41,
                    0
                ],
                "title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering"
                },
                "summary": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark--CareQA--, with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations --Relaxed Perplexity-- to\nmitigate the identified limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark--CareQA--, with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations --Relaxed Perplexity-- to\nmitigate the identified limitations."
                },
                "authors": [
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Pablo Agustin Martin-Torres"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Pablo Bernabeu-Perez"
                    },
                    {
                        "name": "Lucia Urcelay Ganzabal"
                    },
                    {
                        "name": "Marta Gonzalez Mallo"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Enrique Lopez-Cuena"
                    },
                    {
                        "name": "Sergio Alvarez-Napagao"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01022v2",
                "updated": "2025-02-10T16:52:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    52,
                    7,
                    0,
                    41,
                    0
                ],
                "published": "2024-08-02T05:46:17Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    5,
                    46,
                    17,
                    4,
                    215,
                    0
                ],
                "title": "A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence"
                },
                "summary": "Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs."
                },
                "authors": [
                    {
                        "name": "Takahiro Kawashima"
                    },
                    {
                        "name": "Hideitsu Hino"
                    }
                ],
                "author_detail": {
                    "name": "Hideitsu Hino"
                },
                "author": "Hideitsu Hino",
                "arxiv_comment": "Accepted by AISTATS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06663v1",
                "updated": "2025-02-10T16:51:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    51,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:51:03Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    51,
                    3,
                    0,
                    41,
                    0
                ],
                "title": "EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models"
                },
                "summary": "Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM."
                },
                "authors": [
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06661v1",
                "updated": "2025-02-10T16:49:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    49,
                    46,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    49,
                    46,
                    0,
                    41,
                    0
                ],
                "title": "iLOCO: Distribution-Free Inference for Feature Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iLOCO: Distribution-Free Inference for Feature Interactions"
                },
                "summary": "Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions."
                },
                "authors": [
                    {
                        "name": "Camille Little"
                    },
                    {
                        "name": "Lili Zheng"
                    },
                    {
                        "name": "Genevera Allen"
                    }
                ],
                "author_detail": {
                    "name": "Genevera Allen"
                },
                "author": "Genevera Allen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06659v1",
                "updated": "2025-02-10T16:48:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    48,
                    56,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:48:56Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    48,
                    56,
                    0,
                    41,
                    0
                ],
                "title": "Who Taught You That? Tracing Teachers in Model Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Taught You That? Tracing Teachers in Model Distillation"
                },
                "summary": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers."
                },
                "authors": [
                    {
                        "name": "Somin Wadhwa"
                    },
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Silvio Amir"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "Preprint; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06655v1",
                "updated": "2025-02-10T16:45:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    45,
                    18,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:45:18Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    45,
                    18,
                    0,
                    41,
                    0
                ],
                "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Evaluation of Large Language Models from a Causal Perspective"
                },
                "summary": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults."
                },
                "authors": [
                    {
                        "name": "Meilin Chen"
                    },
                    {
                        "name": "Jian Tian"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Di Xie"
                    },
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Jiang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Zhu"
                },
                "author": "Jiang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19020v3",
                "updated": "2025-02-10T16:42:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    23,
                    0,
                    41,
                    0
                ],
                "published": "2024-09-25T07:03:31Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    31,
                    2,
                    269,
                    0
                ],
                "title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications"
                },
                "summary": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research."
                },
                "authors": [
                    {
                        "name": "Sathya Krishnan Suresh"
                    },
                    {
                        "name": "Wu Mengjun"
                    },
                    {
                        "name": "Tushar Pranav"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "13 pages, 1 figure",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06652v1",
                "updated": "2025-02-10T16:42:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:42:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A"
                },
                "summary": "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks."
                },
                "authors": [
                    {
                        "name": "Anna Leschanowsky"
                    },
                    {
                        "name": "Zahra Kolagar"
                    },
                    {
                        "name": "Erion ano"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Dara Hallinan"
                    },
                    {
                        "name": "Emanul A. P. Habets"
                    },
                    {
                        "name": "Birgit Popp"
                    }
                ],
                "author_detail": {
                    "name": "Birgit Popp"
                },
                "author": "Birgit Popp",
                "arxiv_comment": "Submitted to ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06645v1",
                "updated": "2025-02-10T16:35:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    35,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:35:08Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    35,
                    8,
                    0,
                    41,
                    0
                ],
                "title": "Koopman-Equivariant Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Koopman-Equivariant Gaussian Processes"
                },
                "summary": "Credible forecasting and representation learning of dynamical systems are of\never-increasing importance for reliable decision-making. To that end, we\npropose a family of Gaussian processes (GP) for dynamical systems with linear\ntime-invariant responses, which are nonlinear only in initial conditions. This\nlinearity allows us to tractably quantify forecasting and representational\nuncertainty, simultaneously alleviating the challenge of computing the\ndistribution of trajectories from a GP-based dynamical system and enabling a\nnew probabilistic treatment of learning Koopman operator representations. Using\na trajectory-based equivariance -- which we refer to as \\textit{Koopman\nequivariance} -- we obtain a GP model with enhanced generalization\ncapabilities. To allow for large-scale regression, we equip our framework with\nvariational inference based on suitable inducing points. Experiments\ndemonstrate on-par and often better forecasting performance compared to\nkernel-based methods for learning dynamical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credible forecasting and representation learning of dynamical systems are of\never-increasing importance for reliable decision-making. To that end, we\npropose a family of Gaussian processes (GP) for dynamical systems with linear\ntime-invariant responses, which are nonlinear only in initial conditions. This\nlinearity allows us to tractably quantify forecasting and representational\nuncertainty, simultaneously alleviating the challenge of computing the\ndistribution of trajectories from a GP-based dynamical system and enabling a\nnew probabilistic treatment of learning Koopman operator representations. Using\na trajectory-based equivariance -- which we refer to as \\textit{Koopman\nequivariance} -- we obtain a GP model with enhanced generalization\ncapabilities. To allow for large-scale regression, we equip our framework with\nvariational inference based on suitable inducing points. Experiments\ndemonstrate on-par and often better forecasting performance compared to\nkernel-based methods for learning dynamical systems."
                },
                "authors": [
                    {
                        "name": "Petar Bevanda"
                    },
                    {
                        "name": "Max Beier"
                    },
                    {
                        "name": "Armin Lederer"
                    },
                    {
                        "name": "Alexandre Capone"
                    },
                    {
                        "name": "Stefan Sosnowski"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche",
                "arxiv_comment": "Accepted to the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06643v1",
                "updated": "2025-02-10T16:34:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    34,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:34:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    34,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert\n  Placement and Token Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert\n  Placement and Token Routing"
                },
                "summary": "Mixture-of-Experts (MoE) model architecture has emerged as a promising\nsolution for scaling transformer models efficiently, offering sparse activation\nthat reduces computational costs while increasing model capacity. However, as\nMoE models scale, they need to be distributed across GPU devices, thus face\ncritical performance bottlenecks due to their large memory footprint. Expert\nparallelism distributes experts across GPUs, however, faces key challenges\nincluding an unbalanced token routing and expert activation, resulting in\ncommunication tail latency and processing inefficiencies. While existing\nsolutions address some of these issues, they fail to resolve the dual\nchallenges of load imbalance and communication skew. The imbalance in token\nprocessing load across experts causes uneven processing times on different\nGPUs, while communication skew between GPUs leads to unbalanced inter-GPU data\ntransfers. These factors degrade the performance of MoE models by increasing\ntail latency and reducing overall throughput. To address these limitations, we\npropose an Integer Linear Programming (ILP) formulation to optimize expert\nplacement by jointly considering token load, communication, and computation\ncosts. We exploit the property that there is a token routing dependency across\nlayers, where tokens routed to a specific expert in one layer are likely to be\nrouted to a limited set of experts in the subsequent layer. Our solution,\nMoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU\ntoken routing costs and balances token processing across devices, thereby\nreducing tail latency and end-to-end execution time. Experimental results\ndemonstrate 9.3% and 17.5% of end-to-end speedups for single-node and\nmulti-node inference respectively, showcasing the potential of our ILP-based\noptimization for offering expert parallel solutions for next-generation MoEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) model architecture has emerged as a promising\nsolution for scaling transformer models efficiently, offering sparse activation\nthat reduces computational costs while increasing model capacity. However, as\nMoE models scale, they need to be distributed across GPU devices, thus face\ncritical performance bottlenecks due to their large memory footprint. Expert\nparallelism distributes experts across GPUs, however, faces key challenges\nincluding an unbalanced token routing and expert activation, resulting in\ncommunication tail latency and processing inefficiencies. While existing\nsolutions address some of these issues, they fail to resolve the dual\nchallenges of load imbalance and communication skew. The imbalance in token\nprocessing load across experts causes uneven processing times on different\nGPUs, while communication skew between GPUs leads to unbalanced inter-GPU data\ntransfers. These factors degrade the performance of MoE models by increasing\ntail latency and reducing overall throughput. To address these limitations, we\npropose an Integer Linear Programming (ILP) formulation to optimize expert\nplacement by jointly considering token load, communication, and computation\ncosts. We exploit the property that there is a token routing dependency across\nlayers, where tokens routed to a specific expert in one layer are likely to be\nrouted to a limited set of experts in the subsequent layer. Our solution,\nMoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU\ntoken routing costs and balances token processing across devices, thereby\nreducing tail latency and end-to-end execution time. Experimental results\ndemonstrate 9.3% and 17.5% of end-to-end speedups for single-node and\nmulti-node inference respectively, showcasing the potential of our ILP-based\noptimization for offering expert parallel solutions for next-generation MoEs."
                },
                "authors": [
                    {
                        "name": "Seokjin Go"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04419v2",
                "updated": "2025-02-10T16:34:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    34,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T15:20:58Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    20,
                    58,
                    3,
                    37,
                    0
                ],
                "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks"
                },
                "summary": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation."
                },
                "authors": [
                    {
                        "name": "Miaomiao Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "Technical report; 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06635v1",
                "updated": "2025-02-10T16:31:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    31,
                    37,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:31:37Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    31,
                    37,
                    0,
                    41,
                    0
                ],
                "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM"
                },
                "summary": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM."
                },
                "authors": [
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06633v1",
                "updated": "2025-02-10T16:29:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    29,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:29:12Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    29,
                    12,
                    0,
                    41,
                    0
                ],
                "title": "Combining Large Language Models with Static Analyzers for Code Review\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Large Language Models with Static Analyzers for Code Review\n  Generation"
                },
                "summary": "Code review is a crucial but often complex, subjective, and time-consuming\nactivity in software development. Over the past decades, significant efforts\nhave been made to automate this process. Early approaches focused on\nknowledge-based systems (KBS) that apply rule-based mechanisms to detect code\nissues, providing precise feedback but struggling with complex,\ncontext-dependent cases. More recent work has shifted toward fine-tuning\npre-trained language models for code review, enabling broader issue coverage\nbut often at the expense of precision. In this paper, we propose a hybrid\napproach that combines the strengths of KBS and learning-based systems (LBS) to\ngenerate high-quality, comprehensive code reviews. Our method integrates\nknowledge at three distinct stages of the language model pipeline: during data\npreparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented\nGeneration, RAG), and after inference (Naive Concatenation of Outputs, NCO). We\nempirically evaluate our combination strategies against standalone KBS and LBS\nfine-tuned on a real-world dataset. Our results show that these hybrid\nstrategies enhance the relevance, completeness, and overall quality of review\ncomments, effectively bridging the gap between rule-based tools and deep\nlearning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is a crucial but often complex, subjective, and time-consuming\nactivity in software development. Over the past decades, significant efforts\nhave been made to automate this process. Early approaches focused on\nknowledge-based systems (KBS) that apply rule-based mechanisms to detect code\nissues, providing precise feedback but struggling with complex,\ncontext-dependent cases. More recent work has shifted toward fine-tuning\npre-trained language models for code review, enabling broader issue coverage\nbut often at the expense of precision. In this paper, we propose a hybrid\napproach that combines the strengths of KBS and learning-based systems (LBS) to\ngenerate high-quality, comprehensive code reviews. Our method integrates\nknowledge at three distinct stages of the language model pipeline: during data\npreparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented\nGeneration, RAG), and after inference (Naive Concatenation of Outputs, NCO). We\nempirically evaluate our combination strategies against standalone KBS and LBS\nfine-tuned on a real-world dataset. Our results show that these hybrid\nstrategies enhance the relevance, completeness, and overall quality of review\ncomments, effectively bridging the gap between rule-based tools and deep\nlearning models."
                },
                "authors": [
                    {
                        "name": "Imen Jaoua"
                    },
                    {
                        "name": "Oussama Ben Sghaier"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06628v1",
                "updated": "2025-02-10T16:26:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    26,
                    1,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:26:01Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    26,
                    1,
                    0,
                    41,
                    0
                ],
                "title": "Random Variables aren't Random",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Variables aren't Random"
                },
                "summary": "This paper examines the foundational concept of random variables in\nprobability theory and statistical inference, demonstrating that their\nmathematical definition requires no reference to randomization or hypothetical\nrepeated sampling. We show how measure-theoretic probability provides a\nframework for modeling populations through distributions, leading to three key\ncontributions. First, we establish that random variables, properly understood\nas measurable functions, can be fully characterized without appealing to\ninfinite hypothetical samples. Second, we demonstrate how this perspective\nenables statistical inference through logical rather than probabilistic\nreasoning, extending the reductio ad absurdum argument from deductive to\ninductive inference. Third, we show how this framework naturally leads to\ninformation-based assessment of statistical procedures, replacing traditional\ninference metrics that emphasize bias and variance with information-based\napproaches that better describe the families of distributions used in\nparametric inference. This reformulation addresses long-standing debates in\nstatistical inference while providing a more coherent theoretical foundation.\nOur approach offers an alternative to traditional frequentist inference that\nmaintains mathematical rigor while avoiding the philosophical complications\ninherent in repeated sampling interpretations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the foundational concept of random variables in\nprobability theory and statistical inference, demonstrating that their\nmathematical definition requires no reference to randomization or hypothetical\nrepeated sampling. We show how measure-theoretic probability provides a\nframework for modeling populations through distributions, leading to three key\ncontributions. First, we establish that random variables, properly understood\nas measurable functions, can be fully characterized without appealing to\ninfinite hypothetical samples. Second, we demonstrate how this perspective\nenables statistical inference through logical rather than probabilistic\nreasoning, extending the reductio ad absurdum argument from deductive to\ninductive inference. Third, we show how this framework naturally leads to\ninformation-based assessment of statistical procedures, replacing traditional\ninference metrics that emphasize bias and variance with information-based\napproaches that better describe the families of distributions used in\nparametric inference. This reformulation addresses long-standing debates in\nstatistical inference while providing a more coherent theoretical foundation.\nOur approach offers an alternative to traditional frequentist inference that\nmaintains mathematical rigor while avoiding the philosophical complications\ninherent in repeated sampling interpretations."
                },
                "authors": [
                    {
                        "name": "Paul W. Vos"
                    }
                ],
                "author_detail": {
                    "name": "Paul W. Vos"
                },
                "author": "Paul W. Vos",
                "arxiv_comment": "17 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04295v2",
                "updated": "2025-02-10T16:25:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    25,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T18:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO."
                },
                "authors": [
                    {
                        "name": "Yuanye Liu"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05232v2",
                "updated": "2025-02-10T16:22:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    22,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-06T18:02:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds"
                },
                "summary": "Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures."
                },
                "authors": [
                    {
                        "name": "James Beetham"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03380v2",
                "updated": "2025-02-10T16:21:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    21,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-04T12:48:21Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    12,
                    48,
                    21,
                    4,
                    278,
                    0
                ],
                "title": "Identifying perturbation targets through causal differential networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying perturbation targets through causal differential networks"
                },
                "summary": "Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data."
                },
                "authors": [
                    {
                        "name": "Menghua Wu"
                    },
                    {
                        "name": "Umesh Padia"
                    },
                    {
                        "name": "Sean H. Murphy"
                    },
                    {
                        "name": "Regina Barzilay"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Jaakkola"
                },
                "author": "Tommi Jaakkola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06605v1",
                "updated": "2025-02-10T16:02:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    2,
                    26,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:02:26Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    2,
                    26,
                    0,
                    41,
                    0
                ],
                "title": "Quantile Forecast Matching with a Bayesian Quantile Gaussian Process\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Forecast Matching with a Bayesian Quantile Gaussian Process\n  Model"
                },
                "summary": "A set of probabilities along with corresponding quantiles are often used to\ndefine predictive distributions or probabilistic forecasts. These quantile\npredictions offer easily interpreted uncertainty of an event, and quantiles are\ngenerally straightforward to estimate using standard statistical and machine\nlearning methods. However, compared to a distribution defined by a probability\ndensity or cumulative distribution function, a set of quantiles has less\ndistributional information. When given estimated quantiles, it may be desirable\nto estimate a fully defined continuous distribution function. Many researchers\ndo so to make evaluation or ensemble modeling simpler. Most existing methods\nfor fitting a distribution to quantiles lack accurate representation of the\ninherent uncertainty from quantile estimation or are limited in their\napplications. In this manuscript, we present a Gaussian process model, the\nquantile Gaussian process, which is based on established theory of quantile\nfunctions and sample quantiles, to construct a probability distribution given\nestimated quantiles. A Bayesian application of the quantile Gaussian process is\nevaluated for parameter inference and distribution approximation in simulation\nstudies. The quantile Gaussian process is used to approximate the distributions\nof quantile forecasts from the 2023-24 US Centers for Disease Control\ncollaborative flu forecasting initiative. The simulation studies and data\nanalysis show that the quantile Gaussian process leads to accurate inference on\nmodel parameters, estimation of a continuous distribution, and uncertainty\nquantification of sample quantiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A set of probabilities along with corresponding quantiles are often used to\ndefine predictive distributions or probabilistic forecasts. These quantile\npredictions offer easily interpreted uncertainty of an event, and quantiles are\ngenerally straightforward to estimate using standard statistical and machine\nlearning methods. However, compared to a distribution defined by a probability\ndensity or cumulative distribution function, a set of quantiles has less\ndistributional information. When given estimated quantiles, it may be desirable\nto estimate a fully defined continuous distribution function. Many researchers\ndo so to make evaluation or ensemble modeling simpler. Most existing methods\nfor fitting a distribution to quantiles lack accurate representation of the\ninherent uncertainty from quantile estimation or are limited in their\napplications. In this manuscript, we present a Gaussian process model, the\nquantile Gaussian process, which is based on established theory of quantile\nfunctions and sample quantiles, to construct a probability distribution given\nestimated quantiles. A Bayesian application of the quantile Gaussian process is\nevaluated for parameter inference and distribution approximation in simulation\nstudies. The quantile Gaussian process is used to approximate the distributions\nof quantile forecasts from the 2023-24 US Centers for Disease Control\ncollaborative flu forecasting initiative. The simulation studies and data\nanalysis show that the quantile Gaussian process leads to accurate inference on\nmodel parameters, estimation of a continuous distribution, and uncertainty\nquantification of sample quantiles."
                },
                "authors": [
                    {
                        "name": "Spencer Wadsworth"
                    },
                    {
                        "name": "Jarad Niemi"
                    }
                ],
                "author_detail": {
                    "name": "Jarad Niemi"
                },
                "author": "Jarad Niemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06604v1",
                "updated": "2025-02-10T16:01:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    1,
                    55,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:01:55Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    1,
                    55,
                    0,
                    41,
                    0
                ],
                "title": "Do we really have to filter out random noise in pre-training data for\n  language models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we really have to filter out random noise in pre-training data for\n  language models?"
                },
                "summary": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Jinghan Ru"
                    },
                    {
                        "name": "Yuxin Xie"
                    },
                    {
                        "name": "Xianwei Zhuang"
                    },
                    {
                        "name": "Yuguo Yin"
                    },
                    {
                        "name": "Yuexian Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Zou"
                },
                "author": "Yuexian Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06601v1",
                "updated": "2025-02-10T16:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    0,
                    48,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:00:48Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    0,
                    48,
                    0,
                    41,
                    0
                ],
                "title": "Amortized In-Context Bayesian Posterior Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized In-Context Bayesian Posterior Estimation"
                },
                "summary": "Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows."
                },
                "authors": [
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Niels Leif Bracher"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Priyank Jaini"
                    },
                    {
                        "name": "Marcus Brubaker"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Brubaker"
                },
                "author": "Marcus Brubaker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06600v1",
                "updated": "2025-02-10T16:00:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    0,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:00:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    0,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?"
                },
                "summary": "The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments."
                },
                "authors": [
                    {
                        "name": "Gonalo Gomes"
                    },
                    {
                        "name": "Chrysoula Zerva"
                    },
                    {
                        "name": "Bruno Martins"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Martins"
                },
                "author": "Bruno Martins",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06589v1",
                "updated": "2025-02-10T15:54:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    54,
                    34,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:54:34Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    54,
                    34,
                    0,
                    41,
                    0
                ],
                "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training"
                },
                "summary": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Haoming Jiang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Kewei Cheng"
                    },
                    {
                        "name": "Sanket Lokegaonkar"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Qing Ping"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Binxuan Huang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Ruijie Wang"
                    },
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Nasser Zalmout"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted to NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06581v1",
                "updated": "2025-02-10T15:48:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    48,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:48:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    48,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems"
                },
                "summary": "The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field."
                },
                "authors": [
                    {
                        "name": "Linxiao Gong"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Gaoyun Fang"
                    },
                    {
                        "name": "Bobo Ju"
                    },
                    {
                        "name": "Juncen Guo"
                    },
                    {
                        "name": "Xiaoguang Zhu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    }
                ],
                "author_detail": {
                    "name": "Azzedine Boukerche"
                },
                "author": "Azzedine Boukerche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05212v2",
                "updated": "2025-02-10T15:42:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    42,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2024-08-10T05:41:19Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    5,
                    41,
                    19,
                    5,
                    223,
                    0
                ],
                "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions"
                },
                "summary": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks."
                },
                "authors": [
                    {
                        "name": "Michele Miranda"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    },
                    {
                        "name": "Sbastien Bratires"
                    },
                    {
                        "name": "Emanuele Rodol"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Rodol"
                },
                "author": "Emanuele Rodol",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)\n  https://openreview.net/forum?id=Ss9MTTN7OL",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06572v1",
                "updated": "2025-02-10T15:40:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    40,
                    35,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:40:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    40,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM"
                },
                "summary": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 ."
                },
                "authors": [
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Kun-Yang Yu"
                    },
                    {
                        "name": "Shi-Yu Tian"
                    },
                    {
                        "name": "Jiang-Xin Shi"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Pengxiao Song"
                    },
                    {
                        "name": "Yi-Xuan Jin"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Li"
                },
                "author": "Yu-Feng Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06567v1",
                "updated": "2025-02-10T15:34:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    34,
                    42,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:34:42Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    34,
                    42,
                    0,
                    41,
                    0
                ],
                "title": "Membership Inference Risks in Quantized Models: A Theoretical and\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Risks in Quantized Models: A Theoretical and\n  Empirical Study"
                },
                "summary": "Quantizing machine learning models has demonstrated its effectiveness in\nlowering memory and inference costs while maintaining performance levels\ncomparable to the original models. In this work, we investigate the impact of\nquantization procedures on the privacy of data-driven models, specifically\nfocusing on their vulnerability to membership inference attacks. We derive an\nasymptotic theoretical analysis of Membership Inference Security (MIS),\ncharacterizing the privacy implications of quantized algorithm weights against\nthe most powerful (and possibly unknown) attacks. Building on these theoretical\ninsights, we propose a novel methodology to empirically assess and rank the\nprivacy levels of various quantization procedures. Using synthetic datasets, we\ndemonstrate the effectiveness of our approach in assessing the MIS of different\nquantizers. Furthermore, we explore the trade-off between privacy and\nperformance using real-world data and models in the context of molecular\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing machine learning models has demonstrated its effectiveness in\nlowering memory and inference costs while maintaining performance levels\ncomparable to the original models. In this work, we investigate the impact of\nquantization procedures on the privacy of data-driven models, specifically\nfocusing on their vulnerability to membership inference attacks. We derive an\nasymptotic theoretical analysis of Membership Inference Security (MIS),\ncharacterizing the privacy implications of quantized algorithm weights against\nthe most powerful (and possibly unknown) attacks. Building on these theoretical\ninsights, we propose a novel methodology to empirically assess and rank the\nprivacy levels of various quantization procedures. Using synthetic datasets, we\ndemonstrate the effectiveness of our approach in assessing the MIS of different\nquantizers. Furthermore, we explore the trade-off between privacy and\nperformance using real-world data and models in the context of molecular\nmodeling."
                },
                "authors": [
                    {
                        "name": "Eric Aubinais"
                    },
                    {
                        "name": "Philippe Formont"
                    },
                    {
                        "name": "Pablo Piantanida"
                    },
                    {
                        "name": "Elisabeth Gassiat"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth Gassiat"
                },
                "author": "Elisabeth Gassiat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06563v1",
                "updated": "2025-02-10T15:31:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    31,
                    54,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:31:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    31,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation"
                },
                "summary": "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen"
                },
                "authors": [
                    {
                        "name": "Chengwen Qi"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Jinwang Wu"
                    },
                    {
                        "name": "Yuanjun Laili"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06108v2",
                "updated": "2025-02-10T15:28:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    28,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-10T17:01:09Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "title": "Inferring High-Order Couplings with Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring High-Order Couplings with Neural Networks"
                },
                "summary": "Maximum entropy methods, based on the inverse Ising/Potts problem from\nstatistical mechanics, are essential for modeling interactions between pairs of\nvariables in data-driven problems across disciplines such as bioinformatics,\necology, and neuroscience. Despite their considerable success, these methods\ntypically fail to capture higher-order interactions that are often essential\nfor understanding complex systems. Conversely, modern machine learning methods\ncapture these complex interactions, but the computational cost of interpretable\nframeworks makes them impractical for real-world applications. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient way to capture\nstatistical correlations using hidden nodes in a bipartite neural network. In\nthis study, we introduce a new method that maps RBMs to generalized Potts\nmodels, allowing for the extraction of interactions up to any specified order.\nThis method utilizes large-$N$ approximations, enabled by the RBM's simple\nstructure, to extract effective many-body couplings with minimal computational\neffort. Furthermore, we propose a robust framework for extracting higher-order\ninteractions in more complex probabilistic models and a simple gauge-fixing\nmethod within the effective many-body Potts model. Our validation on synthetic\ndatasets confirms the method's ability to recover two- and three-body\ninteractions accurately. When applied to protein sequence data, the framework\ncompetently reconstructs protein contact maps and provides performance\ncomparable to the best inverse Potts models. These findings confirm that RBMs\nare an effective and streamlined tool for exploring higher-order interactions\nwithin complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum entropy methods, based on the inverse Ising/Potts problem from\nstatistical mechanics, are essential for modeling interactions between pairs of\nvariables in data-driven problems across disciplines such as bioinformatics,\necology, and neuroscience. Despite their considerable success, these methods\ntypically fail to capture higher-order interactions that are often essential\nfor understanding complex systems. Conversely, modern machine learning methods\ncapture these complex interactions, but the computational cost of interpretable\nframeworks makes them impractical for real-world applications. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient way to capture\nstatistical correlations using hidden nodes in a bipartite neural network. In\nthis study, we introduce a new method that maps RBMs to generalized Potts\nmodels, allowing for the extraction of interactions up to any specified order.\nThis method utilizes large-$N$ approximations, enabled by the RBM's simple\nstructure, to extract effective many-body couplings with minimal computational\neffort. Furthermore, we propose a robust framework for extracting higher-order\ninteractions in more complex probabilistic models and a simple gauge-fixing\nmethod within the effective many-body Potts model. Our validation on synthetic\ndatasets confirms the method's ability to recover two- and three-body\ninteractions accurately. When applied to protein sequence data, the framework\ncompetently reconstructs protein contact maps and provides performance\ncomparable to the best inverse Potts models. These findings confirm that RBMs\nare an effective and streamlined tool for exploring higher-order interactions\nwithin complex systems."
                },
                "authors": [
                    {
                        "name": "Aurlien Decelle"
                    },
                    {
                        "name": "Alfonso de Jess Navas Gmez"
                    },
                    {
                        "name": "Beatriz Seoane"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Seoane"
                },
                "author": "Beatriz Seoane",
                "arxiv_comment": "16 Pages and 5 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18280v2",
                "updated": "2025-02-10T15:27:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    27,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-30T11:37:40Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models"
                },
                "summary": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner."
                },
                "authors": [
                    {
                        "name": "Haoyu Liang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yunfeng Cai"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05506v2",
                "updated": "2025-02-10T15:26:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    26,
                    28,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-07T02:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    2,
                    34,
                    30,
                    5,
                    342,
                    0
                ],
                "title": "Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation"
                },
                "summary": "We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains."
                },
                "authors": [
                    {
                        "name": "Zebin Wang"
                    },
                    {
                        "name": "Yi Han"
                    },
                    {
                        "name": "Ethan X. Fang"
                    },
                    {
                        "name": "Lan Wang"
                    },
                    {
                        "name": "Junwei Lu"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Lu"
                },
                "author": "Junwei Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06560v1",
                "updated": "2025-02-10T15:25:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation"
                },
                "summary": "The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models."
                },
                "authors": [
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v1",
                "updated": "2025-02-10T15:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level Unit Test Generation Benchmark and Impact\n  of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level Unit Test Generation Benchmark and Impact\n  of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06555v1",
                "updated": "2025-02-10T15:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    23,
                    52,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    23,
                    52,
                    0,
                    41,
                    0
                ],
                "title": "Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?"
                },
                "summary": "Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application."
                },
                "authors": [
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Ryan McKenna"
                    },
                    {
                        "name": "Edo Roth"
                    },
                    {
                        "name": "Albert Cheu"
                    },
                    {
                        "name": "Peter Kairouz"
                    }
                ],
                "author_detail": {
                    "name": "Peter Kairouz"
                },
                "author": "Peter Kairouz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06551v1",
                "updated": "2025-02-10T15:19:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    19,
                    22,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:19:22Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    19,
                    22,
                    0,
                    41,
                    0
                ],
                "title": "Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments"
                },
                "summary": "This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance."
                },
                "authors": [
                    {
                        "name": "Marc Felix Brinner"
                    },
                    {
                        "name": "Sina Zarrie"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarrie"
                },
                "author": "Sina Zarrie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07507v2",
                "updated": "2025-02-10T15:09:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    9,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-10T00:47:59Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    0,
                    47,
                    59,
                    3,
                    284,
                    0
                ],
                "title": "Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)"
                },
                "summary": "Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing."
                },
                "authors": [
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Shreya Shukla"
                    },
                    {
                        "name": "Jose Torres"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Shounak Roychowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shounak Roychowdhury"
                },
                "author": "Shounak Roychowdhury",
                "arxiv_comment": "Accepted to Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v4",
                "updated": "2025-02-10T15:08:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    8,
                    20,
                    0,
                    41,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doruz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "Accepted to NAACL 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v4",
                "updated": "2025-02-10T15:08:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    8,
                    7,
                    0,
                    41,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20973v2",
                "updated": "2025-02-10T15:04:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    4,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-05-31T16:21:05Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    21,
                    5,
                    4,
                    152,
                    0
                ],
                "title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCQ: Low-Rank Codebook based Quantization for Large Language Models"
                },
                "summary": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost."
                },
                "authors": [
                    {
                        "name": "Wen-Pu Cai"
                    },
                    {
                        "name": "Ming-Yang Li"
                    },
                    {
                        "name": "Wu-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Wu-Jun Li"
                },
                "author": "Wu-Jun Li",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06538v1",
                "updated": "2025-02-10T15:04:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    4,
                    9,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:04:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    4,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "ADF22-WEB: Detection of a molecular gas reservoir in a massive quiescent\n  galaxy located in a $z\\approx3$ proto-cluster core",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADF22-WEB: Detection of a molecular gas reservoir in a massive quiescent\n  galaxy located in a $z\\approx3$ proto-cluster core"
                },
                "summary": "We present a study of the molecular gas reservoirs and dust contents in three\nquiescent galaxies (QGs) located in the core of the $z=3.09$ SSA22\nproto-cluster. Using the Atacama Large Millimeter/submillimeter Array (ALMA),\nwe detect CO(3--2) emission in one galaxy, ADF22-QG1, marking the first direct\ndetection of molecular gas in a quiescent galaxy from the early universe. The\ndetected galaxy, ADF22-QG1, has a molecular gas mass of log$M_{\\rm\nH_2}$/M$_\\odot = 10.26 \\pm 0.07$ assuming a CO-to-H$2$ conversion factor\n$\\alpha_{\\rm CO} = 4.4$ (log$M_{\\rm H_2}$/M$_\\odot = 9.52 \\pm 0.07$ for\n$\\alpha_{\\rm CO} = 0.8$), corresponding to a gas mass fraction of $f_{\\rm gas}\n\\approx 14\\%$ (2.5\\%). The gas-to-dust ratio $\\delta _{\\rm gdr}\\gtrsim170$\n($\\delta_{\\rm gdr}\\gtrsim30$) for $\\alpha_{\\rm CO} = 4.4$ ($\\alpha_{\\rm CO}\n=0.8$) is also derived for the first time for a QG at the epoch. For the other\ntwo galaxies, ADF22-QG2 and ADF22-QG3, non detections of CO(3--2) emission\nprovide upper limits, $f_{\\rm gas} \\approx 17\\%$ (3.1\\%) and $f_{\\rm gas}\n\\approx 13\\%$ (2.4\\%), respectively. The inferred gas-consumption history of\nADF22-QG1, based on its star-formation history, suggests that (i) dusty\nstar-forming galaxies (DSFGs) at $z = 4$--$6$ are plausible progenitors, and\n(ii) the cessation of gas accretion from cosmic web filaments plays an\nimportant role in their evolution to quenched systems. Furthermore, the\npresence of a detectable molecular gas reservoir in ADF22-QG1 indicates that\nadditional mechanisms, such as morphological quenching, may be required to\nfully explain its quiescent nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of the molecular gas reservoirs and dust contents in three\nquiescent galaxies (QGs) located in the core of the $z=3.09$ SSA22\nproto-cluster. Using the Atacama Large Millimeter/submillimeter Array (ALMA),\nwe detect CO(3--2) emission in one galaxy, ADF22-QG1, marking the first direct\ndetection of molecular gas in a quiescent galaxy from the early universe. The\ndetected galaxy, ADF22-QG1, has a molecular gas mass of log$M_{\\rm\nH_2}$/M$_\\odot = 10.26 \\pm 0.07$ assuming a CO-to-H$2$ conversion factor\n$\\alpha_{\\rm CO} = 4.4$ (log$M_{\\rm H_2}$/M$_\\odot = 9.52 \\pm 0.07$ for\n$\\alpha_{\\rm CO} = 0.8$), corresponding to a gas mass fraction of $f_{\\rm gas}\n\\approx 14\\%$ (2.5\\%). The gas-to-dust ratio $\\delta _{\\rm gdr}\\gtrsim170$\n($\\delta_{\\rm gdr}\\gtrsim30$) for $\\alpha_{\\rm CO} = 4.4$ ($\\alpha_{\\rm CO}\n=0.8$) is also derived for the first time for a QG at the epoch. For the other\ntwo galaxies, ADF22-QG2 and ADF22-QG3, non detections of CO(3--2) emission\nprovide upper limits, $f_{\\rm gas} \\approx 17\\%$ (3.1\\%) and $f_{\\rm gas}\n\\approx 13\\%$ (2.4\\%), respectively. The inferred gas-consumption history of\nADF22-QG1, based on its star-formation history, suggests that (i) dusty\nstar-forming galaxies (DSFGs) at $z = 4$--$6$ are plausible progenitors, and\n(ii) the cessation of gas accretion from cosmic web filaments plays an\nimportant role in their evolution to quenched systems. Furthermore, the\npresence of a detectable molecular gas reservoir in ADF22-QG1 indicates that\nadditional mechanisms, such as morphological quenching, may be required to\nfully explain its quiescent nature."
                },
                "authors": [
                    {
                        "name": "Hideki Umehata"
                    },
                    {
                        "name": "Mariko Kubo"
                    },
                    {
                        "name": "Kouichiro Nakanishi"
                    }
                ],
                "author_detail": {
                    "name": "Kouichiro Nakanishi"
                },
                "author": "Kouichiro Nakanishi",
                "arxiv_comment": "8 pages, 4 figures, 1 table, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06533v1",
                "updated": "2025-02-10T14:56:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    56,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:56:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    56,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning"
                },
                "summary": "The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage."
                },
                "authors": [
                    {
                        "name": "Jean Vassoyan"
                    },
                    {
                        "name": "Nathanal Beau"
                    },
                    {
                        "name": "Roman Plaud"
                    }
                ],
                "author_detail": {
                    "name": "Roman Plaud"
                },
                "author": "Roman Plaud",
                "arxiv_comment": "11 pages, 6 figures, 5 tables. Accepted for publication in the\n  Findings of the North American Chapter of the Association for Computational\n  Linguistics (NAACL) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04847v2",
                "updated": "2025-02-10T14:51:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    51,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-07T11:36:36Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    36,
                    36,
                    4,
                    38,
                    0
                ],
                "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation"
                },
                "summary": "Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios."
                },
                "authors": [
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhenhui Ye"
                    },
                    {
                        "name": "Pan Xie"
                    },
                    {
                        "name": "Xiang Yin"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Bingyue Peng"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "https://agnjason.github.io/HumanDiT-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06527v1",
                "updated": "2025-02-10T14:50:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    50,
                    32,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:50:32Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    50,
                    32,
                    0,
                    41,
                    0
                ],
                "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers"
                },
                "summary": "Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality."
                },
                "authors": [
                    {
                        "name": "D. She"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Jingxuan Pang"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Qihan Huang"
                    },
                    {
                        "name": "Haobin Tang"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Siming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Siming Fu"
                },
                "author": "Siming Fu",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19903v2",
                "updated": "2025-02-10T14:34:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    34,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2024-06-28T13:16:29Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    13,
                    16,
                    29,
                    4,
                    180,
                    0
                ],
                "title": "Joint estimation of insurance loss development factors using Bayesian\n  hidden Markov models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint estimation of insurance loss development factors using Bayesian\n  hidden Markov models"
                },
                "summary": "Loss development modelling is the actuarial practice of predicting the total\n'ultimate' losses incurred on a set of policies once all claims are reported\nand settled. This poses a challenging prediction task as losses frequently take\nyears to fully emerge from reported claims, and not all claims might yet be\nreported. Loss development models frequently estimate a set of 'link ratios'\nfrom insurance loss triangles, which are multiplicative factors transforming\nlosses at one time point to ultimate. However, link ratios estimated using\nclassical methods typically underestimate ultimate losses and cannot be\nextrapolated outside the domains of the triangle, requiring extension by 'tail\nfactors' from another model. Although flexible, this two-step process relies on\nsubjective decision points that might bias inference. Methods that jointly\nestimate 'body' link ratios and smooth tail factors offer an attractive\nalternative. This paper proposes a novel application of Bayesian hidden Markov\nmodels to loss development modelling, where discrete, latent states\nrepresenting body and tail processes are automatically learned from the data.\nThe hidden Markov development model is found to perform comparably to, and\nfrequently better than, the two-step approach, as well as a latent change-point\nmodel, on numerical examples and industry datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loss development modelling is the actuarial practice of predicting the total\n'ultimate' losses incurred on a set of policies once all claims are reported\nand settled. This poses a challenging prediction task as losses frequently take\nyears to fully emerge from reported claims, and not all claims might yet be\nreported. Loss development models frequently estimate a set of 'link ratios'\nfrom insurance loss triangles, which are multiplicative factors transforming\nlosses at one time point to ultimate. However, link ratios estimated using\nclassical methods typically underestimate ultimate losses and cannot be\nextrapolated outside the domains of the triangle, requiring extension by 'tail\nfactors' from another model. Although flexible, this two-step process relies on\nsubjective decision points that might bias inference. Methods that jointly\nestimate 'body' link ratios and smooth tail factors offer an attractive\nalternative. This paper proposes a novel application of Bayesian hidden Markov\nmodels to loss development modelling, where discrete, latent states\nrepresenting body and tail processes are automatically learned from the data.\nThe hidden Markov development model is found to perform comparably to, and\nfrequently better than, the two-step approach, as well as a latent change-point\nmodel, on numerical examples and industry datasets."
                },
                "authors": [
                    {
                        "name": "Conor Goold"
                    }
                ],
                "author_detail": {
                    "name": "Conor Goold"
                },
                "author": "Conor Goold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06504v1",
                "updated": "2025-02-10T14:21:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    21,
                    51,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:21:51Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    21,
                    51,
                    0,
                    41,
                    0
                ],
                "title": "Variable stars in the VVV globular clusters III. RR Lyrae stars in the\n  inner Galactic globular clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable stars in the VVV globular clusters III. RR Lyrae stars in the\n  inner Galactic globular clusters"
                },
                "summary": "High reddening near the Galactic plane hampers observations and proper\ncharacterization of the globular clusters (GCs) located toward the inner\nregions of the Milky Way. The VISTA Variables in the Via Lactea (VVV) survey\nobserved the Galactic bulge and adjacent disk for several years, providing\nmulti-epoch, near-infrared images for 41 Galactic GCs. Detecting RRLyrae\nvariables belonging to these GCs will aid in their accurate parameterization.\nBy fully leveraging the astrometric, photometric, and variability VVV catalogs,\nwe searched for RRLyrae stars associated with GCs. Our selection criteria,\nbased on proper motions, proximity to the cluster centers, and distances\ninferred from their period-luminosity-metallicity relations, enable us to\naccurately identify the RRLyrae population in these GCs and determine color\nexcesses and distances in a homogeneous manner. Since the VVV catalogs cover\nfrom the innermost regions of the GCs to their outskirts, we can provide a\ncomprehensive picture of the entire RRLyrae population in these GCs. We have\ndiscovered significant RRLyrae populations in two highly reddened Galactic GCs:\nUKS1 and VVV-CL160, previously unknown to host RRLyrae stars. Additionally, we\nhave detected one RRLyrae candidate in each of Terzan4 and Terzan9, also new to\nRRLyrae detection. We further confirm and increase the number of RRLyrae stars\ndetected in 22 other low-latitude Galactic GCs. The RRLyrae distances place\nmost of these GCs within the Galactic bulge, aligning well with the few GCs in\nour sample with reliable Gaia or Hubble Space Telescope measurements. However,\nmost of the VVV GCs lack accurate Gaia distances, and literature distances are\ngenerally significantly smaller than those derived in this work. As a byproduct\nof our analysis, we have obtained the proper motions for all the VVV GCs,\nindependently confirming Gaia results, except for UKS1 and 2MASS-GC02.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High reddening near the Galactic plane hampers observations and proper\ncharacterization of the globular clusters (GCs) located toward the inner\nregions of the Milky Way. The VISTA Variables in the Via Lactea (VVV) survey\nobserved the Galactic bulge and adjacent disk for several years, providing\nmulti-epoch, near-infrared images for 41 Galactic GCs. Detecting RRLyrae\nvariables belonging to these GCs will aid in their accurate parameterization.\nBy fully leveraging the astrometric, photometric, and variability VVV catalogs,\nwe searched for RRLyrae stars associated with GCs. Our selection criteria,\nbased on proper motions, proximity to the cluster centers, and distances\ninferred from their period-luminosity-metallicity relations, enable us to\naccurately identify the RRLyrae population in these GCs and determine color\nexcesses and distances in a homogeneous manner. Since the VVV catalogs cover\nfrom the innermost regions of the GCs to their outskirts, we can provide a\ncomprehensive picture of the entire RRLyrae population in these GCs. We have\ndiscovered significant RRLyrae populations in two highly reddened Galactic GCs:\nUKS1 and VVV-CL160, previously unknown to host RRLyrae stars. Additionally, we\nhave detected one RRLyrae candidate in each of Terzan4 and Terzan9, also new to\nRRLyrae detection. We further confirm and increase the number of RRLyrae stars\ndetected in 22 other low-latitude Galactic GCs. The RRLyrae distances place\nmost of these GCs within the Galactic bulge, aligning well with the few GCs in\nour sample with reliable Gaia or Hubble Space Telescope measurements. However,\nmost of the VVV GCs lack accurate Gaia distances, and literature distances are\ngenerally significantly smaller than those derived in this work. As a byproduct\nof our analysis, we have obtained the proper motions for all the VVV GCs,\nindependently confirming Gaia results, except for UKS1 and 2MASS-GC02."
                },
                "authors": [
                    {
                        "name": "Javier Alonso-Garca"
                    },
                    {
                        "name": "Leigh C. Smith"
                    },
                    {
                        "name": "Jason L. Sanders"
                    },
                    {
                        "name": "Dante Minniti"
                    },
                    {
                        "name": "Mrcio Catelan"
                    },
                    {
                        "name": "Gonzalo Aravena Rojas"
                    },
                    {
                        "name": "Julio A. Carballo-Bello"
                    },
                    {
                        "name": "Jos G. Fernndez-Trincado"
                    },
                    {
                        "name": "Carlos E. Ferreira Lopes"
                    },
                    {
                        "name": "Elisa R. Garro"
                    },
                    {
                        "name": "Zhen Guo"
                    },
                    {
                        "name": "Maren Hempel"
                    },
                    {
                        "name": "Philip W. Lucas"
                    },
                    {
                        "name": "Daniel Majaess"
                    },
                    {
                        "name": "Roberto K. Saito"
                    },
                    {
                        "name": "A. Katherina Vivas"
                    }
                ],
                "author_detail": {
                    "name": "A. Katherina Vivas"
                },
                "author": "A. Katherina Vivas",
                "arxiv_comment": "Accepted for publication in A&A, 21 pages, 5 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14399v2",
                "updated": "2025-02-10T14:11:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    58,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-18T12:02:41Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "title": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning"
                },
                "summary": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo Carvalho"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06494v1",
                "updated": "2025-02-10T14:11:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    32,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:11:32Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    32,
                    0,
                    41,
                    0
                ],
                "title": "GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing"
                },
                "summary": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Zhuoxuan Zhang"
                    },
                    {
                        "name": "Eunhye Ko"
                    },
                    {
                        "name": "Lily Boddy"
                    },
                    {
                        "name": "Chenan Wang"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Alexander Rasgon"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Min Kyung Lee"
                    },
                    {
                        "name": "Chenxi Yuan"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Ying Ding"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "31 pages; the first three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14596v2",
                "updated": "2025-02-10T14:09:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    9,
                    46,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-18T16:49:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Balance Resisting and Accepting Persuasion"
                },
                "summary": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up."
                },
                "authors": [
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NAACL Camera-Ready. Code:\n  https://github.com/esteng/persuasion_balanced_training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06490v1",
                "updated": "2025-02-10T14:08:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:08:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Recent Advances in Discrete Speech Tokens: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Discrete Speech Tokens: A Review"
                },
                "summary": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens."
                },
                "authors": [
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Zhihan Li"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Chongtian Shao"
                    },
                    {
                        "name": "Hanglei Zhang"
                    },
                    {
                        "name": "Chenpeng Du"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "26 pages, 8 figures, 3 tables. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03793v2",
                "updated": "2025-02-10T14:08:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    19,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T05:47:37Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    5,
                    47,
                    37,
                    3,
                    37,
                    0
                ],
                "title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers"
                },
                "summary": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements."
                },
                "authors": [
                    {
                        "name": "Benjamin Clavi"
                    },
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Benjamin Warner"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Warner"
                },
                "author": "Benjamin Warner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10282v2",
                "updated": "2025-02-10T13:58:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    58,
                    44,
                    0,
                    41,
                    0
                ],
                "published": "2023-11-17T02:02:42Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    2,
                    2,
                    42,
                    4,
                    321,
                    0
                ],
                "title": "Joint clustering with alignment for temporal data in a\n  one-point-per-experiment setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint clustering with alignment for temporal data in a\n  one-point-per-experiment setting"
                },
                "summary": "Temporal data, obtained in the setting where it is only possible to observe\none time point per experiment, is widely used in different research fields, yet\nremains insufficiently addressed from the statistical point of view. Such data\noften contain observations of a large number of entities, in which case it is\nof interest to identify a small number of representative behavior types. In\nthis paper, we propose a new method that simultaneously performs clustering and\nalignment of temporal objects inferred from these data, providing insight into\nthe relationships between entities. Simulations confirm the ability of the\nproposed approach to leverage multiple properties of the complex data we target\nsuch as accessible uncertainties, correlations and a small number of time\npoints. We illustrate it on real data encoding cellular response to a radiation\ntreatment with high energy, supported with the results of an enrichment\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal data, obtained in the setting where it is only possible to observe\none time point per experiment, is widely used in different research fields, yet\nremains insufficiently addressed from the statistical point of view. Such data\noften contain observations of a large number of entities, in which case it is\nof interest to identify a small number of representative behavior types. In\nthis paper, we propose a new method that simultaneously performs clustering and\nalignment of temporal objects inferred from these data, providing insight into\nthe relationships between entities. Simulations confirm the ability of the\nproposed approach to leverage multiple properties of the complex data we target\nsuch as accessible uncertainties, correlations and a small number of time\npoints. We illustrate it on real data encoding cellular response to a radiation\ntreatment with high energy, supported with the results of an enrichment\nanalysis."
                },
                "authors": [
                    {
                        "name": "Polina Arsenteva"
                    },
                    {
                        "name": "Mohamed Amine Benadjaoud"
                    },
                    {
                        "name": "Herv Cardot"
                    }
                ],
                "author_detail": {
                    "name": "Herv Cardot"
                },
                "author": "Herv Cardot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06802v2",
                "updated": "2025-02-10T13:55:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    55,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-12T12:52:52Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    52,
                    52,
                    6,
                    12,
                    0
                ],
                "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional\n  Kolmogorov Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Two Types of Scaling Laws from the Perspective of Conditional\n  Kolmogorov Complexity"
                },
                "summary": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the\nrelationships between model loss and the scale of parameters, data, and\ntraining computation. In 2024, OpenAI proposed the second type of Scaling Laws,\ndescribing the relationship between model inference performance and inference\ncomputation. In this paper, we analyze LLMs training and inference processes\nfrom the perspective of lossless compression using conditional Kolmogorov\ncomplexity, and unify these two types of Scaling Laws. We find that both types\nof Scaling Laws improve approximation of conditional Kolmogorov complexity by\nincreasing execution steps of Turing machine. The first type of Scaling Laws\nincreases execution steps by increasing number of model parameters. The second\ntype of Scaling Laws increases execution steps by increasing the number of\nintermediate tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the\nrelationships between model loss and the scale of parameters, data, and\ntraining computation. In 2024, OpenAI proposed the second type of Scaling Laws,\ndescribing the relationship between model inference performance and inference\ncomputation. In this paper, we analyze LLMs training and inference processes\nfrom the perspective of lossless compression using conditional Kolmogorov\ncomplexity, and unify these two types of Scaling Laws. We find that both types\nof Scaling Laws improve approximation of conditional Kolmogorov complexity by\nincreasing execution steps of Turing machine. The first type of Scaling Laws\nincreases execution steps by increasing number of model parameters. The second\ntype of Scaling Laws increases execution steps by increasing the number of\nintermediate tokens."
                },
                "authors": [
                    {
                        "name": "Jun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wan"
                },
                "author": "Jun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04692v2",
                "updated": "2025-02-10T13:52:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    52,
                    51,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-07T06:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "title": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion"
                },
                "summary": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research."
                },
                "authors": [
                    {
                        "name": "Zhenwei Wu"
                    },
                    {
                        "name": "Jinxiong Lu"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06472v1",
                "updated": "2025-02-10T13:51:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    51,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:51:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    51,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment"
                },
                "summary": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments."
                },
                "authors": [
                    {
                        "name": "Yuxing Lu"
                    },
                    {
                        "name": "Jinzhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhuo Wang"
                },
                "author": "Jinzhuo Wang",
                "arxiv_comment": "24 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06470v1",
                "updated": "2025-02-10T13:50:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    50,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:50:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    50,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks"
                },
                "summary": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks."
                },
                "authors": [
                    {
                        "name": "Hieu Minh \"Jord\" Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Minh \"Jord\" Nguyen"
                },
                "author": "Hieu Minh \"Jord\" Nguyen",
                "arxiv_comment": "Advancing Artificial Intelligence through Theory of Mind Workshop,\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07632v2",
                "updated": "2025-02-10T13:48:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    48,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-10T05:54:01Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    54,
                    1,
                    3,
                    284,
                    0
                ],
                "title": "Provable Privacy Attacks on Trained Shallow Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Privacy Attacks on Trained Shallow Neural Networks"
                },
                "summary": "We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis implicit-bias-driven setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis implicit-bias-driven setting."
                },
                "authors": [
                    {
                        "name": "Guy Smorodinsky"
                    },
                    {
                        "name": "Gal Vardi"
                    },
                    {
                        "name": "Itay Safran"
                    }
                ],
                "author_detail": {
                    "name": "Itay Safran"
                },
                "author": "Itay Safran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06462v1",
                "updated": "2025-02-10T13:43:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    43,
                    18,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:43:18Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    43,
                    18,
                    0,
                    41,
                    0
                ],
                "title": "Inference on the cointegration and the attractor spaces via functional\n  approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on the cointegration and the attractor spaces via functional\n  approximation"
                },
                "summary": "This paper discusses semiparametric inference on hypotheses on the\ncointegration and the attractor spaces for I(1) linear processes, using\ncanonical correlation analysis and functional approximation of Brownian\nMotions. It proposes inference criteria based on the estimation of the number\nof common trends in various subsets of variables, and compares them to\nsequences of tests of hypotheses. The exact limit distribution for one of the\ntest statistics is derived in the univariate case. Properties of the\ninferential tools are discussed theoretically and illustrated via a Monte Carlo\nstudy. An empirical analysis of exchange rates is also included.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper discusses semiparametric inference on hypotheses on the\ncointegration and the attractor spaces for I(1) linear processes, using\ncanonical correlation analysis and functional approximation of Brownian\nMotions. It proposes inference criteria based on the estimation of the number\nof common trends in various subsets of variables, and compares them to\nsequences of tests of hypotheses. The exact limit distribution for one of the\ntest statistics is derived in the univariate case. Properties of the\ninferential tools are discussed theoretically and illustrated via a Monte Carlo\nstudy. An empirical analysis of exchange rates is also included."
                },
                "authors": [
                    {
                        "name": "Massimo Franchi"
                    },
                    {
                        "name": "Paolo Paruolo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Paruolo"
                },
                "author": "Paolo Paruolo",
                "arxiv_comment": "28 pages. arXiv admin note: text overlap with arXiv:2411.19572",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06453v1",
                "updated": "2025-02-10T13:31:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    31,
                    46,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:31:46Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    31,
                    46,
                    0,
                    41,
                    0
                ],
                "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations"
                },
                "summary": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels."
                },
                "authors": [
                    {
                        "name": "Kaixuan Huang"
                    },
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Xiang Ji"
                    },
                    {
                        "name": "Jiawei Ge"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Yingqing Guo"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Hui Yuan"
                    },
                    {
                        "name": "Runzhe Wang"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Chi Jin"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17428v2",
                "updated": "2025-02-10T13:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    28,
                    1,
                    0,
                    41,
                    0
                ],
                "published": "2024-03-26T06:50:04Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    6,
                    50,
                    4,
                    1,
                    86,
                    0
                ],
                "title": "Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study"
                },
                "summary": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews."
                },
                "authors": [
                    {
                        "name": "Jae-hee So"
                    },
                    {
                        "name": "Joonhwan Chang"
                    },
                    {
                        "name": "Eunji Kim"
                    },
                    {
                        "name": "Junho Na"
                    },
                    {
                        "name": "JiYeon Choi"
                    },
                    {
                        "name": "Jy-yong Sohn"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    },
                    {
                        "name": "Sang Hui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Sang Hui Chu"
                },
                "author": "Sang Hui Chu",
                "arxiv_doi": "10.2196/58418",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/58418",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "JMIR Form Res 2024;8:e58418",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.12557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.12557v2",
                "updated": "2025-02-10T13:25:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    25,
                    17,
                    0,
                    41,
                    0
                ],
                "published": "2023-07-24T06:46:28Z",
                "published_parsed": [
                    2023,
                    7,
                    24,
                    6,
                    46,
                    28,
                    0,
                    205,
                    0
                ],
                "title": "Robust Bayesian inference for nondestructive one-shot device testing\n  data under competing risk using Hamiltonian Monte Carlo method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian inference for nondestructive one-shot device testing\n  data under competing risk using Hamiltonian Monte Carlo method"
                },
                "summary": "The prevalence of one-shot devices is quite prolific in engineering and\nmedical domains. Unlike typical one-shot devices, nondestructive one-shot\ndevices (NOSD) may survive multiple tests and offer additional data for\nreliability estimation. This study aims to implement the Bayesian approach of\nthe lifetime prognosis of NOSD when failures are subject to multiple risks.\nWith small deviations from the assumed model conditions, conventional\nlikelihood-based Bayesian estimation may result in misleading statistical\ninference, raising the need for a robust Bayesian method. This work develops\nBayesian estimation by exploiting a robustified posterior based on the density\npower divergence measure for NOSD test data. Further, the testing of the\nhypothesis is carried out by applying a proposed Bayes factor derived from the\nrobustified posterior. A flexible Hamiltonian Monte Carlo approach is applied\nto generate posterior samples. Additionally, we assess the extent of resistance\nof the proposed methods to small deviations from the assumed model conditions\nby applying the influence function (IF) approach. In testing of hypothesis, IF\nreflects how outliers impact the decision-making through Bayes factor under\nnull hypothesis. Finally, this analytical development is validated through a\nsimulation study and a data analysis based on cancer data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevalence of one-shot devices is quite prolific in engineering and\nmedical domains. Unlike typical one-shot devices, nondestructive one-shot\ndevices (NOSD) may survive multiple tests and offer additional data for\nreliability estimation. This study aims to implement the Bayesian approach of\nthe lifetime prognosis of NOSD when failures are subject to multiple risks.\nWith small deviations from the assumed model conditions, conventional\nlikelihood-based Bayesian estimation may result in misleading statistical\ninference, raising the need for a robust Bayesian method. This work develops\nBayesian estimation by exploiting a robustified posterior based on the density\npower divergence measure for NOSD test data. Further, the testing of the\nhypothesis is carried out by applying a proposed Bayes factor derived from the\nrobustified posterior. A flexible Hamiltonian Monte Carlo approach is applied\nto generate posterior samples. Additionally, we assess the extent of resistance\nof the proposed methods to small deviations from the assumed model conditions\nby applying the influence function (IF) approach. In testing of hypothesis, IF\nreflects how outliers impact the decision-making through Bayes factor under\nnull hypothesis. Finally, this analytical development is validated through a\nsimulation study and a data analysis based on cancer data."
                },
                "authors": [
                    {
                        "name": "Shanya Baghel"
                    },
                    {
                        "name": "Shuvashree Mondal"
                    }
                ],
                "author_detail": {
                    "name": "Shuvashree Mondal"
                },
                "author": "Shuvashree Mondal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.12557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.12557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62F12, 62NO2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19345v3",
                "updated": "2025-02-10T13:18:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    18,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2024-07-27T21:56:23Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    21,
                    56,
                    23,
                    5,
                    209,
                    0
                ],
                "title": "Inference-Time Selective Debiasing to Enhance Fairness in Text\n  Classification Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Selective Debiasing to Enhance Fairness in Text\n  Classification Models"
                },
                "summary": "We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories."
                },
                "authors": [
                    {
                        "name": "Gleb Kuzmin"
                    },
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06425v1",
                "updated": "2025-02-10T13:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    2,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    2,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Hiroki Watanabe"
                    },
                    {
                        "name": "Motonobu Uchikoshi"
                    }
                ],
                "author_detail": {
                    "name": "Motonobu Uchikoshi"
                },
                "author": "Motonobu Uchikoshi",
                "arxiv_doi": "10.1145/3701716.3715597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06419v1",
                "updated": "2025-02-10T12:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    55,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    55,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have made substantial advancements in the field\nof robotic and autonomous driving. This study presents the first\nOccupancy-based Large Language Model (Occ-LLM), which represents a pioneering\neffort to integrate LLMs with an important representation. To effectively\nencode occupancy as input for the LLM and address the category imbalances\nassociated with occupancy, we propose Motion Separation Variational Autoencoder\n(MS-VAE). This innovative approach utilizes prior knowledge to distinguish\ndynamic objects from static scenes before inputting them into a tailored\nVariational Autoencoder (VAE). This separation enhances the model's capacity to\nconcentrate on dynamic trajectories while effectively reconstructing static\nscenes. The efficacy of Occ-LLM has been validated across key tasks, including\n4D occupancy forecasting, self-ego planning, and occupancy-based scene question\nanswering. Comprehensive evaluations demonstrate that Occ-LLM significantly\nsurpasses existing state-of-the-art methodologies, achieving gains of about 6\\%\nin Intersection over Union (IoU) and 4\\% in mean Intersection over Union (mIoU)\nfor the task of 4D occupancy forecasting. These findings highlight the\ntransformative potential of Occ-LLM in reshaping current paradigms within\nrobotic and autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made substantial advancements in the field\nof robotic and autonomous driving. This study presents the first\nOccupancy-based Large Language Model (Occ-LLM), which represents a pioneering\neffort to integrate LLMs with an important representation. To effectively\nencode occupancy as input for the LLM and address the category imbalances\nassociated with occupancy, we propose Motion Separation Variational Autoencoder\n(MS-VAE). This innovative approach utilizes prior knowledge to distinguish\ndynamic objects from static scenes before inputting them into a tailored\nVariational Autoencoder (VAE). This separation enhances the model's capacity to\nconcentrate on dynamic trajectories while effectively reconstructing static\nscenes. The efficacy of Occ-LLM has been validated across key tasks, including\n4D occupancy forecasting, self-ego planning, and occupancy-based scene question\nanswering. Comprehensive evaluations demonstrate that Occ-LLM significantly\nsurpasses existing state-of-the-art methodologies, achieving gains of about 6\\%\nin Intersection over Union (IoU) and 4\\% in mean Intersection over Union (mIoU)\nfor the task of 4D occupancy forecasting. These findings highlight the\ntransformative potential of Occ-LLM in reshaping current paradigms within\nrobotic and autonomous driving."
                },
                "authors": [
                    {
                        "name": "Tianshuo Xu"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Yingjie Cai"
                    },
                    {
                        "name": "Bingbing Liu"
                    },
                    {
                        "name": "Yingcong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingcong Chen"
                },
                "author": "Yingcong Chen",
                "arxiv_comment": "Accepted in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06415v1",
                "updated": "2025-02-10T12:54:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    54,
                    17,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:54:17Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    54,
                    17,
                    0,
                    41,
                    0
                ],
                "title": "Systematic Outliers in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Outliers in Large Language Models"
                },
                "summary": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers."
                },
                "authors": [
                    {
                        "name": "Yongqi An"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17151v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17151v4",
                "updated": "2025-02-10T12:54:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    54,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2024-05-27T13:26:34Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    13,
                    26,
                    34,
                    0,
                    148,
                    0
                ],
                "title": "Smoke and Mirrors in Causal Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smoke and Mirrors in Causal Downstream Tasks"
                },
                "summary": "Machine Learning and AI have the potential to transform data-driven\nscientific discovery, enabling accurate predictions for several scientific\nphenomena. As many scientific questions are inherently causal, this paper looks\nat the causal inference task of treatment effect estimation, where the outcome\nof interest is recorded in high-dimensional observations in a Randomized\nControlled Trial (RCT). Despite being the simplest possible causal setting and\na perfect fit for deep learning, we theoretically find that many common choices\nin the literature may lead to biased estimates. To test the practical impact of\nthese considerations, we recorded ISTAnt, the first real-world benchmark for\ncausal inference downstream tasks on high-dimensional observations as an RCT\nstudying how garden ants (Lasius neglectus) respond to microparticles applied\nonto their colony members by hygienic grooming. Comparing 6 480 models\nfine-tuned from state-of-the-art visual backbones, we find that the sampling\nand modeling choices significantly affect the accuracy of the causal estimate,\nand that classification accuracy is not a proxy thereof. We further validated\nthe analysis, repeating it on a synthetically generated visual data set\ncontrolling the causal model. Our results suggest that future benchmarks should\ncarefully consider real downstream scientific questions, especially causal\nones. Further, we highlight guidelines for representation learning methods to\nhelp answer causal questions in the sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning and AI have the potential to transform data-driven\nscientific discovery, enabling accurate predictions for several scientific\nphenomena. As many scientific questions are inherently causal, this paper looks\nat the causal inference task of treatment effect estimation, where the outcome\nof interest is recorded in high-dimensional observations in a Randomized\nControlled Trial (RCT). Despite being the simplest possible causal setting and\na perfect fit for deep learning, we theoretically find that many common choices\nin the literature may lead to biased estimates. To test the practical impact of\nthese considerations, we recorded ISTAnt, the first real-world benchmark for\ncausal inference downstream tasks on high-dimensional observations as an RCT\nstudying how garden ants (Lasius neglectus) respond to microparticles applied\nonto their colony members by hygienic grooming. Comparing 6 480 models\nfine-tuned from state-of-the-art visual backbones, we find that the sampling\nand modeling choices significantly affect the accuracy of the causal estimate,\nand that classification accuracy is not a proxy thereof. We further validated\nthe analysis, repeating it on a synthetically generated visual data set\ncontrolling the causal model. Our results suggest that future benchmarks should\ncarefully consider real downstream scientific questions, especially causal\nones. Further, we highlight guidelines for representation learning methods to\nhelp answer causal questions in the sciences."
                },
                "authors": [
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Lukas Lindorfer"
                    },
                    {
                        "name": "Sylvia Cremer"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Francesco Locatello"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Locatello"
                },
                "author": "Francesco Locatello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17151v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17151v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06401v1",
                "updated": "2025-02-10T12:40:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    40,
                    32,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:40:32Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    40,
                    32,
                    0,
                    41,
                    0
                ],
                "title": "Habitizing Diffusion Planning for Efficient and Effective Decision\n  Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Habitizing Diffusion Planning for Efficient and Effective Decision\n  Making"
                },
                "summary": "Diffusion models have shown great promise in decision-making, also known as\ndiffusion planning. However, the slow inference speeds limit their potential\nfor broader real-world applications. Here, we introduce Habi, a general\nframework that transforms powerful but slow diffusion planning models into fast\ndecision-making models, which mimics the cognitive process in the brain that\ncostly goal-directed behavior gradually transitions to efficient habitual\nbehavior with repetitive practice. Even using a laptop CPU, the habitized model\ncan achieve an average 800+ Hz decision-making frequency (faster than previous\ndiffusion planners by orders of magnitude) on standard offline reinforcement\nlearning benchmarks D4RL, while maintaining comparable or even higher\nperformance compared to its corresponding diffusion planner. Our work proposes\na fresh perspective of leveraging powerful diffusion models for real-world\ndecision-making tasks. We also provide robust evaluations and analysis,\noffering insights from both biological and engineering perspectives for\nefficient and effective decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown great promise in decision-making, also known as\ndiffusion planning. However, the slow inference speeds limit their potential\nfor broader real-world applications. Here, we introduce Habi, a general\nframework that transforms powerful but slow diffusion planning models into fast\ndecision-making models, which mimics the cognitive process in the brain that\ncostly goal-directed behavior gradually transitions to efficient habitual\nbehavior with repetitive practice. Even using a laptop CPU, the habitized model\ncan achieve an average 800+ Hz decision-making frequency (faster than previous\ndiffusion planners by orders of magnitude) on standard offline reinforcement\nlearning benchmarks D4RL, while maintaining comparable or even higher\nperformance compared to its corresponding diffusion planner. Our work proposes\na fresh perspective of leveraging powerful diffusion models for real-world\ndecision-making tasks. We also provide robust evaluations and analysis,\noffering insights from both biological and engineering perspectives for\nefficient and effective decision-making."
                },
                "authors": [
                    {
                        "name": "Haofei Lu"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Dongqi Han"
                    }
                ],
                "author_detail": {
                    "name": "Dongqi Han"
                },
                "author": "Dongqi Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06398v1",
                "updated": "2025-02-10T12:36:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    36,
                    57,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:36:57Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    36,
                    57,
                    0,
                    41,
                    0
                ],
                "title": "Learning Counterfactual Outcomes Under Rank Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Counterfactual Outcomes Under Rank Preservation"
                },
                "summary": "Counterfactual inference aims to estimate the counterfactual outcome at the\nindividual level given knowledge of an observed treatment and the factual\noutcome, with broad applications in fields such as epidemiology, econometrics,\nand management science. Previous methods rely on a known structural causal\nmodel (SCM) or assume the homogeneity of the exogenous variable and strict\nmonotonicity between the outcome and exogenous variable. In this paper, we\npropose a principled approach for identifying and estimating the counterfactual\noutcome. We first introduce a simple and intuitive rank preservation assumption\nto identify the counterfactual outcome without relying on a known structural\ncausal model. Building on this, we propose a novel ideal loss for theoretically\nunbiased learning of the counterfactual outcome and further develop a\nkernel-based estimator for its empirical estimation. Our theoretical analysis\nshows that the rank preservation assumption is not stronger than the\nhomogeneity and strict monotonicity assumptions, and shows that the proposed\nideal loss is convex, and the proposed estimator is unbiased. Extensive\nsemi-synthetic and real-world experiments are conducted to demonstrate the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual inference aims to estimate the counterfactual outcome at the\nindividual level given knowledge of an observed treatment and the factual\noutcome, with broad applications in fields such as epidemiology, econometrics,\nand management science. Previous methods rely on a known structural causal\nmodel (SCM) or assume the homogeneity of the exogenous variable and strict\nmonotonicity between the outcome and exogenous variable. In this paper, we\npropose a principled approach for identifying and estimating the counterfactual\noutcome. We first introduce a simple and intuitive rank preservation assumption\nto identify the counterfactual outcome without relying on a known structural\ncausal model. Building on this, we propose a novel ideal loss for theoretically\nunbiased learning of the counterfactual outcome and further develop a\nkernel-based estimator for its empirical estimation. Our theoretical analysis\nshows that the rank preservation assumption is not stronger than the\nhomogeneity and strict monotonicity assumptions, and shows that the proposed\nideal loss is convex, and the proposed estimator is unbiased. Extensive\nsemi-synthetic and real-world experiments are conducted to demonstrate the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Chunyuan Zheng"
                    },
                    {
                        "name": "Yan Zeng"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02765v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02765v3",
                "updated": "2025-02-10T12:35:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    35,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-05-04T22:02:24Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    22,
                    2,
                    24,
                    5,
                    125,
                    0
                ],
                "title": "Has this Fact been Edited? Detecting Knowledge Edits in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Has this Fact been Edited? Detecting Knowledge Edits in Language Models"
                },
                "summary": "Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Jrg Schltterer"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Schltterer"
                },
                "author": "Jrg Schltterer",
                "arxiv_comment": "Accepted at NAACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02765v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02765v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v5",
                "updated": "2025-02-10T12:35:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    35,
                    22,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Section 1.2 (Harnessing Multi-Agent Potential in CAS with OpenAI\n  SWARM) Renamed. Section 5.3 (Future Directions) Updated. Minor typo\n  corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06394v1",
                "updated": "2025-02-10T12:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    30,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    30,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators"
                },
                "summary": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification."
                },
                "authors": [
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Nikita Sushko"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.06788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06788v1",
                "updated": "2025-02-10T18:59:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    58,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:59:58Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    58,
                    0,
                    41,
                    0
                ],
                "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models"
                },
                "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE."
                },
                "authors": [
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Haoge Deng"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06787v1",
                "updated": "2025-02-10T18:59:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    35,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Agentic AI for Spatial Reasoning with a Dynamic API"
                },
                "summary": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/"
                },
                "authors": [
                    {
                        "name": "Damiano Marsili"
                    },
                    {
                        "name": "Rohun Agrawal"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Georgia Gkioxari"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gkioxari"
                },
                "author": "Georgia Gkioxari",
                "arxiv_comment": "Project website: https://glab-caltech.github.io/vadar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08413v2",
                "updated": "2025-02-10T18:57:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    57,
                    27,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-14T20:08:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    8,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data"
                },
                "summary": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling."
                },
                "authors": [
                    {
                        "name": "Jiaxing Qiu"
                    },
                    {
                        "name": "Dongliang Guo"
                    },
                    {
                        "name": "Natalie Papini"
                    },
                    {
                        "name": "Noelle Peace"
                    },
                    {
                        "name": "Cheri A. Levinson"
                    },
                    {
                        "name": "Teague R. Henry"
                    }
                ],
                "author_detail": {
                    "name": "Teague R. Henry"
                },
                "author": "Teague R. Henry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06776v1",
                "updated": "2025-02-10T18:54:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:54:05Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "title": "Towards Internet-Scale Training For Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Internet-Scale Training For Agents"
                },
                "summary": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io."
                },
                "authors": [
                    {
                        "name": "Brandon Trabucco"
                    },
                    {
                        "name": "Gunnar Sigurdsson"
                    },
                    {
                        "name": "Robinson Piramuthu"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Salakhutdinov"
                },
                "author": "Ruslan Salakhutdinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06773v1",
                "updated": "2025-02-10T18:52:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    52,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:52:04Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    52,
                    4,
                    0,
                    41,
                    0
                ],
                "title": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition"
                },
                "summary": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}."
                },
                "authors": [
                    {
                        "name": "Guanghao Ye"
                    },
                    {
                        "name": "Khiem Duc Pham"
                    },
                    {
                        "name": "Xinzhi Zhang"
                    },
                    {
                        "name": "Sivakanth Gopi"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Janardhan Kulkarni"
                    },
                    {
                        "name": "Huseyin A. Inan"
                    }
                ],
                "author_detail": {
                    "name": "Huseyin A. Inan"
                },
                "author": "Huseyin A. Inan",
                "arxiv_comment": "Abstract shortened for arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06772v1",
                "updated": "2025-02-10T18:51:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    51,
                    47,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:51:47Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    51,
                    47,
                    0,
                    41,
                    0
                ],
                "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates"
                },
                "summary": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code: https://github.com/Gen-Verse/ReasonFlux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10626v2",
                "updated": "2025-02-10T18:43:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    43,
                    26,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-14T15:31:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    31,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts"
                },
                "summary": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters."
                },
                "authors": [
                    {
                        "name": "Guorui Zheng"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yuping Zheng"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06621v2",
                "updated": "2025-02-10T18:35:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    35,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2024-06-07T15:28:31Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    15,
                    28,
                    31,
                    4,
                    159,
                    0
                ],
                "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering"
                },
                "summary": "We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems."
                },
                "authors": [
                    {
                        "name": "Harry Li"
                    },
                    {
                        "name": "Gabriel Appleby"
                    },
                    {
                        "name": "Ashley Suh"
                    }
                ],
                "author_detail": {
                    "name": "Ashley Suh"
                },
                "author": "Ashley Suh",
                "arxiv_doi": "10.1109/VIS55277.2024.00031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/VIS55277.2024.00031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.06621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Open-source code: https://github.com/mit-ll/linkq",
                "arxiv_journal_ref": "H. Li, G. Appleby and A. Suh, \"LinkQ: An LLM-Assisted Visual\n  Interface for Knowledge Graph Question-Answering,\" 2024 IEEE Visualization\n  and Visual Analytics (VIS)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00761v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00761v4",
                "updated": "2025-02-10T18:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    26,
                    14,
                    0,
                    41,
                    0
                ],
                "published": "2024-08-01T17:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    59,
                    12,
                    3,
                    214,
                    0
                ],
                "title": "Tamper-Resistant Safeguards for Open-Weight LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tamper-Resistant Safeguards for Open-Weight LLMs"
                },
                "summary": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs."
                },
                "authors": [
                    {
                        "name": "Rishub Tamirisa"
                    },
                    {
                        "name": "Bhrugu Bharathi"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Maxwell Lin"
                    },
                    {
                        "name": "Justin Wang"
                    },
                    {
                        "name": "Rowan Wang"
                    },
                    {
                        "name": "Ron Arel"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Mantas Mazeika"
                    }
                ],
                "author_detail": {
                    "name": "Mantas Mazeika"
                },
                "author": "Mantas Mazeika",
                "arxiv_comment": "Website: https://www.tamper-resistant-safeguards.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00761v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00761v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06742v1",
                "updated": "2025-02-10T18:09:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    9,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:09:53Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    9,
                    53,
                    0,
                    41,
                    0
                ],
                "title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Multi-Normalization for Stateless and Scalable LLM Training"
                },
                "summary": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines."
                },
                "authors": [
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06738v1",
                "updated": "2025-02-10T18:07:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    7,
                    9,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:07:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    7,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "Resurrecting saturated LLM benchmarks with adversarial encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resurrecting saturated LLM benchmarks with adversarial encoding"
                },
                "summary": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks."
                },
                "authors": [
                    {
                        "name": "Igor Ivanov"
                    },
                    {
                        "name": "Dmitrii Volkov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Volkov"
                },
                "author": "Dmitrii Volkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06737v1",
                "updated": "2025-02-10T18:03:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    3,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T18:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    3,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data"
                },
                "summary": "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM."
                },
                "authors": [
                    {
                        "name": "Thomas Zeng"
                    },
                    {
                        "name": "Shuibai Zhang"
                    },
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Christian Classen"
                    },
                    {
                        "name": "Daewon Chae"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Heeju Kim"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Jackson Kunde"
                    },
                    {
                        "name": "Ying Fan"
                    },
                    {
                        "name": "Jungtaek Kim"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06733v1",
                "updated": "2025-02-10T17:57:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    57,
                    15,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    57,
                    15,
                    0,
                    41,
                    0
                ],
                "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining"
                },
                "summary": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance."
                },
                "authors": [
                    {
                        "name": "Daouda Sow"
                    },
                    {
                        "name": "Herbert Woisetschlger"
                    },
                    {
                        "name": "Saikiran Bulusu"
                    },
                    {
                        "name": "Shiqiang Wang"
                    },
                    {
                        "name": "Hans-Arno Jacobsen"
                    },
                    {
                        "name": "Yingbin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yingbin Liang"
                },
                "author": "Yingbin Liang",
                "arxiv_comment": "Accepted for publication at ICLR 2025. Code base available:\n  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06725v1",
                "updated": "2025-02-10T17:54:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    54,
                    30,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:54:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    54,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in\n  Dynamic Environments by Leveraging Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in\n  Dynamic Environments by Leveraging Object Detection"
                },
                "summary": "Autonomous drone navigation in dynamic environments remains a critical\nchallenge, especially when dealing with unpredictable scenarios including\nfast-moving objects with rapidly changing goal positions. While traditional\nplanners and classical optimisation methods have been extensively used to\naddress this dynamic problem, they often face real-time, unpredictable changes\nthat ultimately leads to sub-optimal performance in terms of adaptiveness and\nreal-time decision making. In this work, we propose a novel motion planner,\nAgilePilot, based on Deep Reinforcement Learning (DRL) that is trained in\ndynamic conditions, coupled with real-time Computer Vision (CV) for object\ndetections during flight. The training-to-deployment framework bridges the\nSim2Real gap, leveraging sophisticated reward structures that promotes both\nsafety and agility depending upon environment conditions. The system can\nrapidly adapt to changing environments, while achieving a maximum speed of 3.0\nm/s in real-world scenarios. In comparison, our approach outperforms classical\nalgorithms such as Artificial Potential Field (APF) based motion planner by 3\ntimes, both in performance and tracking accuracy of dynamic targets by using\nvelocity predictions while exhibiting 90% success rate in 75 conducted\nexperiments. This work highlights the effectiveness of DRL in tackling\nreal-time dynamic navigation challenges, offering intelligent safety and\nagility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous drone navigation in dynamic environments remains a critical\nchallenge, especially when dealing with unpredictable scenarios including\nfast-moving objects with rapidly changing goal positions. While traditional\nplanners and classical optimisation methods have been extensively used to\naddress this dynamic problem, they often face real-time, unpredictable changes\nthat ultimately leads to sub-optimal performance in terms of adaptiveness and\nreal-time decision making. In this work, we propose a novel motion planner,\nAgilePilot, based on Deep Reinforcement Learning (DRL) that is trained in\ndynamic conditions, coupled with real-time Computer Vision (CV) for object\ndetections during flight. The training-to-deployment framework bridges the\nSim2Real gap, leveraging sophisticated reward structures that promotes both\nsafety and agility depending upon environment conditions. The system can\nrapidly adapt to changing environments, while achieving a maximum speed of 3.0\nm/s in real-world scenarios. In comparison, our approach outperforms classical\nalgorithms such as Artificial Potential Field (APF) based motion planner by 3\ntimes, both in performance and tracking accuracy of dynamic targets by using\nvelocity predictions while exhibiting 90% success rate in 75 conducted\nexperiments. This work highlights the effectiveness of DRL in tackling\nreal-time dynamic navigation challenges, offering intelligent safety and\nagility."
                },
                "authors": [
                    {
                        "name": "Roohan Ahmed Khan"
                    },
                    {
                        "name": "Valerii Serpiva"
                    },
                    {
                        "name": "Demetros Aschalew"
                    },
                    {
                        "name": "Aleksey Fedoseev"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "Manuscript has been submitted to 2025 INTERNATIONAL CONFERENCE ON\n  UNMANNED AIRCRAFT SYSTEMS (ICUAS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06703v1",
                "updated": "2025-02-10T17:30:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    30,
                    23,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T17:30:23Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    30,
                    23,
                    0,
                    41,
                    0
                ],
                "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling"
                },
                "summary": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18727v2",
                "updated": "2025-02-10T17:27:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    27,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-30T20:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    20,
                    7,
                    44,
                    3,
                    30,
                    0
                ],
                "title": "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks"
                },
                "summary": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms."
                },
                "authors": [
                    {
                        "name": "Mohd. Farhan Israk Soumik"
                    },
                    {
                        "name": "W. K. M. Mithsara"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "arxiv_comment": "Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop\n  on Privacy-Preserving Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v3",
                "updated": "2025-02-10T17:15:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    15,
                    52,
                    0,
                    41,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Coverage-Guided LLM-Based Test Generation"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06669v1",
                "updated": "2025-02-10T16:54:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    54,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:54:03Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    54,
                    3,
                    0,
                    41,
                    0
                ],
                "title": "Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations"
                },
                "summary": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research."
                },
                "authors": [
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Tailai Peng"
                    },
                    {
                        "name": "Xinran Xie"
                    },
                    {
                        "name": "Dekun Lin"
                    },
                    {
                        "name": "Zhe Cui"
                    },
                    {
                        "name": "Zheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Chen"
                },
                "author": "Zheng Chen",
                "arxiv_comment": "to be published in ICONIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06666v1",
                "updated": "2025-02-10T16:52:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    52,
                    39,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:52:39Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    52,
                    39,
                    0,
                    41,
                    0
                ],
                "title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering"
                },
                "summary": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark--CareQA--, with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations --Relaxed Perplexity-- to\nmitigate the identified limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark--CareQA--, with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations --Relaxed Perplexity-- to\nmitigate the identified limitations."
                },
                "authors": [
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Pablo Agustin Martin-Torres"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Pablo Bernabeu-Perez"
                    },
                    {
                        "name": "Lucia Urcelay Ganzabal"
                    },
                    {
                        "name": "Marta Gonzalez Mallo"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Enrique Lopez-Cuena"
                    },
                    {
                        "name": "Sergio Alvarez-Napagao"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06663v1",
                "updated": "2025-02-10T16:51:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    51,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:51:03Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    51,
                    3,
                    0,
                    41,
                    0
                ],
                "title": "EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models"
                },
                "summary": "Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM."
                },
                "authors": [
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Boyan Gao"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06659v1",
                "updated": "2025-02-10T16:48:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    48,
                    56,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:48:56Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    48,
                    56,
                    0,
                    41,
                    0
                ],
                "title": "Who Taught You That? Tracing Teachers in Model Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Taught You That? Tracing Teachers in Model Distillation"
                },
                "summary": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers."
                },
                "authors": [
                    {
                        "name": "Somin Wadhwa"
                    },
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Silvio Amir"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "arxiv_comment": "Preprint; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06656v1",
                "updated": "2025-02-10T16:47:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    47,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:47:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    47,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management"
                },
                "summary": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it."
                },
                "authors": [
                    {
                        "name": "Simeon Campos"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Otter Quarks"
                    }
                ],
                "author_detail": {
                    "name": "Otter Quarks"
                },
                "author": "Otter Quarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06655v1",
                "updated": "2025-02-10T16:45:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    45,
                    18,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:45:18Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    45,
                    18,
                    0,
                    41,
                    0
                ],
                "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Evaluation of Large Language Models from a Causal Perspective"
                },
                "summary": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination has become a significant concern in the LLM\nevaluation community. Previous Agents-as-an-Evaluator address this issue by\ninvolving agents in the generation of questions. Despite their success, the\nbiases in Agents-as-an-Evaluator methods remain largely unexplored. In this\npaper, we present a theoretical formulation of evaluation bias, providing\nvaluable insights into designing unbiased evaluation protocols. Furthermore, we\nidentify two type of bias in Agents-as-an-Evaluator through carefully designed\nprobing tasks on a minimal Agents-as-an-Evaluator setup. To address these\nissues, we propose the Unbiased Evaluator, an evaluation protocol that delivers\na more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive\nexperiments reveal significant room for improvement in current LLMs.\nAdditionally, we demonstrate that the Unbiased Evaluator not only offers strong\nevidence of benchmark contamination but also provides interpretable evaluation\nresults."
                },
                "authors": [
                    {
                        "name": "Meilin Chen"
                    },
                    {
                        "name": "Jian Tian"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Di Xie"
                    },
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Jiang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Zhu"
                },
                "author": "Jiang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19020v3",
                "updated": "2025-02-10T16:42:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    23,
                    0,
                    41,
                    0
                ],
                "published": "2024-09-25T07:03:31Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    31,
                    2,
                    269,
                    0
                ],
                "title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications"
                },
                "summary": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research."
                },
                "authors": [
                    {
                        "name": "Sathya Krishnan Suresh"
                    },
                    {
                        "name": "Wu Mengjun"
                    },
                    {
                        "name": "Tushar Pranav"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "13 pages, 1 figure",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06652v1",
                "updated": "2025-02-10T16:42:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:42:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    42,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A"
                },
                "summary": "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks."
                },
                "authors": [
                    {
                        "name": "Anna Leschanowsky"
                    },
                    {
                        "name": "Zahra Kolagar"
                    },
                    {
                        "name": "Erion ano"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Dara Hallinan"
                    },
                    {
                        "name": "Emanul A. P. Habets"
                    },
                    {
                        "name": "Birgit Popp"
                    }
                ],
                "author_detail": {
                    "name": "Birgit Popp"
                },
                "author": "Birgit Popp",
                "arxiv_comment": "Submitted to ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04419v2",
                "updated": "2025-02-10T16:34:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    34,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T15:20:58Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    20,
                    58,
                    3,
                    37,
                    0
                ],
                "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks"
                },
                "summary": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation."
                },
                "authors": [
                    {
                        "name": "Miaomiao Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "Technical report; 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06635v1",
                "updated": "2025-02-10T16:31:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    31,
                    37,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:31:37Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    31,
                    37,
                    0,
                    41,
                    0
                ],
                "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM"
                },
                "summary": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM."
                },
                "authors": [
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04295v2",
                "updated": "2025-02-10T16:25:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    25,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T18:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO."
                },
                "authors": [
                    {
                        "name": "Yuanye Liu"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05232v2",
                "updated": "2025-02-10T16:22:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    22,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-06T18:02:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    2,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds"
                },
                "summary": "Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures."
                },
                "authors": [
                    {
                        "name": "James Beetham"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13138v2",
                "updated": "2025-02-10T16:13:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    13,
                    52,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-22T12:32:35Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    32,
                    35,
                    2,
                    22,
                    0
                ],
                "title": "Scalability Analysis of 5G-TSN Applications in Indoor Factory Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalability Analysis of 5G-TSN Applications in Indoor Factory Settings"
                },
                "summary": "While technologies such as Time-Sensitive Networking (TSN) improve\ndeterministic behaviour, real-time functionality, and robustness of Ethernet,\nfuture industrial networks aim to be increasingly wireless. While wireless\nnetworks facilitate mobility, reduce cost, and simplify deployment, they do not\nalways provide stringent latency constraints and highly dependable data\ntransmission as required by many manufacturing systems. The advent of 5G, with\nits Ultra-Reliable Low-Latency Communication (URLLC) capabilities, offers\npotential for wireless industrial networks. 5G offers elevated data throughput,\nvery low latency, and negligible jitter. As 5G networks typically include wired\nconnections from the base station to the core network, integration of 5G with\ntime-sensitive networking is essential to provide rigorous QoS standards. This\npaper assesses the scalability of 5G-TSN for various indoor factory\napplications and conditions using OMNET++ simulation. Our research shows that\n5G-TSN has the potential to provide bounded delay for latency-sensitive\napplications in scalable indoor factory settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While technologies such as Time-Sensitive Networking (TSN) improve\ndeterministic behaviour, real-time functionality, and robustness of Ethernet,\nfuture industrial networks aim to be increasingly wireless. While wireless\nnetworks facilitate mobility, reduce cost, and simplify deployment, they do not\nalways provide stringent latency constraints and highly dependable data\ntransmission as required by many manufacturing systems. The advent of 5G, with\nits Ultra-Reliable Low-Latency Communication (URLLC) capabilities, offers\npotential for wireless industrial networks. 5G offers elevated data throughput,\nvery low latency, and negligible jitter. As 5G networks typically include wired\nconnections from the base station to the core network, integration of 5G with\ntime-sensitive networking is essential to provide rigorous QoS standards. This\npaper assesses the scalability of 5G-TSN for various indoor factory\napplications and conditions using OMNET++ simulation. Our research shows that\n5G-TSN has the potential to provide bounded delay for latency-sensitive\napplications in scalable indoor factory settings."
                },
                "authors": [
                    {
                        "name": "Kouros Zanbouri"
                    },
                    {
                        "name": "Md. Noor-A-Rahim"
                    },
                    {
                        "name": "Dirk Pesch"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Pesch"
                },
                "author": "Dirk Pesch",
                "arxiv_comment": "This paper has been accepted for presentation at IEEE Wireless\n  Communications and Networking Conference (WCNC) 2025 Workshops. arXiv admin\n  note: text overlap with arXiv:2501.12792",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06608v1",
                "updated": "2025-02-10T16:07:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    7,
                    54,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:07:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    7,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models"
                },
                "summary": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available."
                },
                "authors": [
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Zi-Xin Zou"
                    },
                    {
                        "name": "Zexiang Liu"
                    },
                    {
                        "name": "Dehu Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Zhipeng Yu"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Ding Liang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yan-Pei Cao"
                },
                "author": "Yan-Pei Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06604v1",
                "updated": "2025-02-10T16:01:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    1,
                    55,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T16:01:55Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    1,
                    55,
                    0,
                    41,
                    0
                ],
                "title": "Do we really have to filter out random noise in pre-training data for\n  language models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we really have to filter out random noise in pre-training data for\n  language models?"
                },
                "summary": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Jinghan Ru"
                    },
                    {
                        "name": "Yuxin Xie"
                    },
                    {
                        "name": "Xianwei Zhuang"
                    },
                    {
                        "name": "Yuguo Yin"
                    },
                    {
                        "name": "Yuexian Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Zou"
                },
                "author": "Yuexian Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06589v1",
                "updated": "2025-02-10T15:54:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    54,
                    34,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:54:34Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    54,
                    34,
                    0,
                    41,
                    0
                ],
                "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training"
                },
                "summary": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Haoming Jiang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Kewei Cheng"
                    },
                    {
                        "name": "Sanket Lokegaonkar"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Qing Ping"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Binxuan Huang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Ruijie Wang"
                    },
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Nasser Zalmout"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Accepted to NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05212v2",
                "updated": "2025-02-10T15:42:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    42,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2024-08-10T05:41:19Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    5,
                    41,
                    19,
                    5,
                    223,
                    0
                ],
                "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions"
                },
                "summary": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks."
                },
                "authors": [
                    {
                        "name": "Michele Miranda"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    },
                    {
                        "name": "Sbastien Bratires"
                    },
                    {
                        "name": "Emanuele Rodol"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Rodol"
                },
                "author": "Emanuele Rodol",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)\n  https://openreview.net/forum?id=Ss9MTTN7OL",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06572v1",
                "updated": "2025-02-10T15:40:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    40,
                    35,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:40:35Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    40,
                    35,
                    0,
                    41,
                    0
                ],
                "title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM"
                },
                "summary": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 ."
                },
                "authors": [
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Kun-Yang Yu"
                    },
                    {
                        "name": "Shi-Yu Tian"
                    },
                    {
                        "name": "Jiang-Xin Shi"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Pengxiao Song"
                    },
                    {
                        "name": "Yi-Xuan Jin"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Yu-Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Li"
                },
                "author": "Yu-Feng Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06563v1",
                "updated": "2025-02-10T15:31:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    31,
                    54,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:31:54Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    31,
                    54,
                    0,
                    41,
                    0
                ],
                "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation"
                },
                "summary": "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen"
                },
                "authors": [
                    {
                        "name": "Chengwen Qi"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Jinwang Wu"
                    },
                    {
                        "name": "Yuanjun Laili"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18280v2",
                "updated": "2025-02-10T15:27:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    27,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-30T11:37:40Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    11,
                    37,
                    40,
                    3,
                    30,
                    0
                ],
                "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models"
                },
                "summary": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner."
                },
                "authors": [
                    {
                        "name": "Haoyu Liang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yunfeng Cai"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05506v2",
                "updated": "2025-02-10T15:26:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    26,
                    28,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-07T02:34:30Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    2,
                    34,
                    30,
                    5,
                    342,
                    0
                ],
                "title": "Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation"
                },
                "summary": "We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains."
                },
                "authors": [
                    {
                        "name": "Zebin Wang"
                    },
                    {
                        "name": "Yi Han"
                    },
                    {
                        "name": "Ethan X. Fang"
                    },
                    {
                        "name": "Lan Wang"
                    },
                    {
                        "name": "Junwei Lu"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Lu"
                },
                "author": "Junwei Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06560v1",
                "updated": "2025-02-10T15:25:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    25,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation"
                },
                "summary": "The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models."
                },
                "authors": [
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v1",
                "updated": "2025-02-10T15:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level Unit Test Generation Benchmark and Impact\n  of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level Unit Test Generation Benchmark and Impact\n  of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06555v1",
                "updated": "2025-02-10T15:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    23,
                    52,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    23,
                    52,
                    0,
                    41,
                    0
                ],
                "title": "Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?"
                },
                "summary": "Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application."
                },
                "authors": [
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Ryan McKenna"
                    },
                    {
                        "name": "Edo Roth"
                    },
                    {
                        "name": "Albert Cheu"
                    },
                    {
                        "name": "Peter Kairouz"
                    }
                ],
                "author_detail": {
                    "name": "Peter Kairouz"
                },
                "author": "Peter Kairouz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06551v1",
                "updated": "2025-02-10T15:19:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    19,
                    22,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T15:19:22Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    19,
                    22,
                    0,
                    41,
                    0
                ],
                "title": "Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments"
                },
                "summary": "This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance."
                },
                "authors": [
                    {
                        "name": "Marc Felix Brinner"
                    },
                    {
                        "name": "Sina Zarrie"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarrie"
                },
                "author": "Sina Zarrie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07507v2",
                "updated": "2025-02-10T15:09:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    9,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-10T00:47:59Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    0,
                    47,
                    59,
                    3,
                    284,
                    0
                ],
                "title": "Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)"
                },
                "summary": "Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing."
                },
                "authors": [
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Shreya Shukla"
                    },
                    {
                        "name": "Jose Torres"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Shounak Roychowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shounak Roychowdhury"
                },
                "author": "Shounak Roychowdhury",
                "arxiv_comment": "Accepted to Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v4",
                "updated": "2025-02-10T15:08:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    8,
                    20,
                    0,
                    41,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doruz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "Accepted to NAACL 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v4",
                "updated": "2025-02-10T15:08:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    8,
                    7,
                    0,
                    41,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20973v2",
                "updated": "2025-02-10T15:04:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    4,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-05-31T16:21:05Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    21,
                    5,
                    4,
                    152,
                    0
                ],
                "title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCQ: Low-Rank Codebook based Quantization for Large Language Models"
                },
                "summary": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost."
                },
                "authors": [
                    {
                        "name": "Wen-Pu Cai"
                    },
                    {
                        "name": "Ming-Yang Li"
                    },
                    {
                        "name": "Wu-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Wu-Jun Li"
                },
                "author": "Wu-Jun Li",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06533v1",
                "updated": "2025-02-10T14:56:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    56,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:56:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    56,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning"
                },
                "summary": "The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage."
                },
                "authors": [
                    {
                        "name": "Jean Vassoyan"
                    },
                    {
                        "name": "Nathanal Beau"
                    },
                    {
                        "name": "Roman Plaud"
                    }
                ],
                "author_detail": {
                    "name": "Roman Plaud"
                },
                "author": "Roman Plaud",
                "arxiv_comment": "11 pages, 6 figures, 5 tables. Accepted for publication in the\n  Findings of the North American Chapter of the Association for Computational\n  Linguistics (NAACL) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03000v2",
                "updated": "2025-02-10T14:34:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    34,
                    4,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-05T08:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    52,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "Armadillo: An Efficient Framework for Numerical Linear Algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Armadillo: An Efficient Framework for Numerical Linear Algebra"
                },
                "summary": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions."
                },
                "authors": [
                    {
                        "name": "Conrad Sanderson"
                    },
                    {
                        "name": "Ryan Curtin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Curtin"
                },
                "author": "Ryan Curtin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99, 65Y04, 65Y15, 65F45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; G.1.3; D.2.3; F.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14399v2",
                "updated": "2025-02-10T14:11:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    58,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-18T12:02:41Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "title": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning"
                },
                "summary": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo Carvalho"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06494v1",
                "updated": "2025-02-10T14:11:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    32,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:11:32Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    11,
                    32,
                    0,
                    41,
                    0
                ],
                "title": "GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing"
                },
                "summary": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Zhuoxuan Zhang"
                    },
                    {
                        "name": "Eunhye Ko"
                    },
                    {
                        "name": "Lily Boddy"
                    },
                    {
                        "name": "Chenan Wang"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Alexander Rasgon"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Min Kyung Lee"
                    },
                    {
                        "name": "Chenxi Yuan"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Ying Ding"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "31 pages; the first three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14596v2",
                "updated": "2025-02-10T14:09:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    9,
                    46,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-18T16:49:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Balance Resisting and Accepting Persuasion"
                },
                "summary": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up."
                },
                "authors": [
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NAACL Camera-Ready. Code:\n  https://github.com/esteng/persuasion_balanced_training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06490v1",
                "updated": "2025-02-10T14:08:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T14:08:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Recent Advances in Discrete Speech Tokens: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Discrete Speech Tokens: A Review"
                },
                "summary": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens."
                },
                "authors": [
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Zhihan Li"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Chongtian Shao"
                    },
                    {
                        "name": "Hanglei Zhang"
                    },
                    {
                        "name": "Chenpeng Du"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "26 pages, 8 figures, 3 tables. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03793v2",
                "updated": "2025-02-10T14:08:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    14,
                    8,
                    19,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-06T05:47:37Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    5,
                    47,
                    37,
                    3,
                    37,
                    0
                ],
                "title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers"
                },
                "summary": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements."
                },
                "authors": [
                    {
                        "name": "Benjamin Clavi"
                    },
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Benjamin Warner"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Warner"
                },
                "author": "Benjamin Warner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06802v2",
                "updated": "2025-02-10T13:55:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    55,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-12T12:52:52Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    52,
                    52,
                    6,
                    12,
                    0
                ],
                "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional\n  Kolmogorov Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Two Types of Scaling Laws from the Perspective of Conditional\n  Kolmogorov Complexity"
                },
                "summary": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the\nrelationships between model loss and the scale of parameters, data, and\ntraining computation. In 2024, OpenAI proposed the second type of Scaling Laws,\ndescribing the relationship between model inference performance and inference\ncomputation. In this paper, we analyze LLMs training and inference processes\nfrom the perspective of lossless compression using conditional Kolmogorov\ncomplexity, and unify these two types of Scaling Laws. We find that both types\nof Scaling Laws improve approximation of conditional Kolmogorov complexity by\nincreasing execution steps of Turing machine. The first type of Scaling Laws\nincreases execution steps by increasing number of model parameters. The second\ntype of Scaling Laws increases execution steps by increasing the number of\nintermediate tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the\nrelationships between model loss and the scale of parameters, data, and\ntraining computation. In 2024, OpenAI proposed the second type of Scaling Laws,\ndescribing the relationship between model inference performance and inference\ncomputation. In this paper, we analyze LLMs training and inference processes\nfrom the perspective of lossless compression using conditional Kolmogorov\ncomplexity, and unify these two types of Scaling Laws. We find that both types\nof Scaling Laws improve approximation of conditional Kolmogorov complexity by\nincreasing execution steps of Turing machine. The first type of Scaling Laws\nincreases execution steps by increasing number of model parameters. The second\ntype of Scaling Laws increases execution steps by increasing the number of\nintermediate tokens."
                },
                "authors": [
                    {
                        "name": "Jun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wan"
                },
                "author": "Jun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04692v2",
                "updated": "2025-02-10T13:52:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    52,
                    51,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-07T06:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "title": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion"
                },
                "summary": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research."
                },
                "authors": [
                    {
                        "name": "Zhenwei Wu"
                    },
                    {
                        "name": "Jinxiong Lu"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06472v1",
                "updated": "2025-02-10T13:51:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    51,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:51:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    51,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment"
                },
                "summary": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments."
                },
                "authors": [
                    {
                        "name": "Yuxing Lu"
                    },
                    {
                        "name": "Jinzhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhuo Wang"
                },
                "author": "Jinzhuo Wang",
                "arxiv_comment": "24 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06470v1",
                "updated": "2025-02-10T13:50:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    50,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:50:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    50,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks"
                },
                "summary": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks."
                },
                "authors": [
                    {
                        "name": "Hieu Minh \"Jord\" Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Minh \"Jord\" Nguyen"
                },
                "author": "Hieu Minh \"Jord\" Nguyen",
                "arxiv_comment": "Advancing Artificial Intelligence through Theory of Mind Workshop,\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06453v1",
                "updated": "2025-02-10T13:31:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    31,
                    46,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:31:46Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    31,
                    46,
                    0,
                    41,
                    0
                ],
                "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations"
                },
                "summary": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels."
                },
                "authors": [
                    {
                        "name": "Kaixuan Huang"
                    },
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Xiang Ji"
                    },
                    {
                        "name": "Jiawei Ge"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Yingqing Guo"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Hui Yuan"
                    },
                    {
                        "name": "Runzhe Wang"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Chi Jin"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17428v2",
                "updated": "2025-02-10T13:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    28,
                    1,
                    0,
                    41,
                    0
                ],
                "published": "2024-03-26T06:50:04Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    6,
                    50,
                    4,
                    1,
                    86,
                    0
                ],
                "title": "Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study"
                },
                "summary": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews."
                },
                "authors": [
                    {
                        "name": "Jae-hee So"
                    },
                    {
                        "name": "Joonhwan Chang"
                    },
                    {
                        "name": "Eunji Kim"
                    },
                    {
                        "name": "Junho Na"
                    },
                    {
                        "name": "JiYeon Choi"
                    },
                    {
                        "name": "Jy-yong Sohn"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    },
                    {
                        "name": "Sang Hui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Sang Hui Chu"
                },
                "author": "Sang Hui Chu",
                "arxiv_doi": "10.2196/58418",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/58418",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "JMIR Form Res 2024;8:e58418",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06440v1",
                "updated": "2025-02-10T13:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    17,
                    34,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    17,
                    34,
                    0,
                    41,
                    0
                ],
                "title": "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding"
                },
                "summary": "The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest\nand collision-free paths for multiple agents in a known, potentially\nobstacle-ridden environment. It is the core challenge for robotic deployments\nin large-scale logistics and transportation. Decentralized learning-based\napproaches have shown great potential for addressing the MAPF problems,\noffering more reactive and scalable solutions. However, existing learning-based\nMAPF methods usually rely on agents making decisions based on a limited field\nof view (FOV), resulting in short-sighted policies and inefficient cooperation\nin complex scenarios. There, a critical challenge is to achieve consensus on\npotential movements between agents based on limited observations and\ncommunications. To tackle this challenge, we introduce a new framework that\napplies sheaf theory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each other through local\nconsensus and utilize them for tightly cooperative decision-making. In\nparticular, sheaf theory provides a mathematical proof of conditions for\nachieving global consensus through local observation. Inspired by this, we\nincorporate a neural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised learning.\nDuring the task, in addition to normal features for MAPF as in previous works,\neach agent distributedly reasons about a learned consensus feature, leading to\nefficient cooperation on pathfinding and collision avoidance. As a result, our\nproposed method demonstrates significant improvements over state-of-the-art\nlearning-based MAPF planners, especially in relatively large and complex\nscenarios, demonstrating its superiority over baselines in various simulations\nand real-world robot experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest\nand collision-free paths for multiple agents in a known, potentially\nobstacle-ridden environment. It is the core challenge for robotic deployments\nin large-scale logistics and transportation. Decentralized learning-based\napproaches have shown great potential for addressing the MAPF problems,\noffering more reactive and scalable solutions. However, existing learning-based\nMAPF methods usually rely on agents making decisions based on a limited field\nof view (FOV), resulting in short-sighted policies and inefficient cooperation\nin complex scenarios. There, a critical challenge is to achieve consensus on\npotential movements between agents based on limited observations and\ncommunications. To tackle this challenge, we introduce a new framework that\napplies sheaf theory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each other through local\nconsensus and utilize them for tightly cooperative decision-making. In\nparticular, sheaf theory provides a mathematical proof of conditions for\nachieving global consensus through local observation. Inspired by this, we\nincorporate a neural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised learning.\nDuring the task, in addition to normal features for MAPF as in previous works,\neach agent distributedly reasons about a learned consensus feature, leading to\nefficient cooperation on pathfinding and collision avoidance. As a result, our\nproposed method demonstrates significant improvements over state-of-the-art\nlearning-based MAPF planners, especially in relatively large and complex\nscenarios, demonstrating its superiority over baselines in various simulations\nand real-world robot experiments."
                },
                "authors": [
                    {
                        "name": "Shuhao Liao"
                    },
                    {
                        "name": "Weihang Xia"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Weiheng Dai"
                    },
                    {
                        "name": "Chengyang He"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "arxiv_comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Robotics and Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06425v1",
                "updated": "2025-02-10T13:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    2,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    2,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Hiroki Watanabe"
                    },
                    {
                        "name": "Motonobu Uchikoshi"
                    }
                ],
                "author_detail": {
                    "name": "Motonobu Uchikoshi"
                },
                "author": "Motonobu Uchikoshi",
                "arxiv_doi": "10.1145/3701716.3715597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06424v1",
                "updated": "2025-02-10T13:00:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    0,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T13:00:49Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    0,
                    49,
                    0,
                    41,
                    0
                ],
                "title": "CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better\n  Interpretability of Intelligent Fault Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better\n  Interpretability of Intelligent Fault Diagnosis"
                },
                "summary": "Neural networks (NNs), with their powerful nonlinear mapping and end-to-end\ncapabilities, are widely applied in mechanical intelligent fault diagnosis\n(IFD). However, as typical black-box models, they pose challenges in\nunderstanding their decision basis and logic, limiting their deployment in\nhigh-reliability scenarios. Hence, various methods have been proposed to\nenhance the interpretability of IFD. Among these, post-hoc approaches can\nprovide explanations without changing model architecture, preserving its\nflexibility and scalability. However, existing post-hoc methods often suffer\nfrom limitations in explanation forms. They either require preprocessing that\ndisrupts the end-to-end nature or overlook fault mechanisms, leading to\nsuboptimal explanations. To address these issues, we derived the\ncyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley\nadditive explanations (SHAP) to the CS domain. CS-SHAP can evaluate\ncontributions from both carrier and modulation frequencies, aligning more\nclosely with fault mechanisms and delivering clearer and more accurate\nexplanations. Three datasets are utilized to validate the superior\ninterpretability of CS-SHAP, ensuring its correctness, reproducibility, and\npractical performance. With open-source code and outstanding interpretability,\nCS-SHAP has the potential to be widely adopted and become the post-hoc\ninterpretability benchmark in IFD, even in other classification tasks. The code\nis available on https://github.com/ChenQian0618/CS-SHAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks (NNs), with their powerful nonlinear mapping and end-to-end\ncapabilities, are widely applied in mechanical intelligent fault diagnosis\n(IFD). However, as typical black-box models, they pose challenges in\nunderstanding their decision basis and logic, limiting their deployment in\nhigh-reliability scenarios. Hence, various methods have been proposed to\nenhance the interpretability of IFD. Among these, post-hoc approaches can\nprovide explanations without changing model architecture, preserving its\nflexibility and scalability. However, existing post-hoc methods often suffer\nfrom limitations in explanation forms. They either require preprocessing that\ndisrupts the end-to-end nature or overlook fault mechanisms, leading to\nsuboptimal explanations. To address these issues, we derived the\ncyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley\nadditive explanations (SHAP) to the CS domain. CS-SHAP can evaluate\ncontributions from both carrier and modulation frequencies, aligning more\nclosely with fault mechanisms and delivering clearer and more accurate\nexplanations. Three datasets are utilized to validate the superior\ninterpretability of CS-SHAP, ensuring its correctness, reproducibility, and\npractical performance. With open-source code and outstanding interpretability,\nCS-SHAP has the potential to be widely adopted and become the post-hoc\ninterpretability benchmark in IFD, even in other classification tasks. The code\nis available on https://github.com/ChenQian0618/CS-SHAP."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xingjian Dong"
                    },
                    {
                        "name": "Kui Hu"
                    },
                    {
                        "name": "Kangkang Chen"
                    },
                    {
                        "name": "Zhike Peng"
                    },
                    {
                        "name": "Guang Meng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Meng"
                },
                "author": "Guang Meng",
                "arxiv_comment": "21 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06419v1",
                "updated": "2025-02-10T12:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    55,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    55,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have made substantial advancements in the field\nof robotic and autonomous driving. This study presents the first\nOccupancy-based Large Language Model (Occ-LLM), which represents a pioneering\neffort to integrate LLMs with an important representation. To effectively\nencode occupancy as input for the LLM and address the category imbalances\nassociated with occupancy, we propose Motion Separation Variational Autoencoder\n(MS-VAE). This innovative approach utilizes prior knowledge to distinguish\ndynamic objects from static scenes before inputting them into a tailored\nVariational Autoencoder (VAE). This separation enhances the model's capacity to\nconcentrate on dynamic trajectories while effectively reconstructing static\nscenes. The efficacy of Occ-LLM has been validated across key tasks, including\n4D occupancy forecasting, self-ego planning, and occupancy-based scene question\nanswering. Comprehensive evaluations demonstrate that Occ-LLM significantly\nsurpasses existing state-of-the-art methodologies, achieving gains of about 6\\%\nin Intersection over Union (IoU) and 4\\% in mean Intersection over Union (mIoU)\nfor the task of 4D occupancy forecasting. These findings highlight the\ntransformative potential of Occ-LLM in reshaping current paradigms within\nrobotic and autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made substantial advancements in the field\nof robotic and autonomous driving. This study presents the first\nOccupancy-based Large Language Model (Occ-LLM), which represents a pioneering\neffort to integrate LLMs with an important representation. To effectively\nencode occupancy as input for the LLM and address the category imbalances\nassociated with occupancy, we propose Motion Separation Variational Autoencoder\n(MS-VAE). This innovative approach utilizes prior knowledge to distinguish\ndynamic objects from static scenes before inputting them into a tailored\nVariational Autoencoder (VAE). This separation enhances the model's capacity to\nconcentrate on dynamic trajectories while effectively reconstructing static\nscenes. The efficacy of Occ-LLM has been validated across key tasks, including\n4D occupancy forecasting, self-ego planning, and occupancy-based scene question\nanswering. Comprehensive evaluations demonstrate that Occ-LLM significantly\nsurpasses existing state-of-the-art methodologies, achieving gains of about 6\\%\nin Intersection over Union (IoU) and 4\\% in mean Intersection over Union (mIoU)\nfor the task of 4D occupancy forecasting. These findings highlight the\ntransformative potential of Occ-LLM in reshaping current paradigms within\nrobotic and autonomous driving."
                },
                "authors": [
                    {
                        "name": "Tianshuo Xu"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Yingjie Cai"
                    },
                    {
                        "name": "Bingbing Liu"
                    },
                    {
                        "name": "Yingcong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yingcong Chen"
                },
                "author": "Yingcong Chen",
                "arxiv_comment": "Accepted in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06415v1",
                "updated": "2025-02-10T12:54:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    54,
                    17,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:54:17Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    54,
                    17,
                    0,
                    41,
                    0
                ],
                "title": "Systematic Outliers in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Outliers in Large Language Models"
                },
                "summary": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers."
                },
                "authors": [
                    {
                        "name": "Yongqi An"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06407v1",
                "updated": "2025-02-10T12:47:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    47,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:47:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    47,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "An Automated Machine Learning Framework for Surgical Suturing Action\n  Detection under Class Imbalance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automated Machine Learning Framework for Surgical Suturing Action\n  Detection under Class Imbalance"
                },
                "summary": "In laparoscopy surgical training and evaluation, real-time detection of\nsurgical actions with interpretable outputs is crucial for automated and\nreal-time instructional feedback and skill development. Such capability would\nenable development of machine guided training systems. This paper presents a\nrapid deployment approach utilizing automated machine learning methods, based\non surgical action data collected from both experienced and trainee surgeons.\nThe proposed approach effectively tackles the challenge of highly imbalanced\nclass distributions, ensuring robust predictions across varying skill levels of\nsurgeons. Additionally, our method partially incorporates model transparency,\naddressing the reliability requirements in medical applications. Compared to\ndeep learning approaches, traditional machine learning models not only\nfacilitate efficient rapid deployment but also offer significant advantages in\ninterpretability. Through experiments, this study demonstrates the potential of\nthis approach to provide quick, reliable and effective real-time detection in\nsurgical training environments",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In laparoscopy surgical training and evaluation, real-time detection of\nsurgical actions with interpretable outputs is crucial for automated and\nreal-time instructional feedback and skill development. Such capability would\nenable development of machine guided training systems. This paper presents a\nrapid deployment approach utilizing automated machine learning methods, based\non surgical action data collected from both experienced and trainee surgeons.\nThe proposed approach effectively tackles the challenge of highly imbalanced\nclass distributions, ensuring robust predictions across varying skill levels of\nsurgeons. Additionally, our method partially incorporates model transparency,\naddressing the reliability requirements in medical applications. Compared to\ndeep learning approaches, traditional machine learning models not only\nfacilitate efficient rapid deployment but also offer significant advantages in\ninterpretability. Through experiments, this study demonstrates the potential of\nthis approach to provide quick, reliable and effective real-time detection in\nsurgical training environments"
                },
                "authors": [
                    {
                        "name": "Baobing Zhang"
                    },
                    {
                        "name": "Paul Sullivan"
                    },
                    {
                        "name": "Benjie Tang"
                    },
                    {
                        "name": "Ghulam Nabi"
                    },
                    {
                        "name": "Mustafa Suphi Erden"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa Suphi Erden"
                },
                "author": "Mustafa Suphi Erden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02765v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02765v3",
                "updated": "2025-02-10T12:35:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    35,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-05-04T22:02:24Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    22,
                    2,
                    24,
                    5,
                    125,
                    0
                ],
                "title": "Has this Fact been Edited? Detecting Knowledge Edits in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Has this Fact been Edited? Detecting Knowledge Edits in Language Models"
                },
                "summary": "Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Jrg Schltterer"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Schltterer"
                },
                "author": "Jrg Schltterer",
                "arxiv_comment": "Accepted at NAACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02765v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02765v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v5",
                "updated": "2025-02-10T12:35:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    35,
                    22,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Section 1.2 (Harnessing Multi-Agent Potential in CAS with OpenAI\n  SWARM) Renamed. Section 5.3 (Future Directions) Updated. Minor typo\n  corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06395v1",
                "updated": "2025-02-10T12:32:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    32,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:32:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    32,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "AppVLM: A Lightweight Vision Language Model for Online App Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AppVLM: A Lightweight Vision Language Model for Online App Control"
                },
                "summary": "The utilisation of foundation models as smartphone assistants, termed app\nagents, is a critical research challenge. These agents aim to execute human\ninstructions on smartphones by interpreting textual instructions and performing\nactions via the device's interface. While promising, current approaches face\nsignificant limitations. Methods that use large proprietary models, such as\nGPT-4o, are computationally expensive, while those that use smaller fine-tuned\nmodels often lack adaptability to out-of-distribution tasks. In this work, we\nintroduce AppVLM, a lightweight Vision-Language Model (VLM). First, we\nfine-tune it offline on the AndroidControl dataset. Then, we refine its policy\nby collecting data from the AndroidWorld environment and performing further\ntraining iterations. Our results indicate that AppVLM achieves the highest\naction prediction accuracy in offline evaluation on the AndroidControl dataset,\ncompared to all evaluated baselines, and matches GPT-4o in online task\ncompletion success rate in the AndroidWorld environment, while being up to ten\ntimes faster. This makes AppVLM a practical and efficient solution for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utilisation of foundation models as smartphone assistants, termed app\nagents, is a critical research challenge. These agents aim to execute human\ninstructions on smartphones by interpreting textual instructions and performing\nactions via the device's interface. While promising, current approaches face\nsignificant limitations. Methods that use large proprietary models, such as\nGPT-4o, are computationally expensive, while those that use smaller fine-tuned\nmodels often lack adaptability to out-of-distribution tasks. In this work, we\nintroduce AppVLM, a lightweight Vision-Language Model (VLM). First, we\nfine-tune it offline on the AndroidControl dataset. Then, we refine its policy\nby collecting data from the AndroidWorld environment and performing further\ntraining iterations. Our results indicate that AppVLM achieves the highest\naction prediction accuracy in offline evaluation on the AndroidControl dataset,\ncompared to all evaluated baselines, and matches GPT-4o in online task\ncompletion success rate in the AndroidWorld environment, while being up to ten\ntimes faster. This makes AppVLM a practical and efficient solution for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Georgios Papoudakis"
                    },
                    {
                        "name": "Thomas Coste"
                    },
                    {
                        "name": "Zhihao Wu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06394v1",
                "updated": "2025-02-10T12:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    30,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    30,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators"
                },
                "summary": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification."
                },
                "authors": [
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Nikita Sushko"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06390v1",
                "updated": "2025-02-10T12:20:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    20,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:20:08Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    20,
                    8,
                    0,
                    41,
                    0
                ],
                "title": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks\n  for VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks\n  for VLMs"
                },
                "summary": "Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List."
                },
                "authors": [
                    {
                        "name": "Aobotao Dai"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Lin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Wang"
                },
                "author": "Lin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06387v1",
                "updated": "2025-02-10T12:15:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    15,
                    27,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T12:15:27Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    15,
                    27,
                    0,
                    41,
                    0
                ],
                "title": "How Humans Help LLMs: Assessing and Incentivizing Human Preference\n  Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Humans Help LLMs: Assessing and Incentivizing Human Preference\n  Annotators"
                },
                "summary": "Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Hanzhao Wang"
                    },
                    {
                        "name": "Zhongyao Ma"
                    },
                    {
                        "name": "Xiaocheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocheng Li"
                },
                "author": "Xiaocheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01523v2",
                "updated": "2025-02-10T12:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    12,
                    0,
                    50,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-02T14:16:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    16,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism"
                },
                "summary": "Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity. In this paper, we show that employing a\nsingle, static strategy results in inefficiency and resource under-utilization,\nhighlighting the need for adaptive approaches to handle the heterogeneous\nworkloads across sequences. To address this, we propose a\nheterogeneity-adaptive sequence parallelism method. For each training step, our\napproach captures the variability in sequence lengths and assigns the optimal\ncombination of scattering strategies based on workload characteristics. We\nmodel this problem as a linear programming optimization and design an efficient\nand effective solver to find the optimal solution. Furthermore, we implement\nour method in a high-performance system that supports adaptive parallelization\nin distributed LLM training. Experimental results demonstrate that our system\noutperforms state-of-the-art training frameworks by up to 1.98x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity. In this paper, we show that employing a\nsingle, static strategy results in inefficiency and resource under-utilization,\nhighlighting the need for adaptive approaches to handle the heterogeneous\nworkloads across sequences. To address this, we propose a\nheterogeneity-adaptive sequence parallelism method. For each training step, our\napproach captures the variability in sequence lengths and assigns the optimal\ncombination of scattering strategies based on workload characteristics. We\nmodel this problem as a linear programming optimization and design an efficient\nand effective solver to find the optimal solution. Furthermore, we implement\nour method in a high-performance system that supports adaptive parallelization\nin distributed LLM training. Experimental results demonstrate that our system\noutperforms state-of-the-art training frameworks by up to 1.98x."
                },
                "authors": [
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Shiju Wang"
                    },
                    {
                        "name": "Shenhan Zhu"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Faming Wu"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06371v1",
                "updated": "2025-02-10T11:40:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    40,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T11:40:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    40,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Simulation as Reality? The Effectiveness of LLM-Generated Data in\n  Open-ended Question Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation as Reality? The Effectiveness of LLM-Generated Data in\n  Open-ended Question Assessment"
                },
                "summary": "The advancement of Artificial Intelligence (AI) has created opportunities for\ne-learning, particularly in automated assessment systems that reduce educators'\nworkload and provide timely feedback to students. However, developing effective\nAI-based assessment tools remains challenging due to the substantial resources\nrequired for collecting and annotating real student data. This study\ninvestigates the potential and gap of simulative data to address this\nlimitation. Through a two-phase experimental study, we examined the\neffectiveness and gap of Large Language Model generated synthetic data in\ntraining educational assessment systems. Our findings reveal that while\nsimulative data demonstrates promising results in training automated assessment\nmodels, outperforming state-of-the-art GPT-4o in most question types, its\neffectiveness has notable limitations. Specifically, models trained on\nsynthetic data show excellent performance in simulated environment but need\nprogress when applied to real-world scenarios. This performance gap highlights\nthe limitations of only using synthetic data in controlled experimental\nsettings for AI training. The absence of real-world noise and biases, which are\nalso present in over-processed real-world data, contributes to this limitation.\nWe recommend that future development of automated assessment agents and other\nAI tools should incorporate a mixture of synthetic and real-world data, or\nintroduce more realistic noise and biases patterns, rather than relying solely\non synthetic or over-processed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Artificial Intelligence (AI) has created opportunities for\ne-learning, particularly in automated assessment systems that reduce educators'\nworkload and provide timely feedback to students. However, developing effective\nAI-based assessment tools remains challenging due to the substantial resources\nrequired for collecting and annotating real student data. This study\ninvestigates the potential and gap of simulative data to address this\nlimitation. Through a two-phase experimental study, we examined the\neffectiveness and gap of Large Language Model generated synthetic data in\ntraining educational assessment systems. Our findings reveal that while\nsimulative data demonstrates promising results in training automated assessment\nmodels, outperforming state-of-the-art GPT-4o in most question types, its\neffectiveness has notable limitations. Specifically, models trained on\nsynthetic data show excellent performance in simulated environment but need\nprogress when applied to real-world scenarios. This performance gap highlights\nthe limitations of only using synthetic data in controlled experimental\nsettings for AI training. The absence of real-world noise and biases, which are\nalso present in over-processed real-world data, contributes to this limitation.\nWe recommend that future development of automated assessment agents and other\nAI tools should incorporate a mixture of synthetic and real-world data, or\nintroduce more realistic noise and biases patterns, rather than relying solely\non synthetic or over-processed data."
                },
                "authors": [
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Wei Lin Wang"
                    },
                    {
                        "name": "Yu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yu Luo"
                },
                "author": "Yu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v3",
                "updated": "2025-02-10T11:35:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    35,
                    28,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT: Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT: Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation quality in each round. In this way,\nwe collect tens of thousands of long-thought MT data, which is used to train\nour DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the\nthought process during machine translation, and outperform vanilla LLMs as well\nas LLMs which are simply fine-tuning on the paired sentences without long\nthought, showing its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation quality in each round. In this way,\nwe collect tens of thousands of long-thought MT data, which is used to train\nour DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the\nthought process during machine translation, and outperform vanilla LLMs as well\nas LLMs which are simply fine-tuning on the paired sentences without long\nthought, showing its effectiveness."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06355v1",
                "updated": "2025-02-10T11:10:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    10,
                    41,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T11:10:41Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    10,
                    41,
                    0,
                    41,
                    0
                ],
                "title": "Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning\n  Approach"
                },
                "summary": "Multimodal transformers integrate diverse data types like images, audio, and\ntext, advancing tasks such as audio-visual understanding and image-text\nretrieval; yet their high parameterization limits deployment on\nresource-constrained edge devices. Split Learning (SL), which partitions models\nat a designated cut-layer to offload compute-intensive operations to the\nserver, offers a promising approach for distributed training of multimodal\ntransformers, though its application remains underexplored. We present MPSL, a\nparallel SL approach for computational efficient fine-tuning of multimodal\ntransformers in a distributed manner, while eliminating label sharing, client\nsynchronization, and per-client sub-model management. MPSL employs lightweight\nclient-side tokenizers and a unified modality-agnostic encoder, allowing\nflexible adaptation to task-specific needs. Our evaluation across 7 multimodal\ndatasets demonstrates that MPSL matches or outperforms Federated Learning,\nreduces client-side computations by 250x, and achieves superior scalability in\ncommunication cost with model growth. Through extensive analysis, we highlight\ntask suitability, trade-offs, and scenarios where MPSL excels, inspiring\nfurther exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal transformers integrate diverse data types like images, audio, and\ntext, advancing tasks such as audio-visual understanding and image-text\nretrieval; yet their high parameterization limits deployment on\nresource-constrained edge devices. Split Learning (SL), which partitions models\nat a designated cut-layer to offload compute-intensive operations to the\nserver, offers a promising approach for distributed training of multimodal\ntransformers, though its application remains underexplored. We present MPSL, a\nparallel SL approach for computational efficient fine-tuning of multimodal\ntransformers in a distributed manner, while eliminating label sharing, client\nsynchronization, and per-client sub-model management. MPSL employs lightweight\nclient-side tokenizers and a unified modality-agnostic encoder, allowing\nflexible adaptation to task-specific needs. Our evaluation across 7 multimodal\ndatasets demonstrates that MPSL matches or outperforms Federated Learning,\nreduces client-side computations by 250x, and achieves superior scalability in\ncommunication cost with model growth. Through extensive analysis, we highlight\ntask suitability, trade-offs, and scenarios where MPSL excels, inspiring\nfurther exploration."
                },
                "authors": [
                    {
                        "name": "Timo Fudala"
                    },
                    {
                        "name": "Vasileios Tsouvalas"
                    },
                    {
                        "name": "Nirvana Meratnia"
                    }
                ],
                "author_detail": {
                    "name": "Nirvana Meratnia"
                },
                "author": "Nirvana Meratnia",
                "arxiv_comment": "10 pages, 4 figures, submitted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12586v2",
                "updated": "2025-02-10T11:04:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    4,
                    12,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-16T14:04:26Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    4,
                    26,
                    2,
                    290,
                    0
                ],
                "title": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits"
                },
                "summary": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_comment": "Accepted at NAACL Main 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06351v1",
                "updated": "2025-02-10T11:00:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    0,
                    24,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T11:00:24Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    11,
                    0,
                    24,
                    0,
                    41,
                    0
                ],
                "title": "Calibrating LLMs with Information-Theoretic Evidential Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating LLMs with Information-Theoretic Evidential Deep Learning"
                },
                "summary": "Fine-tuned large language models (LLMs) often exhibit overconfidence,\nparticularly when trained on small datasets, resulting in poor calibration and\ninaccurate uncertainty estimates. Evidential Deep Learning (EDL), an\nuncertainty-aware approach, enables uncertainty estimation in a single forward\npass, making it a promising method for calibrating fine-tuned LLMs. However,\ndespite its computational efficiency, EDL is prone to overfitting, as its\ntraining objective can result in overly concentrated probability distributions.\nTo mitigate this, we propose regularizing EDL by incorporating an information\nbottleneck (IB). Our approach IB-EDL suppresses spurious information in the\nevidence generated by the model and encourages truly predictive information to\ninfluence both the predictions and uncertainty estimates. Extensive experiments\nacross various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms\nboth existing EDL and non-EDL approaches. By improving the trustworthiness of\nLLMs, IB-EDL facilitates their broader adoption in domains requiring high\nlevels of confidence calibration. Code is available at\nhttps://github.com/sandylaker/ib-edl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuned large language models (LLMs) often exhibit overconfidence,\nparticularly when trained on small datasets, resulting in poor calibration and\ninaccurate uncertainty estimates. Evidential Deep Learning (EDL), an\nuncertainty-aware approach, enables uncertainty estimation in a single forward\npass, making it a promising method for calibrating fine-tuned LLMs. However,\ndespite its computational efficiency, EDL is prone to overfitting, as its\ntraining objective can result in overly concentrated probability distributions.\nTo mitigate this, we propose regularizing EDL by incorporating an information\nbottleneck (IB). Our approach IB-EDL suppresses spurious information in the\nevidence generated by the model and encourages truly predictive information to\ninfluence both the predictions and uncertainty estimates. Extensive experiments\nacross various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms\nboth existing EDL and non-EDL approaches. By improving the trustworthiness of\nLLMs, IB-EDL facilitates their broader adoption in domains requiring high\nlevels of confidence calibration. Code is available at\nhttps://github.com/sandylaker/ib-edl."
                },
                "authors": [
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "David Rgamer"
                    },
                    {
                        "name": "Bernd Bischl"
                    },
                    {
                        "name": "Mina Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Mina Rezaei"
                },
                "author": "Mina Rezaei",
                "arxiv_comment": "18 pages; 3 figures; accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06348v1",
                "updated": "2025-02-10T10:58:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    58,
                    9,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:58:09Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    58,
                    9,
                    0,
                    41,
                    0
                ],
                "title": "AiRacleX: Automated Detection of Price Oracle Manipulations via\n  LLM-Driven Knowledge Mining and Prompt Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AiRacleX: Automated Detection of Price Oracle Manipulations via\n  LLM-Driven Knowledge Mining and Prompt Generation"
                },
                "summary": "Decentralized finance applications depend on accurate price oracles to ensure\nsecure transactions, yet these oracles are highly vulnerable to manipulation,\nenabling attackers to exploit smart contract vulnerabilities for unfair asset\nvaluation and financial gain. Detecting such manipulations traditionally relies\non the manual effort of experienced experts, presenting significant challenges.\nIn this paper, we propose a novel LLM-driven framework that automates the\ndetection of price oracle manipulations by leveraging the complementary\nstrengths of different LLM models. Our approach begins with domain-specific\nknowledge extraction, where an LLM model synthesizes precise insights about\nprice oracle vulnerabilities from top-tier academic papers, eliminating the\nneed for profound expertise from developers or auditors. This knowledge forms\nthe foundation for a second LLM model to generate structured, context-aware\nchain of thought prompts, which guide a third LLM model in accurately\nidentifying manipulation patterns in smart contracts. We validate the framework\neffectiveness through experiments on 60 known vulnerabilities from 46\nreal-world DeFi attacks or projects spanning 2021 to 2023. The best performing\ncombination of LLMs (Haiku-Haiku-4o-mini) identified by AiRacleX demonstrate a\n2.58-times improvement in recall (0.667 vs 0.259) compared to the\nstate-of-the-art tool GPTScan, while maintaining comparable precision.\nFurthermore, our framework demonstrates the feasibility of replacing commercial\nmodels with open-source alternatives, enhancing privacy and security for\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized finance applications depend on accurate price oracles to ensure\nsecure transactions, yet these oracles are highly vulnerable to manipulation,\nenabling attackers to exploit smart contract vulnerabilities for unfair asset\nvaluation and financial gain. Detecting such manipulations traditionally relies\non the manual effort of experienced experts, presenting significant challenges.\nIn this paper, we propose a novel LLM-driven framework that automates the\ndetection of price oracle manipulations by leveraging the complementary\nstrengths of different LLM models. Our approach begins with domain-specific\nknowledge extraction, where an LLM model synthesizes precise insights about\nprice oracle vulnerabilities from top-tier academic papers, eliminating the\nneed for profound expertise from developers or auditors. This knowledge forms\nthe foundation for a second LLM model to generate structured, context-aware\nchain of thought prompts, which guide a third LLM model in accurately\nidentifying manipulation patterns in smart contracts. We validate the framework\neffectiveness through experiments on 60 known vulnerabilities from 46\nreal-world DeFi attacks or projects spanning 2021 to 2023. The best performing\ncombination of LLMs (Haiku-Haiku-4o-mini) identified by AiRacleX demonstrate a\n2.58-times improvement in recall (0.667 vs 0.259) compared to the\nstate-of-the-art tool GPTScan, while maintaining comparable precision.\nFurthermore, our framework demonstrates the feasibility of replacing commercial\nmodels with open-source alternatives, enhancing privacy and security for\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Qingsong Wei"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Rick Siow Mong Goh"
                    }
                ],
                "author_detail": {
                    "name": "Rick Siow Mong Goh"
                },
                "author": "Rick Siow Mong Goh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06062v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06062v4",
                "updated": "2025-02-10T10:55:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    55,
                    8,
                    0,
                    41,
                    0
                ],
                "published": "2024-10-08T14:09:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    14,
                    9,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs"
                },
                "summary": "We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org."
                },
                "authors": [
                    {
                        "name": "Vincent Emonet"
                    },
                    {
                        "name": "Jerven Bolleman"
                    },
                    {
                        "name": "Severine Duvaud"
                    },
                    {
                        "name": "Tarcisio Mendes de Farias"
                    },
                    {
                        "name": "Ana Claudia Sima"
                    }
                ],
                "author_detail": {
                    "name": "Ana Claudia Sima"
                },
                "author": "Ana Claudia Sima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06062v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06062v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06329v1",
                "updated": "2025-02-10T10:29:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    29,
                    28,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:29:28Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    29,
                    28,
                    0,
                    41,
                    0
                ],
                "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expect the Unexpected: FailSafe Long Context QA for Finance"
                },
                "summary": "We propose a new long-context financial benchmark, FailSafeQA, designed to\ntest the robustness and context-awareness of LLMs against six variations in\nhuman-interface interactions in LLM-based query-answer systems within finance.\nWe concentrate on two case studies: Query Failure and Context Failure. In the\nQuery Failure scenario, we perturb the original query to vary in domain\nexpertise, completeness, and linguistic accuracy. In the Context Failure case,\nwe simulate the uploads of degraded, irrelevant, and empty documents. We employ\nthe LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained\nrating criteria to define and calculate Robustness, Context Grounding, and\nCompliance scores for 24 off-the-shelf models. The results suggest that\nalthough some models excel at mitigating input perturbations, they must balance\nrobust answering with the ability to refrain from hallucinating. Notably,\nPalmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained\nstrong baseline performance but encountered challenges in sustaining robust\npredictions in 17% of test cases. On the other hand, the most robust model,\nOpenAI o3-mini, fabricated information in 41% of tested cases. The results\ndemonstrate that even high-performing models have significant room for\nimprovement and highlight the role of FailSafeQA as a tool for developing LLMs\noptimized for dependability in financial applications. The dataset is available\nat: https://huggingface.co/datasets/Writer/FailSafeQA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new long-context financial benchmark, FailSafeQA, designed to\ntest the robustness and context-awareness of LLMs against six variations in\nhuman-interface interactions in LLM-based query-answer systems within finance.\nWe concentrate on two case studies: Query Failure and Context Failure. In the\nQuery Failure scenario, we perturb the original query to vary in domain\nexpertise, completeness, and linguistic accuracy. In the Context Failure case,\nwe simulate the uploads of degraded, irrelevant, and empty documents. We employ\nthe LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained\nrating criteria to define and calculate Robustness, Context Grounding, and\nCompliance scores for 24 off-the-shelf models. The results suggest that\nalthough some models excel at mitigating input perturbations, they must balance\nrobust answering with the ability to refrain from hallucinating. Notably,\nPalmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained\nstrong baseline performance but encountered challenges in sustaining robust\npredictions in 17% of test cases. On the other hand, the most robust model,\nOpenAI o3-mini, fabricated information in 41% of tested cases. The results\ndemonstrate that even high-performing models have significant room for\nimprovement and highlight the role of FailSafeQA as a tool for developing LLMs\noptimized for dependability in financial applications. The dataset is available\nat: https://huggingface.co/datasets/Writer/FailSafeQA"
                },
                "authors": [
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Dmytro Mozolevskyi"
                    },
                    {
                        "name": "Muayad Ali"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06316v1",
                "updated": "2025-02-10T10:09:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    9,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:09:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    9,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the\n  Correspondence between Patent Claim and Prior Art",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the\n  Correspondence between Patent Claim and Prior Art"
                },
                "summary": "Assessing the novelty of patent claims is a critical yet challenging task\ntraditionally performed by patent examiners. While advancements in NLP have\nenabled progress in various patent-related tasks, novelty assessment remains\nunexplored. This paper introduces a novel challenge by evaluating the ability\nof large language models (LLMs) to assess patent novelty by comparing claims\nwith cited prior art documents, following the process similar to that of patent\nexaminers done. We present the first dataset specifically designed for novelty\nevaluation, derived from real patent examination cases, and analyze the\ncapabilities of LLMs to address this task. Our study reveals that while\nclassification models struggle to effectively assess novelty, generative models\nmake predictions with a reasonable level of accuracy, and their explanations\nare accurate enough to understand the relationship between the target patent\nand prior art. These findings demonstrate the potential of LLMs to assist in\npatent evaluation, reducing the workload for both examiners and applicants. Our\ncontributions highlight the limitations of current models and provide a\nfoundation for improving AI-driven patent analysis through advanced models and\nrefined datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the novelty of patent claims is a critical yet challenging task\ntraditionally performed by patent examiners. While advancements in NLP have\nenabled progress in various patent-related tasks, novelty assessment remains\nunexplored. This paper introduces a novel challenge by evaluating the ability\nof large language models (LLMs) to assess patent novelty by comparing claims\nwith cited prior art documents, following the process similar to that of patent\nexaminers done. We present the first dataset specifically designed for novelty\nevaluation, derived from real patent examination cases, and analyze the\ncapabilities of LLMs to address this task. Our study reveals that while\nclassification models struggle to effectively assess novelty, generative models\nmake predictions with a reasonable level of accuracy, and their explanations\nare accurate enough to understand the relationship between the target patent\nand prior art. These findings demonstrate the potential of LLMs to assist in\npatent evaluation, reducing the workload for both examiners and applicants. Our\ncontributions highlight the limitations of current models and provide a\nfoundation for improving AI-driven patent analysis through advanced models and\nrefined datasets."
                },
                "authors": [
                    {
                        "name": "Hayato Ikoma"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06298v1",
                "updated": "2025-02-10T09:40:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    40,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T09:40:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    40,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual\n  Questions in Southeast Asia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual\n  Questions in Southeast Asia"
                },
                "summary": "This study introduces two novel benchmarks, SeaExam and SeaBench, designed to\nevaluate the capabilities of Large Language Models (LLMs) in Southeast Asian\n(SEA) application scenarios. Unlike existing multilingual datasets primarily\nderived from English translations, these benchmarks are constructed based on\nreal-world scenarios from SEA regions. SeaExam draws from regional educational\nexams to form a comprehensive dataset that encompasses subjects such as local\nhistory and literature. In contrast, SeaBench is crafted around multi-turn,\nopen-ended tasks that reflect daily interactions within SEA communities. Our\nevaluations demonstrate that SeaExam and SeaBench more effectively discern LLM\nperformance on SEA language tasks compared to their translated benchmarks. This\nhighlights the importance of using real-world queries to assess the\nmultilingual capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces two novel benchmarks, SeaExam and SeaBench, designed to\nevaluate the capabilities of Large Language Models (LLMs) in Southeast Asian\n(SEA) application scenarios. Unlike existing multilingual datasets primarily\nderived from English translations, these benchmarks are constructed based on\nreal-world scenarios from SEA regions. SeaExam draws from regional educational\nexams to form a comprehensive dataset that encompasses subjects such as local\nhistory and literature. In contrast, SeaBench is crafted around multi-turn,\nopen-ended tasks that reflect daily interactions within SEA communities. Our\nevaluations demonstrate that SeaExam and SeaBench more effectively discern LLM\nperformance on SEA language tasks compared to their translated benchmarks. This\nhighlights the importance of using real-world queries to assess the\nmultilingual capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Chaoqun Liu"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Mahani Aljunied"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Accepted to Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12517v2",
                "updated": "2025-02-10T09:37:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    37,
                    59,
                    0,
                    41,
                    0
                ],
                "published": "2024-09-19T07:15:58Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    15,
                    58,
                    3,
                    263,
                    0
                ],
                "title": "Scaling FP8 training to trillion-token LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling FP8 training to trillion-token LLMs"
                },
                "summary": "We train, for the first time, large language models using FP8 precision on\ndatasets up to 2 trillion tokens -- a 20-fold increase over previous limits.\nThrough these extended training runs, we uncover critical instabilities in FP8\ntraining that were not observable in earlier works with shorter durations. We\ntrace these instabilities to outlier amplification by the SwiGLU activation\nfunction. Interestingly, we show, both analytically and empirically, that this\namplification happens only over prolonged training periods, and link it to a\nSwiGLU weight alignment process. To address this newly identified issue, we\nintroduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training\nwithout altering function behavior. We also demonstrate, for the first time,\nFP8 quantization of both Adam optimizer moments. Combining these innovations,\nwe successfully train a 7B parameter model using FP8 precision on 256 Intel\nGaudi2 accelerators, achieving on-par results with the BF16 baseline while\ndelivering up to a $\\sim 34 \\%$ throughput improvement. A reference\nimplementation is supplied in\nhttps://github.com/Anonymous1252022/Megatron-DeepSpeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We train, for the first time, large language models using FP8 precision on\ndatasets up to 2 trillion tokens -- a 20-fold increase over previous limits.\nThrough these extended training runs, we uncover critical instabilities in FP8\ntraining that were not observable in earlier works with shorter durations. We\ntrace these instabilities to outlier amplification by the SwiGLU activation\nfunction. Interestingly, we show, both analytically and empirically, that this\namplification happens only over prolonged training periods, and link it to a\nSwiGLU weight alignment process. To address this newly identified issue, we\nintroduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training\nwithout altering function behavior. We also demonstrate, for the first time,\nFP8 quantization of both Adam optimizer moments. Combining these innovations,\nwe successfully train a 7B parameter model using FP8 precision on 256 Intel\nGaudi2 accelerators, achieving on-par results with the BF16 baseline while\ndelivering up to a $\\sim 34 \\%$ throughput improvement. A reference\nimplementation is supplied in\nhttps://github.com/Anonymous1252022/Megatron-DeepSpeed."
                },
                "authors": [
                    {
                        "name": "Maxim Fishman"
                    },
                    {
                        "name": "Brian Chmiel"
                    },
                    {
                        "name": "Ron Banner"
                    },
                    {
                        "name": "Daniel Soudry"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Soudry"
                },
                "author": "Daniel Soudry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06279v1",
                "updated": "2025-02-10T09:23:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    23,
                    3,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T09:23:03Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    23,
                    3,
                    0,
                    41,
                    0
                ],
                "title": "DebateBench: A Challenging Long Context Reasoning Benchmark For Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DebateBench: A Challenging Long Context Reasoning Benchmark For Large\n  Language Models"
                },
                "summary": "We introduce DebateBench, a novel dataset consisting of an extensive\ncollection of transcripts and metadata from some of the world's most\nprestigious competitive debates. The dataset consists of British Parliamentary\ndebates from prestigious debating tournaments on diverse topics, annotated with\ndetailed speech-level scores and house rankings sourced from official\nadjudication data. We curate 256 speeches across 32 debates with each debate\nbeing over 1 hour long with each input being an average of 32,000 tokens.\nDesigned to capture long-context, large-scale reasoning tasks, DebateBench\nprovides a benchmark for evaluating modern large language models (LLMs) on\ntheir ability to engage in argumentation, deliberation, and alignment with\nhuman experts. To do well on DebateBench, the LLMs must perform in-context\nlearning to understand the rules and evaluation criteria of the debates, then\nanalyze 8 seven minute long speeches and reason about the arguments presented\nby all speakers to give the final results. Our preliminary evaluation using GPT\no1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on\nDebateBench, highlighting the need to develop more sophisticated techniques for\nimproving their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DebateBench, a novel dataset consisting of an extensive\ncollection of transcripts and metadata from some of the world's most\nprestigious competitive debates. The dataset consists of British Parliamentary\ndebates from prestigious debating tournaments on diverse topics, annotated with\ndetailed speech-level scores and house rankings sourced from official\nadjudication data. We curate 256 speeches across 32 debates with each debate\nbeing over 1 hour long with each input being an average of 32,000 tokens.\nDesigned to capture long-context, large-scale reasoning tasks, DebateBench\nprovides a benchmark for evaluating modern large language models (LLMs) on\ntheir ability to engage in argumentation, deliberation, and alignment with\nhuman experts. To do well on DebateBench, the LLMs must perform in-context\nlearning to understand the rules and evaluation criteria of the debates, then\nanalyze 8 seven minute long speeches and reason about the arguments presented\nby all speakers to give the final results. Our preliminary evaluation using GPT\no1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on\nDebateBench, highlighting the need to develop more sophisticated techniques for\nimproving their performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Tiwari"
                    },
                    {
                        "name": "Aryan Seth"
                    },
                    {
                        "name": "Adi Mukherjee"
                    },
                    {
                        "name": "Kaavya Mer"
                    },
                    {
                        "name": "Kavish"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06258v1",
                "updated": "2025-02-10T08:48:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    48,
                    10,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T08:48:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    48,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Emergent Response Planning in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Response Planning in LLM"
                },
                "summary": "In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structural attributes}$ (response length, reasoning steps),\n$\\textit{content attributes}$ (character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavioral\nattributes}$ (answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggests potential applications\nfor improving transparency and generation control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structural attributes}$ (response length, reasoning steps),\n$\\textit{content attributes}$ (character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavioral\nattributes}$ (answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggests potential applications\nfor improving transparency and generation control."
                },
                "authors": [
                    {
                        "name": "Zhichen Dong"
                    },
                    {
                        "name": "Zhanhui Zhou"
                    },
                    {
                        "name": "Zhixuan Liu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Chaochao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Lu"
                },
                "author": "Chaochao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06257v1",
                "updated": "2025-02-10T08:45:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    45,
                    56,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T08:45:56Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    45,
                    56,
                    0,
                    41,
                    0
                ],
                "title": "K-ON: Stacking Knowledge On the Head Layer of Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-ON: Stacking Knowledge On the Head Layer of Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved various natural language processing (NLP) tasks. Typically, LLMs are\ntrained to predict the next token, aligning well with many NLP tasks. However,\nin knowledge graph (KG) scenarios, entities are the fundamental units and\nidentifying an entity requires at least several tokens. This leads to a\ngranularity mismatch between KGs and natural languages. To address this issue,\nwe propose K-ON, which integrates KG knowledge into the LLM by employing\nmultiple head layers for next k-step prediction. K-ON can not only generate\nentity-level results in one step, but also enables contrastive loss against\nentities, which is the most powerful tool in KG representation learning.\nExperimental results show that K-ON outperforms state-of-the-art methods that\nincorporate text and even the other modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved various natural language processing (NLP) tasks. Typically, LLMs are\ntrained to predict the next token, aligning well with many NLP tasks. However,\nin knowledge graph (KG) scenarios, entities are the fundamental units and\nidentifying an entity requires at least several tokens. This leads to a\ngranularity mismatch between KGs and natural languages. To address this issue,\nwe propose K-ON, which integrates KG knowledge into the LLM by employing\nmultiple head layers for next k-step prediction. K-ON can not only generate\nentity-level results in one step, but also enables contrastive loss against\nentities, which is the most powerful tool in KG representation learning.\nExperimental results show that K-ON outperforms state-of-the-art methods that\nincorporate text and even the other modalities."
                },
                "authors": [
                    {
                        "name": "Lingbing Guo"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "AAAI 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06253v1",
                "updated": "2025-02-10T08:37:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    37,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T08:37:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    37,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Find Central Dogma Again",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find Central Dogma Again"
                },
                "summary": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences.This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs.We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the English similarity\njudgment dataset from PAWS-X. When tested on a dataset for DNA and protein\nsequence alignment judgment, the fine-tuned model achieved a classification\naccuracy of 76%. The study also analyzed factors contributing to this zero-shot\ncapability, including model training stability and types of training data.This\nresearch demonstrates that LLMs can, through the transfer of natural language\ncapabilities and solely relying on the analysis of sequences themselves,\nrediscover the central dogma without prior knowledge of it. This study opens a\nnew door for AI-driven biological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences.This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs.We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the English similarity\njudgment dataset from PAWS-X. When tested on a dataset for DNA and protein\nsequence alignment judgment, the fine-tuned model achieved a classification\naccuracy of 76%. The study also analyzed factors contributing to this zero-shot\ncapability, including model training stability and types of training data.This\nresearch demonstrates that LLMs can, through the transfer of natural language\ncapabilities and solely relying on the analysis of sequences themselves,\nrediscover the central dogma without prior knowledge of it. This study opens a\nnew door for AI-driven biological research."
                },
                "authors": [
                    {
                        "name": "Wang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Liang"
                },
                "author": "Wang Liang",
                "arxiv_comment": "21 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16633v2",
                "updated": "2025-02-10T08:13:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    13,
                    17,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-21T13:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    58,
                    27,
                    5,
                    356,
                    0
                ],
                "title": "POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks\n  against Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks\n  against Embodied AI"
                },
                "summary": "Embodied AI systems are rapidly evolving due to the integration of LLMs as\nplanning modules, which transform complex instructions into executable\npolicies. However, LLMs are vulnerable to jailbreak attacks, which can generate\nmalicious content. This paper investigates the feasibility and rationale behind\napplying traditional LLM jailbreak attacks to EAI systems. We aim to answer\nthree questions: (1) Do traditional LLM jailbreak attacks apply to EAI systems?\n(2) What challenges arise if they do not? and (3) How can we defend against EAI\njailbreak attacks? To this end, we first measure existing LLM-based EAI systems\nusing a newly constructed dataset, i.e., the Harmful-RLbench. Our study\nconfirms that traditional LLM jailbreak attacks are not directly applicable to\nEAI systems and identifies two unique challenges. First, the harmful text does\nnot necessarily constitute harmful policies. Second, even if harmful policies\ncan be generated, they are not necessarily executable by the EAI systems, which\nlimits the potential risk. To facilitate a more comprehensive security\nanalysis, we refine and introduce POEX, a novel red teaming framework that\noptimizes adversarial suffixes to induce harmful yet executable policies\nagainst EAI systems. The design of POEX employs adversarial constraints, policy\nevaluators, and suffix optimization to ensure successful policy execution while\nevading safety detection inside an EAI system. Experiments on the real-world\nrobotic arm and simulator using Harmful-RLbench demonstrate the efficacy,\nhighlighting severe safety vulnerabilities and high transferability across\nmodels. Finally, we propose prompt-based and model-based defenses, achieving an\n85% success rate in mitigating attacks and enhancing safety awareness in EAI\nsystems. Our findings underscore the urgent need for robust security measures\nto ensure the safe deployment of EAI in critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI systems are rapidly evolving due to the integration of LLMs as\nplanning modules, which transform complex instructions into executable\npolicies. However, LLMs are vulnerable to jailbreak attacks, which can generate\nmalicious content. This paper investigates the feasibility and rationale behind\napplying traditional LLM jailbreak attacks to EAI systems. We aim to answer\nthree questions: (1) Do traditional LLM jailbreak attacks apply to EAI systems?\n(2) What challenges arise if they do not? and (3) How can we defend against EAI\njailbreak attacks? To this end, we first measure existing LLM-based EAI systems\nusing a newly constructed dataset, i.e., the Harmful-RLbench. Our study\nconfirms that traditional LLM jailbreak attacks are not directly applicable to\nEAI systems and identifies two unique challenges. First, the harmful text does\nnot necessarily constitute harmful policies. Second, even if harmful policies\ncan be generated, they are not necessarily executable by the EAI systems, which\nlimits the potential risk. To facilitate a more comprehensive security\nanalysis, we refine and introduce POEX, a novel red teaming framework that\noptimizes adversarial suffixes to induce harmful yet executable policies\nagainst EAI systems. The design of POEX employs adversarial constraints, policy\nevaluators, and suffix optimization to ensure successful policy execution while\nevading safety detection inside an EAI system. Experiments on the real-world\nrobotic arm and simulator using Harmful-RLbench demonstrate the efficacy,\nhighlighting severe safety vulnerabilities and high transferability across\nmodels. Finally, we propose prompt-based and model-based defenses, achieving an\n85% success rate in mitigating attacks and enhancing safety awareness in EAI\nsystems. Our findings underscore the urgent need for robust security measures\nto ensure the safe deployment of EAI in critical applications."
                },
                "authors": [
                    {
                        "name": "Xuancun Lu"
                    },
                    {
                        "name": "Zhengxian Huang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Xiaoyu ji"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu",
                "arxiv_comment": "Homepage: https://poex-eai-jailbreak.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06233v1",
                "updated": "2025-02-10T08:10:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    10,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T08:10:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    10,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Confidence Improves Self-Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Improves Self-Consistency in LLMs"
                },
                "summary": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic."
                },
                "authors": [
                    {
                        "name": "Amir Taubenfeld"
                    },
                    {
                        "name": "Tom Sheffer"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Amir Feder"
                    },
                    {
                        "name": "Ariel Goldstein"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Gal Yona"
                    }
                ],
                "author_detail": {
                    "name": "Gal Yona"
                },
                "author": "Gal Yona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18457v2",
                "updated": "2025-02-10T07:46:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    46,
                    6,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-30T16:15:38Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    15,
                    38,
                    3,
                    30,
                    0
                ],
                "title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering"
                },
                "summary": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and\nretrieval-augmented settings. We also found that increasing the number of\nlanguages involved in CALM training leads to higher accuracy and consistency.\nWe offer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and\nretrieval-augmented settings. We also found that increasing the number of\nlanguages involved in CALM training leads to higher accuracy and consistency.\nWe offer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00426v2",
                "updated": "2025-02-10T07:44:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    44,
                    35,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-01T13:20:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    20,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification"
                },
                "summary": "Recently, adapting Vision Language Models (VLMs) to zero-shot visual\nclassification by tuning class embedding with a few prompts (Test-time Prompt\nTuning, TPT) or replacing class names with generated visual samples\n(support-set) has shown promising results. However, TPT cannot avoid the\nsemantic gap between modalities while the support-set cannot be tuned. To this\nend, we draw on each other's strengths and propose a novel framework namely\nTEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It\nfirst dilates the support-set with multiple prompts (Multi-prompting\nSupport-set Dilation, MSD) and then erodes the support-set via learnable\nweights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE).\nSpecifically, i) MSD expands the support samples for each class based on\nmultiple prompts enquired from LLMs to enrich the diversity of the support-set.\nii) TSE tunes the support-set with factorized learnable weights according to\nthe temporal prediction consistency in a self-supervised manner to dig pivotal\nsupporting cues for each class. $\\textbf{TEST-V}$ achieves state-of-the-art\nresults across four benchmarks and has good interpretability for the\nsupport-set dilation and erosion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, adapting Vision Language Models (VLMs) to zero-shot visual\nclassification by tuning class embedding with a few prompts (Test-time Prompt\nTuning, TPT) or replacing class names with generated visual samples\n(support-set) has shown promising results. However, TPT cannot avoid the\nsemantic gap between modalities while the support-set cannot be tuned. To this\nend, we draw on each other's strengths and propose a novel framework namely\nTEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It\nfirst dilates the support-set with multiple prompts (Multi-prompting\nSupport-set Dilation, MSD) and then erodes the support-set via learnable\nweights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE).\nSpecifically, i) MSD expands the support samples for each class based on\nmultiple prompts enquired from LLMs to enrich the diversity of the support-set.\nii) TSE tunes the support-set with factorized learnable weights according to\nthe temporal prediction consistency in a self-supervised manner to dig pivotal\nsupporting cues for each class. $\\textbf{TEST-V}$ achieves state-of-the-art\nresults across four benchmarks and has good interpretability for the\nsupport-set dilation and erosion."
                },
                "authors": [
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Hongyu Qu"
                    },
                    {
                        "name": "Xiaoyu Du"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Jinhui Tang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06215v1",
                "updated": "2025-02-10T07:33:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    33,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T07:33:49Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    33,
                    49,
                    0,
                    41,
                    0
                ],
                "title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83\n  Software Engineering Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83\n  Software Engineering Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Yunbo Lyu"
                    },
                    {
                        "name": "Jianming Chang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Dan Huang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06207v1",
                "updated": "2025-02-10T07:14:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    14,
                    26,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T07:14:26Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    14,
                    26,
                    0,
                    41,
                    0
                ],
                "title": "Unveiling the Capabilities of Large Language Models in Detecting\n  Offensive Language with Annotation Disagreement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Capabilities of Large Language Models in Detecting\n  Offensive Language with Annotation Disagreement"
                },
                "summary": "LLMs are widely used for offensive language detection due to their advanced\ncapability. However, the challenges posed by human annotation disagreement in\nreal-world datasets remain underexplored. These disagreement samples are\ndifficult to detect due to their ambiguous nature. Additionally, the confidence\nof LLMs in processing disagreement samples can provide valuable insights into\ntheir alignment with human annotators. To address this gap, we systematically\nevaluate the ability of LLMs to detect offensive language with annotation\ndisagreement. We compare the binary accuracy of multiple LLMs across varying\nannotation agreement levels and analyze the relationship between LLM confidence\nand annotation agreement. Furthermore, we investigate the impact of\ndisagreement samples on LLM decision-making during few-shot learning and\ninstruction fine-tuning. Our findings highlight the challenges posed by\ndisagreement samples and offer guidance for improving LLM-based offensive\nlanguage detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are widely used for offensive language detection due to their advanced\ncapability. However, the challenges posed by human annotation disagreement in\nreal-world datasets remain underexplored. These disagreement samples are\ndifficult to detect due to their ambiguous nature. Additionally, the confidence\nof LLMs in processing disagreement samples can provide valuable insights into\ntheir alignment with human annotators. To address this gap, we systematically\nevaluate the ability of LLMs to detect offensive language with annotation\ndisagreement. We compare the binary accuracy of multiple LLMs across varying\nannotation agreement levels and analyze the relationship between LLM confidence\nand annotation agreement. Furthermore, we investigate the impact of\ndisagreement samples on LLM decision-making during few-shot learning and\ninstruction fine-tuning. Our findings highlight the challenges posed by\ndisagreement samples and offer guidance for improving LLM-based offensive\nlanguage detection."
                },
                "authors": [
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Kaichun Wang"
                    },
                    {
                        "name": "Kelaiti Xiao"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin",
                "arxiv_comment": "17 pages, submitted to the ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06205v1",
                "updated": "2025-02-10T07:04:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    4,
                    32,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T07:04:32Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    4,
                    32,
                    0,
                    41,
                    0
                ],
                "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Peiying Yu"
                    },
                    {
                        "name": "Dingmin Wang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Ongong work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06204v1",
                "updated": "2025-02-10T07:03:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    3,
                    0,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T07:03:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    7,
                    3,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Non-literal Understanding of Number Words by Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-literal Understanding of Number Words by Language Models"
                },
                "summary": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Polina Tsvilodub"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jan-Philipp Frnken"
                    },
                    {
                        "name": "Michael Franke"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "12 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06197v1",
                "updated": "2025-02-10T06:51:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    51,
                    50,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T06:51:50Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    51,
                    50,
                    0,
                    41,
                    0
                ],
                "title": "Timing Matters: How Using LLMs at Different Timings Influences Writers'\n  Perceptions and Ideation Outcomes in AI-Assisted Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timing Matters: How Using LLMs at Different Timings Influences Writers'\n  Perceptions and Ideation Outcomes in AI-Assisted Ideation"
                },
                "summary": "Large Language Models (LLMs) have been widely used to support ideation in the\nwriting process. However, whether generating ideas with the help of LLMs leads\nto idea fixation or idea expansion is unclear. This study examines how\ndifferent timings of LLM usage - either at the beginning or after independent\nideation - affect people's perceptions and ideation outcomes in a writing task.\nIn a controlled experiment with 60 participants, we found that using LLMs from\nthe beginning reduced the number of original ideas and lowered creative\nself-efficacy and self-credit, mediated by changes in autonomy and ownership.\nWe discuss the challenges and opportunities associated with using LLMs to\nassist in idea generation. We propose delaying the use of LLMs to support\nideation while considering users' self-efficacy, autonomy, and ownership of the\nideation outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used to support ideation in the\nwriting process. However, whether generating ideas with the help of LLMs leads\nto idea fixation or idea expansion is unclear. This study examines how\ndifferent timings of LLM usage - either at the beginning or after independent\nideation - affect people's perceptions and ideation outcomes in a writing task.\nIn a controlled experiment with 60 participants, we found that using LLMs from\nthe beginning reduced the number of original ideas and lowered creative\nself-efficacy and self-credit, mediated by changes in autonomy and ownership.\nWe discuss the challenges and opportunities associated with using LLMs to\nassist in idea generation. We propose delaying the use of LLMs to support\nideation while considering users' self-efficacy, autonomy, and ownership of the\nideation outcomes."
                },
                "authors": [
                    {
                        "name": "Peinuan Qin"
                    },
                    {
                        "name": "Chi-Lan Yang"
                    },
                    {
                        "name": "Jingshu Li"
                    },
                    {
                        "name": "Jing Wen"
                    },
                    {
                        "name": "Yi-Chieh Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Chieh Lee"
                },
                "author": "Yi-Chieh Lee",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06193v1",
                "updated": "2025-02-10T06:49:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T06:49:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering"
                },
                "summary": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored. In\nthis paper, we empirically explore LLM-as-a-judge methods for evaluating SE\ntasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide..."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Jiyu Guo"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Guodong Fan"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_comment": "Accepted by ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04643v2",
                "updated": "2025-02-10T06:46:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    46,
                    48,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-07T04:07:36Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    7,
                    36,
                    4,
                    38,
                    0
                ],
                "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Elicitation: A New Attack Vector for Large Language Models"
                },
                "summary": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions."
                },
                "authors": [
                    {
                        "name": "Brian Formento"
                    },
                    {
                        "name": "Chuan Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "Published in ICLR 2025. The code is publicly available at\n  https://github.com/Aniloid2/Confidence_Elicitation_Attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06173v1",
                "updated": "2025-02-10T05:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    54,
                    36,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:54:36Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    54,
                    36,
                    0,
                    41,
                    0
                ],
                "title": "Uncertainty-Aware Adaptation of Large Language Models for\n  Protein-Protein Interaction Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Adaptation of Large Language Models for\n  Protein-Protein Interaction Analysis"
                },
                "summary": "Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research."
                },
                "authors": [
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Tianle Wang"
                    },
                    {
                        "name": "Gilchan Park"
                    },
                    {
                        "name": "Kriti Chopra"
                    },
                    {
                        "name": "Nicholas Jeon"
                    },
                    {
                        "name": "Xiaoning Qian"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Jun Yoon"
                },
                "author": "Byung-Jun Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]