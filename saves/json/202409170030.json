[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.09030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09030v1",
                "updated": "2024-09-13T17:55:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    58,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:55:58Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    58,
                    4,
                    257,
                    0
                ],
                "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents in Software Engineering: Survey, Landscape, and Vision"
                },
                "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE."
                },
                "authors": [
                    {
                        "name": "Yanxian Huang"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Qianxiang Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanlin Wang"
                },
                "author": "Yanlin Wang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09029v1",
                "updated": "2024-09-13T17:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    48,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:55:48Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    48,
                    4,
                    257,
                    0
                ],
                "title": "Learning Theory Informed Priors for Bayesian Inference: A Case Study\n  with Early Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Theory Informed Priors for Bayesian Inference: A Case Study\n  with Early Dark Energy"
                },
                "summary": "Cosmological models are often motivated and formulated in the language of\nparticle physics, using quantities such as the axion decay constant, but tested\nagainst data using ostensibly physical quantities, such as energy density\nratios, assuming uniform priors on the latter. This approach neglects priors on\nthe model from fundamental theory, including from particle physics and string\ntheory, such as the preference for sub-Planckian axion decay constants. We\nintroduce a novel approach to learning theory-informed priors for Bayesian\ninference using normalizing flows (NF), a flexible generative machine learning\ntechnique that generates priors on model parameters when analytic expressions\nare unavailable or difficult to compute. As a test case, we focus on early dark\nenergy (EDE), a model designed to address the Hubble tension. Rather than using\nuniform priors on the $\\textit{phenomenological}$ EDE parameters $f_{\\rm EDE}$\nand $z_c$, we train a NF on EDE cosmologies informed by theory expectations for\naxion masses and decay constants. Our method recovers known constraints in this\nrepresentation while being $\\sim 300,000$ times more efficient in terms of\ntotal CPU compute time. Applying our NF to $\\textit{Planck}$ and BOSS data, we\nobtain the first theory-informed constraints on EDE, finding $f_{\\rm EDE}\n\\lesssim 0.02$ at $95\\%$ confidence with an $H_0$ consistent with\n$\\textit{Planck}$, but in $\\sim 6\\sigma$ tension with SH0ES. This yields the\nstrongest constraints on EDE to date, additionally challenging its role in\nresolving the Hubble tension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological models are often motivated and formulated in the language of\nparticle physics, using quantities such as the axion decay constant, but tested\nagainst data using ostensibly physical quantities, such as energy density\nratios, assuming uniform priors on the latter. This approach neglects priors on\nthe model from fundamental theory, including from particle physics and string\ntheory, such as the preference for sub-Planckian axion decay constants. We\nintroduce a novel approach to learning theory-informed priors for Bayesian\ninference using normalizing flows (NF), a flexible generative machine learning\ntechnique that generates priors on model parameters when analytic expressions\nare unavailable or difficult to compute. As a test case, we focus on early dark\nenergy (EDE), a model designed to address the Hubble tension. Rather than using\nuniform priors on the $\\textit{phenomenological}$ EDE parameters $f_{\\rm EDE}$\nand $z_c$, we train a NF on EDE cosmologies informed by theory expectations for\naxion masses and decay constants. Our method recovers known constraints in this\nrepresentation while being $\\sim 300,000$ times more efficient in terms of\ntotal CPU compute time. Applying our NF to $\\textit{Planck}$ and BOSS data, we\nobtain the first theory-informed constraints on EDE, finding $f_{\\rm EDE}\n\\lesssim 0.02$ at $95\\%$ confidence with an $H_0$ consistent with\n$\\textit{Planck}$, but in $\\sim 6\\sigma$ tension with SH0ES. This yields the\nstrongest constraints on EDE to date, additionally challenging its role in\nresolving the Hubble tension."
                },
                "authors": [
                    {
                        "name": "Michael W. Toomey"
                    },
                    {
                        "name": "Mikhail M. Ivanov"
                    },
                    {
                        "name": "Evan McDonough"
                    }
                ],
                "author_detail": {
                    "name": "Evan McDonough"
                },
                "author": "Evan McDonough",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09018v1",
                "updated": "2024-09-13T17:45:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    45,
                    53,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:45:53Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    45,
                    53,
                    4,
                    257,
                    0
                ],
                "title": "An Efficient and Streaming Audio Visual Active Speaker Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Streaming Audio Visual Active Speaker Detection System"
                },
                "summary": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost."
                },
                "authors": [
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Mohammad Sekhavat"
                    },
                    {
                        "name": "Max Horton"
                    },
                    {
                        "name": "Danny Tormoen"
                    },
                    {
                        "name": "Devang Naik"
                    }
                ],
                "author_detail": {
                    "name": "Devang Naik"
                },
                "author": "Devang Naik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09013v1",
                "updated": "2024-09-13T17:41:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    41,
                    12,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:41:12Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    41,
                    12,
                    4,
                    257,
                    0
                ],
                "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents"
                },
                "summary": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents."
                },
                "authors": [
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Sanketh Rangreji"
                    },
                    {
                        "name": "Anubha Kabra"
                    },
                    {
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09010v1",
                "updated": "2024-09-13T17:38:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    38,
                    47,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:38:47Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    38,
                    47,
                    4,
                    257,
                    0
                ],
                "title": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering"
                },
                "summary": "Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper."
                },
                "authors": [
                    {
                        "name": "Kanchan Shivashankar"
                    },
                    {
                        "name": "Nadine Steinmetz"
                    }
                ],
                "author_detail": {
                    "name": "Nadine Steinmetz"
                },
                "author": "Nadine Steinmetz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09007v1",
                "updated": "2024-09-13T17:37:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    37,
                    34,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:37:34Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    37,
                    34,
                    4,
                    257,
                    0
                ],
                "title": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear\n  Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear\n  Complexity"
                },
                "summary": "Learning representations on large graphs is a long-standing challenge due to\nthe inter-dependence nature. Transformers recently have shown promising\nperformance on small graphs thanks to its global attention for capturing\nall-pair interactions beyond observed structures. Existing approaches tend to\ninherit the spirit of Transformers in language and vision tasks, and embrace\ncomplicated architectures by stacking deep attention-based propagation layers.\nIn this paper, we attempt to evaluate the necessity of adopting multi-layer\nattentions in Transformers on graphs, which considerably restricts the\nefficiency. Specifically, we analyze a generic hybrid propagation layer,\ncomprised of all-pair attention and graph-based propagation, and show that\nmulti-layer propagation can be reduced to one-layer propagation, with the same\ncapability for representation learning. It suggests a new technical path for\nbuilding powerful and efficient Transformers on graphs, particularly through\nsimplifying model architectures without sacrificing expressiveness. As\nexemplified by this work, we propose a Simplified Single-layer Graph\nTransformers (SGFormer), whose main component is a single-layer global\nattention that scales linearly w.r.t. graph sizes and requires none of any\napproximation for accommodating all-pair interactions. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M, yielding\norders-of-magnitude inference acceleration over peer Transformers on\nmedium-sized graphs, and demonstrates competitiveness with limited labeled\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning representations on large graphs is a long-standing challenge due to\nthe inter-dependence nature. Transformers recently have shown promising\nperformance on small graphs thanks to its global attention for capturing\nall-pair interactions beyond observed structures. Existing approaches tend to\ninherit the spirit of Transformers in language and vision tasks, and embrace\ncomplicated architectures by stacking deep attention-based propagation layers.\nIn this paper, we attempt to evaluate the necessity of adopting multi-layer\nattentions in Transformers on graphs, which considerably restricts the\nefficiency. Specifically, we analyze a generic hybrid propagation layer,\ncomprised of all-pair attention and graph-based propagation, and show that\nmulti-layer propagation can be reduced to one-layer propagation, with the same\ncapability for representation learning. It suggests a new technical path for\nbuilding powerful and efficient Transformers on graphs, particularly through\nsimplifying model architectures without sacrificing expressiveness. As\nexemplified by this work, we propose a Simplified Single-layer Graph\nTransformers (SGFormer), whose main component is a single-layer global\nattention that scales linearly w.r.t. graph sizes and requires none of any\napproximation for accommodating all-pair interactions. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M, yielding\norders-of-magnitude inference acceleration over peer Transformers on\nmedium-sized graphs, and demonstrates competitiveness with limited labeled\ndata."
                },
                "authors": [
                    {
                        "name": "Qitian Wu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Hengrui Zhang"
                    },
                    {
                        "name": "David Wipf"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Extended version of NeurIPS2023 contribution arXiv:2306.10759",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07868v2",
                "updated": "2024-09-13T17:15:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    15,
                    33,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-12T04:49:11Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    4,
                    49,
                    11,
                    2,
                    164,
                    0
                ],
                "title": "Bridging multiple worlds: multi-marginal optimal transport for causal\n  partial-identification problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging multiple worlds: multi-marginal optimal transport for causal\n  partial-identification problem"
                },
                "summary": "Under the prevalent potential outcome model in causal inference, each unit is\nassociated with multiple potential outcomes but at most one of which is\nobserved, leading to many causal quantities being only partially identified.\nThe inherent missing data issue echoes the multi-marginal optimal transport\n(MOT) problem, where marginal distributions are known, but how the marginals\ncouple to form the joint distribution is unavailable. In this paper, we cast\nthe causal partial identification problem in the framework of MOT with $K$\nmargins and $d$-dimensional outcomes and obtain the exact partial identified\nset. In order to estimate the partial identified set via MOT, statistically, we\nestablish a convergence rate of the plug-in MOT estimator for the $\\ell_2$ cost\nfunction stemming from the variance minimization problem and prove it is\nminimax optimal for arbitrary $K$ and $d \\le 4$. We also extend the convergence\nresult to general quadratic objective functions. Numerically, we demonstrate\nthe efficacy of our method over synthetic datasets and several real-world\ndatasets where our proposal consistently outperforms the baseline by a\nsignificant margin (over 70%). In addition, we provide efficient off-the-shelf\nimplementations of MOT with general objective functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under the prevalent potential outcome model in causal inference, each unit is\nassociated with multiple potential outcomes but at most one of which is\nobserved, leading to many causal quantities being only partially identified.\nThe inherent missing data issue echoes the multi-marginal optimal transport\n(MOT) problem, where marginal distributions are known, but how the marginals\ncouple to form the joint distribution is unavailable. In this paper, we cast\nthe causal partial identification problem in the framework of MOT with $K$\nmargins and $d$-dimensional outcomes and obtain the exact partial identified\nset. In order to estimate the partial identified set via MOT, statistically, we\nestablish a convergence rate of the plug-in MOT estimator for the $\\ell_2$ cost\nfunction stemming from the variance minimization problem and prove it is\nminimax optimal for arbitrary $K$ and $d \\le 4$. We also extend the convergence\nresult to general quadratic objective functions. Numerically, we demonstrate\nthe efficacy of our method over synthetic datasets and several real-world\ndatasets where our proposal consistently outperforms the baseline by a\nsignificant margin (over 70%). In addition, we provide efficient off-the-shelf\nimplementations of MOT with general objective functions."
                },
                "authors": [
                    {
                        "name": "Zijun Gao"
                    },
                    {
                        "name": "Shu Ge"
                    },
                    {
                        "name": "Jian Qian"
                    }
                ],
                "author_detail": {
                    "name": "Jian Qian"
                },
                "author": "Jian Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2102.07008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2102.07008v2",
                "updated": "2024-09-13T17:15:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    15,
                    1,
                    4,
                    257,
                    0
                ],
                "published": "2021-02-13T21:55:09Z",
                "published_parsed": [
                    2021,
                    2,
                    13,
                    21,
                    55,
                    9,
                    5,
                    44,
                    0
                ],
                "title": "A Distance Covariance-based Estimator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distance Covariance-based Estimator"
                },
                "summary": "This paper introduces an estimator that considerably weakens the conventional\nrelevance condition of instrumental variable (IV) methods, allowing for\ninstruments that are weakly correlated, uncorrelated, or even mean-independent\nbut not independent of endogenous covariates. Under the relevance condition,\nthe estimator achieves consistent estimation and reliable inference without\nrequiring instrument excludability, and it remains robust even when the first\nmoment of the disturbance term does not exist. In contrast to conventional IV\nmethods, it maximises the set of feasible instruments in any empirical setting.\nUnder a weak conditional median independence condition on pairwise differences\nin disturbances and mild regularity assumptions, identification holds, and the\nestimator is consistent and asymptotically normal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an estimator that considerably weakens the conventional\nrelevance condition of instrumental variable (IV) methods, allowing for\ninstruments that are weakly correlated, uncorrelated, or even mean-independent\nbut not independent of endogenous covariates. Under the relevance condition,\nthe estimator achieves consistent estimation and reliable inference without\nrequiring instrument excludability, and it remains robust even when the first\nmoment of the disturbance term does not exist. In contrast to conventional IV\nmethods, it maximises the set of feasible instruments in any empirical setting.\nUnder a weak conditional median independence condition on pairwise differences\nin disturbances and mild regularity assumptions, identification holds, and the\nestimator is consistent and asymptotically normal."
                },
                "authors": [
                    {
                        "name": "Emmanuel Selorm Tsyawo"
                    },
                    {
                        "name": "Abdul-Nasah Soale"
                    }
                ],
                "author_detail": {
                    "name": "Abdul-Nasah Soale"
                },
                "author": "Abdul-Nasah Soale",
                "arxiv_comment": "Second draft. 55 pages total and 22 pages main text. New results\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2102.07008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2102.07008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08985v1",
                "updated": "2024-09-13T16:58:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    58,
                    6,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T16:58:06Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    58,
                    6,
                    4,
                    257,
                    0
                ],
                "title": "Clean Label Attacks against SLU Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean Label Attacks against SLU Systems"
                },
                "summary": "Poisoning backdoor attacks involve an adversary manipulating the training\ndata to induce certain behaviors in the victim model by inserting a trigger in\nthe signal at inference time. We adapted clean label backdoor (CLBD)-data\npoisoning attacks, which do not modify the training labels, on state-of-the-art\nspeech recognition models that support/perform a Spoken Language Understanding\ntask, achieving 99.8% attack success rate by poisoning 10% of the training\ndata. We analyzed how varying the signal-strength of the poison, percent of\nsamples poisoned, and choice of trigger impact the attack. We also found that\nCLBD attacks are most successful when applied to training samples that are\ninherently hard for a proxy model. Using this strategy, we achieved an attack\nsuccess rate of 99.3% by poisoning a meager 1.5% of the training data. Finally,\nwe applied two previously developed defenses against gradient-based attacks,\nand found that they attain mixed success against poisoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning backdoor attacks involve an adversary manipulating the training\ndata to induce certain behaviors in the victim model by inserting a trigger in\nthe signal at inference time. We adapted clean label backdoor (CLBD)-data\npoisoning attacks, which do not modify the training labels, on state-of-the-art\nspeech recognition models that support/perform a Spoken Language Understanding\ntask, achieving 99.8% attack success rate by poisoning 10% of the training\ndata. We analyzed how varying the signal-strength of the poison, percent of\nsamples poisoned, and choice of trigger impact the attack. We also found that\nCLBD attacks are most successful when applied to training samples that are\ninherently hard for a proxy model. Using this strategy, we achieved an attack\nsuccess rate of 99.3% by poisoning a meager 1.5% of the training data. Finally,\nwe applied two previously developed defenses against gradient-based attacks,\nand found that they attain mixed success against poisoning."
                },
                "authors": [
                    {
                        "name": "Henry Li Xinyuan"
                    },
                    {
                        "name": "Sonal Joshi"
                    },
                    {
                        "name": "Thomas Thebaud"
                    },
                    {
                        "name": "Jesus Villalba"
                    },
                    {
                        "name": "Najim Dehak"
                    },
                    {
                        "name": "Sanjeev Khudanpur"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Khudanpur"
                },
                "author": "Sanjeev Khudanpur",
                "arxiv_comment": "Accepted at IEEE SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08963v1",
                "updated": "2024-09-13T16:29:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    29,
                    25,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T16:29:25Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    29,
                    25,
                    4,
                    257,
                    0
                ],
                "title": "Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance"
                },
                "summary": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems."
                },
                "authors": [
                    {
                        "name": "Lucio La Cava"
                    },
                    {
                        "name": "Andrea Tagarelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Tagarelli"
                },
                "author": "Andrea Tagarelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08958v1",
                "updated": "2024-09-13T16:23:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    23,
                    17,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T16:23:17Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    23,
                    17,
                    4,
                    257,
                    0
                ],
                "title": "PINNfluence: Influence Functions for Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNfluence: Influence Functions for Physics-Informed Neural Networks"
                },
                "summary": "Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies."
                },
                "authors": [
                    {
                        "name": "Jonas R. Naujoks"
                    },
                    {
                        "name": "Aleksander Krasowski"
                    },
                    {
                        "name": "Moritz Weckbecker"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "René P. Klausen"
                    }
                ],
                "author_detail": {
                    "name": "René P. Klausen"
                },
                "author": "René P. Klausen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08937v1",
                "updated": "2024-09-13T15:55:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    55,
                    59,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:55:59Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    55,
                    59,
                    4,
                    257,
                    0
                ],
                "title": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations,\n  Data Quality Assessment, and Cognitive Forcing Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations,\n  Data Quality Assessment, and Cognitive Forcing Functions"
                },
                "summary": "In this paper, we investigate the impact of hallucinations and cognitive\nforcing functions in human-AI collaborative text generation tasks, focusing on\nthe use of Large Language Models (LLMs) to assist in generating high-quality\nconversational data. LLMs require data for fine-tuning, a crucial step in\nenhancing their performance. In the context of conversational customer support,\nthe data takes the form of a conversation between a human customer and an agent\nand can be generated with an AI assistant. In our inquiry, involving 11 users\nwho each completed 8 tasks, resulting in a total of 88 tasks, we found that the\npresence of hallucinations negatively impacts the quality of data. We also find\nthat, although the cognitive forcing function does not always mitigate the\ndetrimental effects of hallucinations on data quality, the presence of\ncognitive forcing functions and hallucinations together impacts data quality\nand influences how users leverage the AI responses presented to them. Our\nanalysis of user behavior reveals distinct patterns of reliance on AI-generated\nresponses, highlighting the importance of managing hallucinations in\nAI-generated content within conversational AI contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the impact of hallucinations and cognitive\nforcing functions in human-AI collaborative text generation tasks, focusing on\nthe use of Large Language Models (LLMs) to assist in generating high-quality\nconversational data. LLMs require data for fine-tuning, a crucial step in\nenhancing their performance. In the context of conversational customer support,\nthe data takes the form of a conversation between a human customer and an agent\nand can be generated with an AI assistant. In our inquiry, involving 11 users\nwho each completed 8 tasks, resulting in a total of 88 tasks, we found that the\npresence of hallucinations negatively impacts the quality of data. We also find\nthat, although the cognitive forcing function does not always mitigate the\ndetrimental effects of hallucinations on data quality, the presence of\ncognitive forcing functions and hallucinations together impacts data quality\nand influences how users leverage the AI responses presented to them. Our\nanalysis of user behavior reveals distinct patterns of reliance on AI-generated\nresponses, highlighting the importance of managing hallucinations in\nAI-generated content within conversational AI contexts."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Marina Danilevsky"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Casey Dugan"
                    },
                    {
                        "name": "Michelle Bachman"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Bachman"
                },
                "author": "Michelle Bachman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08934v1",
                "updated": "2024-09-13T15:53:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    53,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:53:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    53,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "Proactive Recommendation in Social Networks: Steering User Interest via\n  Neighbor Influence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Recommendation in Social Networks: Steering User Interest via\n  Neighbor Influence"
                },
                "summary": "Recommending items solely catering to users' historical interests narrows\nusers' horizons. Recent works have considered steering target users beyond\ntheir historical interests by directly adjusting items exposed to them.\nHowever, the recommended items for direct steering might not align perfectly\nwith users' interests evolution, detrimentally affecting target users'\nexperience. To avoid this issue, we propose a new task named Proactive\nRecommendation in Social Networks (PRSN) that indirectly steers users' interest\nby utilizing the influence of social neighbors, i.e., indirect steering by\nadjusting the exposure of a target item to target users' neighbors. The key to\nPRSN lies in answering an interventional question: what would a target user's\nfeedback be on a target item if the item is exposed to the user's different\nneighbors? To answer this question, we resort to causal inference and formalize\nPRSN as: (1) estimating the potential feedback of a user on an item, under the\nnetwork interference by the item's exposure to the user's neighbors; and (2)\nadjusting the exposure of a target item to target users' neighbors to trade off\nsteering performance and the damage to the neighbors' experience. To this end,\nwe propose a Neighbor Interference Recommendation (NIRec) framework with two\nkey modules: (1)an interference representation-based estimation module for\nmodeling potential feedback; and (2) a post-learning-based optimization module\nfor optimizing a target item's exposure to trade off steering performance and\nthe neighbors' experience by greedy search. We conduct extensive\nsemi-simulation experiments based on three real-world datasets, validating the\nsteering effectiveness of NIRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommending items solely catering to users' historical interests narrows\nusers' horizons. Recent works have considered steering target users beyond\ntheir historical interests by directly adjusting items exposed to them.\nHowever, the recommended items for direct steering might not align perfectly\nwith users' interests evolution, detrimentally affecting target users'\nexperience. To avoid this issue, we propose a new task named Proactive\nRecommendation in Social Networks (PRSN) that indirectly steers users' interest\nby utilizing the influence of social neighbors, i.e., indirect steering by\nadjusting the exposure of a target item to target users' neighbors. The key to\nPRSN lies in answering an interventional question: what would a target user's\nfeedback be on a target item if the item is exposed to the user's different\nneighbors? To answer this question, we resort to causal inference and formalize\nPRSN as: (1) estimating the potential feedback of a user on an item, under the\nnetwork interference by the item's exposure to the user's neighbors; and (2)\nadjusting the exposure of a target item to target users' neighbors to trade off\nsteering performance and the damage to the neighbors' experience. To this end,\nwe propose a Neighbor Interference Recommendation (NIRec) framework with two\nkey modules: (1)an interference representation-based estimation module for\nmodeling potential feedback; and (2) a post-learning-based optimization module\nfor optimizing a target item's exposure to trade off steering performance and\nthe neighbors' experience by greedy search. We conduct extensive\nsemi-simulation experiments based on three real-world datasets, validating the\nsteering effectiveness of NIRec."
                },
                "authors": [
                    {
                        "name": "Hang Pan"
                    },
                    {
                        "name": "Shuxian Bi"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08931v1",
                "updated": "2024-09-13T15:47:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    47,
                    50,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:47:50Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    47,
                    50,
                    4,
                    257,
                    0
                ],
                "title": "LLM-based Weak Supervision Framework for Query Intent Classification in\n  Video Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Weak Supervision Framework for Query Intent Classification in\n  Video Search"
                },
                "summary": "Streaming services have reshaped how we discover and engage with digital\nentertainment. Despite these advancements, effectively understanding the wide\nspectrum of user search queries continues to pose a significant challenge. An\naccurate query understanding system that can handle a variety of entities that\nrepresent different user intents is essential for delivering an enhanced user\nexperience. We can build such a system by training a natural language\nunderstanding (NLU) model; however, obtaining high-quality labeled training\ndata in this specialized domain is a substantial obstacle. Manual annotation is\ncostly and impractical for capturing users' vast vocabulary variations. To\naddress this, we introduce a novel approach that leverages large language\nmodels (LLMs) through weak supervision to automatically annotate a vast\ncollection of user search queries. Using prompt engineering and a diverse set\nof LLM personas, we generate training data that matches human annotator\nexpectations. By incorporating domain knowledge via Chain of Thought and\nIn-Context Learning, our approach leverages the labeled data to train\nlow-latency models optimized for real-time inference. Extensive evaluations\ndemonstrated that our approach outperformed the baseline with an average\nrelative gain of 113% in recall. Furthermore, our novel prompt engineering\nframework yields higher quality LLM-generated data to be used for weak\nsupervision; we observed 47.60% improvement over baseline in agreement rate\nbetween LLM predictions and human annotations with respect to F1 score,\nweighted according to the distribution of occurrences of the search queries.\nOur persona selection routing mechanism further adds an additional 3.67%\nincrease in weighted F1 score on top of our novel prompt engineering framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming services have reshaped how we discover and engage with digital\nentertainment. Despite these advancements, effectively understanding the wide\nspectrum of user search queries continues to pose a significant challenge. An\naccurate query understanding system that can handle a variety of entities that\nrepresent different user intents is essential for delivering an enhanced user\nexperience. We can build such a system by training a natural language\nunderstanding (NLU) model; however, obtaining high-quality labeled training\ndata in this specialized domain is a substantial obstacle. Manual annotation is\ncostly and impractical for capturing users' vast vocabulary variations. To\naddress this, we introduce a novel approach that leverages large language\nmodels (LLMs) through weak supervision to automatically annotate a vast\ncollection of user search queries. Using prompt engineering and a diverse set\nof LLM personas, we generate training data that matches human annotator\nexpectations. By incorporating domain knowledge via Chain of Thought and\nIn-Context Learning, our approach leverages the labeled data to train\nlow-latency models optimized for real-time inference. Extensive evaluations\ndemonstrated that our approach outperformed the baseline with an average\nrelative gain of 113% in recall. Furthermore, our novel prompt engineering\nframework yields higher quality LLM-generated data to be used for weak\nsupervision; we observed 47.60% improvement over baseline in agreement rate\nbetween LLM predictions and human annotations with respect to F1 score,\nweighted according to the distribution of occurrences of the search queries.\nOur persona selection routing mechanism further adds an additional 3.67%\nincrease in weighted F1 score on top of our novel prompt engineering framework."
                },
                "authors": [
                    {
                        "name": "Farnoosh Javadi"
                    },
                    {
                        "name": "Phanideep Gampa"
                    },
                    {
                        "name": "Alyssa Woo"
                    },
                    {
                        "name": "Xingxing Geng"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Jose Sepulveda"
                    },
                    {
                        "name": "Belhassen Bayar"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08928v1",
                "updated": "2024-09-13T15:45:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:45:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "Self-Organized State-Space Models with Artificial Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Organized State-Space Models with Artificial Dynamics"
                },
                "summary": "In this paper we consider a state-space model (SSM) parametrized by some\nparameter $\\theta$, and our aim is to perform joint parameter and state\ninference. A simple idea to perform this task, which almost dates back to the\norigin of the Kalman filter, is to replace the static parameter $\\theta$ by a\nMarkov chain $(\\theta_t)_{t\\geq 0}$ on the parameter space and then to apply a\nstandard filtering algorithm to the extended, or self-organized SSM. However,\nthe practical implementation of this idea in a theoretically justified way has\nremained an open problem. In this paper we fill this gap by introducing various\npossible constructions of the Markov chain $(\\theta_t)_{t\\geq 0}$ that ensure\nthe validity of the self-organized SSM (SO-SSM) for joint parameter and state\ninference. Notably, we show that theoretically valid SO-SSMs can be defined\neven if $\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|$ converges to 0 slowly as\n$t\\rightarrow\\infty$. This result is important since, as illustrated in our\nnumerical experiments, such models can be efficiently approximated using\nstandard particle filter algorithms. While the idea studied in this work was\nfirst introduced for online inference in SSMs, it has also been proved to be\nuseful for computing the maximum likelihood estimator (MLE) of a given SSM,\nsince iterated filtering algorithms can be seen as particle filters applied to\nSO-SSMs for which the target parameter value is the MLE of interest. Based on\nthis observation, we also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical results tailored to these specific applications of SO-SSMs, and as\na result, we introduce new iterated filtering algorithms. From a practical\npoint of view, the algorithms introduced in this work have the merit of being\nsimple to implement and only requiring minimal tuning to perform well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider a state-space model (SSM) parametrized by some\nparameter $\\theta$, and our aim is to perform joint parameter and state\ninference. A simple idea to perform this task, which almost dates back to the\norigin of the Kalman filter, is to replace the static parameter $\\theta$ by a\nMarkov chain $(\\theta_t)_{t\\geq 0}$ on the parameter space and then to apply a\nstandard filtering algorithm to the extended, or self-organized SSM. However,\nthe practical implementation of this idea in a theoretically justified way has\nremained an open problem. In this paper we fill this gap by introducing various\npossible constructions of the Markov chain $(\\theta_t)_{t\\geq 0}$ that ensure\nthe validity of the self-organized SSM (SO-SSM) for joint parameter and state\ninference. Notably, we show that theoretically valid SO-SSMs can be defined\neven if $\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|$ converges to 0 slowly as\n$t\\rightarrow\\infty$. This result is important since, as illustrated in our\nnumerical experiments, such models can be efficiently approximated using\nstandard particle filter algorithms. While the idea studied in this work was\nfirst introduced for online inference in SSMs, it has also been proved to be\nuseful for computing the maximum likelihood estimator (MLE) of a given SSM,\nsince iterated filtering algorithms can be seen as particle filters applied to\nSO-SSMs for which the target parameter value is the MLE of interest. Based on\nthis observation, we also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical results tailored to these specific applications of SO-SSMs, and as\na result, we introduce new iterated filtering algorithms. From a practical\npoint of view, the algorithms introduced in this work have the merit of being\nsimple to implement and only requiring minimal tuning to perform well."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Mathieu Gerber"
                    },
                    {
                        "name": "Christophe Andrieu"
                    },
                    {
                        "name": "Randal Douc"
                    }
                ],
                "author_detail": {
                    "name": "Randal Douc"
                },
                "author": "Randal Douc",
                "arxiv_comment": "115 pages (including a 77-page long appendix), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v2",
                "updated": "2024-09-13T15:45:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    10,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Coverage-Guided LLM-Based Test Generation"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains a challenge. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp iteratively improves test coverage, interleaving\ncoverage analysis with dialogs with the LLM that steer it to refine tests so\nthat they increase coverage of lines and branches. We evaluate our prototype\nCoverUp implementation across a benchmark of challenging code derived from\nopen-source Python projects, and show that CoverUp substantially improves on\nthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based test\ngenerator, CoverUp achieves a per-module median line+branch coverage of 80%\n(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUp\nachieves an overall line+branch coverage of 90% (vs. 77%). We show that\nCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,\ncontributing to nearly 40% of its successes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains a challenge. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp iteratively improves test coverage, interleaving\ncoverage analysis with dialogs with the LLM that steer it to refine tests so\nthat they increase coverage of lines and branches. We evaluate our prototype\nCoverUp implementation across a benchmark of challenging code derived from\nopen-source Python projects, and show that CoverUp substantially improves on\nthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based test\ngenerator, CoverUp achieves a per-module median line+branch coverage of 80%\n(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUp\nachieves an overall line+branch coverage of 90% (vs. 77%). We show that\nCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,\ncontributing to nearly 40% of its successes."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08924v1",
                "updated": "2024-09-13T15:39:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    39,
                    29,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:39:29Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    39,
                    29,
                    4,
                    257,
                    0
                ],
                "title": "Regression-based proximal causal inference for right-censored\n  time-to-event data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression-based proximal causal inference for right-censored\n  time-to-event data"
                },
                "summary": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'."
                },
                "authors": [
                    {
                        "name": "Kendrick Li"
                    },
                    {
                        "name": "George C. Linderman"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08904v1",
                "updated": "2024-09-13T15:15:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    15,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:15:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    15,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal\n  Robots Guided by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal\n  Robots Guided by Large Language Models"
                },
                "summary": "Training and deploying reinforcement learning (RL) policies for robots,\nespecially in accomplishing specific tasks, presents substantial challenges.\nRecent advancements have explored diverse reward function designs, training\ntechniques, simulation-to-reality (sim-to-real) transfers, and performance\nanalysis methodologies, yet these still require significant human intervention.\nThis paper introduces an end-to-end framework for training and deploying RL\npolicies, guided by Large Language Models (LLMs), and evaluates its\neffectiveness on bipedal robots. The framework consists of three interconnected\nmodules: an LLM-guided reward function design module, an RL training module\nleveraging prior work, and a sim-to-real homomorphic evaluation module. This\ndesign significantly reduces the need for human input by utilizing only\nessential simulation and deployment platforms, with the option to incorporate\nhuman-engineered strategies and historical data. We detail the construction of\nthese modules, their advantages over traditional approaches, and demonstrate\nthe framework's capability to autonomously develop and refine controlling\nstrategies for bipedal robot locomotion, showcasing its potential to operate\nindependently of human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying reinforcement learning (RL) policies for robots,\nespecially in accomplishing specific tasks, presents substantial challenges.\nRecent advancements have explored diverse reward function designs, training\ntechniques, simulation-to-reality (sim-to-real) transfers, and performance\nanalysis methodologies, yet these still require significant human intervention.\nThis paper introduces an end-to-end framework for training and deploying RL\npolicies, guided by Large Language Models (LLMs), and evaluates its\neffectiveness on bipedal robots. The framework consists of three interconnected\nmodules: an LLM-guided reward function design module, an RL training module\nleveraging prior work, and a sim-to-real homomorphic evaluation module. This\ndesign significantly reduces the need for human input by utilizing only\nessential simulation and deployment platforms, with the option to incorporate\nhuman-engineered strategies and historical data. We detail the construction of\nthese modules, their advantages over traditional approaches, and demonstrate\nthe framework's capability to autonomously develop and refine controlling\nstrategies for bipedal robot locomotion, showcasing its potential to operate\nindependently of human intervention."
                },
                "authors": [
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Wentao He"
                    },
                    {
                        "name": "Chenyu Gu"
                    },
                    {
                        "name": "Jiaheng Du"
                    },
                    {
                        "name": "Fuwei Tan"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Junguo Lu"
                    }
                ],
                "author_detail": {
                    "name": "Junguo Lu"
                },
                "author": "Junguo Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08899v1",
                "updated": "2024-09-13T15:12:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    12,
                    42,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:12:42Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    12,
                    42,
                    4,
                    257,
                    0
                ],
                "title": "Inferring hyperuniformity from local structures via persistent homology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring hyperuniformity from local structures via persistent homology"
                },
                "summary": "Hyperuniformity refers to the suppression of density fluctuations at large\nscales. Typical for ordered systems, this property also emerges in several\ndisordered physical and biological systems, where it is particularly relevant\nto understand mechanisms of pattern formation and to exploit peculiar\nattributes, e.g., interaction with light and transport phenomena. While\nhyperuniformity is a global property, it has been shown in [Phys. Rev. Research\n6, 023107 (2024)] that global hyperuniform characteristics systematically\ncorrelate with topological properties representative of local arrangements. In\nthis work, building on this information, we explore and assess the inverse\nrelationship between hyperuniformity and local structures in point\ndistributions as described by persistent homology. Standard machine learning\nalgorithms trained on persistence diagrams are shown to detect hyperuniformity\nwith high accuracy. Therefore, we demonstrate that the information on patterns'\nlocal structure allows for inferring hyperuniformity. Then, addressing more\nquantitative aspects, we show that parameters defining hyperuniformity\nglobally, for instance entering the structure factor, can be reconstructed by\ncomparing persistence diagrams of targeted patterns with reference ones. We\nalso explore the generation of patterns entailing given topological properties.\nThe results of this study pave the way for advanced analysis of hyperuniform\npatterns including local information, and introduce basic concepts for their\ninverse design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperuniformity refers to the suppression of density fluctuations at large\nscales. Typical for ordered systems, this property also emerges in several\ndisordered physical and biological systems, where it is particularly relevant\nto understand mechanisms of pattern formation and to exploit peculiar\nattributes, e.g., interaction with light and transport phenomena. While\nhyperuniformity is a global property, it has been shown in [Phys. Rev. Research\n6, 023107 (2024)] that global hyperuniform characteristics systematically\ncorrelate with topological properties representative of local arrangements. In\nthis work, building on this information, we explore and assess the inverse\nrelationship between hyperuniformity and local structures in point\ndistributions as described by persistent homology. Standard machine learning\nalgorithms trained on persistence diagrams are shown to detect hyperuniformity\nwith high accuracy. Therefore, we demonstrate that the information on patterns'\nlocal structure allows for inferring hyperuniformity. Then, addressing more\nquantitative aspects, we show that parameters defining hyperuniformity\nglobally, for instance entering the structure factor, can be reconstructed by\ncomparing persistence diagrams of targeted patterns with reference ones. We\nalso explore the generation of patterns entailing given topological properties.\nThe results of this study pave the way for advanced analysis of hyperuniform\npatterns including local information, and introduce basic concepts for their\ninverse design."
                },
                "authors": [
                    {
                        "name": "Abel H. G. Milor"
                    },
                    {
                        "name": "Marco Salvalaglio"
                    }
                ],
                "author_detail": {
                    "name": "Marco Salvalaglio"
                },
                "author": "Marco Salvalaglio",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08892v1",
                "updated": "2024-09-13T15:07:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    7,
                    22,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:07:22Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    7,
                    22,
                    4,
                    257,
                    0
                ],
                "title": "Exploring Action-Centric Representations Through the Lens of\n  Rate-Distortion Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Action-Centric Representations Through the Lens of\n  Rate-Distortion Theory"
                },
                "summary": "Organisms have to keep track of the information in the environment that is\nrelevant for adaptive behaviour. Transmitting information in an economical and\nefficient way becomes crucial for limited-resourced agents living in\nhigh-dimensional environments. The efficient coding hypothesis claims that\norganisms seek to maximize the information about the sensory input in an\nefficient manner. Under Bayesian inference, this means that the role of the\nbrain is to efficiently allocate resources in order to make predictions about\nthe hidden states that cause sensory data. However, neither of those frameworks\naccounts for how that information is exploited downstream, leaving aside the\naction-oriented role of the perceptual system. Rate-distortion theory, which\ndefines optimal lossy compression under constraints, has gained attention as a\nformal framework to explore goal-oriented efficient coding. In this work, we\nexplore action-centric representations in the context of rate-distortion\ntheory. We also provide a mathematical definition of abstractions and we argue\nthat, as a summary of the relevant details, they can be used to fix the content\nof action-centric representations. We model action-centric representations\nusing VAEs and we find that such representations i) are efficient lossy\ncompressions of the data; ii) capture the task-dependent invariances necessary\nto achieve successful behaviour; and iii) are not in service of reconstructing\nthe data. Thus, we conclude that full reconstruction of the data is rarely\nneeded to achieve optimal behaviour, consistent with a teleological approach to\nperception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organisms have to keep track of the information in the environment that is\nrelevant for adaptive behaviour. Transmitting information in an economical and\nefficient way becomes crucial for limited-resourced agents living in\nhigh-dimensional environments. The efficient coding hypothesis claims that\norganisms seek to maximize the information about the sensory input in an\nefficient manner. Under Bayesian inference, this means that the role of the\nbrain is to efficiently allocate resources in order to make predictions about\nthe hidden states that cause sensory data. However, neither of those frameworks\naccounts for how that information is exploited downstream, leaving aside the\naction-oriented role of the perceptual system. Rate-distortion theory, which\ndefines optimal lossy compression under constraints, has gained attention as a\nformal framework to explore goal-oriented efficient coding. In this work, we\nexplore action-centric representations in the context of rate-distortion\ntheory. We also provide a mathematical definition of abstractions and we argue\nthat, as a summary of the relevant details, they can be used to fix the content\nof action-centric representations. We model action-centric representations\nusing VAEs and we find that such representations i) are efficient lossy\ncompressions of the data; ii) capture the task-dependent invariances necessary\nto achieve successful behaviour; and iii) are not in service of reconstructing\nthe data. Thus, we conclude that full reconstruction of the data is rarely\nneeded to achieve optimal behaviour, consistent with a teleological approach to\nperception."
                },
                "authors": [
                    {
                        "name": "Miguel de Llanza Varona"
                    },
                    {
                        "name": "Christopher L. Buckley"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "arxiv_journal_ref": "4th International Workshop on Active Inference, 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2108.04674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2108.04674v2",
                "updated": "2024-09-13T15:00:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    0,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2021-08-10T13:25:29Z",
                "published_parsed": [
                    2021,
                    8,
                    10,
                    13,
                    25,
                    29,
                    1,
                    222,
                    0
                ],
                "title": "Natural Language Processing with Commonsense Knowledge: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing with Commonsense Knowledge: A Survey"
                },
                "summary": "Commonsense knowledge is essential for advancing natural language processing\n(NLP) by enabling models to engage in human-like reasoning, which requires a\ndeeper understanding of context and often involves making inferences based on\nimplicit external knowledge. This paper explores the integration of commonsense\nknowledge into various NLP tasks. We begin by reviewing prominent commonsense\nknowledge bases and then discuss the benchmarks used to evaluate the\ncommonsense reasoning capabilities of NLP models, particularly language models.\nFurthermore, we highlight key methodologies for incorporating commonsense\nknowledge and their applications across different NLP tasks. The paper also\nexamines the challenges and emerging trends in enhancing NLP systems with\ncommonsense reasoning. All literature referenced in this survey can be accessed\nvia our GitHub repository: https://github.com/yuboxie/awesome-commonsense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense knowledge is essential for advancing natural language processing\n(NLP) by enabling models to engage in human-like reasoning, which requires a\ndeeper understanding of context and often involves making inferences based on\nimplicit external knowledge. This paper explores the integration of commonsense\nknowledge into various NLP tasks. We begin by reviewing prominent commonsense\nknowledge bases and then discuss the benchmarks used to evaluate the\ncommonsense reasoning capabilities of NLP models, particularly language models.\nFurthermore, we highlight key methodologies for incorporating commonsense\nknowledge and their applications across different NLP tasks. The paper also\nexamines the challenges and emerging trends in enhancing NLP systems with\ncommonsense reasoning. All literature referenced in this survey can be accessed\nvia our GitHub repository: https://github.com/yuboxie/awesome-commonsense."
                },
                "authors": [
                    {
                        "name": "Yubo Xie"
                    },
                    {
                        "name": "Zonghui Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Fanyuan Meng"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Fahui Miao"
                    },
                    {
                        "name": "Pearl Pu"
                    }
                ],
                "author_detail": {
                    "name": "Pearl Pu"
                },
                "author": "Pearl Pu",
                "arxiv_comment": "20 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2108.04674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2108.04674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02331v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02331v3",
                "updated": "2024-09-13T14:55:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    55,
                    12,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-03T23:04:28Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    23,
                    4,
                    28,
                    1,
                    247,
                    0
                ],
                "title": "A parameterization of anisotropic Gaussian fields with penalized\n  complexity priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A parameterization of anisotropic Gaussian fields with penalized\n  complexity priors"
                },
                "summary": "Gaussian random fields (GFs) are fundamental tools in spatial modeling and\ncan be represented flexibly and efficiently as solutions to stochastic partial\ndifferential equations (SPDEs). The SPDEs depend on specific parameters, which\nenforce various field behaviors and can be estimated using Bayesian inference.\nHowever, the likelihood typically only provides limited insights into the\ncovariance structure under in-fill asymptotics. In response, it is essential to\nleverage priors to achieve appropriate, meaningful covariance structures in the\nposterior. This study introduces a smooth, invertible parameterization of the\ncorrelation length and diffusion matrix of an anisotropic GF and constructs\npenalized complexity (PC) priors for the model when the parameters are constant\nin space. The formulated prior is weakly informative, effectively penalizing\ncomplexity by pushing the correlation range toward infinity and the anisotropy\nto zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian random fields (GFs) are fundamental tools in spatial modeling and\ncan be represented flexibly and efficiently as solutions to stochastic partial\ndifferential equations (SPDEs). The SPDEs depend on specific parameters, which\nenforce various field behaviors and can be estimated using Bayesian inference.\nHowever, the likelihood typically only provides limited insights into the\ncovariance structure under in-fill asymptotics. In response, it is essential to\nleverage priors to achieve appropriate, meaningful covariance structures in the\nposterior. This study introduces a smooth, invertible parameterization of the\ncorrelation length and diffusion matrix of an anisotropic GF and constructs\npenalized complexity (PC) priors for the model when the parameters are constant\nin space. The formulated prior is weakly informative, effectively penalizing\ncomplexity by pushing the correlation range toward infinity and the anisotropy\nto zero."
                },
                "authors": [
                    {
                        "name": "Liam Llamazares-Elias"
                    },
                    {
                        "name": "Jonas Latz"
                    },
                    {
                        "name": "Finn Lindgren"
                    }
                ],
                "author_detail": {
                    "name": "Finn Lindgren"
                },
                "author": "Finn Lindgren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02331v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02331v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08887v1",
                "updated": "2024-09-13T14:54:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    54,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:54:37Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    54,
                    37,
                    4,
                    257,
                    0
                ],
                "title": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark"
                },
                "summary": "Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.13918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.13918v3",
                "updated": "2024-09-13T14:48:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    48,
                    57,
                    4,
                    257,
                    0
                ],
                "published": "2022-09-28T08:46:48Z",
                "published_parsed": [
                    2022,
                    9,
                    28,
                    8,
                    46,
                    48,
                    2,
                    271,
                    0
                ],
                "title": "Inference in generalized linear models with robustness to misspecified\n  variances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in generalized linear models with robustness to misspecified\n  variances"
                },
                "summary": "Generalized linear models usually assume a common dispersion parameter, an\nassumption that is seldom true in practice. Consequently, standard parametric\nmethods may suffer appreciable loss of type I error control. As an alternative,\nwe present a semi-parametric group-invariance method based on sign flipping of\nscore contributions. Our method requires only the correct specification of the\nmean model, but is robust against any misspecification of the variance. We\npresent tests for single as well as multiple regression coefficients. The test\nis asymptotically valid but shows excellent performance in small samples. We\nillustrate the method using RNA sequencing count data, for which it is\ndifficult to model the overdispersion correctly. The method is available in the\nR library flipscores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized linear models usually assume a common dispersion parameter, an\nassumption that is seldom true in practice. Consequently, standard parametric\nmethods may suffer appreciable loss of type I error control. As an alternative,\nwe present a semi-parametric group-invariance method based on sign flipping of\nscore contributions. Our method requires only the correct specification of the\nmean model, but is robust against any misspecification of the variance. We\npresent tests for single as well as multiple regression coefficients. The test\nis asymptotically valid but shows excellent performance in small samples. We\nillustrate the method using RNA sequencing count data, for which it is\ndifficult to model the overdispersion correctly. The method is available in the\nR library flipscores."
                },
                "authors": [
                    {
                        "name": "Riccardo De Santis"
                    },
                    {
                        "name": "Jelle J. Goeman"
                    },
                    {
                        "name": "Jesse Hemerik"
                    },
                    {
                        "name": "Samuel Davenport"
                    },
                    {
                        "name": "Livio Finos"
                    }
                ],
                "author_detail": {
                    "name": "Livio Finos"
                },
                "author": "Livio Finos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.13918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.13918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03275v2",
                "updated": "2024-09-13T14:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    42,
                    8,
                    4,
                    257,
                    0
                ],
                "published": "2024-04-04T07:59:24Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    7,
                    59,
                    24,
                    3,
                    95,
                    0
                ],
                "title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Ilche Georgievski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08864v1",
                "updated": "2024-09-13T14:26:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    26,
                    58,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:26:58Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    26,
                    58,
                    4,
                    257,
                    0
                ],
                "title": "Exploring Graph Structure Comprehension Ability of Multimodal Large\n  Language Models: Case Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Graph Structure Comprehension Ability of Multimodal Large\n  Language Models: Case Studies"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in processing\nvarious data structures, including graphs. While previous research has focused\non developing textual encoding methods for graph representation, the emergence\nof multimodal LLMs presents a new frontier for graph comprehension. These\nadvanced models, capable of processing both text and images, offer potential\nimprovements in graph understanding by incorporating visual representations\nalongside traditional textual data. This study investigates the impact of graph\nvisualisations on LLM performance across a range of benchmark tasks at node,\nedge, and graph levels. Our experiments compare the effectiveness of multimodal\napproaches against purely textual graph representations. The results provide\nvaluable insights into both the potential and limitations of leveraging visual\ngraph modalities to enhance LLMs' graph structure comprehension abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in processing\nvarious data structures, including graphs. While previous research has focused\non developing textual encoding methods for graph representation, the emergence\nof multimodal LLMs presents a new frontier for graph comprehension. These\nadvanced models, capable of processing both text and images, offer potential\nimprovements in graph understanding by incorporating visual representations\nalongside traditional textual data. This study investigates the impact of graph\nvisualisations on LLM performance across a range of benchmark tasks at node,\nedge, and graph levels. Our experiments compare the effectiveness of multimodal\napproaches against purely textual graph representations. The results provide\nvaluable insights into both the potential and limitations of leveraging visual\ngraph modalities to enhance LLMs' graph structure comprehension abilities."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Zhong"
                    },
                    {
                        "name": "Davide Mottin"
                    }
                ],
                "author_detail": {
                    "name": "Davide Mottin"
                },
                "author": "Davide Mottin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10127v2",
                "updated": "2024-09-13T14:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    21,
                    5,
                    4,
                    257,
                    0
                ],
                "published": "2024-05-16T14:26:53Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    14,
                    26,
                    53,
                    3,
                    137,
                    0
                ],
                "title": "Mergers of hairy black holes: Constraining topological couplings from\n  entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mergers of hairy black holes: Constraining topological couplings from\n  entropy"
                },
                "summary": "Hairy black-holes are a unique prediction of certain theories that extend\nGeneral Relativity (GR) with a scalar field. The presence of scalar hair is\nreflected non-trivially in the entropy of the black hole along with any\ntopological coupling that may be present in the action. Demanding that a system\nof two merging black holes obeys the global second law of thermodynamics\nimposes a bound on this topological coupling coefficient. In this work we study\nhow this bound is pushed from its GR value by the presence of scalar hair by\nconsidering estimates of binary black-hole merger parameters through inference\nstudies of both mock and real gravitational-wave (GW) events. Although the\nscalar charge may produce a statistically significant deviation of the change\nin entropy over the GR prediction, we find no evidence of this happening in the\ndata from real GW events taken from GWTC-1. We also find the entropy change to\nbe susceptible to biases arising out of GW inferences which ends up being two\norders of magnitude larger, therefore overwhelming any change, if at all,\ninduced by the scalar hair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hairy black-holes are a unique prediction of certain theories that extend\nGeneral Relativity (GR) with a scalar field. The presence of scalar hair is\nreflected non-trivially in the entropy of the black hole along with any\ntopological coupling that may be present in the action. Demanding that a system\nof two merging black holes obeys the global second law of thermodynamics\nimposes a bound on this topological coupling coefficient. In this work we study\nhow this bound is pushed from its GR value by the presence of scalar hair by\nconsidering estimates of binary black-hole merger parameters through inference\nstudies of both mock and real gravitational-wave (GW) events. Although the\nscalar charge may produce a statistically significant deviation of the change\nin entropy over the GR prediction, we find no evidence of this happening in the\ndata from real GW events taken from GWTC-1. We also find the entropy change to\nbe susceptible to biases arising out of GW inferences which ends up being two\norders of magnitude larger, therefore overwhelming any change, if at all,\ninduced by the scalar hair."
                },
                "authors": [
                    {
                        "name": "Kabir Chakravarti"
                    },
                    {
                        "name": "Amit Reza"
                    },
                    {
                        "name": "Leonardo G. Trombetta"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo G. Trombetta"
                },
                "author": "Leonardo G. Trombetta",
                "arxiv_doi": "10.1103/PhysRevD.110.064032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.064032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.10127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 6 figures. Published version",
                "arxiv_journal_ref": "Phys. Rev. D 110, 064032 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08846v1",
                "updated": "2024-09-13T14:04:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition"
                },
                "summary": "Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io ."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Chang Hu"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08845v1",
                "updated": "2024-09-13T14:03:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    3,
                    49,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    3,
                    49,
                    4,
                    257,
                    0
                ],
                "title": "AIPO: Improving Training Objective for Iterative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPO: Improving Training Objective for Iterative Preference Optimization"
                },
                "summary": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO."
                },
                "authors": [
                    {
                        "name": "Yaojie Shen"
                    },
                    {
                        "name": "Xinyao Wang"
                    },
                    {
                        "name": "Yulei Niu"
                    },
                    {
                        "name": "Ying Zhou"
                    },
                    {
                        "name": "Lexin Tang"
                    },
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Longyin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Longyin Wen"
                },
                "author": "Longyin Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05462v2",
                "updated": "2024-09-13T13:36:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    36,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-09T09:42:46Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    42,
                    46,
                    0,
                    253,
                    0
                ],
                "title": "Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning"
                },
                "summary": "For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples."
                },
                "authors": [
                    {
                        "name": "Jibin Jia"
                    },
                    {
                        "name": "Peihao Dong"
                    },
                    {
                        "name": "Fuhui Zhou"
                    },
                    {
                        "name": "Qihui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Wu"
                },
                "author": "Qihui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08822v1",
                "updated": "2024-09-13T13:36:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    36,
                    30,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:36:30Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    36,
                    30,
                    4,
                    257,
                    0
                ],
                "title": "Development of a Compton Imager Setup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Compton Imager Setup"
                },
                "summary": "Hard X-ray photons with energies in the range of hundreds of keV typically\nundergo Compton scattering when they are incident on a detector. In this\nprocess, an incident photon deposits a fraction of its energy at the point of\nincidence and continues onward with a change in direction that depends on the\namount of energy deposited. By using a pair of detectors to detect the point of\nincidence and the direction of the scattered photon, we can calculate the\nscattering direction and angle. The position of a source in the sky can be\nreconstructed using many Compton photon pairs from a source. We demonstrate\nthis principle in the laboratory by using a pair of Cadmium Zinc Telluride\ndetectors sensitive in the energy range of 20-200 keV. The laboratory setup\nconsists of the two detectors placed perpendicular to each other in a\nlead-lined box. The detectors are read out by a custom-programmed Xilinx PYNQ\nFPGA board, and data is then transferred to a PC. The detectors are first\ncalibrated using lines from $^{241}\\mathrm{Am}$, $^{155}\\mathrm{Eu}$ and\n$^{133}\\mathrm{Ba}$ sources. We irradiated the detectors with a collimated\n$^{133}\\mathrm{Ba}$ source and identified Compton scattering events for the 356\nkeV line. We run a Compton reconstruction algorithm and correctly infer the\nlocation of the source in the detector frame. This comprises a successful\ntechnology demonstration for a Compton imaging camera in the Hard X-ray regime.\nWe present the details of our setup, the data acquisition process, and software\nalgorithms, and showcase our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard X-ray photons with energies in the range of hundreds of keV typically\nundergo Compton scattering when they are incident on a detector. In this\nprocess, an incident photon deposits a fraction of its energy at the point of\nincidence and continues onward with a change in direction that depends on the\namount of energy deposited. By using a pair of detectors to detect the point of\nincidence and the direction of the scattered photon, we can calculate the\nscattering direction and angle. The position of a source in the sky can be\nreconstructed using many Compton photon pairs from a source. We demonstrate\nthis principle in the laboratory by using a pair of Cadmium Zinc Telluride\ndetectors sensitive in the energy range of 20-200 keV. The laboratory setup\nconsists of the two detectors placed perpendicular to each other in a\nlead-lined box. The detectors are read out by a custom-programmed Xilinx PYNQ\nFPGA board, and data is then transferred to a PC. The detectors are first\ncalibrated using lines from $^{241}\\mathrm{Am}$, $^{155}\\mathrm{Eu}$ and\n$^{133}\\mathrm{Ba}$ sources. We irradiated the detectors with a collimated\n$^{133}\\mathrm{Ba}$ source and identified Compton scattering events for the 356\nkeV line. We run a Compton reconstruction algorithm and correctly infer the\nlocation of the source in the detector frame. This comprises a successful\ntechnology demonstration for a Compton imaging camera in the Hard X-ray regime.\nWe present the details of our setup, the data acquisition process, and software\nalgorithms, and showcase our results."
                },
                "authors": [
                    {
                        "name": "Anuraag Arya"
                    },
                    {
                        "name": "Harmanjeet Singh Bilkhu"
                    },
                    {
                        "name": "Sandeep Vishwakarma"
                    },
                    {
                        "name": "Hrishikesh Belatikar"
                    },
                    {
                        "name": "Varun Bhalerao"
                    },
                    {
                        "name": "Abhijeet Ghodgaonkar"
                    },
                    {
                        "name": "Jayprakash G. Koyande"
                    },
                    {
                        "name": "Aditi Marathe"
                    },
                    {
                        "name": "N. P. S. Mithun"
                    },
                    {
                        "name": "Sanjoli Narang"
                    },
                    {
                        "name": "Sudhanshu Nimbalkar"
                    },
                    {
                        "name": "Pranav Page"
                    },
                    {
                        "name": "Sourav Palit"
                    },
                    {
                        "name": "Arpit Patel"
                    },
                    {
                        "name": "Amit Shetye"
                    },
                    {
                        "name": "Siddharth Tallur"
                    },
                    {
                        "name": "Shriharsh Tendulkar"
                    },
                    {
                        "name": "Santosh Vadawale"
                    },
                    {
                        "name": "Gaurav Waratkar"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Waratkar"
                },
                "author": "Gaurav Waratkar",
                "arxiv_comment": "17 pages, 10 figures, submitted for publication in Journal of\n  Astronomical Telescopes, Instruments, and Systems (JATIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03824v4",
                "updated": "2024-09-13T13:36:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    36,
                    5,
                    4,
                    257,
                    0
                ],
                "published": "2024-02-06T09:11:20Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    9,
                    11,
                    20,
                    1,
                    37,
                    0
                ],
                "title": "A call for embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A call for embodied AI"
                },
                "summary": "We propose Embodied AI as the next fundamental step in the pursuit of\nArtificial General Intelligence, juxtaposing it against current AI\nadvancements, particularly Large Language Models. We traverse the evolution of\nthe embodiment concept across diverse fields - philosophy, psychology,\nneuroscience, and robotics - to highlight how EAI distinguishes itself from the\nclassical paradigm of static learning. By broadening the scope of Embodied AI,\nwe introduce a theoretical framework based on cognitive architectures,\nemphasizing perception, action, memory, and learning as essential components of\nan embodied agent. This framework is aligned with Friston's active inference\nprinciple, offering a comprehensive approach to EAI development. Despite the\nprogress made in the field of AI, substantial challenges, such as the\nformulation of a novel AI learning theory and the innovation of advanced\nhardware, persist. Our discussion lays down a foundational guideline for future\nEmbodied AI research. Highlighting the importance of creating Embodied AI\nagents capable of seamless communication, collaboration, and coexistence with\nhumans and other intelligent entities within real-world environments, we aim to\nsteer the AI community towards addressing the multifaceted challenges and\nseizing the opportunities that lie ahead in the quest for AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Embodied AI as the next fundamental step in the pursuit of\nArtificial General Intelligence, juxtaposing it against current AI\nadvancements, particularly Large Language Models. We traverse the evolution of\nthe embodiment concept across diverse fields - philosophy, psychology,\nneuroscience, and robotics - to highlight how EAI distinguishes itself from the\nclassical paradigm of static learning. By broadening the scope of Embodied AI,\nwe introduce a theoretical framework based on cognitive architectures,\nemphasizing perception, action, memory, and learning as essential components of\nan embodied agent. This framework is aligned with Friston's active inference\nprinciple, offering a comprehensive approach to EAI development. Despite the\nprogress made in the field of AI, substantial challenges, such as the\nformulation of a novel AI learning theory and the innovation of advanced\nhardware, persist. Our discussion lays down a foundational guideline for future\nEmbodied AI research. Highlighting the importance of creating Embodied AI\nagents capable of seamless communication, collaboration, and coexistence with\nhumans and other intelligent entities within real-world environments, we aim to\nsteer the AI community towards addressing the multifaceted challenges and\nseizing the opportunities that lie ahead in the quest for AGI."
                },
                "authors": [
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Jonas Gonzalez-Billandon"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "arxiv_comment": "Published in ICML 2024 Position paper track",
                "arxiv_journal_ref": "PMLR 235:39493-39508, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08820v1",
                "updated": "2024-09-13T13:34:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    34,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:34:32Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    34,
                    32,
                    4,
                    257,
                    0
                ],
                "title": "A RAG Approach for Generating Competency Questions in Ontology\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG Approach for Generating Competency Questions in Ontology\n  Engineering"
                },
                "summary": "Competency question (CQ) formulation is central to several ontology\ndevelopment and evaluation methodologies. Traditionally, the task of crafting\nthese competency questions heavily relies on the effort of domain experts and\nknowledge engineers which is often time-consuming and labor-intensive. With the\nemergence of Large Language Models (LLMs), there arises the possibility to\nautomate and enhance this process. Unlike other similar works which use\nexisting ontologies or knowledge graphs as input to LLMs, we present a\nretrieval-augmented generation (RAG) approach that uses LLMs for the automatic\ngeneration of CQs given a set of scientific papers considered to be a domain\nknowledge base. We investigate its performance and specifically, we study the\nimpact of different number of papers to the RAG and different temperature\nsetting of the LLM. We conduct experiments using GPT-4 on two domain ontology\nengineering tasks and compare results against ground-truth CQs constructed by\ndomain experts. Empirical assessments on the results, utilizing evaluation\nmetrics (precision and consistency), reveal that compared to zero-shot\nprompting, adding relevant domain knowledge to the RAG improves the performance\nof LLMs on generating CQs for concrete ontology engineering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competency question (CQ) formulation is central to several ontology\ndevelopment and evaluation methodologies. Traditionally, the task of crafting\nthese competency questions heavily relies on the effort of domain experts and\nknowledge engineers which is often time-consuming and labor-intensive. With the\nemergence of Large Language Models (LLMs), there arises the possibility to\nautomate and enhance this process. Unlike other similar works which use\nexisting ontologies or knowledge graphs as input to LLMs, we present a\nretrieval-augmented generation (RAG) approach that uses LLMs for the automatic\ngeneration of CQs given a set of scientific papers considered to be a domain\nknowledge base. We investigate its performance and specifically, we study the\nimpact of different number of papers to the RAG and different temperature\nsetting of the LLM. We conduct experiments using GPT-4 on two domain ontology\nengineering tasks and compare results against ground-truth CQs constructed by\ndomain experts. Empirical assessments on the results, utilizing evaluation\nmetrics (precision and consistency), reveal that compared to zero-shot\nprompting, adding relevant domain knowledge to the RAG improves the performance\nof LLMs on generating CQs for concrete ontology engineering tasks."
                },
                "authors": [
                    {
                        "name": "Xueli Pan"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    },
                    {
                        "name": "Victor de Boer"
                    },
                    {
                        "name": "Zhisheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhisheng Huang"
                },
                "author": "Zhisheng Huang",
                "arxiv_journal_ref": "MTST2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08813v1",
                "updated": "2024-09-13T13:24:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:24:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Weak LLM is Secretly a Strong Teacher for Alignment"
                },
                "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08811v1",
                "updated": "2024-09-13T13:19:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    19,
                    48,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:19:48Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    19,
                    48,
                    4,
                    257,
                    0
                ],
                "title": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with\n  LLM-driven AI Agents in a Real-time Shared Workspace Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with\n  LLM-driven AI Agents in a Real-time Shared Workspace Task"
                },
                "summary": "Theory of Mind (ToM) significantly impacts human collaboration and\ncommunication as a crucial capability to understand others. When AI agents with\nToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in\nsuch human-AI teams (HATs). The MToM process, which involves interactive\ncommunication and ToM-based strategy adjustment, affects the team's performance\nand collaboration process. To explore the MToM process, we conducted a\nmixed-design experiment using a large language model-driven AI agent with ToM\nand communication modules in a real-time shared-workspace task. We find that\nthe agent's ToM capability does not significantly impact team performance but\nenhances human understanding of the agent and the feeling of being understood.\nMost participants in our study believe verbal communication increases human\nburden, and the results show that bidirectional communication leads to lower\nHAT performance. We discuss the results' implications for designing AI agents\nthat collaborate with humans in real-time shared workspace tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) significantly impacts human collaboration and\ncommunication as a crucial capability to understand others. When AI agents with\nToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in\nsuch human-AI teams (HATs). The MToM process, which involves interactive\ncommunication and ToM-based strategy adjustment, affects the team's performance\nand collaboration process. To explore the MToM process, we conducted a\nmixed-design experiment using a large language model-driven AI agent with ToM\nand communication modules in a real-time shared-workspace task. We find that\nthe agent's ToM capability does not significantly impact team performance but\nenhances human understanding of the agent and the feeling of being understood.\nMost participants in our study believe verbal communication increases human\nburden, and the results show that bidirectional communication leads to lower\nHAT performance. We discuss the results' implications for designing AI agents\nthat collaborate with humans in real-time shared workspace tasks."
                },
                "authors": [
                    {
                        "name": "Shao Zhang"
                    },
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Yongshan Chen"
                    },
                    {
                        "name": "Landi Gao"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "arxiv_comment": "34 pages, Preprint Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02373v2",
                "updated": "2024-09-13T13:09:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    9,
                    41,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-05T10:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    10,
                    53,
                    51,
                    0,
                    218,
                    0
                ],
                "title": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants"
                },
                "summary": "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize contextual integrity (CI), a\nframework that equates privacy with the appropriate flow of information in a\ngiven context. In particular, we design and evaluate a number of strategies to\nsteer assistants' information-sharing actions to be CI compliant. Our\nevaluation is based on a novel form filling benchmark composed of human\nannotations of common webform applications, and it reveals that prompting\nfrontier LLMs to perform CI-based reasoning yields strong results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize contextual integrity (CI), a\nframework that equates privacy with the appropriate flow of information in a\ngiven context. In particular, we design and evaluate a number of strategies to\nsteer assistants' information-sharing actions to be CI compliant. Our\nevaluation is based on a novel form filling benchmark composed of human\nannotations of common webform applications, and it reveals that prompting\nfrontier LLMs to perform CI-based reasoning yields strong results."
                },
                "authors": [
                    {
                        "name": "Sahra Ghalebikesabi"
                    },
                    {
                        "name": "Eugene Bagdasaryan"
                    },
                    {
                        "name": "Ren Yi"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Aneesh Pappu"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Laura Weidinger"
                    },
                    {
                        "name": "Robert Stanforth"
                    },
                    {
                        "name": "Leonard Berrada"
                    },
                    {
                        "name": "Pushmeet Kohli"
                    },
                    {
                        "name": "Po-Sen Huang"
                    },
                    {
                        "name": "Borja Balle"
                    }
                ],
                "author_detail": {
                    "name": "Borja Balle"
                },
                "author": "Borja Balle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08795v1",
                "updated": "2024-09-13T12:59:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment"
                },
                "summary": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data."
                },
                "authors": [
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Vincent Cheung"
                    },
                    {
                        "name": "Hayato Nishioka"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Shinichi Furuya"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Furuya"
                },
                "author": "Shinichi Furuya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08792v1",
                "updated": "2024-09-13T12:55:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    55,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:55:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    55,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes"
                },
                "summary": "In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science."
                },
                "authors": [
                    {
                        "name": "Luis Rita"
                    },
                    {
                        "name": "Josh Southern"
                    },
                    {
                        "name": "Ivan Laponogov"
                    },
                    {
                        "name": "Kyle Higgins"
                    },
                    {
                        "name": "Kirill Veselkov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Veselkov"
                },
                "author": "Kirill Veselkov",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08788v1",
                "updated": "2024-09-13T12:50:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    50,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:50:36Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    50,
                    36,
                    4,
                    257,
                    0
                ],
                "title": "Electrocardiogram Report Generation and Question Answering via\n  Retrieval-Augmented Self-Supervised Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram Report Generation and Question Answering via\n  Retrieval-Augmented Self-Supervised Modeling"
                },
                "summary": "Interpreting electrocardiograms (ECGs) and generating comprehensive reports\nremain challenging tasks in cardiology, often requiring specialized expertise\nand significant time investment. To address these critical issues, we propose\nECG-ReGen, a retrieval-based approach for ECG-to-text report generation and\nquestion answering. Our method leverages a self-supervised learning for the ECG\nencoder, enabling efficient similarity searches and report retrieval. By\ncombining pre-training with dynamic retrieval and Large Language Model\n(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers\nrelated queries, with the potential of improving patient care. Experiments\nconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior\nperformance in both in-domain and cross-domain scenarios for report generation.\nFurthermore, our approach exhibits competitive performance on ECG-QA dataset\ncompared to fully supervised methods when utilizing off-the-shelf LLMs for\nzero-shot question answering. This approach, effectively combining\nself-supervised encoder and LLMs, offers a scalable and efficient solution for\naccurate ECG interpretation, holding significant potential to enhance clinical\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting electrocardiograms (ECGs) and generating comprehensive reports\nremain challenging tasks in cardiology, often requiring specialized expertise\nand significant time investment. To address these critical issues, we propose\nECG-ReGen, a retrieval-based approach for ECG-to-text report generation and\nquestion answering. Our method leverages a self-supervised learning for the ECG\nencoder, enabling efficient similarity searches and report retrieval. By\ncombining pre-training with dynamic retrieval and Large Language Model\n(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers\nrelated queries, with the potential of improving patient care. Experiments\nconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior\nperformance in both in-domain and cross-domain scenarios for report generation.\nFurthermore, our approach exhibits competitive performance on ECG-QA dataset\ncompared to fully supervised methods when utilizing off-the-shelf LLMs for\nzero-shot question answering. This approach, effectively combining\nself-supervised encoder and LLMs, offers a scalable and efficient solution for\naccurate ECG interpretation, holding significant potential to enhance clinical\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v1",
                "updated": "2024-09-13T12:34:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What You Say = What You Want? Teaching Humans to Articulate Requirements\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You Say = What You Want? Teaching Humans to Articulate Requirements\n  for LLMs"
                },
                "summary": "Prompting ChatGPT to achieve complex goals (e.g., creating a customer support\nchatbot) often demands meticulous prompt engineering, including aspects like\nfluent writing and chain-of-thought techniques. While emerging prompt\noptimizers can automatically refine many of these aspects, we argue that\nclearly conveying customized requirements (e.g., how to handle diverse inputs)\nremains a human-centric challenge. In this work, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a study with 30 novices, we\nshow that requirement-focused training doubles novices' prompting performance,\nsignificantly outperforming conventional prompt engineering training and prompt\noptimization. We also demonstrate that high-quality LLM outputs are directly\ntied to the quality of input requirements. Our work paves the way for more\neffective task delegation in human-LLM collaborative prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting ChatGPT to achieve complex goals (e.g., creating a customer support\nchatbot) often demands meticulous prompt engineering, including aspects like\nfluent writing and chain-of-thought techniques. While emerging prompt\noptimizers can automatically refine many of these aspects, we argue that\nclearly conveying customized requirements (e.g., how to handle diverse inputs)\nremains a human-centric challenge. In this work, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a study with 30 novices, we\nshow that requirement-focused training doubles novices' prompting performance,\nsignificantly outperforming conventional prompt engineering training and prompt\noptimization. We also demonstrate that high-quality LLM outputs are directly\ntied to the quality of input requirements. Our work paves the way for more\neffective task delegation in human-LLM collaborative prompting."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v4",
                "updated": "2024-09-13T12:28:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    28,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16318v2",
                "updated": "2024-09-13T12:27:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    27,
                    44,
                    4,
                    257,
                    0
                ],
                "published": "2024-01-29T17:21:41Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    17,
                    21,
                    41,
                    0,
                    29,
                    0
                ],
                "title": "Defining and Extracting generalizable interaction primitives from DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Extracting generalizable interaction primitives from DNNs"
                },
                "summary": "Faithfully summarizing the knowledge encoded by a deep neural network (DNN)\ninto a few symbolic primitive patterns without losing much information\nrepresents a core challenge in explainable AI. To this end, Ren et al. (2024)\nhave derived a series of theorems to prove that the inference score of a DNN\ncan be explained as a small set of interactions between input variables.\nHowever, the lack of generalization power makes it still hard to consider such\ninteractions as faithful primitive patterns encoded by the DNN. Therefore,\ngiven different DNNs trained for the same task, we develop a new method to\nextract interactions that are shared by these DNNs. Experiments show that the\nextracted interactions can better reflect common knowledge shared by different\nDNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfully summarizing the knowledge encoded by a deep neural network (DNN)\ninto a few symbolic primitive patterns without losing much information\nrepresents a core challenge in explainable AI. To this end, Ren et al. (2024)\nhave derived a series of theorems to prove that the inference score of a DNN\ncan be explained as a small set of interactions between input variables.\nHowever, the lack of generalization power makes it still hard to consider such\ninteractions as faithful primitive patterns encoded by the DNN. Therefore,\ngiven different DNNs trained for the same task, we develop a new method to\nextract interactions that are shared by these DNNs. Experiments show that the\nextracted interactions can better reflect common knowledge shared by different\nDNNs."
                },
                "authors": [
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Siyu Lou"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08769v1",
                "updated": "2024-09-13T12:21:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    21,
                    25,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:21:25Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    21,
                    25,
                    4,
                    257,
                    0
                ],
                "title": "Causal Transformer for Fusion and Pose Estimation in Deep Visual\n  Inertial Odometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Transformer for Fusion and Pose Estimation in Deep Visual\n  Inertial Odometry"
                },
                "summary": "In recent years, transformer-based architectures become the de facto standard\nfor sequence modeling in deep learning frameworks. Inspired by the successful\nexamples, we propose a causal visual-inertial fusion transformer (VIFT) for\npose estimation in deep visual-inertial odometry. This study aims to improve\npose estimation accuracy by leveraging the attention mechanisms in\ntransformers, which better utilize historical data compared to the recurrent\nneural network (RNN) based methods seen in recent methods. Transformers\ntypically require large-scale data for training. To address this issue, we\nutilize inductive biases for deep VIO networks. Since latent visual-inertial\nfeature vectors encompass essential information for pose estimation, we employ\ntransformers to refine pose estimates by updating latent vectors temporally.\nOur study also examines the impact of data imbalance and rotation learning\nmethods in supervised end-to-end learning of visual inertial odometry by\nutilizing specialized gradients in backpropagation for the elements of SE$(3)$\ngroup. The proposed method is end-to-end trainable and requires only a\nmonocular camera and IMU during inference. Experimental results demonstrate\nthat VIFT increases the accuracy of monocular VIO networks, achieving\nstate-of-the-art results when compared to previous methods on the KITTI\ndataset. The code will be made available at https://github.com/ybkurt/VIFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, transformer-based architectures become the de facto standard\nfor sequence modeling in deep learning frameworks. Inspired by the successful\nexamples, we propose a causal visual-inertial fusion transformer (VIFT) for\npose estimation in deep visual-inertial odometry. This study aims to improve\npose estimation accuracy by leveraging the attention mechanisms in\ntransformers, which better utilize historical data compared to the recurrent\nneural network (RNN) based methods seen in recent methods. Transformers\ntypically require large-scale data for training. To address this issue, we\nutilize inductive biases for deep VIO networks. Since latent visual-inertial\nfeature vectors encompass essential information for pose estimation, we employ\ntransformers to refine pose estimates by updating latent vectors temporally.\nOur study also examines the impact of data imbalance and rotation learning\nmethods in supervised end-to-end learning of visual inertial odometry by\nutilizing specialized gradients in backpropagation for the elements of SE$(3)$\ngroup. The proposed method is end-to-end trainable and requires only a\nmonocular camera and IMU during inference. Experimental results demonstrate\nthat VIFT increases the accuracy of monocular VIO networks, achieving\nstate-of-the-art results when compared to previous methods on the KITTI\ndataset. The code will be made available at https://github.com/ybkurt/VIFT."
                },
                "authors": [
                    {
                        "name": "Yunus Bilge Kurt"
                    },
                    {
                        "name": "Ahmet Akman"
                    },
                    {
                        "name": "A. Aydın Alatan"
                    }
                ],
                "author_detail": {
                    "name": "A. Aydın Alatan"
                },
                "author": "A. Aydın Alatan",
                "arxiv_comment": "Accepted to ECCV 2024 2nd Workshop on Vision-Centric Autonomous\n  Driving (VCAD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08760v1",
                "updated": "2024-09-13T12:09:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    9,
                    9,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:09:09Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    9,
                    9,
                    4,
                    257,
                    0
                ],
                "title": "Online Network Inference from Graph-Stationary Signals with Hidden Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Network Inference from Graph-Stationary Signals with Hidden Nodes"
                },
                "summary": "Graph learning is the fundamental task of estimating unknown graph\nconnectivity from available data. Typical approaches assume that not only is\nall information available simultaneously but also that all nodes can be\nobserved. However, in many real-world scenarios, data can neither be known\ncompletely nor obtained all at once. We present a novel method for online graph\nestimation that accounts for the presence of hidden nodes. We consider signals\nthat are stationary on the underlying graph, which provides a model for the\nunknown connections to hidden nodes. We then formulate a convex optimization\nproblem for graph learning from streaming, incomplete graph signals. We solve\nthe proposed problem through an efficient proximal gradient algorithm that can\nrun in real-time as data arrives sequentially. Additionally, we provide\ntheoretical conditions under which our online algorithm is similar to\nbatch-wise solutions. Through experimental results on synthetic and real-world\ndata, we demonstrate the viability of our approach for online graph learning in\nthe presence of missing observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning is the fundamental task of estimating unknown graph\nconnectivity from available data. Typical approaches assume that not only is\nall information available simultaneously but also that all nodes can be\nobserved. However, in many real-world scenarios, data can neither be known\ncompletely nor obtained all at once. We present a novel method for online graph\nestimation that accounts for the presence of hidden nodes. We consider signals\nthat are stationary on the underlying graph, which provides a model for the\nunknown connections to hidden nodes. We then formulate a convex optimization\nproblem for graph learning from streaming, incomplete graph signals. We solve\nthe proposed problem through an efficient proximal gradient algorithm that can\nrun in real-time as data arrives sequentially. Additionally, we provide\ntheoretical conditions under which our online algorithm is similar to\nbatch-wise solutions. Through experimental results on synthetic and real-world\ndata, we demonstrate the viability of our approach for online graph learning in\nthe presence of missing observations."
                },
                "authors": [
                    {
                        "name": "Andrei Buciulea"
                    },
                    {
                        "name": "Madeline Navarro"
                    },
                    {
                        "name": "Samuel Rey"
                    },
                    {
                        "name": "Santiago Segarra"
                    },
                    {
                        "name": "Antonio G. Marques"
                    }
                ],
                "author_detail": {
                    "name": "Antonio G. Marques"
                },
                "author": "Antonio G. Marques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01154v3",
                "updated": "2024-09-13T11:46:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    46,
                    33,
                    4,
                    257,
                    0
                ],
                "published": "2024-01-02T11:08:39Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    11,
                    8,
                    39,
                    1,
                    2,
                    0
                ],
                "title": "Applying Bayesian Data Analysis for Causal Inference about Requirements\n  Quality: A Controlled Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Bayesian Data Analysis for Causal Inference about Requirements\n  Quality: A Controlled Experiment"
                },
                "summary": "It is commonly accepted that the quality of requirements specifications\nimpacts subsequent software engineering activities. However, we still lack\nempirical evidence to support organizations in deciding whether their\nrequirements are good enough or impede subsequent activities. We aim to\ncontribute empirical evidence to the effect that requirements quality defects\nhave on a software engineering activity that depends on this requirement. We\nconduct a controlled experiment in which 25 participants from industry and\nuniversity generate domain models from four natural language requirements\ncontaining different quality defects. We evaluate the resulting models using\nboth frequentist and Bayesian data analysis. Contrary to our expectations, our\nresults show that the use of passive voice only has a minor impact on the\nresulting domain models. The use of ambiguous pronouns, however, shows a strong\neffect on various properties of the resulting domain models. Most notably,\nambiguous pronouns lead to incorrect associations in domain models. Despite\nbeing equally advised against by literature and frequentist methods, the\nBayesian data analysis shows that the two investigated quality defects have\nvastly different impacts on software engineering activities and, hence, deserve\ndifferent levels of attention. Our employed method can be further utilized by\nresearchers to improve reliable, detailed empirical evidence on requirements\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly accepted that the quality of requirements specifications\nimpacts subsequent software engineering activities. However, we still lack\nempirical evidence to support organizations in deciding whether their\nrequirements are good enough or impede subsequent activities. We aim to\ncontribute empirical evidence to the effect that requirements quality defects\nhave on a software engineering activity that depends on this requirement. We\nconduct a controlled experiment in which 25 participants from industry and\nuniversity generate domain models from four natural language requirements\ncontaining different quality defects. We evaluate the resulting models using\nboth frequentist and Bayesian data analysis. Contrary to our expectations, our\nresults show that the use of passive voice only has a minor impact on the\nresulting domain models. The use of ambiguous pronouns, however, shows a strong\neffect on various properties of the resulting domain models. Most notably,\nambiguous pronouns lead to incorrect associations in domain models. Despite\nbeing equally advised against by literature and frequentist methods, the\nBayesian data analysis shows that the two investigated quality defects have\nvastly different impacts on software engineering activities and, hence, deserve\ndifferent levels of attention. Our employed method can be further utilized by\nresearchers to improve reliable, detailed empirical evidence on requirements\nquality."
                },
                "authors": [
                    {
                        "name": "Julian Frattini"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Richard Torkar"
                    },
                    {
                        "name": "Lloyd Montgomery"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Jannik Fischbach"
                    },
                    {
                        "name": "Daniel Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Mendez"
                },
                "author": "Daniel Mendez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08738v1",
                "updated": "2024-09-13T11:37:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    37,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T11:37:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    37,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "DataliVR: Transformation of Data Literacy Education through Virtual\n  Reality with ChatGPT-Powered Enhancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataliVR: Transformation of Data Literacy Education through Virtual\n  Reality with ChatGPT-Powered Enhancements"
                },
                "summary": "Data literacy is essential in today's data-driven world, emphasizing\nindividuals' abilities to effectively manage data and extract meaningful\ninsights. However, traditional classroom-based educational approaches often\nstruggle to fully address the multifaceted nature of data literacy. As\neducation undergoes digital transformation, innovative technologies such as\nVirtual Reality (VR) offer promising avenues for immersive and engaging\nlearning experiences. This paper introduces DataliVR, a pioneering VR\napplication aimed at enhancing the data literacy skills of university students\nwithin a contextual and gamified virtual learning environment. By integrating\nLarge Language Models (LLMs) like ChatGPT as a conversational artificial\nintelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides\npersonalized learning assistance, enriching user learning experiences. Our\nstudy employed an experimental approach, with chatbot availability as the\nindependent variable, analyzing learning experiences and outcomes as dependent\nvariables with a sample of thirty participants. Our approach underscores the\neffectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering\ndata literacy skills. Moreover, our study examines the impact of the\nChatGPT-based AI chatbot on users' learning, revealing significant effects on\nboth learning experiences and outcomes. Our study presents a robust tool for\nfostering data literacy skills, contributing significantly to the digital\nadvancement of data literacy education through cutting-edge VR and AI\ntechnologies. Moreover, our research provides valuable insights and\nimplications for future research endeavors aiming to integrate LLMs (e.g.,\nChatGPT) into educational VR platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data literacy is essential in today's data-driven world, emphasizing\nindividuals' abilities to effectively manage data and extract meaningful\ninsights. However, traditional classroom-based educational approaches often\nstruggle to fully address the multifaceted nature of data literacy. As\neducation undergoes digital transformation, innovative technologies such as\nVirtual Reality (VR) offer promising avenues for immersive and engaging\nlearning experiences. This paper introduces DataliVR, a pioneering VR\napplication aimed at enhancing the data literacy skills of university students\nwithin a contextual and gamified virtual learning environment. By integrating\nLarge Language Models (LLMs) like ChatGPT as a conversational artificial\nintelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides\npersonalized learning assistance, enriching user learning experiences. Our\nstudy employed an experimental approach, with chatbot availability as the\nindependent variable, analyzing learning experiences and outcomes as dependent\nvariables with a sample of thirty participants. Our approach underscores the\neffectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering\ndata literacy skills. Moreover, our study examines the impact of the\nChatGPT-based AI chatbot on users' learning, revealing significant effects on\nboth learning experiences and outcomes. Our study presents a robust tool for\nfostering data literacy skills, contributing significantly to the digital\nadvancement of data literacy education through cutting-edge VR and AI\ntechnologies. Moreover, our research provides valuable insights and\nimplications for future research endeavors aiming to integrate LLMs (e.g.,\nChatGPT) into educational VR platforms."
                },
                "authors": [
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Haochun Huai"
                    },
                    {
                        "name": "Sena Yildiz-Degirmenci"
                    },
                    {
                        "name": "Maria Bannert"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "10 pages, this paper was accepted to ISMAR2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18140v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18140v3",
                "updated": "2024-09-13T11:36:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    36,
                    49,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-26T07:44:27Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    7,
                    44,
                    27,
                    2,
                    178,
                    0
                ],
                "title": "Exclusive Style Removal for Cross Domain Novel Class Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exclusive Style Removal for Cross Domain Novel Class Discovery"
                },
                "summary": "As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module."
                },
                "authors": [
                    {
                        "name": "Yicheng Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Junmin Liu"
                    },
                    {
                        "name": "Kai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kai Sun"
                },
                "author": "Kai Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18140v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18140v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19936v2",
                "updated": "2024-09-13T11:18:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    18,
                    42,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-28T14:05:47Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    14,
                    5,
                    47,
                    4,
                    180,
                    0
                ],
                "title": "Deep Learning of Multivariate Extremes via a Geometric Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning of Multivariate Extremes via a Geometric Representation"
                },
                "summary": "The study of geometric extremes, where extremal dependence properties are\ninferred from the deterministic limiting shapes of scaled sample clouds,\nprovides an exciting approach to modelling the extremes of multivariate data.\nThese shapes, termed limit sets, link together several popular extremal\ndependence modelling frameworks. Although the geometric approach is becoming an\nincreasingly popular modelling tool, current inference techniques are limited\nto a low dimensional setting (d < 5), and generally require rigid modelling\nassumptions. In this work, we propose a range of novel theoretical results to\naid with the implementation of the geometric extremes framework and introduce\nthe first approach to modelling limit sets using deep learning. By leveraging\nneural networks, we construct asymptotically-justified yet flexible\nsemi-parametric models for extremal dependence of high-dimensional data. We\nshowcase the efficacy of our deep approach by modelling the complex extremal\ndependencies between meteorological and oceanographic variables in the North\nSea off the coast of the UK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of geometric extremes, where extremal dependence properties are\ninferred from the deterministic limiting shapes of scaled sample clouds,\nprovides an exciting approach to modelling the extremes of multivariate data.\nThese shapes, termed limit sets, link together several popular extremal\ndependence modelling frameworks. Although the geometric approach is becoming an\nincreasingly popular modelling tool, current inference techniques are limited\nto a low dimensional setting (d < 5), and generally require rigid modelling\nassumptions. In this work, we propose a range of novel theoretical results to\naid with the implementation of the geometric extremes framework and introduce\nthe first approach to modelling limit sets using deep learning. By leveraging\nneural networks, we construct asymptotically-justified yet flexible\nsemi-parametric models for extremal dependence of high-dimensional data. We\nshowcase the efficacy of our deep approach by modelling the complex extremal\ndependencies between meteorological and oceanographic variables in the North\nSea off the coast of the UK."
                },
                "authors": [
                    {
                        "name": "Callum J. R. Murphy-Barltrop"
                    },
                    {
                        "name": "Reetam Majumder"
                    },
                    {
                        "name": "Jordan Richards"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Richards"
                },
                "author": "Jordan Richards",
                "arxiv_comment": "Includes supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08717v1",
                "updated": "2024-09-13T11:02:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T11:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents"
                },
                "summary": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Zhicheng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dong"
                },
                "author": "Zhicheng Dong",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08715v1",
                "updated": "2024-09-13T11:00:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    0,
                    17,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T11:00:17Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    0,
                    17,
                    4,
                    257,
                    0
                ],
                "title": "On spiked eigenvalues of a renormalized sample covariance matrix from\n  multi-population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On spiked eigenvalues of a renormalized sample covariance matrix from\n  multi-population"
                },
                "summary": "Sample covariance matrices from multi-population typically exhibit several\nlarge spiked eigenvalues, which stem from differences between population means\nand are crucial for inference on the underlying data structure. This paper\ninvestigates the asymptotic properties of spiked eigenvalues of a renormalized\nsample covariance matrices from multi-population in the ultrahigh dimensional\ncontext where the dimension-to-sample size ratio p/n go to infinity. The first-\nand second-order convergence of these spikes are established based on\nasymptotic properties of three types of sesquilinear forms from\nmulti-population. These findings are further applied to two scenarios,including\ndetermination of total number of subgroups and a new criterion for evaluating\nclustering results in the absence of true labels. Additionally, we provide a\nunified framework with p/n->c\\in (0,\\infty] that integrates the asymptotic\nresults in both high and ultrahigh dimensional settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample covariance matrices from multi-population typically exhibit several\nlarge spiked eigenvalues, which stem from differences between population means\nand are crucial for inference on the underlying data structure. This paper\ninvestigates the asymptotic properties of spiked eigenvalues of a renormalized\nsample covariance matrices from multi-population in the ultrahigh dimensional\ncontext where the dimension-to-sample size ratio p/n go to infinity. The first-\nand second-order convergence of these spikes are established based on\nasymptotic properties of three types of sesquilinear forms from\nmulti-population. These findings are further applied to two scenarios,including\ndetermination of total number of subgroups and a new criterion for evaluating\nclustering results in the absence of true labels. Additionally, we provide a\nunified framework with p/n->c\\in (0,\\infty] that integrates the asymptotic\nresults in both high and ultrahigh dimensional settings."
                },
                "authors": [
                    {
                        "name": "Weiming Li"
                    },
                    {
                        "name": "Zeng Li"
                    },
                    {
                        "name": "Junpeng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Junpeng Zhu"
                },
                "author": "Junpeng Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08712v1",
                "updated": "2024-09-13T10:59:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    59,
                    24,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:59:24Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    59,
                    24,
                    4,
                    257,
                    0
                ],
                "title": "Layerwise Change of Knowledge in Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layerwise Change of Knowledge in Neural Networks"
                },
                "summary": "This paper aims to explain how a deep neural network (DNN) gradually extracts\nnew knowledge and forgets noisy features through layers in forward propagation.\nUp to now, although the definition of knowledge encoded by the DNN has not\nreached a consensus, Previous studies have derived a series of mathematical\nevidence to take interactions as symbolic primitive inference patterns encoded\nby a DNN. We extend the definition of interactions and, for the first time,\nextract interactions encoded by intermediate layers. We quantify and track the\nnewly emerged interactions and the forgotten interactions in each layer during\nthe forward propagation, which shed new light on the learning behavior of DNNs.\nThe layer-wise change of interactions also reveals the change of the\ngeneralization capacity and instability of feature representations of a DNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to explain how a deep neural network (DNN) gradually extracts\nnew knowledge and forgets noisy features through layers in forward propagation.\nUp to now, although the definition of knowledge encoded by the DNN has not\nreached a consensus, Previous studies have derived a series of mathematical\nevidence to take interactions as symbolic primitive inference patterns encoded\nby a DNN. We extend the definition of interactions and, for the first time,\nextract interactions encoded by intermediate layers. We quantify and track the\nnewly emerged interactions and the forgotten interactions in each layer during\nthe forward propagation, which shed new light on the learning behavior of DNNs.\nThe layer-wise change of interactions also reveals the change of the\ngeneralization capacity and instability of feature representations of a DNN."
                },
                "authors": [
                    {
                        "name": "Xu Cheng"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Zhaoran Peng"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Tian Han"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05629v2",
                "updated": "2024-09-13T10:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    49,
                    21,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-10T20:48:40Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    20,
                    48,
                    40,
                    5,
                    223,
                    0
                ],
                "title": "Quantum-secure multiparty deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-secure multiparty deep learning"
                },
                "summary": "Secure multiparty computation enables the joint evaluation of multivariate\nfunctions across distributed users while ensuring the privacy of their local\ninputs. This field has become increasingly urgent due to the exploding demand\nfor computationally intensive deep learning inference. These computations are\ntypically offloaded to cloud computing servers, leading to vulnerabilities that\ncan compromise the security of the clients' data. To solve this problem, we\nintroduce a linear algebra engine that leverages the quantum nature of light\nfor information-theoretically secure multiparty computation using only\nconventional telecommunication components. We apply this linear algebra engine\nto deep learning and derive rigorous upper bounds on the information leakage of\nboth the deep neural network weights and the client's data via the Holevo and\nthe Cram\\'er-Rao bounds, respectively. Applied to the MNIST classification\ntask, we obtain test accuracies exceeding $96\\%$ while leaking less than $0.1$\nbits per weight symbol and $0.01$ bits per data symbol. This weight leakage is\nan order of magnitude below the minimum bit precision required for accurate\ndeep learning using state-of-the-art quantization techniques. Our work lays the\nfoundation for practical quantum-secure computation and unlocks secure cloud\ndeep learning as a field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure multiparty computation enables the joint evaluation of multivariate\nfunctions across distributed users while ensuring the privacy of their local\ninputs. This field has become increasingly urgent due to the exploding demand\nfor computationally intensive deep learning inference. These computations are\ntypically offloaded to cloud computing servers, leading to vulnerabilities that\ncan compromise the security of the clients' data. To solve this problem, we\nintroduce a linear algebra engine that leverages the quantum nature of light\nfor information-theoretically secure multiparty computation using only\nconventional telecommunication components. We apply this linear algebra engine\nto deep learning and derive rigorous upper bounds on the information leakage of\nboth the deep neural network weights and the client's data via the Holevo and\nthe Cram\\'er-Rao bounds, respectively. Applied to the MNIST classification\ntask, we obtain test accuracies exceeding $96\\%$ while leaking less than $0.1$\nbits per weight symbol and $0.01$ bits per data symbol. This weight leakage is\nan order of magnitude below the minimum bit precision required for accurate\ndeep learning using state-of-the-art quantization techniques. Our work lays the\nfoundation for practical quantum-secure computation and unlocks secure cloud\ndeep learning as a field."
                },
                "authors": [
                    {
                        "name": "Kfir Sulimany"
                    },
                    {
                        "name": "Sri Krishna Vadlamani"
                    },
                    {
                        "name": "Ryan Hamerly"
                    },
                    {
                        "name": "Prahlad Iyengar"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08706v1",
                "updated": "2024-09-13T10:48:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:48:35Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "title": "L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp ."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Akanksha Salunke"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08703v1",
                "updated": "2024-09-13T10:43:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    43,
                    18,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:43:18Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    43,
                    18,
                    4,
                    257,
                    0
                ],
                "title": "NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for\n  Click-Through Rate Prediction"
                },
                "summary": "Click-through-rate (CTR) prediction plays an important role in online\nadvertising and ad recommender systems. In the past decade, maximizing CTR has\nbeen the main focus of model development and solution creation. Therefore,\nresearchers and practitioners have proposed various models and solutions to\nenhance the effectiveness of CTR prediction. Most of the existing literature\nfocuses on capturing either implicit or explicit feature interactions. Although\nimplicit interactions are successfully captured in some studies, explicit\ninteractions present a challenge for achieving high CTR by extracting both\nlow-order and high-order feature interactions. Unnecessary and irrelevant\nfeatures may cause high computational time and low prediction performance.\nFurthermore, certain features may perform well with specific predictive models\nwhile underperforming with others. Also, feature distribution may fluctuate due\nto traffic variations. Most importantly, in live production environments,\nresources are limited, and the time for inference is just as crucial as\ntraining time. Because of all these reasons, feature selection is one of the\nmost important factors in enhancing CTR prediction model performance. Simple\nfilter-based feature selection algorithms do not perform well and they are not\nsufficient. An effective and efficient feature selection algorithm is needed to\nconsistently filter the most useful features during live CTR prediction\nprocess. In this paper, we propose a heuristic algorithm named Neighborhood\nSearch with Heuristic-based Feature Selection (NeSHFS) to enhance CTR\nprediction performance while reducing dimensionality and training time costs.\nWe conduct comprehensive experiments on three public datasets to validate the\nefficiency and effectiveness of our proposed solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-through-rate (CTR) prediction plays an important role in online\nadvertising and ad recommender systems. In the past decade, maximizing CTR has\nbeen the main focus of model development and solution creation. Therefore,\nresearchers and practitioners have proposed various models and solutions to\nenhance the effectiveness of CTR prediction. Most of the existing literature\nfocuses on capturing either implicit or explicit feature interactions. Although\nimplicit interactions are successfully captured in some studies, explicit\ninteractions present a challenge for achieving high CTR by extracting both\nlow-order and high-order feature interactions. Unnecessary and irrelevant\nfeatures may cause high computational time and low prediction performance.\nFurthermore, certain features may perform well with specific predictive models\nwhile underperforming with others. Also, feature distribution may fluctuate due\nto traffic variations. Most importantly, in live production environments,\nresources are limited, and the time for inference is just as crucial as\ntraining time. Because of all these reasons, feature selection is one of the\nmost important factors in enhancing CTR prediction model performance. Simple\nfilter-based feature selection algorithms do not perform well and they are not\nsufficient. An effective and efficient feature selection algorithm is needed to\nconsistently filter the most useful features during live CTR prediction\nprocess. In this paper, we propose a heuristic algorithm named Neighborhood\nSearch with Heuristic-based Feature Selection (NeSHFS) to enhance CTR\nprediction performance while reducing dimensionality and training time costs.\nWe conduct comprehensive experiments on three public datasets to validate the\nefficiency and effectiveness of our proposed solution."
                },
                "authors": [
                    {
                        "name": "Dogukan Aksu"
                    },
                    {
                        "name": "Ismail Hakki Toroslu"
                    },
                    {
                        "name": "Hasan Davulcu"
                    }
                ],
                "author_detail": {
                    "name": "Hasan Davulcu"
                },
                "author": "Hasan Davulcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08695v1",
                "updated": "2024-09-13T10:27:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    27,
                    27,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:27:27Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    27,
                    27,
                    4,
                    257,
                    0
                ],
                "title": "Precision Aquaculture: An Integrated Computer Vision and IoT Approach\n  for Optimized Tilapia Feeding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision Aquaculture: An Integrated Computer Vision and IoT Approach\n  for Optimized Tilapia Feeding"
                },
                "summary": "Traditional fish farming practices often lead to inefficient feeding,\nresulting in environmental issues and reduced productivity. We developed an\ninnovative system combining computer vision and IoT technologies for precise\nTilapia feeding. Our solution uses real-time IoT sensors to monitor water\nquality parameters and computer vision algorithms to analyze fish size and\ncount, determining optimal feed amounts. A mobile app enables remote monitoring\nand control. We utilized YOLOv8 for keypoint detection to measure Tilapia\nweight from length, achieving \\textbf{94\\%} precision on 3,500 annotated\nimages. Pixel-based measurements were converted to centimeters using depth\nestimation for accurate feeding calculations. Our method, with data collection\nmirroring inference conditions, significantly improved results. Preliminary\nestimates suggest this approach could increase production up to 58 times\ncompared to traditional farms. Our models, code, and dataset are\nopen-source~\\footnote{The code, dataset, and models are available upon\nreasonable request.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional fish farming practices often lead to inefficient feeding,\nresulting in environmental issues and reduced productivity. We developed an\ninnovative system combining computer vision and IoT technologies for precise\nTilapia feeding. Our solution uses real-time IoT sensors to monitor water\nquality parameters and computer vision algorithms to analyze fish size and\ncount, determining optimal feed amounts. A mobile app enables remote monitoring\nand control. We utilized YOLOv8 for keypoint detection to measure Tilapia\nweight from length, achieving \\textbf{94\\%} precision on 3,500 annotated\nimages. Pixel-based measurements were converted to centimeters using depth\nestimation for accurate feeding calculations. Our method, with data collection\nmirroring inference conditions, significantly improved results. Preliminary\nestimates suggest this approach could increase production up to 58 times\ncompared to traditional farms. Our models, code, and dataset are\nopen-source~\\footnote{The code, dataset, and models are available upon\nreasonable request."
                },
                "authors": [
                    {
                        "name": "Rania Hossam"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Walid Gomaa"
                    }
                ],
                "author_detail": {
                    "name": "Walid Gomaa"
                },
                "author": "Walid Gomaa",
                "arxiv_comment": "8 pages, 6 figures, 3 tables, 21th International Conference on\n  Informatics in Control, Automation, and Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08692v1",
                "updated": "2024-09-13T10:22:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    22,
                    8,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:22:08Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    22,
                    8,
                    4,
                    257,
                    0
                ],
                "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with\n  Plausible Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B4: Towards Optimal Assessment of Plausible Code Solutions with\n  Plausible Tests"
                },
                "summary": "Selecting the best code solution from multiple generated ones is an essential\ntask in code generation, which can be achieved by using some reliable\nvalidators (e.g., developer-written test cases) for assistance. Since reliable\ntest cases are not always available and can be expensive to build in practice,\nresearchers propose to automatically generate test cases to assess code\nsolutions. However, when both code solutions and test cases are plausible and\nnot reliable, selecting the best solution becomes challenging. Although some\nheuristic strategies have been proposed to tackle this problem, they lack a\nstrong theoretical guarantee and it is still an open question whether an\noptimal selection strategy exists. Our work contributes in two ways. First, we\nshow that within a Bayesian framework, the optimal selection strategy can be\ndefined based on the posterior probability of the observed passing states\nbetween solutions and tests. The problem of identifying the best solution is\nthen framed as an integer programming problem. Second, we propose an efficient\napproach for approximating this optimal (yet uncomputable) strategy, where the\napproximation error is bounded by the correctness of prior knowledge. We then\nincorporate effective prior knowledge to tailor code generation tasks. Both\ntheoretical and empirical studies confirm that existing heuristics are limited\nin selecting the best solutions with plausible test cases. Our proposed\napproximated optimal strategy B4 significantly surpasses existing heuristics in\nselecting code solutions generated by large language models (LLMs) with\nLLM-generated tests, achieving a relative performance improvement by up to 50%\nover the strongest heuristic and 246% over the random selection in the most\nchallenging scenarios. Our code is publicly available at\nhttps://github.com/ZJU-CTAG/B4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting the best code solution from multiple generated ones is an essential\ntask in code generation, which can be achieved by using some reliable\nvalidators (e.g., developer-written test cases) for assistance. Since reliable\ntest cases are not always available and can be expensive to build in practice,\nresearchers propose to automatically generate test cases to assess code\nsolutions. However, when both code solutions and test cases are plausible and\nnot reliable, selecting the best solution becomes challenging. Although some\nheuristic strategies have been proposed to tackle this problem, they lack a\nstrong theoretical guarantee and it is still an open question whether an\noptimal selection strategy exists. Our work contributes in two ways. First, we\nshow that within a Bayesian framework, the optimal selection strategy can be\ndefined based on the posterior probability of the observed passing states\nbetween solutions and tests. The problem of identifying the best solution is\nthen framed as an integer programming problem. Second, we propose an efficient\napproach for approximating this optimal (yet uncomputable) strategy, where the\napproximation error is bounded by the correctness of prior knowledge. We then\nincorporate effective prior knowledge to tailor code generation tasks. Both\ntheoretical and empirical studies confirm that existing heuristics are limited\nin selecting the best solutions with plausible test cases. Our proposed\napproximated optimal strategy B4 significantly surpasses existing heuristics in\nselecting code solutions generated by large language models (LLMs) with\nLLM-generated tests, achieving a relative performance improvement by up to 50%\nover the strongest heuristic and 246% over the random selection in the most\nchallenging scenarios. Our code is publicly available at\nhttps://github.com/ZJU-CTAG/B4."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "He Tao"
                    },
                    {
                        "name": "Yusu Hong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "arxiv_comment": "accepted by ASE' 24 (full paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08688v1",
                "updated": "2024-09-13T10:15:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    15,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:15:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    15,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "GenMapping: Unleashing the Potential of Inverse Perspective Mapping for\n  Robust Online HD Map Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenMapping: Unleashing the Potential of Inverse Perspective Mapping for\n  Robust Online HD Map Construction"
                },
                "summary": "Online High-Definition (HD) maps have emerged as the preferred option for\nautonomous driving, overshadowing the counterpart offline HD maps due to\nflexible update capability and lower maintenance costs. However, contemporary\nonline HD map models embed parameters of visual sensors into training,\nresulting in a significant decrease in generalization performance when applied\nto visual sensors with different parameters. Inspired by the inherent potential\nof Inverse Perspective Mapping (IPM), where camera parameters are decoupled\nfrom the training process, we have designed a universal map generation\nframework, GenMapping. The framework is established with a triadic synergy\narchitecture, including principal and dual auxiliary branches. When faced with\na coarse road image with local distortion translated via IPM, the principal\nbranch learns robust global features under the state space models. The two\nauxiliary branches are a dense perspective branch and a sparse prior branch.\nThe former exploits the correlation information between static and moving\nobjects, whereas the latter introduces the prior knowledge of OpenStreetMap\n(OSM). The triple-enhanced merging module is crafted to synergistically\nintegrate the unique spatial features from all three branches. To further\nimprove generalization capabilities, a Cross-View Map Learning (CVML) scheme is\nleveraged to realize joint learning within the common space. Additionally, a\nBidirectional Data Augmentation (BiDA) module is introduced to mitigate\nreliance on datasets concurrently. A thorough array of experimental results\nshows that the proposed model surpasses current state-of-the-art methods in\nboth semantic mapping and vectorized mapping, while also maintaining a rapid\ninference speed. The source code will be publicly available at\nhttps://github.com/lynn-yu/GenMapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online High-Definition (HD) maps have emerged as the preferred option for\nautonomous driving, overshadowing the counterpart offline HD maps due to\nflexible update capability and lower maintenance costs. However, contemporary\nonline HD map models embed parameters of visual sensors into training,\nresulting in a significant decrease in generalization performance when applied\nto visual sensors with different parameters. Inspired by the inherent potential\nof Inverse Perspective Mapping (IPM), where camera parameters are decoupled\nfrom the training process, we have designed a universal map generation\nframework, GenMapping. The framework is established with a triadic synergy\narchitecture, including principal and dual auxiliary branches. When faced with\na coarse road image with local distortion translated via IPM, the principal\nbranch learns robust global features under the state space models. The two\nauxiliary branches are a dense perspective branch and a sparse prior branch.\nThe former exploits the correlation information between static and moving\nobjects, whereas the latter introduces the prior knowledge of OpenStreetMap\n(OSM). The triple-enhanced merging module is crafted to synergistically\nintegrate the unique spatial features from all three branches. To further\nimprove generalization capabilities, a Cross-View Map Learning (CVML) scheme is\nleveraged to realize joint learning within the common space. Additionally, a\nBidirectional Data Augmentation (BiDA) module is introduced to mitigate\nreliance on datasets concurrently. A thorough array of experimental results\nshows that the proposed model surpasses current state-of-the-art methods in\nboth semantic mapping and vectorized mapping, while also maintaining a rapid\ninference speed. The source code will be publicly available at\nhttps://github.com/lynn-yu/GenMapping."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "You Yao"
                    },
                    {
                        "name": "Zhiyong Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Li"
                },
                "author": "Zhiyong Li",
                "arxiv_comment": "The source code will be publicly available at\n  https://github.com/lynn-yu/GenMapping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08683v1",
                "updated": "2024-09-13T10:03:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    3,
                    22,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:03:22Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    3,
                    22,
                    4,
                    257,
                    0
                ],
                "title": "Large-scale simulations of vortex Majorana zero modes in topological\n  crystalline insulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale simulations of vortex Majorana zero modes in topological\n  crystalline insulators"
                },
                "summary": "Topological crystalline insulators are known to support multiple Majorana\nzero modes (MZMs) at a single vortex, their hybridization is forbidden by a\nmagnetic mirror symmetry $M_T$. Due to the limited energy resolution of\nscanning tunneling microscopes and the very small energy spacing of trivial\nbound states, it remains challenging to directly probe and demonstrate the\nexistence of multiple MZMs. In this work, we propose to demonstrate the\nexistence of MZMs by studying the hybridization of multiple MZMs in a symmetry\nbreaking field. The different responses of trivial bound states and MZMs can be\ninferred from their spatial distribution in the vortex. However, the\ntheoretical simulations are very demanding since it requires an extremely large\nsystem in real space. By utilizing the kernel polynomial method, we can\nefficiently simulate large lattices with over $10^8$ orbitals to compute the\nlocal density of states which bridges the gap between theoretical studies based\non minimal models and experimental measurements. We show that the spatial\ndistribution of MZMs and trivial vortex bound states indeed differs drastically\nin tilted magnetic fields. The zero-bias peak elongates when the magnetic field\npreserves $M_T$, while it splits when $M_T$ is broken, giving rise to an\nanisotropic magnetic response. Since the bulk of SnTe are metallic, we also\nstudy the robustness of MZMs against the bulk states, and clarify when can the\nMZMs produce a pronounced anisotropic magnetic response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological crystalline insulators are known to support multiple Majorana\nzero modes (MZMs) at a single vortex, their hybridization is forbidden by a\nmagnetic mirror symmetry $M_T$. Due to the limited energy resolution of\nscanning tunneling microscopes and the very small energy spacing of trivial\nbound states, it remains challenging to directly probe and demonstrate the\nexistence of multiple MZMs. In this work, we propose to demonstrate the\nexistence of MZMs by studying the hybridization of multiple MZMs in a symmetry\nbreaking field. The different responses of trivial bound states and MZMs can be\ninferred from their spatial distribution in the vortex. However, the\ntheoretical simulations are very demanding since it requires an extremely large\nsystem in real space. By utilizing the kernel polynomial method, we can\nefficiently simulate large lattices with over $10^8$ orbitals to compute the\nlocal density of states which bridges the gap between theoretical studies based\non minimal models and experimental measurements. We show that the spatial\ndistribution of MZMs and trivial vortex bound states indeed differs drastically\nin tilted magnetic fields. The zero-bias peak elongates when the magnetic field\npreserves $M_T$, while it splits when $M_T$ is broken, giving rise to an\nanisotropic magnetic response. Since the bulk of SnTe are metallic, we also\nstudy the robustness of MZMs against the bulk states, and clarify when can the\nMZMs produce a pronounced anisotropic magnetic response."
                },
                "authors": [
                    {
                        "name": "Chun Yu Wan"
                    },
                    {
                        "name": "Yujun Zhao"
                    },
                    {
                        "name": "Yaoyi Li"
                    },
                    {
                        "name": "Jinfeng Jia"
                    },
                    {
                        "name": "Junwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liu"
                },
                "author": "Junwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04167v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04167v5",
                "updated": "2024-09-13T09:47:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    47,
                    29,
                    4,
                    257,
                    0
                ],
                "published": "2024-04-05T15:20:02Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    15,
                    20,
                    2,
                    4,
                    96,
                    0
                ],
                "title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model"
                },
                "summary": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models."
                },
                "authors": [
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Ding Pan"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04167v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04167v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01829v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01829v2",
                "updated": "2024-09-13T09:39:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    39,
                    53,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-03T22:56:40Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    22,
                    56,
                    40,
                    0,
                    155,
                    0
                ],
                "title": "FaçAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaçAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction"
                },
                "summary": "We introduce a neuro-symbolic transformer-based model that converts flat,\nsegmented facade structures into procedural definitions using a custom-designed\nsplit grammar. To facilitate this, we first develop a semi-complex split\ngrammar tailored for architectural facades and then generate a dataset\ncomprising of facades alongside their corresponding procedural representations.\nThis dataset is used to train our transformer model to convert segmented, flat\nfacades into the procedural language of our grammar. During inference, the\nmodel applies this learned transformation to new facade segmentations,\nproviding a procedural representation that users can adjust to generate varied\nfacade designs. This method not only automates the conversion of static facade\nimages into dynamic, editable procedural formats but also enhances the design\nflexibility, allowing for easy modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neuro-symbolic transformer-based model that converts flat,\nsegmented facade structures into procedural definitions using a custom-designed\nsplit grammar. To facilitate this, we first develop a semi-complex split\ngrammar tailored for architectural facades and then generate a dataset\ncomprising of facades alongside their corresponding procedural representations.\nThis dataset is used to train our transformer model to convert segmented, flat\nfacades into the procedural language of our grammar. During inference, the\nmodel applies this learned transformation to new facade segmentations,\nproviding a procedural representation that users can adjust to generate varied\nfacade designs. This method not only automates the conversion of static facade\nimages into dynamic, editable procedural formats but also enhances the design\nflexibility, allowing for easy modifications."
                },
                "authors": [
                    {
                        "name": "Aleksander Plocharski"
                    },
                    {
                        "name": "Jan Swidzinski"
                    },
                    {
                        "name": "Joanna Porter-Sobieraj"
                    },
                    {
                        "name": "Przemyslaw Musialski"
                    }
                ],
                "author_detail": {
                    "name": "Przemyslaw Musialski"
                },
                "author": "Przemyslaw Musialski",
                "arxiv_comment": "11 pages, 11 figures, in ACM SIGGRAPH Asia 2024 Conference Papers\n  Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01829v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01829v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5; I.2.2; I.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15649v3",
                "updated": "2024-09-13T09:36:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    36,
                    18,
                    4,
                    257,
                    0
                ],
                "published": "2023-11-27T09:20:23Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    9,
                    20,
                    23,
                    0,
                    331,
                    0
                ],
                "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks"
                },
                "summary": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality."
                },
                "authors": [
                    {
                        "name": "Yaran Chen"
                    },
                    {
                        "name": "Wenbo Cui"
                    },
                    {
                        "name": "Yuanwen Chen"
                    },
                    {
                        "name": "Mining Tan"
                    },
                    {
                        "name": "Xinyao Zhang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01939v2",
                "updated": "2024-09-13T09:22:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    22,
                    38,
                    4,
                    257,
                    0
                ],
                "published": "2023-05-03T07:32:28Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    7,
                    32,
                    28,
                    2,
                    123,
                    0
                ],
                "title": "Where We Have Arrived in Proving the Emergence of Sparse Symbolic\n  Concepts in AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where We Have Arrived in Proving the Emergence of Sparse Symbolic\n  Concepts in AI Models"
                },
                "summary": "This study aims to prove the emergence of symbolic concepts (or more\nprecisely, sparse primitive inference patterns) in well-trained deep neural\nnetworks (DNNs). Specifically, we prove the following three conditions for the\nemergence. (i) The high-order derivatives of the network output with respect to\nthe input variables are all zero. (ii) The DNN can be used on occluded samples\nand when the input sample is less occluded, the DNN will yield higher\nconfidence. (iii) The confidence of the DNN does not significantly degrade on\noccluded samples. These conditions are quite common, and we prove that under\nthese conditions, the DNN will only encode a relatively small number of sparse\ninteractions between input variables. Moreover, we can consider such\ninteractions as symbolic primitive inference patterns encoded by a DNN, because\nwe show that inference scores of the DNN on an exponentially large number of\nrandomly masked samples can always be well mimicked by numerical effects of\njust a few interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to prove the emergence of symbolic concepts (or more\nprecisely, sparse primitive inference patterns) in well-trained deep neural\nnetworks (DNNs). Specifically, we prove the following three conditions for the\nemergence. (i) The high-order derivatives of the network output with respect to\nthe input variables are all zero. (ii) The DNN can be used on occluded samples\nand when the input sample is less occluded, the DNN will yield higher\nconfidence. (iii) The confidence of the DNN does not significantly degrade on\noccluded samples. These conditions are quite common, and we prove that under\nthese conditions, the DNN will only encode a relatively small number of sparse\ninteractions between input variables. Moreover, we can consider such\ninteractions as symbolic primitive inference patterns encoded by a DNN, because\nwe show that inference scores of the DNN on an exponentially large number of\nrandomly masked samples can always be well mimicked by numerical effects of\njust a few interactions."
                },
                "authors": [
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Jiayang Gao"
                    },
                    {
                        "name": "Wen Shen"
                    },
                    {
                        "name": "Quanshi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanshi Zhang"
                },
                "author": "Quanshi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08660v1",
                "updated": "2024-09-13T09:20:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    20,
                    42,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T09:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    20,
                    42,
                    4,
                    257,
                    0
                ],
                "title": "Online Learning Of Expanding Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Learning Of Expanding Graphs"
                },
                "summary": "This paper addresses the problem of online network topology inference for\nexpanding graphs from a stream of spatiotemporal signals. Online algorithms for\ndynamic graph learning are crucial in delay-sensitive applications or when\nchanges in topology occur rapidly. While existing works focus on inferring the\nconnectivity within a fixed set of nodes, in practice, the graph can grow as\nnew nodes join the network. This poses additional challenges like modeling\ntemporal dynamics involving signals and graphs of different sizes. This growth\nalso increases the computational complexity of the learning process, which may\nbecome prohibitive. To the best of our knowledge, this is the first work to\ntackle this setting. We propose a general online algorithm based on projected\nproximal gradient descent that accounts for the increasing graph size at each\niteration. Recursively updating the sample covariance matrix is a key aspect of\nour approach. We introduce a strategy that enables different types of updates\nfor nodes that just joined the network and for previously existing nodes. To\nprovide further insights into the proposed method, we specialize it in Gaussian\nMarkov random field settings, where we analyze the computational complexity and\ncharacterize the dynamic cumulative regret. Finally, we demonstrate the\neffectiveness of the proposed approach using both controlled experiments and\nreal-world datasets from epidemic and financial networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of online network topology inference for\nexpanding graphs from a stream of spatiotemporal signals. Online algorithms for\ndynamic graph learning are crucial in delay-sensitive applications or when\nchanges in topology occur rapidly. While existing works focus on inferring the\nconnectivity within a fixed set of nodes, in practice, the graph can grow as\nnew nodes join the network. This poses additional challenges like modeling\ntemporal dynamics involving signals and graphs of different sizes. This growth\nalso increases the computational complexity of the learning process, which may\nbecome prohibitive. To the best of our knowledge, this is the first work to\ntackle this setting. We propose a general online algorithm based on projected\nproximal gradient descent that accounts for the increasing graph size at each\niteration. Recursively updating the sample covariance matrix is a key aspect of\nour approach. We introduce a strategy that enables different types of updates\nfor nodes that just joined the network and for previously existing nodes. To\nprovide further insights into the proposed method, we specialize it in Gaussian\nMarkov random field settings, where we analyze the computational complexity and\ncharacterize the dynamic cumulative regret. Finally, we demonstrate the\neffectiveness of the proposed approach using both controlled experiments and\nreal-world datasets from epidemic and financial networks."
                },
                "authors": [
                    {
                        "name": "Samuel Rey"
                    },
                    {
                        "name": "Bishwadeep Das"
                    },
                    {
                        "name": "Elvin Isufi"
                    }
                ],
                "author_detail": {
                    "name": "Elvin Isufi"
                },
                "author": "Elvin Isufi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08659v1",
                "updated": "2024-09-13T09:18:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    18,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T09:18:32Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    18,
                    32,
                    4,
                    257,
                    0
                ],
                "title": "A Modified Initial Mass Function of the First Stars with Explodability\n  Theory under Different Enrichment Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modified Initial Mass Function of the First Stars with Explodability\n  Theory under Different Enrichment Scenarios"
                },
                "summary": "The most metal-poor stars record the earliest metal enrichment triggered by\nPopulation III stars. By comparing observed abundance patterns with theoretical\nyields of metal-free stars, physical properties of their first star progenitors\ncan be inferred, including zero-age main-sequence mass and explosion energy. In\nthis work, the initial mass distribution (IMF) of first stars is obtained from\nthe largest analysis to date of 406 very metal-poor stars with the newest\nLAMOST/Subaru high-resolution spectroscopic observations. However, the mass\ndistribution fails to be consistent with the Salpeter IMF, which is also\nreported by previous studies. Here we modify the standard power-law function\nwith explodability theory. The mass distribution of Population III stars could\nbe well explained by ensuring the initial metal enrichment to originate from\nsuccessful supernova explosions. Based on the modified power-law function, we\nsuggest an extremely top-heavy or nearly flat initial mass function with a\nlarge explosion energy exponent. This indicates that supernova explodability\nshould be considered in the earliest metal enrichment process in the Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most metal-poor stars record the earliest metal enrichment triggered by\nPopulation III stars. By comparing observed abundance patterns with theoretical\nyields of metal-free stars, physical properties of their first star progenitors\ncan be inferred, including zero-age main-sequence mass and explosion energy. In\nthis work, the initial mass distribution (IMF) of first stars is obtained from\nthe largest analysis to date of 406 very metal-poor stars with the newest\nLAMOST/Subaru high-resolution spectroscopic observations. However, the mass\ndistribution fails to be consistent with the Salpeter IMF, which is also\nreported by previous studies. Here we modify the standard power-law function\nwith explodability theory. The mass distribution of Population III stars could\nbe well explained by ensuring the initial metal enrichment to originate from\nsuccessful supernova explosions. Based on the modified power-law function, we\nsuggest an extremely top-heavy or nearly flat initial mass function with a\nlarge explosion energy exponent. This indicates that supernova explodability\nshould be considered in the earliest metal enrichment process in the Universe."
                },
                "authors": [
                    {
                        "name": "Ruizheng Jiang"
                    },
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Haining Li"
                    },
                    {
                        "name": "Qianfan Xing"
                    }
                ],
                "author_detail": {
                    "name": "Qianfan Xing"
                },
                "author": "Qianfan Xing",
                "arxiv_comment": "15 pages, 5 figures, 1 table, accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08642v1",
                "updated": "2024-09-13T08:59:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T08:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "title": "CPL: Critical Planning Step Learning Boosts LLM Generalization in\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPL: Critical Planning Step Learning Boosts LLM Generalization in\n  Reasoning Tasks"
                },
                "summary": "Post-training large language models (LLMs) to develop reasoning capabilities\nhas proven effective across diverse domains, such as mathematical reasoning and\ncode generation. However, existing methods primarily focus on improving\ntask-specific reasoning but have not adequately addressed the model's\ngeneralization capabilities across a broader range of reasoning tasks. To\ntackle this challenge, we introduce Critical Planning Step Learning (CPL),\nwhich leverages Monte Carlo Tree Search (MCTS) to explore diverse planning\nsteps in multi-step reasoning tasks. Based on long-term outcomes, CPL learns\nstep-level planning preferences to improve the model's planning capabilities\nand, consequently, its general reasoning capabilities. Furthermore, while\neffective in many scenarios for aligning LLMs, existing preference learning\napproaches like Direct Preference Optimization (DPO) struggle with complex\nmulti-step reasoning tasks due to their inability to capture fine-grained\nsupervision at each step. We propose Step-level Advantage Preference\nOptimization (Step-APO), which integrates an advantage estimate for step-level\npreference pairs obtained via MCTS into the DPO. This enables the model to more\neffectively learn critical intermediate planning steps, thereby further\nimproving its generalization in reasoning tasks. Experimental results\ndemonstrate that our method, trained exclusively on GSM8K and MATH, not only\nsignificantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also\nenhances out-of-domain reasoning benchmarks, such as ARC-C (+4.0%), BBH\n(+1.8%), MMLU-STEM (+2.2%), and MMLU (+0.9%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training large language models (LLMs) to develop reasoning capabilities\nhas proven effective across diverse domains, such as mathematical reasoning and\ncode generation. However, existing methods primarily focus on improving\ntask-specific reasoning but have not adequately addressed the model's\ngeneralization capabilities across a broader range of reasoning tasks. To\ntackle this challenge, we introduce Critical Planning Step Learning (CPL),\nwhich leverages Monte Carlo Tree Search (MCTS) to explore diverse planning\nsteps in multi-step reasoning tasks. Based on long-term outcomes, CPL learns\nstep-level planning preferences to improve the model's planning capabilities\nand, consequently, its general reasoning capabilities. Furthermore, while\neffective in many scenarios for aligning LLMs, existing preference learning\napproaches like Direct Preference Optimization (DPO) struggle with complex\nmulti-step reasoning tasks due to their inability to capture fine-grained\nsupervision at each step. We propose Step-level Advantage Preference\nOptimization (Step-APO), which integrates an advantage estimate for step-level\npreference pairs obtained via MCTS into the DPO. This enables the model to more\neffectively learn critical intermediate planning steps, thereby further\nimproving its generalization in reasoning tasks. Experimental results\ndemonstrate that our method, trained exclusively on GSM8K and MATH, not only\nsignificantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also\nenhances out-of-domain reasoning benchmarks, such as ARC-C (+4.0%), BBH\n(+1.8%), MMLU-STEM (+2.2%), and MMLU (+0.9%)."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Jing Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jing Bai"
                },
                "author": "Jing Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07319v2",
                "updated": "2024-09-13T08:30:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    30,
                    38,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-12T05:06:07Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    5,
                    6,
                    7,
                    1,
                    72,
                    0
                ],
                "title": "Efficient Diffusion Model for Image Restoration by Residual Shifting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Model for Image Restoration by Residual Shifting"
                },
                "summary": "While diffusion-based image restoration (IR) methods have achieved remarkable\nsuccess, they are still limited by the low inference speed attributed to the\nnecessity of executing hundreds or even thousands of sampling steps. Existing\nacceleration sampling techniques, though seeking to expedite the process,\ninevitably sacrifice performance to some extent, resulting in over-blurry\nrestored outcomes. To address this issue, this study proposes a novel and\nefficient diffusion model for IR that significantly reduces the required number\nof diffusion steps. Our method avoids the need for post-acceleration during\ninference, thereby avoiding the associated performance deterioration.\nSpecifically, our proposed method establishes a Markov chain that facilitates\nthe transitions between the high-quality and low-quality images by shifting\ntheir residuals, substantially improving the transition efficiency. A carefully\nformulated noise schedule is devised to flexibly control the shifting speed and\nthe noise strength during the diffusion process. Extensive experimental\nevaluations demonstrate that the proposed method achieves superior or\ncomparable performance to current state-of-the-art methods on three classical\nIR tasks, namely image super-resolution, image inpainting, and blind face\nrestoration, \\textit{\\textbf{even only with four sampling steps}}. Our code and\nmodel are publicly available at \\url{https://github.com/zsyOAOA/ResShift}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion-based image restoration (IR) methods have achieved remarkable\nsuccess, they are still limited by the low inference speed attributed to the\nnecessity of executing hundreds or even thousands of sampling steps. Existing\nacceleration sampling techniques, though seeking to expedite the process,\ninevitably sacrifice performance to some extent, resulting in over-blurry\nrestored outcomes. To address this issue, this study proposes a novel and\nefficient diffusion model for IR that significantly reduces the required number\nof diffusion steps. Our method avoids the need for post-acceleration during\ninference, thereby avoiding the associated performance deterioration.\nSpecifically, our proposed method establishes a Markov chain that facilitates\nthe transitions between the high-quality and low-quality images by shifting\ntheir residuals, substantially improving the transition efficiency. A carefully\nformulated noise schedule is devised to flexibly control the shifting speed and\nthe noise strength during the diffusion process. Extensive experimental\nevaluations demonstrate that the proposed method achieves superior or\ncomparable performance to current state-of-the-art methods on three classical\nIR tasks, namely image super-resolution, image inpainting, and blind face\nrestoration, \\textit{\\textbf{even only with four sampling steps}}. Our code and\nmodel are publicly available at \\url{https://github.com/zsyOAOA/ResShift}."
                },
                "authors": [
                    {
                        "name": "Zongsheng Yue"
                    },
                    {
                        "name": "Jianyi Wang"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "arxiv_comment": "Accepted by TPAMI@2024. Code: https://github.com/zsyOAOA/ResShift",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08622v1",
                "updated": "2024-09-13T08:19:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    19,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T08:19:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    19,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking"
                },
                "summary": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for pluralistic alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for pluralistic alignment."
                },
                "authors": [
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03793v2",
                "updated": "2024-09-13T08:14:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    14,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-03T10:14:51Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    14,
                    51,
                    1,
                    247,
                    0
                ],
                "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures"
                },
                "summary": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Ishaan Domkundwar"
                    },
                    {
                        "name": "Mukunda N S"
                    },
                    {
                        "name": "Ishaan Bhola"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Bhola"
                },
                "author": "Ishaan Bhola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10718v2",
                "updated": "2024-09-13T08:09:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    9,
                    50,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-20T10:40:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?"
                },
                "summary": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhao"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Annan Li"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12196v3",
                "updated": "2024-09-13T07:51:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    51,
                    57,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-18T01:51:16Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    1,
                    51,
                    16,
                    1,
                    170,
                    0
                ],
                "title": "CITADEL: Context Similarity Based Deep Learning Framework Bug Finding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITADEL: Context Similarity Based Deep Learning Framework Bug Finding"
                },
                "summary": "With deep learning (DL) technology becoming an integral part of the new\nintelligent software, tools of DL framework testing and bug-finding are in high\ndemand. Existing DL framework testing tools have limited coverage on bug types.\nFor example, they lack the capability of finding performance bugs, which are\ncritical for DL model training and inference regarding performance, economics,\nand the environment. This problem is challenging due to the difficulty of\ngetting test oracles of performance bugs. Moreover, existing tools are\ninefficient, generating hundreds of test cases with few trigger bugs. In this\npaper, we propose Citadel, a method that accelerates the finding of bugs in\nterms of efficiency and effectiveness. We observe that many DL framework bugs\nare similar due to the similarity of operators and algorithms belonging to the\nsame family (e.g., Conv2D and Conv3D). Orthogonal to existing bug-finding\ntools, Citadel aims to find new bugs that are similar to reported ones that\nhave known test oracles. It works by first collecting existing bug reports and\nidentifying problematic APIs. Citadel defines context similarity to measure the\nsimilarity of DL framework API pairs and automatically generates test cases\nwith oracles for APIs that are similar to the problematic APIs in existing bug\nreports. Citadel respectively covers 1,436 PyTorch and 5,380 TensorFlow APIs\nand effectively detects 77 and 74 API bugs, many of which, e.g., 11 performance\nbugs, cannot be detected by existing tools. Moreover, a remarkable 35.40% of\nthe test cases generated by Citadel can trigger bugs, which significantly\ntranscends the state-of-the-art method (3.90%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With deep learning (DL) technology becoming an integral part of the new\nintelligent software, tools of DL framework testing and bug-finding are in high\ndemand. Existing DL framework testing tools have limited coverage on bug types.\nFor example, they lack the capability of finding performance bugs, which are\ncritical for DL model training and inference regarding performance, economics,\nand the environment. This problem is challenging due to the difficulty of\ngetting test oracles of performance bugs. Moreover, existing tools are\ninefficient, generating hundreds of test cases with few trigger bugs. In this\npaper, we propose Citadel, a method that accelerates the finding of bugs in\nterms of efficiency and effectiveness. We observe that many DL framework bugs\nare similar due to the similarity of operators and algorithms belonging to the\nsame family (e.g., Conv2D and Conv3D). Orthogonal to existing bug-finding\ntools, Citadel aims to find new bugs that are similar to reported ones that\nhave known test oracles. It works by first collecting existing bug reports and\nidentifying problematic APIs. Citadel defines context similarity to measure the\nsimilarity of DL framework API pairs and automatically generates test cases\nwith oracles for APIs that are similar to the problematic APIs in existing bug\nreports. Citadel respectively covers 1,436 PyTorch and 5,380 TensorFlow APIs\nand effectively detects 77 and 74 API bugs, many of which, e.g., 11 performance\nbugs, cannot be detected by existing tools. Moreover, a remarkable 35.40% of\nthe test cases generated by Citadel can trigger bugs, which significantly\ntranscends the state-of-the-art method (3.90%)."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Shiwei Wang"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05058v3",
                "updated": "2024-09-13T07:42:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    42,
                    59,
                    4,
                    257,
                    0
                ],
                "published": "2023-12-08T14:26:45Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    14,
                    26,
                    45,
                    4,
                    342,
                    0
                ],
                "title": "Spatial and Temporal Hierarchy for Autonomous Navigation using Active\n  Inference in Minigrid Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial and Temporal Hierarchy for Autonomous Navigation using Active\n  Inference in Minigrid Environment"
                },
                "summary": "Robust evidence suggests that humans explore their environment using a\ncombination of topological landmarks and coarse-grained path integration. This\napproach relies on identifiable environmental features (topological landmarks)\nin tandem with estimations of distance and direction (coarse-grained path\nintegration) to construct cognitive maps of the surroundings. This cognitive\nmap is believed to exhibit a hierarchical structure, allowing efficient\nplanning when solving complex navigation tasks. Inspired by human behaviour,\nthis paper presents a scalable hierarchical active inference model for\nautonomous navigation, exploration, and goal-oriented behaviour. The model uses\nvisual observation and motion perception to combine curiosity-driven\nexploration with goal-oriented behaviour. Motion is planned using different\nlevels of reasoning, i.e., from context to place to motion. This allows for\nefficient navigation in new spaces and rapid progress toward a target. By\nincorporating these human navigational strategies and their hierarchical\nrepresentation of the environment, this model proposes a new solution for\nautonomous navigation and exploration. The approach is validated through\nsimulations in a mini-grid environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust evidence suggests that humans explore their environment using a\ncombination of topological landmarks and coarse-grained path integration. This\napproach relies on identifiable environmental features (topological landmarks)\nin tandem with estimations of distance and direction (coarse-grained path\nintegration) to construct cognitive maps of the surroundings. This cognitive\nmap is believed to exhibit a hierarchical structure, allowing efficient\nplanning when solving complex navigation tasks. Inspired by human behaviour,\nthis paper presents a scalable hierarchical active inference model for\nautonomous navigation, exploration, and goal-oriented behaviour. The model uses\nvisual observation and motion perception to combine curiosity-driven\nexploration with goal-oriented behaviour. Motion is planned using different\nlevels of reasoning, i.e., from context to place to motion. This allows for\nefficient navigation in new spaces and rapid progress toward a target. By\nincorporating these human navigational strategies and their hierarchical\nrepresentation of the environment, this model proposes a new solution for\nautonomous navigation and exploration. The approach is validated through\nsimulations in a mini-grid environment."
                },
                "authors": [
                    {
                        "name": "Daria de Tinguy"
                    },
                    {
                        "name": "Toon van de Maele"
                    },
                    {
                        "name": "Tim Verbelen"
                    },
                    {
                        "name": "Bart Dhoedt"
                    }
                ],
                "author_detail": {
                    "name": "Bart Dhoedt"
                },
                "author": "Bart Dhoedt",
                "arxiv_doi": "10.3390/e26010083",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/e26010083",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2309.09864",
                "arxiv_journal_ref": "Entropy 2024, 26, 83, Special Issue From Functional Imaging to\n  Free Energy Dedicated to Professor Karl Friston on the Occasion of His 65th\n  Birthday",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08597v1",
                "updated": "2024-09-13T07:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    47,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T07:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    47,
                    4,
                    257,
                    0
                ],
                "title": "LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented\n  Generation"
                },
                "summary": "Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations."
                },
                "authors": [
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Xianghui He"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08596v1",
                "updated": "2024-09-13T07:28:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T07:28:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings."
                },
                "authors": [
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zhaoqing Li"
                    },
                    {
                        "name": "Yuejiao Wang"
                    },
                    {
                        "name": "Wenxuan Wu"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07003v2",
                "updated": "2024-09-13T07:19:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    19,
                    16,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-11T06:55:32Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    6,
                    55,
                    32,
                    1,
                    163,
                    0
                ],
                "title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context\n  Graph-based Retrieval and Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context\n  Graph-based Retrieval and Language Model"
                },
                "summary": "The performance of repository-level code completion depends upon the\neffective leverage of both general and repository-specific knowledge. Despite\nthe impressive capability of code LLMs in general code completion tasks, they\noften exhibit less satisfactory performance on repository-level completion due\nto the lack of repository-specific knowledge in these LLMs. To address this\nproblem, we propose GraphCoder, a retrieval-augmented code completion framework\nthat leverages LLMs' general code knowledge and the repository-specific\nknowledge via a graph-based retrieval-generation process. In particular,\nGraphCoder captures the context of completion target more accurately through\ncode context graph (CCG) that consists of control-flow, data- and\ncontrol-dependence between code statements, a more structured way to capture\nthe completion target context than the sequence-based context used in existing\nretrieval-augmented approaches; based on CCG, GraphCoder further employs a\ncoarse-to-fine retrieval process to locate context-similar code snippets with\nthe completion target from the current repository. Experimental results\ndemonstrate both the effectiveness and efficiency of GraphCoder: Compared to\nbaseline retrieval-augmented methods, GraphCoder achieves higher exact match\n(EM) on average, with increases of +6.06 in code match and +6.23 in identifier\nmatch, while using less time and space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of repository-level code completion depends upon the\neffective leverage of both general and repository-specific knowledge. Despite\nthe impressive capability of code LLMs in general code completion tasks, they\noften exhibit less satisfactory performance on repository-level completion due\nto the lack of repository-specific knowledge in these LLMs. To address this\nproblem, we propose GraphCoder, a retrieval-augmented code completion framework\nthat leverages LLMs' general code knowledge and the repository-specific\nknowledge via a graph-based retrieval-generation process. In particular,\nGraphCoder captures the context of completion target more accurately through\ncode context graph (CCG) that consists of control-flow, data- and\ncontrol-dependence between code statements, a more structured way to capture\nthe completion target context than the sequence-based context used in existing\nretrieval-augmented approaches; based on CCG, GraphCoder further employs a\ncoarse-to-fine retrieval process to locate context-similar code snippets with\nthe completion target from the current repository. Experimental results\ndemonstrate both the effectiveness and efficiency of GraphCoder: Compared to\nbaseline retrieval-augmented methods, GraphCoder achieves higher exact match\n(EM) on average, with increases of +6.06 in code match and +6.23 in identifier\nmatch, while using less time and space."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Ailun Yu"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08583v1",
                "updated": "2024-09-13T07:02:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    2,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T07:02:36Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    2,
                    36,
                    4,
                    257,
                    0
                ],
                "title": "LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling"
                },
                "summary": "Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice\nConversion (VC), enabling the transformation of one singer's voice into another\nwhile preserving musical elements such as melody, rhythm, and timbre.\nTraditional SVC methods have limitations in terms of audio quality, data\nrequirements, and computational complexity. In this paper, we propose LHQ-SVC,\na lightweight, CPU-compatible model based on the SVC framework and diffusion\nmodel, designed to reduce model size and computational demand without\nsacrificing performance. We incorporate features to improve inference quality,\nand optimize for CPU execution by using performance tuning tools and parallel\ncomputing frameworks. Our experiments demonstrate that LHQ-SVC maintains\ncompetitive performance, with significant improvements in processing speed and\nefficiency across different devices. The results suggest that LHQ-SVC can meet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice\nConversion (VC), enabling the transformation of one singer's voice into another\nwhile preserving musical elements such as melody, rhythm, and timbre.\nTraditional SVC methods have limitations in terms of audio quality, data\nrequirements, and computational complexity. In this paper, we propose LHQ-SVC,\na lightweight, CPU-compatible model based on the SVC framework and diffusion\nmodel, designed to reduce model size and computational demand without\nsacrificing performance. We incorporate features to improve inference quality,\nand optimize for CPU execution by using performance tuning tools and parallel\ncomputing frameworks. Our experiments demonstrate that LHQ-SVC maintains\ncompetitive performance, with significant improvements in processing speed and\nefficiency across different devices. The results suggest that LHQ-SVC can meet"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Xin Lai"
                    },
                    {
                        "name": "Muyang Ye"
                    },
                    {
                        "name": "Anran Zhu"
                    },
                    {
                        "name": "Zixi Wang"
                    },
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zhiyuan Zhou"
                    },
                    {
                        "name": "Weijie Niu"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Niu"
                },
                "author": "Weijie Niu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05693v2",
                "updated": "2024-09-13T06:57:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    57,
                    1,
                    4,
                    257,
                    0
                ],
                "published": "2024-07-08T07:47:30Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    7,
                    47,
                    30,
                    0,
                    190,
                    0
                ],
                "title": "Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation"
                },
                "summary": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL."
                },
                "authors": [
                    {
                        "name": "Jian Qian"
                    },
                    {
                        "name": "Miao Sun"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Ruizhi Hun"
                    },
                    {
                        "name": "Patrick Chiang"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Chiang"
                },
                "author": "Patrick Chiang",
                "arxiv_comment": "Accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08564v1",
                "updated": "2024-09-13T06:34:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    34,
                    15,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:34:15Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    34,
                    15,
                    4,
                    257,
                    0
                ],
                "title": "Cracking the Code: Multi-domain LLM Evaluation on Real-World\n  Professional Exams in Indonesia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code: Multi-domain LLM Evaluation on Real-World\n  Professional Exams in Indonesia"
                },
                "summary": "While knowledge evaluation in large language models has predominantly focused\non academic subjects like math and physics, these assessments often fail to\ncapture the practical demands of real-world professions. In this paper, we\nintroduce IndoCareer, a dataset comprising 8,834 multiple-choice questions\ndesigned to evaluate performance in vocational and professional certification\nexams across various fields. With a focus on Indonesia, IndoCareer provides\nrich local contexts, spanning six key sectors: (1) healthcare, (2) insurance\nand finance, (3) creative and design, (4) tourism and hospitality, (5)\neducation and training, and (6) law. Our comprehensive evaluation of 27 large\nlanguage models shows that these models struggle particularly in fields with\nstrong local contexts, such as insurance and finance. Additionally, while using\nthe entire dataset, shuffling answer options generally maintains consistent\nevaluation results across models, but it introduces instability specifically in\nthe insurance and finance sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While knowledge evaluation in large language models has predominantly focused\non academic subjects like math and physics, these assessments often fail to\ncapture the practical demands of real-world professions. In this paper, we\nintroduce IndoCareer, a dataset comprising 8,834 multiple-choice questions\ndesigned to evaluate performance in vocational and professional certification\nexams across various fields. With a focus on Indonesia, IndoCareer provides\nrich local contexts, spanning six key sectors: (1) healthcare, (2) insurance\nand finance, (3) creative and design, (4) tourism and hospitality, (5)\neducation and training, and (6) law. Our comprehensive evaluation of 27 large\nlanguage models shows that these models struggle particularly in fields with\nstrong local contexts, such as insurance and finance. Additionally, while using\nthe entire dataset, shuffling answer options generally maintains consistent\nevaluation results across models, but it introduces instability specifically in\nthe insurance and finance sectors."
                },
                "authors": [
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08561v1",
                "updated": "2024-09-13T06:29:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    29,
                    20,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:29:20Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    29,
                    20,
                    4,
                    257,
                    0
                ],
                "title": "Expediting and Elevating Large Language Model Reasoning via Hidden\n  Chain-of-Thought Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expediting and Elevating Large Language Model Reasoning via Hidden\n  Chain-of-Thought Decoding"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications."
                },
                "authors": [
                    {
                        "name": "Tianqiao Liu"
                    },
                    {
                        "name": "Zui Chen"
                    },
                    {
                        "name": "Zitao Liu"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Weiqi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Weiqi Luo"
                },
                "author": "Weiqi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08554v1",
                "updated": "2024-09-13T06:13:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    13,
                    55,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:13:55Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    13,
                    55,
                    4,
                    257,
                    0
                ],
                "title": "LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study"
                },
                "summary": "Grapheme-to-phoneme (G2P) conversion is critical in speech processing,\nparticularly for applications like speech synthesis. G2P systems must possess\nlinguistic understanding and contextual awareness of languages with polyphone\nwords and context-dependent phonemes. Large language models (LLMs) have\nrecently demonstrated significant potential in various language tasks,\nsuggesting that their phonetic knowledge could be leveraged for G2P. In this\npaper, we evaluate the performance of LLMs in G2P conversion and introduce\nprompting and post-processing methods that enhance LLM outputs without\nadditional training or labeled data. We also present a benchmarking dataset\ndesigned to assess G2P performance on sentence-level phonetic challenges of the\nPersian language. Our results show that by applying the proposed methods, LLMs\ncan outperform traditional G2P tools, even in an underrepresented language like\nPersian, highlighting the potential of developing LLM-aided G2P systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grapheme-to-phoneme (G2P) conversion is critical in speech processing,\nparticularly for applications like speech synthesis. G2P systems must possess\nlinguistic understanding and contextual awareness of languages with polyphone\nwords and context-dependent phonemes. Large language models (LLMs) have\nrecently demonstrated significant potential in various language tasks,\nsuggesting that their phonetic knowledge could be leveraged for G2P. In this\npaper, we evaluate the performance of LLMs in G2P conversion and introduce\nprompting and post-processing methods that enhance LLM outputs without\nadditional training or labeled data. We also present a benchmarking dataset\ndesigned to assess G2P performance on sentence-level phonetic challenges of the\nPersian language. Our results show that by applying the proposed methods, LLMs\ncan outperform traditional G2P tools, even in an underrepresented language like\nPersian, highlighting the potential of developing LLM-aided G2P systems."
                },
                "authors": [
                    {
                        "name": "Mahta Fetrat Qharabagh"
                    },
                    {
                        "name": "Zahra Dehghanian"
                    },
                    {
                        "name": "Hamid R. Rabiee"
                    }
                ],
                "author_detail": {
                    "name": "Hamid R. Rabiee"
                },
                "author": "Hamid R. Rabiee",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08551v1",
                "updated": "2024-09-13T06:10:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    10,
                    54,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:10:54Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    10,
                    54,
                    4,
                    257,
                    0
                ],
                "title": "Think Twice Before You Act: Improving Inverse Problem Solving With MCMC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice Before You Act: Improving Inverse Problem Solving With MCMC"
                },
                "summary": "Recent studies demonstrate that diffusion models can serve as a strong prior\nfor solving inverse problems. A prominent example is Diffusion Posterior\nSampling (DPS), which approximates the posterior distribution of data given the\nmeasure using Tweedie's formula. Despite the merits of being versatile in\nsolving various inverse problems without re-training, the performance of DPS is\nhindered by the fact that this posterior approximation can be inaccurate\nespecially for high noise levels. Therefore, we propose \\textbf{D}iffusion\n\\textbf{P}osterior \\textbf{MC}MC (\\textbf{DPMC}), a novel inference algorithm\nbased on Annealed MCMC to solve inverse problems with pretrained diffusion\nmodels. We define a series of intermediate distributions inspired by the\napproximated conditional distributions used by DPS. Through annealed MCMC\nsampling, we encourage the samples to follow each intermediate distribution\nmore closely before moving to the next distribution at a lower noise level, and\ntherefore reduce the accumulated error along the path. We test our algorithm in\nvarious inverse problems, including super resolution, Gaussian deblurring,\nmotion deblurring, inpainting, and phase retrieval. Our algorithm outperforms\nDPS with less number of evaluations across nearly all tasks, and is competitive\namong existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies demonstrate that diffusion models can serve as a strong prior\nfor solving inverse problems. A prominent example is Diffusion Posterior\nSampling (DPS), which approximates the posterior distribution of data given the\nmeasure using Tweedie's formula. Despite the merits of being versatile in\nsolving various inverse problems without re-training, the performance of DPS is\nhindered by the fact that this posterior approximation can be inaccurate\nespecially for high noise levels. Therefore, we propose \\textbf{D}iffusion\n\\textbf{P}osterior \\textbf{MC}MC (\\textbf{DPMC}), a novel inference algorithm\nbased on Annealed MCMC to solve inverse problems with pretrained diffusion\nmodels. We define a series of intermediate distributions inspired by the\napproximated conditional distributions used by DPS. Through annealed MCMC\nsampling, we encourage the samples to follow each intermediate distribution\nmore closely before moving to the next distribution at a lower noise level, and\ntherefore reduce the accumulated error along the path. We test our algorithm in\nvarious inverse problems, including super resolution, Gaussian deblurring,\nmotion deblurring, inpainting, and phase retrieval. Our algorithm outperforms\nDPS with less number of evaluations across nearly all tasks, and is competitive\namong existing approaches."
                },
                "authors": [
                    {
                        "name": "Yaxuan Zhu"
                    },
                    {
                        "name": "Zehao Dou"
                    },
                    {
                        "name": "Haoxin Zheng"
                    },
                    {
                        "name": "Yasi Zhang"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Ruiqi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Ruiqi Gao"
                },
                "author": "Ruiqi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05116v2",
                "updated": "2024-09-13T06:00:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    0,
                    3,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-08T14:56:25Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    56,
                    25,
                    6,
                    252,
                    0
                ],
                "title": "Diffusion-based Speech Enhancement with Schrödinger Bridge and\n  Symmetric Noise Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Speech Enhancement with Schrödinger Bridge and\n  Symmetric Noise Schedule"
                },
                "summary": "Recently, diffusion-based generative models have demonstrated remarkable\nperformance in speech enhancement tasks. However, these methods still encounter\nchallenges, including the lack of structural information and poor performance\nin low Signal-to-Noise Ratio (SNR) scenarios. To overcome these challenges, we\npropose the Schr\\\"oodinger Bridge-based Speech Enhancement (SBSE) method, which\nlearns the diffusion processes directly between the noisy input and the clean\ndistribution, unlike conventional diffusion-based speech enhancement systems\nthat learn data to Gaussian distributions. To enhance performance in extremely\nnoisy conditions, we introduce a two-stage system incorporating ratio mask\ninformation into the diffusion-based generative model. Our experimental results\nshow that our proposed SBSE method outperforms all the baseline models and\nachieves state-of-the-art performance, especially in low SNR conditions.\nImportantly, only a few inference steps are required to achieve the best\nresult.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based generative models have demonstrated remarkable\nperformance in speech enhancement tasks. However, these methods still encounter\nchallenges, including the lack of structural information and poor performance\nin low Signal-to-Noise Ratio (SNR) scenarios. To overcome these challenges, we\npropose the Schr\\\"oodinger Bridge-based Speech Enhancement (SBSE) method, which\nlearns the diffusion processes directly between the noisy input and the clean\ndistribution, unlike conventional diffusion-based speech enhancement systems\nthat learn data to Gaussian distributions. To enhance performance in extremely\nnoisy conditions, we introduce a two-stage system incorporating ratio mask\ninformation into the diffusion-based generative model. Our experimental results\nshow that our proposed SBSE method outperforms all the baseline models and\nachieves state-of-the-art performance, especially in low SNR conditions.\nImportantly, only a few inference steps are required to achieve the best\nresult."
                },
                "authors": [
                    {
                        "name": "Siyi Wang"
                    },
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Andrew Harper"
                    },
                    {
                        "name": "Paul Kendrick"
                    },
                    {
                        "name": "Mathieu Salzmann"
                    },
                    {
                        "name": "Milos Cernak"
                    }
                ],
                "author_detail": {
                    "name": "Milos Cernak"
                },
                "author": "Milos Cernak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13745v3",
                "updated": "2024-09-13T05:50:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    50,
                    11,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-25T07:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation"
                },
                "summary": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation."
                },
                "authors": [
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12632v2",
                "updated": "2024-09-13T05:43:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    43,
                    14,
                    4,
                    257,
                    0
                ],
                "published": "2024-07-17T15:00:35Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    15,
                    0,
                    35,
                    2,
                    199,
                    0
                ],
                "title": "CerberusDet: Unified Multi-Dataset Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CerberusDet: Unified Multi-Dataset Object Detection"
                },
                "summary": "Conventional object detection models are usually limited by the data on which\nthey were trained and by the category logic they define. With the recent rise\nof Language-Visual Models, new methods have emerged that are not restricted to\nthese fixed categories. Despite their flexibility, such Open Vocabulary\ndetection models still fall short in accuracy compared to traditional models\nwith fixed classes. At the same time, more accurate data-specific models face\nchallenges when there is a need to extend classes or merge different datasets\nfor training. The latter often cannot be combined due to different logics or\nconflicting class definitions, making it difficult to improve a model without\ncompromising its performance. In this paper, we introduce CerberusDet, a\nframework with a multi-headed model designed for handling multiple object\ndetection tasks. Proposed model is built on the YOLO architecture and\nefficiently shares visual features from both backbone and neck components,\nwhile maintaining separate task heads. This approach allows CerberusDet to\nperform very efficiently while still delivering optimal results. We evaluated\nthe model on the PASCAL VOC dataset and Objects365 dataset to demonstrate its\nabilities. CerberusDet achieved state-of-the-art results with 36% less\ninference time. The more tasks are trained together, the more efficient the\nproposed model becomes compared to running individual models sequentially. The\ntraining and inference code, as well as the model, are available as open-source\n(https://github.com/ai-forever/CerberusDet).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional object detection models are usually limited by the data on which\nthey were trained and by the category logic they define. With the recent rise\nof Language-Visual Models, new methods have emerged that are not restricted to\nthese fixed categories. Despite their flexibility, such Open Vocabulary\ndetection models still fall short in accuracy compared to traditional models\nwith fixed classes. At the same time, more accurate data-specific models face\nchallenges when there is a need to extend classes or merge different datasets\nfor training. The latter often cannot be combined due to different logics or\nconflicting class definitions, making it difficult to improve a model without\ncompromising its performance. In this paper, we introduce CerberusDet, a\nframework with a multi-headed model designed for handling multiple object\ndetection tasks. Proposed model is built on the YOLO architecture and\nefficiently shares visual features from both backbone and neck components,\nwhile maintaining separate task heads. This approach allows CerberusDet to\nperform very efficiently while still delivering optimal results. We evaluated\nthe model on the PASCAL VOC dataset and Objects365 dataset to demonstrate its\nabilities. CerberusDet achieved state-of-the-art results with 36% less\ninference time. The more tasks are trained together, the more efficient the\nproposed model becomes compared to running individual models sequentially. The\ntraining and inference code, as well as the model, are available as open-source\n(https://github.com/ai-forever/CerberusDet)."
                },
                "authors": [
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Mikhail Chernyshov"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.4.0; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08544v1",
                "updated": "2024-09-13T05:39:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    39,
                    0,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T05:39:00Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    39,
                    0,
                    4,
                    257,
                    0
                ],
                "title": "Causal GNNs: A GNN-Driven Instrumental Variable Approach for Causal\n  Inference in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal GNNs: A GNN-Driven Instrumental Variable Approach for Causal\n  Inference in Networks"
                },
                "summary": "As network data applications continue to expand, causal inference within\nnetworks has garnered increasing attention. However, hidden confounders\ncomplicate the estimation of causal effects. Most methods rely on the strong\nignorability assumption, which presumes the absence of hidden confounders-an\nassumption that is both difficult to validate and often unrealistic in\npractice. To address this issue, we propose CgNN, a novel approach that\nleverages network structure as instrumental variables (IVs), combined with\ngraph neural networks (GNNs) and attention mechanisms, to mitigate hidden\nconfounder bias and improve causal effect estimation. By utilizing network\nstructure as IVs, we reduce confounder bias while preserving the correlation\nwith treatment. Our integration of attention mechanisms enhances robustness and\nimproves the identification of important nodes. Validated on two real-world\ndatasets, our results demonstrate that CgNN effectively mitigates hidden\nconfounder bias and offers a robust GNN-driven IV framework for causal\ninference in complex network data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As network data applications continue to expand, causal inference within\nnetworks has garnered increasing attention. However, hidden confounders\ncomplicate the estimation of causal effects. Most methods rely on the strong\nignorability assumption, which presumes the absence of hidden confounders-an\nassumption that is both difficult to validate and often unrealistic in\npractice. To address this issue, we propose CgNN, a novel approach that\nleverages network structure as instrumental variables (IVs), combined with\ngraph neural networks (GNNs) and attention mechanisms, to mitigate hidden\nconfounder bias and improve causal effect estimation. By utilizing network\nstructure as IVs, we reduce confounder bias while preserving the correlation\nwith treatment. Our integration of attention mechanisms enhances robustness and\nimproves the identification of important nodes. Validated on two real-world\ndatasets, our results demonstrate that CgNN effectively mitigates hidden\nconfounder bias and offers a robust GNN-driven IV framework for causal\ninference in complex network data."
                },
                "authors": [
                    {
                        "name": "Xiaojing Du"
                    },
                    {
                        "name": "Feiyu Yang"
                    },
                    {
                        "name": "Wentao Gao"
                    },
                    {
                        "name": "Xiongren Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiongren Chen"
                },
                "author": "Xiongren Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v3",
                "updated": "2024-09-13T05:34:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08543v1",
                "updated": "2024-09-13T05:33:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    33,
                    9,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T05:33:09Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    33,
                    9,
                    4,
                    257,
                    0
                ],
                "title": "ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and\n  Low-Rank Adaptation via Instruction-Tuned Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and\n  Low-Rank Adaptation via Instruction-Tuned Large Language Model"
                },
                "summary": "Recommender Systems (RS) play a pivotal role in boosting user satisfaction by\nproviding personalized product suggestions in domains such as e-commerce and\nentertainment. This study examines the integration of multimodal data text and\naudio into large language models (LLMs) with the aim of enhancing\nrecommendation performance. Traditional text and audio recommenders encounter\nlimitations such as the cold-start problem, and recent advancements in LLMs,\nwhile promising, are computationally expensive. To address these issues,\nLow-Rank Adaptation (LoRA) is introduced, which enhances efficiency without\ncompromising performance. The ATFLRec framework is proposed to integrate audio\nand text modalities into a multimodal recommendation system, utilizing various\nLoRA configurations and modality fusion techniques. Results indicate that\nATFLRec outperforms baseline models, including traditional and graph neural\nnetwork-based approaches, achieving higher AUC scores. Furthermore, separate\nfine-tuning of audio and text data with distinct LoRA modules yields optimal\nperformance, with different pooling methods and Mel filter bank numbers\nsignificantly impacting performance. This research offers valuable insights\ninto optimizing multimodal recommender systems and advancing the integration of\ndiverse data modalities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender Systems (RS) play a pivotal role in boosting user satisfaction by\nproviding personalized product suggestions in domains such as e-commerce and\nentertainment. This study examines the integration of multimodal data text and\naudio into large language models (LLMs) with the aim of enhancing\nrecommendation performance. Traditional text and audio recommenders encounter\nlimitations such as the cold-start problem, and recent advancements in LLMs,\nwhile promising, are computationally expensive. To address these issues,\nLow-Rank Adaptation (LoRA) is introduced, which enhances efficiency without\ncompromising performance. The ATFLRec framework is proposed to integrate audio\nand text modalities into a multimodal recommendation system, utilizing various\nLoRA configurations and modality fusion techniques. Results indicate that\nATFLRec outperforms baseline models, including traditional and graph neural\nnetwork-based approaches, achieving higher AUC scores. Furthermore, separate\nfine-tuning of audio and text data with distinct LoRA modules yields optimal\nperformance, with different pooling methods and Mel filter bank numbers\nsignificantly impacting performance. This research offers valuable insights\ninto optimizing multimodal recommender systems and advancing the integration of\ndiverse data modalities in LLMs."
                },
                "authors": [
                    {
                        "name": "Zezheng Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zezheng Qin"
                },
                "author": "Zezheng Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04811v2",
                "updated": "2024-09-13T05:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    19,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-09T01:45:39Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    1,
                    45,
                    39,
                    4,
                    222,
                    0
                ],
                "title": "h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM\n  Safety Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM\n  Safety Assessment"
                },
                "summary": "The safety of Large Language Models (LLMs) remains a critical concern due to\na lack of adequate benchmarks for systematically evaluating their ability to\nresist generating harmful content. Previous efforts towards automated red\nteaming involve static or templated sets of illicit requests and adversarial\nprompts which have limited utility given jailbreak attacks' evolving and\ncomposable nature. We propose a novel dynamic benchmark of composable jailbreak\nattacks to move beyond static datasets and taxonomies of attacks and harms. Our\napproach consists of three components collectively called h4rm3l: (1) a\ndomain-specific language that formally expresses jailbreak attacks as\ncompositions of parameterized prompt transformation primitives, (2)\nbandit-based few-shot program synthesis algorithms that generate novel attacks\noptimized to penetrate the safety filters of a target black box LLM, and (3)\nopen-source automated red-teaming software employing the previous two\ncomponents. We use h4rm3l to generate a dataset of 2656 successful novel\njailbreak attacks targeting 6 state-of-the-art (SOTA) open-source and\nproprietary LLMs. Several of our synthesized attacks are more effective than\npreviously reported ones, with Attack Success Rates exceeding 90% on SOTA\nclosed language models such as claude-3-haiku and GPT4-o. By generating\ndatasets of jailbreak attacks in a unified formal representation, h4rm3l\nenables reproducible benchmarking and automated red-teaming, contributes to\nunderstanding LLM safety limitations, and supports the development of robust\ndefenses in an increasingly LLM-integrated world.\n  Warning: This paper and related research artifacts contain offensive and\npotentially disturbing prompts and model-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety of Large Language Models (LLMs) remains a critical concern due to\na lack of adequate benchmarks for systematically evaluating their ability to\nresist generating harmful content. Previous efforts towards automated red\nteaming involve static or templated sets of illicit requests and adversarial\nprompts which have limited utility given jailbreak attacks' evolving and\ncomposable nature. We propose a novel dynamic benchmark of composable jailbreak\nattacks to move beyond static datasets and taxonomies of attacks and harms. Our\napproach consists of three components collectively called h4rm3l: (1) a\ndomain-specific language that formally expresses jailbreak attacks as\ncompositions of parameterized prompt transformation primitives, (2)\nbandit-based few-shot program synthesis algorithms that generate novel attacks\noptimized to penetrate the safety filters of a target black box LLM, and (3)\nopen-source automated red-teaming software employing the previous two\ncomponents. We use h4rm3l to generate a dataset of 2656 successful novel\njailbreak attacks targeting 6 state-of-the-art (SOTA) open-source and\nproprietary LLMs. Several of our synthesized attacks are more effective than\npreviously reported ones, with Attack Success Rates exceeding 90% on SOTA\nclosed language models such as claude-3-haiku and GPT4-o. By generating\ndatasets of jailbreak attacks in a unified formal representation, h4rm3l\nenables reproducible benchmarking and automated red-teaming, contributes to\nunderstanding LLM safety limitations, and supports the development of robust\ndefenses in an increasingly LLM-integrated world.\n  Warning: This paper and related research artifacts contain offensive and\npotentially disturbing prompts and model-generated content."
                },
                "authors": [
                    {
                        "name": "Moussa Koulako Bala Doumbouya"
                    },
                    {
                        "name": "Ananjan Nandi"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Davide Ghilardi"
                    },
                    {
                        "name": "Anna Goldie"
                    },
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.0; I.2.1; I.2.5; I.2.7; K.6.5; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v4",
                "updated": "2024-09-13T05:12:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    12,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v2",
                "updated": "2024-09-13T04:54:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    54,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on nVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various LLMs and\ntoken lengths, with a particular focus on the bottleneck caused by CPU-GPU data\ntransfers via PCIe. Our results indicate that while there is minimal\ncomputational overhead within the GPU, the overall performance penalty is\nprimarily attributable to data transfer. For the majority of typical LLM\nqueries, the overhead remains below 5%, with larger models and longer sequences\nexperiencing nearly zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on nVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various LLMs and\ntoken lengths, with a particular focus on the bottleneck caused by CPU-GPU data\ntransfers via PCIe. Our results indicate that while there is minimal\ncomputational overhead within the GPU, the overall performance penalty is\nprimarily attributable to data transfer. For the majority of typical LLM\nqueries, the overhead remains below 5%, with larger models and longer sequences\nexperiencing nearly zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Peng Deng"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14515v2",
                "updated": "2024-09-13T04:25:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    25,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-25T11:33:52Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    33,
                    52,
                    6,
                    238,
                    0
                ],
                "title": "A Joint Learning Model with Variational Interaction for Multilingual\n  Program Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Model with Variational Interaction for Multilingual\n  Program Translation"
                },
                "summary": "Programs implemented in various programming languages form the foundation of\nsoftware applications. To alleviate the burden of program migration and\nfacilitate the development of software systems, automated program translation\nacross languages has garnered significant attention. Previous approaches\nprimarily focus on pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel data is\ndifficult to collect for some language pairs, and the distribution of program\nsemantics across languages can shift, posing challenges for pairwise program\ntranslation. In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior to separately\nlearning from bilingual parallel data. We propose Variational Interaction for\nMultilingual Program Translation~(VIM-PT), a disentanglement-based generative\napproach that jointly trains a unified model for multilingual program\ntranslation across multiple languages. VIM-PT disentangles code into\nlanguage-shared and language-specific features, using variational inference and\ninteraction information with a novel lower bound, then achieves program\ntranslation through conditional generation. VIM-PT demonstrates four\nadvantages: 1) captures language-shared information more accurately from\nvarious implementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel data, 3)\naddresses the distribution shift of program semantics across languages, 4) and\nserves as a unified model, reducing deployment complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programs implemented in various programming languages form the foundation of\nsoftware applications. To alleviate the burden of program migration and\nfacilitate the development of software systems, automated program translation\nacross languages has garnered significant attention. Previous approaches\nprimarily focus on pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel data is\ndifficult to collect for some language pairs, and the distribution of program\nsemantics across languages can shift, posing challenges for pairwise program\ntranslation. In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior to separately\nlearning from bilingual parallel data. We propose Variational Interaction for\nMultilingual Program Translation~(VIM-PT), a disentanglement-based generative\napproach that jointly trains a unified model for multilingual program\ntranslation across multiple languages. VIM-PT disentangles code into\nlanguage-shared and language-specific features, using variational inference and\ninteraction information with a novel lower bound, then achieves program\ntranslation through conditional generation. VIM-PT demonstrates four\nadvantages: 1) captures language-shared information more accurately from\nvarious implementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel data, 3)\naddresses the distribution shift of program semantics across languages, 4) and\nserves as a unified model, reducing deployment complexity."
                },
                "authors": [
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07276v2",
                "updated": "2024-09-13T04:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    16,
                    55,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-11T13:49:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM"
                },
                "summary": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08523v1",
                "updated": "2024-09-13T04:06:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    6,
                    0,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T04:06:00Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    6,
                    0,
                    4,
                    257,
                    0
                ],
                "title": "Eir: Thai Medical Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eir: Thai Medical Large Language Models"
                },
                "summary": "We present Eir Thai Medical LLM, a large language model with 8 billion\nparameters, specifically designed to enhance the accuracy of handling medical\ntasks in the Thai language. This model focuses on providing clear and\neasy-to-understand answers for both healthcare professionals and patients,\nthereby improving the efficiency of diagnosis and treatment processes. Human\nevaluation was conducted to ensure that the model adheres to care standards and\nprovides unbiased answers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop Eir\nThai Medical LLM. Our evaluation employed multiple questioning strategies,\nincluding zero-shot, few-shot, chain-of-thought reasoning, and\nensemble/self-consistency voting methods. Our model outperformed commercially\navailable Thai-language large language models by more than 10%. In addition, we\ndeveloped enhanced model testing tailored for clinical use in Thai across 18\nclinical tasks, where our model exceeded GPT-4o performance by more than 11%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Eir Thai Medical LLM, a large language model with 8 billion\nparameters, specifically designed to enhance the accuracy of handling medical\ntasks in the Thai language. This model focuses on providing clear and\neasy-to-understand answers for both healthcare professionals and patients,\nthereby improving the efficiency of diagnosis and treatment processes. Human\nevaluation was conducted to ensure that the model adheres to care standards and\nprovides unbiased answers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop Eir\nThai Medical LLM. Our evaluation employed multiple questioning strategies,\nincluding zero-shot, few-shot, chain-of-thought reasoning, and\nensemble/self-consistency voting methods. Our model outperformed commercially\navailable Thai-language large language models by more than 10%. In addition, we\ndeveloped enhanced model testing tailored for clinical use in Thai across 18\nclinical tasks, where our model exceeded GPT-4o performance by more than 11%"
                },
                "authors": [
                    {
                        "name": "Yutthakorn Thiprak"
                    },
                    {
                        "name": "Rungtam Ngodngamthaweesuk"
                    },
                    {
                        "name": "Songtam Ngodngamtaweesuk"
                    }
                ],
                "author_detail": {
                    "name": "Songtam Ngodngamtaweesuk"
                },
                "author": "Songtam Ngodngamtaweesuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07556v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07556v4",
                "updated": "2024-09-13T03:59:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    59,
                    49,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-12T11:40:44Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    11,
                    40,
                    44,
                    1,
                    72,
                    0
                ],
                "title": "Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information."
                },
                "authors": [
                    {
                        "name": "Tian Yu"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Accepted to ACL 2024 Findings. Code is available at:\n  https://github.com/ictnlp/TACS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07556v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07556v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08518v1",
                "updated": "2024-09-13T03:34:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    34,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T03:34:37Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    34,
                    37,
                    4,
                    257,
                    0
                ],
                "title": "Anytime Continual Learning for Open Vocabulary Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime Continual Learning for Open Vocabulary Classification"
                },
                "summary": "We propose an approach for anytime continual learning (AnytimeCL) for open\nvocabulary image classification. The AnytimeCL problem aims to break away from\nbatch training and rigid models by requiring that a system can predict any set\nof labels at any time and efficiently update and improve when receiving one or\nmore training samples at any time. Despite the challenging goal, we achieve\nsubstantial improvements over recent methods. We propose a dynamic weighting\nbetween predictions of a partially fine-tuned model and a fixed open vocabulary\nmodel that enables continual improvement when training samples are available\nfor a subset of a task's labels. We also propose an attention-weighted PCA\ncompression of training features that reduces storage and computation with\nlittle impact to model accuracy. Our methods are validated with experiments\nthat test flexibility of learning and inference. Code is available at\nhttps://github.com/jessemelpolio/AnytimeCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an approach for anytime continual learning (AnytimeCL) for open\nvocabulary image classification. The AnytimeCL problem aims to break away from\nbatch training and rigid models by requiring that a system can predict any set\nof labels at any time and efficiently update and improve when receiving one or\nmore training samples at any time. Despite the challenging goal, we achieve\nsubstantial improvements over recent methods. We propose a dynamic weighting\nbetween predictions of a partially fine-tuned model and a fixed open vocabulary\nmodel that enables continual improvement when training samples are available\nfor a subset of a task's labels. We also propose an attention-weighted PCA\ncompression of training features that reduces storage and computation with\nlittle impact to model accuracy. Our methods are validated with experiments\nthat test flexibility of learning and inference. Code is available at\nhttps://github.com/jessemelpolio/AnytimeCL."
                },
                "authors": [
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Yiming Gong"
                    },
                    {
                        "name": "Derek Hoiem"
                    }
                ],
                "author_detail": {
                    "name": "Derek Hoiem"
                },
                "author": "Derek Hoiem",
                "arxiv_comment": "To appear at ECCV 2024 as Oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11022v2",
                "updated": "2024-09-13T03:27:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    27,
                    51,
                    4,
                    257,
                    0
                ],
                "published": "2024-04-17T02:57:31Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    2,
                    57,
                    31,
                    2,
                    108,
                    0
                ],
                "title": "The Radius Distribution of M dwarf-hosted Planets and its Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radius Distribution of M dwarf-hosted Planets and its Evolution"
                },
                "summary": "M dwarf stars are the most promising hosts for detection and characterization\nof small and potentially habitable planets, and provide leverage relative to\nsolar-type stars to test models of planet formation and evolution. Using\n\\emph{Gaia} astrometry, adaptive optics imaging, and calibrated gyrochronologic\nrelations to estimate stellar properties and filter binaries we refined the\nradii of 117 Kepler Objects of Interest (confirmed or candidate planets)\ntransiting 74 single late K- and early M-type stars, and assigned stellar\nrotation-based ages to 113 of these. We constructed the radius distribution of\n115 small ($<4R_{\\oplus}$) planets and assessed its evolution. As for\nsolar-type stars, the inferred distribution contains distinct populations of\n\"super-Earths\" (at $\\approx$1.3$_{\\oplus}$) and \"sub-Neptunes\" (at\n$\\approx$2.2$R_{\\oplus}$) separated by a gap or \"valley\" at\n$\\approx$1.7$_{\\oplus}$ that has a period dependence that is significantly\nweaker (power law index of -0.03$^{+0.01}_{-0.03}$) than for solar-type stars.\nSub-Neptunes are largely absent at short periods ($<$2 days) and high\nirradiance, a feature analogous to the \"Neptune desert\" observed around\nsolar-type stars. The relative number of sub-Neptunes to super-Earths declines\nbetween the younger and older halves of the sample (median age 3.86 Gyr),\nalthough the formal significance is low ($p = 0.08$) because of the small\nsample size. The decline in sub-Neptunes appears to be more pronounced on wider\norbits and low stellar irradiance. This is not due to detection bias and\nsuggests a role for H$_2$O as steam in inflating the radii of sub-Neptunes\nand/or regulating the escape of H/He from them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M dwarf stars are the most promising hosts for detection and characterization\nof small and potentially habitable planets, and provide leverage relative to\nsolar-type stars to test models of planet formation and evolution. Using\n\\emph{Gaia} astrometry, adaptive optics imaging, and calibrated gyrochronologic\nrelations to estimate stellar properties and filter binaries we refined the\nradii of 117 Kepler Objects of Interest (confirmed or candidate planets)\ntransiting 74 single late K- and early M-type stars, and assigned stellar\nrotation-based ages to 113 of these. We constructed the radius distribution of\n115 small ($<4R_{\\oplus}$) planets and assessed its evolution. As for\nsolar-type stars, the inferred distribution contains distinct populations of\n\"super-Earths\" (at $\\approx$1.3$_{\\oplus}$) and \"sub-Neptunes\" (at\n$\\approx$2.2$R_{\\oplus}$) separated by a gap or \"valley\" at\n$\\approx$1.7$_{\\oplus}$ that has a period dependence that is significantly\nweaker (power law index of -0.03$^{+0.01}_{-0.03}$) than for solar-type stars.\nSub-Neptunes are largely absent at short periods ($<$2 days) and high\nirradiance, a feature analogous to the \"Neptune desert\" observed around\nsolar-type stars. The relative number of sub-Neptunes to super-Earths declines\nbetween the younger and older halves of the sample (median age 3.86 Gyr),\nalthough the formal significance is low ($p = 0.08$) because of the small\nsample size. The decline in sub-Neptunes appears to be more pronounced on wider\norbits and low stellar irradiance. This is not due to detection bias and\nsuggests a role for H$_2$O as steam in inflating the radii of sub-Neptunes\nand/or regulating the escape of H/He from them."
                },
                "authors": [
                    {
                        "name": "Eric Gaidos"
                    },
                    {
                        "name": "Aleezah Ali"
                    },
                    {
                        "name": "Adam L. Kraus"
                    },
                    {
                        "name": "Jason F. Rowe"
                    }
                ],
                "author_detail": {
                    "name": "Jason F. Rowe"
                },
                "author": "Jason F. Rowe",
                "arxiv_comment": "Accepted to MNRAS on 2024 September 12. Received 2024 September 7; in\n  original form 2023 August 14. Table 1 available in machine readable form as\n  ancillary file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03612v2",
                "updated": "2024-09-13T03:16:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    16,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-02-06T00:55:06Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    0,
                    55,
                    6,
                    1,
                    37,
                    0
                ],
                "title": "Privacy risk in GeoData: A survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy risk in GeoData: A survey"
                },
                "summary": "With the ubiquitous use of location-based services, large-scale\nindividual-level location data has been widely collected through\nlocation-awareness devices. The widespread exposure of such location data poses\nsignificant privacy risks to users, as it can lead to re-identification, the\ninference of sensitive information, and even physical threats. In this survey,\nwe analyse different geomasking techniques proposed to protect individuals'\nprivacy in geodata. We propose a taxonomy to characterise these techniques\nacross various dimensions. We then highlight the shortcomings of current\ntechniques and discuss avenues for future research. Our proposed taxonomy\nserves as a practical resource for data custodians, offering them a means to\nnavigate the extensive array of existing privacy mechanisms and to identify\nthose that align most effectively with their specific requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ubiquitous use of location-based services, large-scale\nindividual-level location data has been widely collected through\nlocation-awareness devices. The widespread exposure of such location data poses\nsignificant privacy risks to users, as it can lead to re-identification, the\ninference of sensitive information, and even physical threats. In this survey,\nwe analyse different geomasking techniques proposed to protect individuals'\nprivacy in geodata. We propose a taxonomy to characterise these techniques\nacross various dimensions. We then highlight the shortcomings of current\ntechniques and discuss avenues for future research. Our proposed taxonomy\nserves as a practical resource for data custodians, offering them a means to\nnavigate the extensive array of existing privacy mechanisms and to identify\nthose that align most effectively with their specific requirements."
                },
                "authors": [
                    {
                        "name": "Mahrokh Abdollahi Lorestani"
                    },
                    {
                        "name": "Thilina Ranbaduge"
                    },
                    {
                        "name": "Thierry Rakotoarivelo"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Rakotoarivelo"
                },
                "author": "Thierry Rakotoarivelo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06816v2",
                "updated": "2024-09-13T03:12:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    12,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-10T18:52:40Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    52,
                    40,
                    1,
                    254,
                    0
                ],
                "title": "LLM-Enhanced Software Patch Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Software Patch Localization"
                },
                "summary": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}."
                },
                "authors": [
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Di Tang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Haixu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Haixu Tang"
                },
                "author": "Haixu Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12665v2",
                "updated": "2024-09-13T03:07:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    7,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-07-17T15:48:39Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    15,
                    48,
                    39,
                    2,
                    199,
                    0
                ],
                "title": "Patch-Level Training for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch-Level Training for Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}."
                },
                "authors": [
                    {
                        "name": "Chenze Shao"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06040v2",
                "updated": "2024-09-13T03:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    6,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-09T20:02:20Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    2,
                    20,
                    0,
                    253,
                    0
                ],
                "title": "Discovery of Two Ultra-Diffuse Galaxies with Unusually Bright Globular\n  Cluster Luminosity Functions via a Mark-Dependently Thinned Point Process\n  (MATHPOP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of Two Ultra-Diffuse Galaxies with Unusually Bright Globular\n  Cluster Luminosity Functions via a Mark-Dependently Thinned Point Process\n  (MATHPOP)"
                },
                "summary": "We present \\textsc{Mathpop}, a novel method to infer the globular cluster\n(GC) counts in ultra-diffuse galaxies (UDGs) and low-surface brightness\ngalaxies (LSBGs). Many known UDGs have a surprisingly high ratio of GC number\nto surface brightness. However, standard methods to infer GC counts in UDGs\nface various challenges, such as photometric measurement uncertainties, GC\nmembership uncertainties, and assumptions about the GC luminosity functions\n(GCLFs). \\textsc{Mathpop} tackles these challenges using the mark-dependent\nthinned point process, enabling joint inference of the spatial and magnitude\ndistributions of GCs. In doing so, \\textsc{Mathpop} allows us to infer and\nquantify the uncertainties in both GC counts and GCLFs with minimal\nassumptions. As a precursor to \\textsc{Mathpop}, we also address the data\nuncertainties coming from the selection process of GC candidates: we obtain\nprobabilistic GC candidates instead of the traditional binary classification\nbased on the color--magnitude diagram. We apply \\textsc{Mathpop} to 40 LSBGs in\nthe Perseus cluster using GC catalogs from a \\textit{Hubble Space Telescope}\nimaging program. We then compare our results to those from an independent study\nusing the standard method. We further calibrate and validate our approach\nthrough extensive simulations. Our approach reveals two LSBGs having GCLF\nturnover points much brighter than the canonical value with Bayes' factor being\n$\\sim4.5$ and $\\sim2.5$, respectively. An additional crude maximum-likelihood\nestimation shows that their GCLF TO points are approximately $0.9$~mag and\n$1.1$~mag brighter than the canonical value, with $p$-value $\\sim 10^{-8}$ and\n$\\sim 10^{-5}$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textsc{Mathpop}, a novel method to infer the globular cluster\n(GC) counts in ultra-diffuse galaxies (UDGs) and low-surface brightness\ngalaxies (LSBGs). Many known UDGs have a surprisingly high ratio of GC number\nto surface brightness. However, standard methods to infer GC counts in UDGs\nface various challenges, such as photometric measurement uncertainties, GC\nmembership uncertainties, and assumptions about the GC luminosity functions\n(GCLFs). \\textsc{Mathpop} tackles these challenges using the mark-dependent\nthinned point process, enabling joint inference of the spatial and magnitude\ndistributions of GCs. In doing so, \\textsc{Mathpop} allows us to infer and\nquantify the uncertainties in both GC counts and GCLFs with minimal\nassumptions. As a precursor to \\textsc{Mathpop}, we also address the data\nuncertainties coming from the selection process of GC candidates: we obtain\nprobabilistic GC candidates instead of the traditional binary classification\nbased on the color--magnitude diagram. We apply \\textsc{Mathpop} to 40 LSBGs in\nthe Perseus cluster using GC catalogs from a \\textit{Hubble Space Telescope}\nimaging program. We then compare our results to those from an independent study\nusing the standard method. We further calibrate and validate our approach\nthrough extensive simulations. Our approach reveals two LSBGs having GCLF\nturnover points much brighter than the canonical value with Bayes' factor being\n$\\sim4.5$ and $\\sim2.5$, respectively. An additional crude maximum-likelihood\nestimation shows that their GCLF TO points are approximately $0.9$~mag and\n$1.1$~mag brighter than the canonical value, with $p$-value $\\sim 10^{-8}$ and\n$\\sim 10^{-5}$, respectively."
                },
                "authors": [
                    {
                        "name": "Dayi Li"
                    },
                    {
                        "name": "Gwendolyn Eadie"
                    },
                    {
                        "name": "Patrick Brown"
                    },
                    {
                        "name": "William Harris"
                    },
                    {
                        "name": "Roberto Abraham"
                    },
                    {
                        "name": "Pieter van Dokkum"
                    },
                    {
                        "name": "Steven Janssens"
                    },
                    {
                        "name": "Samantha Berek"
                    },
                    {
                        "name": "Shany Danieli"
                    },
                    {
                        "name": "Aaron Romanowsky"
                    },
                    {
                        "name": "Joshua Speagle"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Speagle"
                },
                "author": "Joshua Speagle",
                "arxiv_comment": "8 figures, 5 tables; submitted to ApJ, comments are welcomed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06658v2",
                "updated": "2024-09-13T02:59:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    2,
                    59,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-13T06:15:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "ComGPT: Detecting Local Community Structure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComGPT: Detecting Local Community Structure with Large Language Models"
                },
                "summary": "Large Language Models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks, such as node classification. Despite their strong\nabilities in graph reasoning tasks, they lack specific domain knowledge and\nhave a weaker understanding of community-related graph information, which\nhinders their capabilities in the community detection task. Moreover, local\ncommunity detection algorithms based on seed expansion, referred to as seed\nexpansion algorithms, often face the seed-dependent problem, community\ndiffusion, and free rider effect. To use LLMs to overcome the above\nshortcomings, we explore a GPT-guided seed expansion algorithm named ComGPT.\nComGPT iteratively selects potential nodes by local modularity M from the\ndetected community's neighbors, and subsequently employs LLMs to choose the\nnode to join the detected community from these selected potential nodes. To\naddress the above issues faced by LLMs, we improve graph encoding method,\ncalled Incident, by incorporating community knowledge to improve LLMs's\nunderstanding of community-related graph information. Additionally, we design\nthe NSG (Node Selection Guide) prompt to enhance LLMs' understanding of\ncommunity characteristics. Experimental results demonstrate that ComGPT\noutperforms the comparison methods, thereby confirming the effectiveness of the\nimproved graph encoding method and prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks, such as node classification. Despite their strong\nabilities in graph reasoning tasks, they lack specific domain knowledge and\nhave a weaker understanding of community-related graph information, which\nhinders their capabilities in the community detection task. Moreover, local\ncommunity detection algorithms based on seed expansion, referred to as seed\nexpansion algorithms, often face the seed-dependent problem, community\ndiffusion, and free rider effect. To use LLMs to overcome the above\nshortcomings, we explore a GPT-guided seed expansion algorithm named ComGPT.\nComGPT iteratively selects potential nodes by local modularity M from the\ndetected community's neighbors, and subsequently employs LLMs to choose the\nnode to join the detected community from these selected potential nodes. To\naddress the above issues faced by LLMs, we improve graph encoding method,\ncalled Incident, by incorporating community knowledge to improve LLMs's\nunderstanding of community-related graph information. Additionally, we design\nthe NSG (Node Selection Guide) prompt to enhance LLMs' understanding of\ncommunity characteristics. Experimental results demonstrate that ComGPT\noutperforms the comparison methods, thereby confirming the effectiveness of the\nimproved graph encoding method and prompts."
                },
                "authors": [
                    {
                        "name": "Li Ni"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Lin Mu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Wenjian Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenjian Luo"
                },
                "author": "Wenjian Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.09030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09030v1",
                "updated": "2024-09-13T17:55:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    58,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:55:58Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    55,
                    58,
                    4,
                    257,
                    0
                ],
                "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents in Software Engineering: Survey, Landscape, and Vision"
                },
                "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE."
                },
                "authors": [
                    {
                        "name": "Yanxian Huang"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Qianxiang Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanlin Wang"
                },
                "author": "Yanlin Wang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09018v1",
                "updated": "2024-09-13T17:45:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    45,
                    53,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:45:53Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    45,
                    53,
                    4,
                    257,
                    0
                ],
                "title": "An Efficient and Streaming Audio Visual Active Speaker Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Streaming Audio Visual Active Speaker Detection System"
                },
                "summary": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost."
                },
                "authors": [
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Mohammad Sekhavat"
                    },
                    {
                        "name": "Max Horton"
                    },
                    {
                        "name": "Danny Tormoen"
                    },
                    {
                        "name": "Devang Naik"
                    }
                ],
                "author_detail": {
                    "name": "Devang Naik"
                },
                "author": "Devang Naik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09013v1",
                "updated": "2024-09-13T17:41:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    41,
                    12,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:41:12Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    41,
                    12,
                    4,
                    257,
                    0
                ],
                "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents"
                },
                "summary": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents."
                },
                "authors": [
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Sanketh Rangreji"
                    },
                    {
                        "name": "Anubha Kabra"
                    },
                    {
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09010v1",
                "updated": "2024-09-13T17:38:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    38,
                    47,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T17:38:47Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    38,
                    47,
                    4,
                    257,
                    0
                ],
                "title": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contri(e)ve: Context + Retrieve for Scholarly Question Answering"
                },
                "summary": "Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper."
                },
                "authors": [
                    {
                        "name": "Kanchan Shivashankar"
                    },
                    {
                        "name": "Nadine Steinmetz"
                    }
                ],
                "author_detail": {
                    "name": "Nadine Steinmetz"
                },
                "author": "Nadine Steinmetz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08967v1",
                "updated": "2024-09-13T16:33:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    33,
                    18,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T16:33:18Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    33,
                    18,
                    4,
                    257,
                    0
                ],
                "title": "Modeling Rational Adaptation of Visual Search to Hierarchical Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Rational Adaptation of Visual Search to Hierarchical Structures"
                },
                "summary": "Efficient attention deployment in visual search is limited by human visual\nmemory, yet this limitation can be offset by exploiting the environment's\nstructure. This paper introduces a computational cognitive model that simulates\nhow the human visual system uses visual hierarchies to prevent refixations in\nsequential attention deployment. The model adopts computational rationality,\npositing behaviors as adaptations to cognitive constraints and environmental\nstructures. In contrast to earlier models that predict search performance for\nhierarchical information, our model does not include predefined assumptions\nabout particular search strategies. Instead, our model's search strategy\nemerges as a result of adapting to the environment through reinforcement\nlearning algorithms. In an experiment with human participants we test the\nmodel's prediction that structured environments reduce visual search times\ncompared to random tasks. Our model's predictions correspond well with human\nsearch performance across various set sizes for both structured and\nunstructured visual layouts. Our work improves understanding of the adaptive\nnature of visual search in hierarchically structured environments and informs\nthe design of optimized search spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient attention deployment in visual search is limited by human visual\nmemory, yet this limitation can be offset by exploiting the environment's\nstructure. This paper introduces a computational cognitive model that simulates\nhow the human visual system uses visual hierarchies to prevent refixations in\nsequential attention deployment. The model adopts computational rationality,\npositing behaviors as adaptations to cognitive constraints and environmental\nstructures. In contrast to earlier models that predict search performance for\nhierarchical information, our model does not include predefined assumptions\nabout particular search strategies. Instead, our model's search strategy\nemerges as a result of adapting to the environment through reinforcement\nlearning algorithms. In an experiment with human participants we test the\nmodel's prediction that structured environments reduce visual search times\ncompared to random tasks. Our model's predictions correspond well with human\nsearch performance across various set sizes for both structured and\nunstructured visual layouts. Our work improves understanding of the adaptive\nnature of visual search in hierarchically structured environments and informs\nthe design of optimized search spaces."
                },
                "authors": [
                    {
                        "name": "Saku Sourulahti"
                    },
                    {
                        "name": "Christian P Janssen"
                    },
                    {
                        "name": "Jussi PP Jokinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi PP Jokinen"
                },
                "author": "Jussi PP Jokinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08963v1",
                "updated": "2024-09-13T16:29:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    29,
                    25,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T16:29:25Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    29,
                    25,
                    4,
                    257,
                    0
                ],
                "title": "Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance"
                },
                "summary": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems."
                },
                "authors": [
                    {
                        "name": "Lucio La Cava"
                    },
                    {
                        "name": "Andrea Tagarelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Tagarelli"
                },
                "author": "Andrea Tagarelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08937v1",
                "updated": "2024-09-13T15:55:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    55,
                    59,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:55:59Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    55,
                    59,
                    4,
                    257,
                    0
                ],
                "title": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations,\n  Data Quality Assessment, and Cognitive Forcing Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations,\n  Data Quality Assessment, and Cognitive Forcing Functions"
                },
                "summary": "In this paper, we investigate the impact of hallucinations and cognitive\nforcing functions in human-AI collaborative text generation tasks, focusing on\nthe use of Large Language Models (LLMs) to assist in generating high-quality\nconversational data. LLMs require data for fine-tuning, a crucial step in\nenhancing their performance. In the context of conversational customer support,\nthe data takes the form of a conversation between a human customer and an agent\nand can be generated with an AI assistant. In our inquiry, involving 11 users\nwho each completed 8 tasks, resulting in a total of 88 tasks, we found that the\npresence of hallucinations negatively impacts the quality of data. We also find\nthat, although the cognitive forcing function does not always mitigate the\ndetrimental effects of hallucinations on data quality, the presence of\ncognitive forcing functions and hallucinations together impacts data quality\nand influences how users leverage the AI responses presented to them. Our\nanalysis of user behavior reveals distinct patterns of reliance on AI-generated\nresponses, highlighting the importance of managing hallucinations in\nAI-generated content within conversational AI contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the impact of hallucinations and cognitive\nforcing functions in human-AI collaborative text generation tasks, focusing on\nthe use of Large Language Models (LLMs) to assist in generating high-quality\nconversational data. LLMs require data for fine-tuning, a crucial step in\nenhancing their performance. In the context of conversational customer support,\nthe data takes the form of a conversation between a human customer and an agent\nand can be generated with an AI assistant. In our inquiry, involving 11 users\nwho each completed 8 tasks, resulting in a total of 88 tasks, we found that the\npresence of hallucinations negatively impacts the quality of data. We also find\nthat, although the cognitive forcing function does not always mitigate the\ndetrimental effects of hallucinations on data quality, the presence of\ncognitive forcing functions and hallucinations together impacts data quality\nand influences how users leverage the AI responses presented to them. Our\nanalysis of user behavior reveals distinct patterns of reliance on AI-generated\nresponses, highlighting the importance of managing hallucinations in\nAI-generated content within conversational AI contexts."
                },
                "authors": [
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Marina Danilevsky"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Casey Dugan"
                    },
                    {
                        "name": "Michelle Bachman"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Bachman"
                },
                "author": "Michelle Bachman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08931v1",
                "updated": "2024-09-13T15:47:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    47,
                    50,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:47:50Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    47,
                    50,
                    4,
                    257,
                    0
                ],
                "title": "LLM-based Weak Supervision Framework for Query Intent Classification in\n  Video Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Weak Supervision Framework for Query Intent Classification in\n  Video Search"
                },
                "summary": "Streaming services have reshaped how we discover and engage with digital\nentertainment. Despite these advancements, effectively understanding the wide\nspectrum of user search queries continues to pose a significant challenge. An\naccurate query understanding system that can handle a variety of entities that\nrepresent different user intents is essential for delivering an enhanced user\nexperience. We can build such a system by training a natural language\nunderstanding (NLU) model; however, obtaining high-quality labeled training\ndata in this specialized domain is a substantial obstacle. Manual annotation is\ncostly and impractical for capturing users' vast vocabulary variations. To\naddress this, we introduce a novel approach that leverages large language\nmodels (LLMs) through weak supervision to automatically annotate a vast\ncollection of user search queries. Using prompt engineering and a diverse set\nof LLM personas, we generate training data that matches human annotator\nexpectations. By incorporating domain knowledge via Chain of Thought and\nIn-Context Learning, our approach leverages the labeled data to train\nlow-latency models optimized for real-time inference. Extensive evaluations\ndemonstrated that our approach outperformed the baseline with an average\nrelative gain of 113% in recall. Furthermore, our novel prompt engineering\nframework yields higher quality LLM-generated data to be used for weak\nsupervision; we observed 47.60% improvement over baseline in agreement rate\nbetween LLM predictions and human annotations with respect to F1 score,\nweighted according to the distribution of occurrences of the search queries.\nOur persona selection routing mechanism further adds an additional 3.67%\nincrease in weighted F1 score on top of our novel prompt engineering framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming services have reshaped how we discover and engage with digital\nentertainment. Despite these advancements, effectively understanding the wide\nspectrum of user search queries continues to pose a significant challenge. An\naccurate query understanding system that can handle a variety of entities that\nrepresent different user intents is essential for delivering an enhanced user\nexperience. We can build such a system by training a natural language\nunderstanding (NLU) model; however, obtaining high-quality labeled training\ndata in this specialized domain is a substantial obstacle. Manual annotation is\ncostly and impractical for capturing users' vast vocabulary variations. To\naddress this, we introduce a novel approach that leverages large language\nmodels (LLMs) through weak supervision to automatically annotate a vast\ncollection of user search queries. Using prompt engineering and a diverse set\nof LLM personas, we generate training data that matches human annotator\nexpectations. By incorporating domain knowledge via Chain of Thought and\nIn-Context Learning, our approach leverages the labeled data to train\nlow-latency models optimized for real-time inference. Extensive evaluations\ndemonstrated that our approach outperformed the baseline with an average\nrelative gain of 113% in recall. Furthermore, our novel prompt engineering\nframework yields higher quality LLM-generated data to be used for weak\nsupervision; we observed 47.60% improvement over baseline in agreement rate\nbetween LLM predictions and human annotations with respect to F1 score,\nweighted according to the distribution of occurrences of the search queries.\nOur persona selection routing mechanism further adds an additional 3.67%\nincrease in weighted F1 score on top of our novel prompt engineering framework."
                },
                "authors": [
                    {
                        "name": "Farnoosh Javadi"
                    },
                    {
                        "name": "Phanideep Gampa"
                    },
                    {
                        "name": "Alyssa Woo"
                    },
                    {
                        "name": "Xingxing Geng"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Jose Sepulveda"
                    },
                    {
                        "name": "Belhassen Bayar"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v2",
                "updated": "2024-09-13T15:45:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    10,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Coverage-Guided LLM-Based Test Generation"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains a challenge. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp iteratively improves test coverage, interleaving\ncoverage analysis with dialogs with the LLM that steer it to refine tests so\nthat they increase coverage of lines and branches. We evaluate our prototype\nCoverUp implementation across a benchmark of challenging code derived from\nopen-source Python projects, and show that CoverUp substantially improves on\nthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based test\ngenerator, CoverUp achieves a per-module median line+branch coverage of 80%\n(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUp\nachieves an overall line+branch coverage of 90% (vs. 77%). We show that\nCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,\ncontributing to nearly 40% of its successes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains a challenge. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp iteratively improves test coverage, interleaving\ncoverage analysis with dialogs with the LLM that steer it to refine tests so\nthat they increase coverage of lines and branches. We evaluate our prototype\nCoverUp implementation across a benchmark of challenging code derived from\nopen-source Python projects, and show that CoverUp substantially improves on\nthe state of the art. Compared to CodaMosa, a hybrid search/LLM-based test\ngenerator, CoverUp achieves a per-module median line+branch coverage of 80%\n(vs. 47%). Compared to MuTAP, a mutation/LLM-based test generator, CoverUp\nachieves an overall line+branch coverage of 90% (vs. 77%). We show that\nCoverUp's iterative, coverage-guided approach is crucial to its effectiveness,\ncontributing to nearly 40% of its successes."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08904v1",
                "updated": "2024-09-13T15:15:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    15,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T15:15:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    15,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal\n  Robots Guided by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyBipe: An End-to-End Framework for Training and Deploying Bipedal\n  Robots Guided by Large Language Models"
                },
                "summary": "Training and deploying reinforcement learning (RL) policies for robots,\nespecially in accomplishing specific tasks, presents substantial challenges.\nRecent advancements have explored diverse reward function designs, training\ntechniques, simulation-to-reality (sim-to-real) transfers, and performance\nanalysis methodologies, yet these still require significant human intervention.\nThis paper introduces an end-to-end framework for training and deploying RL\npolicies, guided by Large Language Models (LLMs), and evaluates its\neffectiveness on bipedal robots. The framework consists of three interconnected\nmodules: an LLM-guided reward function design module, an RL training module\nleveraging prior work, and a sim-to-real homomorphic evaluation module. This\ndesign significantly reduces the need for human input by utilizing only\nessential simulation and deployment platforms, with the option to incorporate\nhuman-engineered strategies and historical data. We detail the construction of\nthese modules, their advantages over traditional approaches, and demonstrate\nthe framework's capability to autonomously develop and refine controlling\nstrategies for bipedal robot locomotion, showcasing its potential to operate\nindependently of human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying reinforcement learning (RL) policies for robots,\nespecially in accomplishing specific tasks, presents substantial challenges.\nRecent advancements have explored diverse reward function designs, training\ntechniques, simulation-to-reality (sim-to-real) transfers, and performance\nanalysis methodologies, yet these still require significant human intervention.\nThis paper introduces an end-to-end framework for training and deploying RL\npolicies, guided by Large Language Models (LLMs), and evaluates its\neffectiveness on bipedal robots. The framework consists of three interconnected\nmodules: an LLM-guided reward function design module, an RL training module\nleveraging prior work, and a sim-to-real homomorphic evaluation module. This\ndesign significantly reduces the need for human input by utilizing only\nessential simulation and deployment platforms, with the option to incorporate\nhuman-engineered strategies and historical data. We detail the construction of\nthese modules, their advantages over traditional approaches, and demonstrate\nthe framework's capability to autonomously develop and refine controlling\nstrategies for bipedal robot locomotion, showcasing its potential to operate\nindependently of human intervention."
                },
                "authors": [
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Wentao He"
                    },
                    {
                        "name": "Chenyu Gu"
                    },
                    {
                        "name": "Jiaheng Du"
                    },
                    {
                        "name": "Fuwei Tan"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Junguo Lu"
                    }
                ],
                "author_detail": {
                    "name": "Junguo Lu"
                },
                "author": "Junguo Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08887v1",
                "updated": "2024-09-13T14:54:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    54,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:54:37Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    54,
                    37,
                    4,
                    257,
                    0
                ],
                "title": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark"
                },
                "summary": "Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03275v2",
                "updated": "2024-09-13T14:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    42,
                    8,
                    4,
                    257,
                    0
                ],
                "published": "2024-04-04T07:59:24Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    7,
                    59,
                    24,
                    3,
                    95,
                    0
                ],
                "title": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large\n  Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have sparked a revolution\nacross many research fields. In robotics, the integration of common-sense\nknowledge from LLMs into task and motion planning has drastically advanced the\nfield by unlocking unprecedented levels of context awareness. Despite their\nvast collection of knowledge, large language models may generate infeasible\nplans due to hallucinations or missing domain information. To address these\nchallenges and improve plan feasibility and computational efficiency, we\nintroduce DELTA, a novel LLM-informed task planning approach. By using scene\ngraphs as environment representations within LLMs, DELTA achieves rapid\ngeneration of precise planning problem descriptions. To enhance planning\nperformance, DELTA decomposes long-term task goals with LLMs into an\nautoregressive sequence of sub-goals, enabling automated task planners to\nefficiently solve complex problems. In our extensive evaluation, we show that\nDELTA enables an efficient and fully automatic task planning pipeline,\nachieving higher planning success rates and significantly shorter planning\ntimes compared to the state of the art."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Ilche Georgievski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08864v1",
                "updated": "2024-09-13T14:26:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    26,
                    58,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:26:58Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    26,
                    58,
                    4,
                    257,
                    0
                ],
                "title": "Exploring Graph Structure Comprehension Ability of Multimodal Large\n  Language Models: Case Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Graph Structure Comprehension Ability of Multimodal Large\n  Language Models: Case Studies"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in processing\nvarious data structures, including graphs. While previous research has focused\non developing textual encoding methods for graph representation, the emergence\nof multimodal LLMs presents a new frontier for graph comprehension. These\nadvanced models, capable of processing both text and images, offer potential\nimprovements in graph understanding by incorporating visual representations\nalongside traditional textual data. This study investigates the impact of graph\nvisualisations on LLM performance across a range of benchmark tasks at node,\nedge, and graph levels. Our experiments compare the effectiveness of multimodal\napproaches against purely textual graph representations. The results provide\nvaluable insights into both the potential and limitations of leveraging visual\ngraph modalities to enhance LLMs' graph structure comprehension abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in processing\nvarious data structures, including graphs. While previous research has focused\non developing textual encoding methods for graph representation, the emergence\nof multimodal LLMs presents a new frontier for graph comprehension. These\nadvanced models, capable of processing both text and images, offer potential\nimprovements in graph understanding by incorporating visual representations\nalongside traditional textual data. This study investigates the impact of graph\nvisualisations on LLM performance across a range of benchmark tasks at node,\nedge, and graph levels. Our experiments compare the effectiveness of multimodal\napproaches against purely textual graph representations. The results provide\nvaluable insights into both the potential and limitations of leveraging visual\ngraph modalities to enhance LLMs' graph structure comprehension abilities."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Zhong"
                    },
                    {
                        "name": "Davide Mottin"
                    }
                ],
                "author_detail": {
                    "name": "Davide Mottin"
                },
                "author": "Davide Mottin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08846v1",
                "updated": "2024-09-13T14:04:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition"
                },
                "summary": "Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io ."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Chang Hu"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08845v1",
                "updated": "2024-09-13T14:03:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    3,
                    49,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T14:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    3,
                    49,
                    4,
                    257,
                    0
                ],
                "title": "AIPO: Improving Training Objective for Iterative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPO: Improving Training Objective for Iterative Preference Optimization"
                },
                "summary": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO."
                },
                "authors": [
                    {
                        "name": "Yaojie Shen"
                    },
                    {
                        "name": "Xinyao Wang"
                    },
                    {
                        "name": "Yulei Niu"
                    },
                    {
                        "name": "Ying Zhou"
                    },
                    {
                        "name": "Lexin Tang"
                    },
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Longyin Wen"
                    }
                ],
                "author_detail": {
                    "name": "Longyin Wen"
                },
                "author": "Longyin Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08820v1",
                "updated": "2024-09-13T13:34:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    34,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:34:32Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    34,
                    32,
                    4,
                    257,
                    0
                ],
                "title": "A RAG Approach for Generating Competency Questions in Ontology\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG Approach for Generating Competency Questions in Ontology\n  Engineering"
                },
                "summary": "Competency question (CQ) formulation is central to several ontology\ndevelopment and evaluation methodologies. Traditionally, the task of crafting\nthese competency questions heavily relies on the effort of domain experts and\nknowledge engineers which is often time-consuming and labor-intensive. With the\nemergence of Large Language Models (LLMs), there arises the possibility to\nautomate and enhance this process. Unlike other similar works which use\nexisting ontologies or knowledge graphs as input to LLMs, we present a\nretrieval-augmented generation (RAG) approach that uses LLMs for the automatic\ngeneration of CQs given a set of scientific papers considered to be a domain\nknowledge base. We investigate its performance and specifically, we study the\nimpact of different number of papers to the RAG and different temperature\nsetting of the LLM. We conduct experiments using GPT-4 on two domain ontology\nengineering tasks and compare results against ground-truth CQs constructed by\ndomain experts. Empirical assessments on the results, utilizing evaluation\nmetrics (precision and consistency), reveal that compared to zero-shot\nprompting, adding relevant domain knowledge to the RAG improves the performance\nof LLMs on generating CQs for concrete ontology engineering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competency question (CQ) formulation is central to several ontology\ndevelopment and evaluation methodologies. Traditionally, the task of crafting\nthese competency questions heavily relies on the effort of domain experts and\nknowledge engineers which is often time-consuming and labor-intensive. With the\nemergence of Large Language Models (LLMs), there arises the possibility to\nautomate and enhance this process. Unlike other similar works which use\nexisting ontologies or knowledge graphs as input to LLMs, we present a\nretrieval-augmented generation (RAG) approach that uses LLMs for the automatic\ngeneration of CQs given a set of scientific papers considered to be a domain\nknowledge base. We investigate its performance and specifically, we study the\nimpact of different number of papers to the RAG and different temperature\nsetting of the LLM. We conduct experiments using GPT-4 on two domain ontology\nengineering tasks and compare results against ground-truth CQs constructed by\ndomain experts. Empirical assessments on the results, utilizing evaluation\nmetrics (precision and consistency), reveal that compared to zero-shot\nprompting, adding relevant domain knowledge to the RAG improves the performance\nof LLMs on generating CQs for concrete ontology engineering tasks."
                },
                "authors": [
                    {
                        "name": "Xueli Pan"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    },
                    {
                        "name": "Victor de Boer"
                    },
                    {
                        "name": "Zhisheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhisheng Huang"
                },
                "author": "Zhisheng Huang",
                "arxiv_journal_ref": "MTST2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08813v1",
                "updated": "2024-09-13T13:24:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:24:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Weak LLM is Secretly a Strong Teacher for Alignment"
                },
                "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08811v1",
                "updated": "2024-09-13T13:19:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    19,
                    48,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T13:19:48Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    19,
                    48,
                    4,
                    257,
                    0
                ],
                "title": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with\n  LLM-driven AI Agents in a Real-time Shared Workspace Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with\n  LLM-driven AI Agents in a Real-time Shared Workspace Task"
                },
                "summary": "Theory of Mind (ToM) significantly impacts human collaboration and\ncommunication as a crucial capability to understand others. When AI agents with\nToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in\nsuch human-AI teams (HATs). The MToM process, which involves interactive\ncommunication and ToM-based strategy adjustment, affects the team's performance\nand collaboration process. To explore the MToM process, we conducted a\nmixed-design experiment using a large language model-driven AI agent with ToM\nand communication modules in a real-time shared-workspace task. We find that\nthe agent's ToM capability does not significantly impact team performance but\nenhances human understanding of the agent and the feeling of being understood.\nMost participants in our study believe verbal communication increases human\nburden, and the results show that bidirectional communication leads to lower\nHAT performance. We discuss the results' implications for designing AI agents\nthat collaborate with humans in real-time shared workspace tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) significantly impacts human collaboration and\ncommunication as a crucial capability to understand others. When AI agents with\nToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in\nsuch human-AI teams (HATs). The MToM process, which involves interactive\ncommunication and ToM-based strategy adjustment, affects the team's performance\nand collaboration process. To explore the MToM process, we conducted a\nmixed-design experiment using a large language model-driven AI agent with ToM\nand communication modules in a real-time shared-workspace task. We find that\nthe agent's ToM capability does not significantly impact team performance but\nenhances human understanding of the agent and the feeling of being understood.\nMost participants in our study believe verbal communication increases human\nburden, and the results show that bidirectional communication leads to lower\nHAT performance. We discuss the results' implications for designing AI agents\nthat collaborate with humans in real-time shared workspace tasks."
                },
                "authors": [
                    {
                        "name": "Shao Zhang"
                    },
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Yongshan Chen"
                    },
                    {
                        "name": "Landi Gao"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "arxiv_comment": "34 pages, Preprint Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02373v2",
                "updated": "2024-09-13T13:09:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    9,
                    41,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-05T10:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    10,
                    53,
                    51,
                    0,
                    218,
                    0
                ],
                "title": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants"
                },
                "summary": "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize contextual integrity (CI), a\nframework that equates privacy with the appropriate flow of information in a\ngiven context. In particular, we design and evaluate a number of strategies to\nsteer assistants' information-sharing actions to be CI compliant. Our\nevaluation is based on a novel form filling benchmark composed of human\nannotations of common webform applications, and it reveals that prompting\nfrontier LLMs to perform CI-based reasoning yields strong results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced AI assistants combine frontier LLMs and tool access to autonomously\nperform complex tasks on behalf of users. While the helpfulness of such\nassistants can increase dramatically with access to user information including\nemails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer\ninformation-sharing assistants to behave in accordance with privacy\nexpectations, we propose to operationalize contextual integrity (CI), a\nframework that equates privacy with the appropriate flow of information in a\ngiven context. In particular, we design and evaluate a number of strategies to\nsteer assistants' information-sharing actions to be CI compliant. Our\nevaluation is based on a novel form filling benchmark composed of human\nannotations of common webform applications, and it reveals that prompting\nfrontier LLMs to perform CI-based reasoning yields strong results."
                },
                "authors": [
                    {
                        "name": "Sahra Ghalebikesabi"
                    },
                    {
                        "name": "Eugene Bagdasaryan"
                    },
                    {
                        "name": "Ren Yi"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Aneesh Pappu"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Laura Weidinger"
                    },
                    {
                        "name": "Robert Stanforth"
                    },
                    {
                        "name": "Leonard Berrada"
                    },
                    {
                        "name": "Pushmeet Kohli"
                    },
                    {
                        "name": "Po-Sen Huang"
                    },
                    {
                        "name": "Borja Balle"
                    }
                ],
                "author_detail": {
                    "name": "Borja Balle"
                },
                "author": "Borja Balle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08795v1",
                "updated": "2024-09-13T12:59:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment"
                },
                "summary": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data."
                },
                "authors": [
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Vincent Cheung"
                    },
                    {
                        "name": "Hayato Nishioka"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Shinichi Furuya"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Furuya"
                },
                "author": "Shinichi Furuya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08792v1",
                "updated": "2024-09-13T12:55:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    55,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:55:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    55,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes"
                },
                "summary": "In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science."
                },
                "authors": [
                    {
                        "name": "Luis Rita"
                    },
                    {
                        "name": "Josh Southern"
                    },
                    {
                        "name": "Ivan Laponogov"
                    },
                    {
                        "name": "Kyle Higgins"
                    },
                    {
                        "name": "Kirill Veselkov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Veselkov"
                },
                "author": "Kirill Veselkov",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08788v1",
                "updated": "2024-09-13T12:50:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    50,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:50:36Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    50,
                    36,
                    4,
                    257,
                    0
                ],
                "title": "Electrocardiogram Report Generation and Question Answering via\n  Retrieval-Augmented Self-Supervised Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram Report Generation and Question Answering via\n  Retrieval-Augmented Self-Supervised Modeling"
                },
                "summary": "Interpreting electrocardiograms (ECGs) and generating comprehensive reports\nremain challenging tasks in cardiology, often requiring specialized expertise\nand significant time investment. To address these critical issues, we propose\nECG-ReGen, a retrieval-based approach for ECG-to-text report generation and\nquestion answering. Our method leverages a self-supervised learning for the ECG\nencoder, enabling efficient similarity searches and report retrieval. By\ncombining pre-training with dynamic retrieval and Large Language Model\n(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers\nrelated queries, with the potential of improving patient care. Experiments\nconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior\nperformance in both in-domain and cross-domain scenarios for report generation.\nFurthermore, our approach exhibits competitive performance on ECG-QA dataset\ncompared to fully supervised methods when utilizing off-the-shelf LLMs for\nzero-shot question answering. This approach, effectively combining\nself-supervised encoder and LLMs, offers a scalable and efficient solution for\naccurate ECG interpretation, holding significant potential to enhance clinical\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting electrocardiograms (ECGs) and generating comprehensive reports\nremain challenging tasks in cardiology, often requiring specialized expertise\nand significant time investment. To address these critical issues, we propose\nECG-ReGen, a retrieval-based approach for ECG-to-text report generation and\nquestion answering. Our method leverages a self-supervised learning for the ECG\nencoder, enabling efficient similarity searches and report retrieval. By\ncombining pre-training with dynamic retrieval and Large Language Model\n(LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers\nrelated queries, with the potential of improving patient care. Experiments\nconducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior\nperformance in both in-domain and cross-domain scenarios for report generation.\nFurthermore, our approach exhibits competitive performance on ECG-QA dataset\ncompared to fully supervised methods when utilizing off-the-shelf LLMs for\nzero-shot question answering. This approach, effectively combining\nself-supervised encoder and LLMs, offers a scalable and efficient solution for\naccurate ECG interpretation, holding significant potential to enhance clinical\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v1",
                "updated": "2024-09-13T12:34:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What You Say = What You Want? Teaching Humans to Articulate Requirements\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You Say = What You Want? Teaching Humans to Articulate Requirements\n  for LLMs"
                },
                "summary": "Prompting ChatGPT to achieve complex goals (e.g., creating a customer support\nchatbot) often demands meticulous prompt engineering, including aspects like\nfluent writing and chain-of-thought techniques. While emerging prompt\noptimizers can automatically refine many of these aspects, we argue that\nclearly conveying customized requirements (e.g., how to handle diverse inputs)\nremains a human-centric challenge. In this work, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a study with 30 novices, we\nshow that requirement-focused training doubles novices' prompting performance,\nsignificantly outperforming conventional prompt engineering training and prompt\noptimization. We also demonstrate that high-quality LLM outputs are directly\ntied to the quality of input requirements. Our work paves the way for more\neffective task delegation in human-LLM collaborative prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting ChatGPT to achieve complex goals (e.g., creating a customer support\nchatbot) often demands meticulous prompt engineering, including aspects like\nfluent writing and chain-of-thought techniques. While emerging prompt\noptimizers can automatically refine many of these aspects, we argue that\nclearly conveying customized requirements (e.g., how to handle diverse inputs)\nremains a human-centric challenge. In this work, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a study with 30 novices, we\nshow that requirement-focused training doubles novices' prompting performance,\nsignificantly outperforming conventional prompt engineering training and prompt\noptimization. We also demonstrate that high-quality LLM outputs are directly\ntied to the quality of input requirements. Our work paves the way for more\neffective task delegation in human-LLM collaborative prompting."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v4",
                "updated": "2024-09-13T12:28:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    28,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08738v1",
                "updated": "2024-09-13T11:37:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    37,
                    40,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T11:37:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    37,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "DataliVR: Transformation of Data Literacy Education through Virtual\n  Reality with ChatGPT-Powered Enhancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataliVR: Transformation of Data Literacy Education through Virtual\n  Reality with ChatGPT-Powered Enhancements"
                },
                "summary": "Data literacy is essential in today's data-driven world, emphasizing\nindividuals' abilities to effectively manage data and extract meaningful\ninsights. However, traditional classroom-based educational approaches often\nstruggle to fully address the multifaceted nature of data literacy. As\neducation undergoes digital transformation, innovative technologies such as\nVirtual Reality (VR) offer promising avenues for immersive and engaging\nlearning experiences. This paper introduces DataliVR, a pioneering VR\napplication aimed at enhancing the data literacy skills of university students\nwithin a contextual and gamified virtual learning environment. By integrating\nLarge Language Models (LLMs) like ChatGPT as a conversational artificial\nintelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides\npersonalized learning assistance, enriching user learning experiences. Our\nstudy employed an experimental approach, with chatbot availability as the\nindependent variable, analyzing learning experiences and outcomes as dependent\nvariables with a sample of thirty participants. Our approach underscores the\neffectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering\ndata literacy skills. Moreover, our study examines the impact of the\nChatGPT-based AI chatbot on users' learning, revealing significant effects on\nboth learning experiences and outcomes. Our study presents a robust tool for\nfostering data literacy skills, contributing significantly to the digital\nadvancement of data literacy education through cutting-edge VR and AI\ntechnologies. Moreover, our research provides valuable insights and\nimplications for future research endeavors aiming to integrate LLMs (e.g.,\nChatGPT) into educational VR platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data literacy is essential in today's data-driven world, emphasizing\nindividuals' abilities to effectively manage data and extract meaningful\ninsights. However, traditional classroom-based educational approaches often\nstruggle to fully address the multifaceted nature of data literacy. As\neducation undergoes digital transformation, innovative technologies such as\nVirtual Reality (VR) offer promising avenues for immersive and engaging\nlearning experiences. This paper introduces DataliVR, a pioneering VR\napplication aimed at enhancing the data literacy skills of university students\nwithin a contextual and gamified virtual learning environment. By integrating\nLarge Language Models (LLMs) like ChatGPT as a conversational artificial\nintelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides\npersonalized learning assistance, enriching user learning experiences. Our\nstudy employed an experimental approach, with chatbot availability as the\nindependent variable, analyzing learning experiences and outcomes as dependent\nvariables with a sample of thirty participants. Our approach underscores the\neffectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering\ndata literacy skills. Moreover, our study examines the impact of the\nChatGPT-based AI chatbot on users' learning, revealing significant effects on\nboth learning experiences and outcomes. Our study presents a robust tool for\nfostering data literacy skills, contributing significantly to the digital\nadvancement of data literacy education through cutting-edge VR and AI\ntechnologies. Moreover, our research provides valuable insights and\nimplications for future research endeavors aiming to integrate LLMs (e.g.,\nChatGPT) into educational VR platforms."
                },
                "authors": [
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Haochun Huai"
                    },
                    {
                        "name": "Sena Yildiz-Degirmenci"
                    },
                    {
                        "name": "Maria Bannert"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "10 pages, this paper was accepted to ISMAR2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08717v1",
                "updated": "2024-09-13T11:02:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T11:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents"
                },
                "summary": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Zhicheng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dong"
                },
                "author": "Zhicheng Dong",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08706v1",
                "updated": "2024-09-13T10:48:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:48:35Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    48,
                    35,
                    4,
                    257,
                    0
                ],
                "title": "L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp ."
                },
                "authors": [
                    {
                        "name": "Pritika Rohera"
                    },
                    {
                        "name": "Chaitrali Ginimav"
                    },
                    {
                        "name": "Akanksha Salunke"
                    },
                    {
                        "name": "Gayatri Sawant"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08692v1",
                "updated": "2024-09-13T10:22:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    22,
                    8,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T10:22:08Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    10,
                    22,
                    8,
                    4,
                    257,
                    0
                ],
                "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with\n  Plausible Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B4: Towards Optimal Assessment of Plausible Code Solutions with\n  Plausible Tests"
                },
                "summary": "Selecting the best code solution from multiple generated ones is an essential\ntask in code generation, which can be achieved by using some reliable\nvalidators (e.g., developer-written test cases) for assistance. Since reliable\ntest cases are not always available and can be expensive to build in practice,\nresearchers propose to automatically generate test cases to assess code\nsolutions. However, when both code solutions and test cases are plausible and\nnot reliable, selecting the best solution becomes challenging. Although some\nheuristic strategies have been proposed to tackle this problem, they lack a\nstrong theoretical guarantee and it is still an open question whether an\noptimal selection strategy exists. Our work contributes in two ways. First, we\nshow that within a Bayesian framework, the optimal selection strategy can be\ndefined based on the posterior probability of the observed passing states\nbetween solutions and tests. The problem of identifying the best solution is\nthen framed as an integer programming problem. Second, we propose an efficient\napproach for approximating this optimal (yet uncomputable) strategy, where the\napproximation error is bounded by the correctness of prior knowledge. We then\nincorporate effective prior knowledge to tailor code generation tasks. Both\ntheoretical and empirical studies confirm that existing heuristics are limited\nin selecting the best solutions with plausible test cases. Our proposed\napproximated optimal strategy B4 significantly surpasses existing heuristics in\nselecting code solutions generated by large language models (LLMs) with\nLLM-generated tests, achieving a relative performance improvement by up to 50%\nover the strongest heuristic and 246% over the random selection in the most\nchallenging scenarios. Our code is publicly available at\nhttps://github.com/ZJU-CTAG/B4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting the best code solution from multiple generated ones is an essential\ntask in code generation, which can be achieved by using some reliable\nvalidators (e.g., developer-written test cases) for assistance. Since reliable\ntest cases are not always available and can be expensive to build in practice,\nresearchers propose to automatically generate test cases to assess code\nsolutions. However, when both code solutions and test cases are plausible and\nnot reliable, selecting the best solution becomes challenging. Although some\nheuristic strategies have been proposed to tackle this problem, they lack a\nstrong theoretical guarantee and it is still an open question whether an\noptimal selection strategy exists. Our work contributes in two ways. First, we\nshow that within a Bayesian framework, the optimal selection strategy can be\ndefined based on the posterior probability of the observed passing states\nbetween solutions and tests. The problem of identifying the best solution is\nthen framed as an integer programming problem. Second, we propose an efficient\napproach for approximating this optimal (yet uncomputable) strategy, where the\napproximation error is bounded by the correctness of prior knowledge. We then\nincorporate effective prior knowledge to tailor code generation tasks. Both\ntheoretical and empirical studies confirm that existing heuristics are limited\nin selecting the best solutions with plausible test cases. Our proposed\napproximated optimal strategy B4 significantly surpasses existing heuristics in\nselecting code solutions generated by large language models (LLMs) with\nLLM-generated tests, achieving a relative performance improvement by up to 50%\nover the strongest heuristic and 246% over the random selection in the most\nchallenging scenarios. Our code is publicly available at\nhttps://github.com/ZJU-CTAG/B4."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "He Tao"
                    },
                    {
                        "name": "Yusu Hong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "arxiv_comment": "accepted by ASE' 24 (full paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04167v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04167v5",
                "updated": "2024-09-13T09:47:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    47,
                    29,
                    4,
                    257,
                    0
                ],
                "published": "2024-04-05T15:20:02Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    15,
                    20,
                    2,
                    4,
                    96,
                    0
                ],
                "title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model"
                },
                "summary": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models."
                },
                "authors": [
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Ding Pan"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04167v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04167v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15649v3",
                "updated": "2024-09-13T09:36:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    9,
                    36,
                    18,
                    4,
                    257,
                    0
                ],
                "published": "2023-11-27T09:20:23Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    9,
                    20,
                    23,
                    0,
                    331,
                    0
                ],
                "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks"
                },
                "summary": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality."
                },
                "authors": [
                    {
                        "name": "Yaran Chen"
                    },
                    {
                        "name": "Wenbo Cui"
                    },
                    {
                        "name": "Yuanwen Chen"
                    },
                    {
                        "name": "Mining Tan"
                    },
                    {
                        "name": "Xinyao Zhang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08642v1",
                "updated": "2024-09-13T08:59:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T08:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    59,
                    31,
                    4,
                    257,
                    0
                ],
                "title": "CPL: Critical Planning Step Learning Boosts LLM Generalization in\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPL: Critical Planning Step Learning Boosts LLM Generalization in\n  Reasoning Tasks"
                },
                "summary": "Post-training large language models (LLMs) to develop reasoning capabilities\nhas proven effective across diverse domains, such as mathematical reasoning and\ncode generation. However, existing methods primarily focus on improving\ntask-specific reasoning but have not adequately addressed the model's\ngeneralization capabilities across a broader range of reasoning tasks. To\ntackle this challenge, we introduce Critical Planning Step Learning (CPL),\nwhich leverages Monte Carlo Tree Search (MCTS) to explore diverse planning\nsteps in multi-step reasoning tasks. Based on long-term outcomes, CPL learns\nstep-level planning preferences to improve the model's planning capabilities\nand, consequently, its general reasoning capabilities. Furthermore, while\neffective in many scenarios for aligning LLMs, existing preference learning\napproaches like Direct Preference Optimization (DPO) struggle with complex\nmulti-step reasoning tasks due to their inability to capture fine-grained\nsupervision at each step. We propose Step-level Advantage Preference\nOptimization (Step-APO), which integrates an advantage estimate for step-level\npreference pairs obtained via MCTS into the DPO. This enables the model to more\neffectively learn critical intermediate planning steps, thereby further\nimproving its generalization in reasoning tasks. Experimental results\ndemonstrate that our method, trained exclusively on GSM8K and MATH, not only\nsignificantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also\nenhances out-of-domain reasoning benchmarks, such as ARC-C (+4.0%), BBH\n(+1.8%), MMLU-STEM (+2.2%), and MMLU (+0.9%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training large language models (LLMs) to develop reasoning capabilities\nhas proven effective across diverse domains, such as mathematical reasoning and\ncode generation. However, existing methods primarily focus on improving\ntask-specific reasoning but have not adequately addressed the model's\ngeneralization capabilities across a broader range of reasoning tasks. To\ntackle this challenge, we introduce Critical Planning Step Learning (CPL),\nwhich leverages Monte Carlo Tree Search (MCTS) to explore diverse planning\nsteps in multi-step reasoning tasks. Based on long-term outcomes, CPL learns\nstep-level planning preferences to improve the model's planning capabilities\nand, consequently, its general reasoning capabilities. Furthermore, while\neffective in many scenarios for aligning LLMs, existing preference learning\napproaches like Direct Preference Optimization (DPO) struggle with complex\nmulti-step reasoning tasks due to their inability to capture fine-grained\nsupervision at each step. We propose Step-level Advantage Preference\nOptimization (Step-APO), which integrates an advantage estimate for step-level\npreference pairs obtained via MCTS into the DPO. This enables the model to more\neffectively learn critical intermediate planning steps, thereby further\nimproving its generalization in reasoning tasks. Experimental results\ndemonstrate that our method, trained exclusively on GSM8K and MATH, not only\nsignificantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also\nenhances out-of-domain reasoning benchmarks, such as ARC-C (+4.0%), BBH\n(+1.8%), MMLU-STEM (+2.2%), and MMLU (+0.9%)."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Jing Bai"
                    }
                ],
                "author_detail": {
                    "name": "Jing Bai"
                },
                "author": "Jing Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08622v1",
                "updated": "2024-09-13T08:19:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    19,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T08:19:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    19,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking"
                },
                "summary": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for pluralistic alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for pluralistic alignment."
                },
                "authors": [
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03793v2",
                "updated": "2024-09-13T08:14:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    14,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-03T10:14:51Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    10,
                    14,
                    51,
                    1,
                    247,
                    0
                ],
                "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures"
                },
                "summary": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Ishaan Domkundwar"
                    },
                    {
                        "name": "Mukunda N S"
                    },
                    {
                        "name": "Ishaan Bhola"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Bhola"
                },
                "author": "Ishaan Bhola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10718v2",
                "updated": "2024-09-13T08:09:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    9,
                    50,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-20T10:40:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    40,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?"
                },
                "summary": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhao"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Annan Li"
                    },
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08597v1",
                "updated": "2024-09-13T07:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    47,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T07:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    47,
                    4,
                    257,
                    0
                ],
                "title": "LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented\n  Generation"
                },
                "summary": "Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations."
                },
                "authors": [
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Xianghui He"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08596v1",
                "updated": "2024-09-13T07:28:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T07:28:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    28,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings."
                },
                "authors": [
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zhaoqing Li"
                    },
                    {
                        "name": "Yuejiao Wang"
                    },
                    {
                        "name": "Wenxuan Wu"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07003v2",
                "updated": "2024-09-13T07:19:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    7,
                    19,
                    16,
                    4,
                    257,
                    0
                ],
                "published": "2024-06-11T06:55:32Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    6,
                    55,
                    32,
                    1,
                    163,
                    0
                ],
                "title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context\n  Graph-based Retrieval and Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context\n  Graph-based Retrieval and Language Model"
                },
                "summary": "The performance of repository-level code completion depends upon the\neffective leverage of both general and repository-specific knowledge. Despite\nthe impressive capability of code LLMs in general code completion tasks, they\noften exhibit less satisfactory performance on repository-level completion due\nto the lack of repository-specific knowledge in these LLMs. To address this\nproblem, we propose GraphCoder, a retrieval-augmented code completion framework\nthat leverages LLMs' general code knowledge and the repository-specific\nknowledge via a graph-based retrieval-generation process. In particular,\nGraphCoder captures the context of completion target more accurately through\ncode context graph (CCG) that consists of control-flow, data- and\ncontrol-dependence between code statements, a more structured way to capture\nthe completion target context than the sequence-based context used in existing\nretrieval-augmented approaches; based on CCG, GraphCoder further employs a\ncoarse-to-fine retrieval process to locate context-similar code snippets with\nthe completion target from the current repository. Experimental results\ndemonstrate both the effectiveness and efficiency of GraphCoder: Compared to\nbaseline retrieval-augmented methods, GraphCoder achieves higher exact match\n(EM) on average, with increases of +6.06 in code match and +6.23 in identifier\nmatch, while using less time and space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of repository-level code completion depends upon the\neffective leverage of both general and repository-specific knowledge. Despite\nthe impressive capability of code LLMs in general code completion tasks, they\noften exhibit less satisfactory performance on repository-level completion due\nto the lack of repository-specific knowledge in these LLMs. To address this\nproblem, we propose GraphCoder, a retrieval-augmented code completion framework\nthat leverages LLMs' general code knowledge and the repository-specific\nknowledge via a graph-based retrieval-generation process. In particular,\nGraphCoder captures the context of completion target more accurately through\ncode context graph (CCG) that consists of control-flow, data- and\ncontrol-dependence between code statements, a more structured way to capture\nthe completion target context than the sequence-based context used in existing\nretrieval-augmented approaches; based on CCG, GraphCoder further employs a\ncoarse-to-fine retrieval process to locate context-similar code snippets with\nthe completion target from the current repository. Experimental results\ndemonstrate both the effectiveness and efficiency of GraphCoder: Compared to\nbaseline retrieval-augmented methods, GraphCoder achieves higher exact match\n(EM) on average, with increases of +6.06 in code match and +6.23 in identifier\nmatch, while using less time and space."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Ailun Yu"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Qianxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianxiang Wang"
                },
                "author": "Qianxiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05693v2",
                "updated": "2024-09-13T06:57:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    57,
                    1,
                    4,
                    257,
                    0
                ],
                "published": "2024-07-08T07:47:30Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    7,
                    47,
                    30,
                    0,
                    190,
                    0
                ],
                "title": "Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation"
                },
                "summary": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL."
                },
                "authors": [
                    {
                        "name": "Jian Qian"
                    },
                    {
                        "name": "Miao Sun"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Ruizhi Hun"
                    },
                    {
                        "name": "Patrick Chiang"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Chiang"
                },
                "author": "Patrick Chiang",
                "arxiv_comment": "Accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08564v1",
                "updated": "2024-09-13T06:34:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    34,
                    15,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:34:15Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    34,
                    15,
                    4,
                    257,
                    0
                ],
                "title": "Cracking the Code: Multi-domain LLM Evaluation on Real-World\n  Professional Exams in Indonesia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code: Multi-domain LLM Evaluation on Real-World\n  Professional Exams in Indonesia"
                },
                "summary": "While knowledge evaluation in large language models has predominantly focused\non academic subjects like math and physics, these assessments often fail to\ncapture the practical demands of real-world professions. In this paper, we\nintroduce IndoCareer, a dataset comprising 8,834 multiple-choice questions\ndesigned to evaluate performance in vocational and professional certification\nexams across various fields. With a focus on Indonesia, IndoCareer provides\nrich local contexts, spanning six key sectors: (1) healthcare, (2) insurance\nand finance, (3) creative and design, (4) tourism and hospitality, (5)\neducation and training, and (6) law. Our comprehensive evaluation of 27 large\nlanguage models shows that these models struggle particularly in fields with\nstrong local contexts, such as insurance and finance. Additionally, while using\nthe entire dataset, shuffling answer options generally maintains consistent\nevaluation results across models, but it introduces instability specifically in\nthe insurance and finance sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While knowledge evaluation in large language models has predominantly focused\non academic subjects like math and physics, these assessments often fail to\ncapture the practical demands of real-world professions. In this paper, we\nintroduce IndoCareer, a dataset comprising 8,834 multiple-choice questions\ndesigned to evaluate performance in vocational and professional certification\nexams across various fields. With a focus on Indonesia, IndoCareer provides\nrich local contexts, spanning six key sectors: (1) healthcare, (2) insurance\nand finance, (3) creative and design, (4) tourism and hospitality, (5)\neducation and training, and (6) law. Our comprehensive evaluation of 27 large\nlanguage models shows that these models struggle particularly in fields with\nstrong local contexts, such as insurance and finance. Additionally, while using\nthe entire dataset, shuffling answer options generally maintains consistent\nevaluation results across models, but it introduces instability specifically in\nthe insurance and finance sectors."
                },
                "authors": [
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08561v1",
                "updated": "2024-09-13T06:29:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    29,
                    20,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:29:20Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    29,
                    20,
                    4,
                    257,
                    0
                ],
                "title": "Expediting and Elevating Large Language Model Reasoning via Hidden\n  Chain-of-Thought Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expediting and Elevating Large Language Model Reasoning via Hidden\n  Chain-of-Thought Decoding"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications."
                },
                "authors": [
                    {
                        "name": "Tianqiao Liu"
                    },
                    {
                        "name": "Zui Chen"
                    },
                    {
                        "name": "Zitao Liu"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Weiqi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Weiqi Luo"
                },
                "author": "Weiqi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08554v1",
                "updated": "2024-09-13T06:13:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    13,
                    55,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T06:13:55Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    6,
                    13,
                    55,
                    4,
                    257,
                    0
                ],
                "title": "LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study"
                },
                "summary": "Grapheme-to-phoneme (G2P) conversion is critical in speech processing,\nparticularly for applications like speech synthesis. G2P systems must possess\nlinguistic understanding and contextual awareness of languages with polyphone\nwords and context-dependent phonemes. Large language models (LLMs) have\nrecently demonstrated significant potential in various language tasks,\nsuggesting that their phonetic knowledge could be leveraged for G2P. In this\npaper, we evaluate the performance of LLMs in G2P conversion and introduce\nprompting and post-processing methods that enhance LLM outputs without\nadditional training or labeled data. We also present a benchmarking dataset\ndesigned to assess G2P performance on sentence-level phonetic challenges of the\nPersian language. Our results show that by applying the proposed methods, LLMs\ncan outperform traditional G2P tools, even in an underrepresented language like\nPersian, highlighting the potential of developing LLM-aided G2P systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grapheme-to-phoneme (G2P) conversion is critical in speech processing,\nparticularly for applications like speech synthesis. G2P systems must possess\nlinguistic understanding and contextual awareness of languages with polyphone\nwords and context-dependent phonemes. Large language models (LLMs) have\nrecently demonstrated significant potential in various language tasks,\nsuggesting that their phonetic knowledge could be leveraged for G2P. In this\npaper, we evaluate the performance of LLMs in G2P conversion and introduce\nprompting and post-processing methods that enhance LLM outputs without\nadditional training or labeled data. We also present a benchmarking dataset\ndesigned to assess G2P performance on sentence-level phonetic challenges of the\nPersian language. Our results show that by applying the proposed methods, LLMs\ncan outperform traditional G2P tools, even in an underrepresented language like\nPersian, highlighting the potential of developing LLM-aided G2P systems."
                },
                "authors": [
                    {
                        "name": "Mahta Fetrat Qharabagh"
                    },
                    {
                        "name": "Zahra Dehghanian"
                    },
                    {
                        "name": "Hamid R. Rabiee"
                    }
                ],
                "author_detail": {
                    "name": "Hamid R. Rabiee"
                },
                "author": "Hamid R. Rabiee",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13745v3",
                "updated": "2024-09-13T05:50:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    50,
                    11,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-25T07:10:36Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    7,
                    10,
                    36,
                    6,
                    238,
                    0
                ],
                "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation"
                },
                "summary": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation."
                },
                "authors": [
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v3",
                "updated": "2024-09-13T05:34:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08543v1",
                "updated": "2024-09-13T05:33:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    33,
                    9,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T05:33:09Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    33,
                    9,
                    4,
                    257,
                    0
                ],
                "title": "ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and\n  Low-Rank Adaptation via Instruction-Tuned Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and\n  Low-Rank Adaptation via Instruction-Tuned Large Language Model"
                },
                "summary": "Recommender Systems (RS) play a pivotal role in boosting user satisfaction by\nproviding personalized product suggestions in domains such as e-commerce and\nentertainment. This study examines the integration of multimodal data text and\naudio into large language models (LLMs) with the aim of enhancing\nrecommendation performance. Traditional text and audio recommenders encounter\nlimitations such as the cold-start problem, and recent advancements in LLMs,\nwhile promising, are computationally expensive. To address these issues,\nLow-Rank Adaptation (LoRA) is introduced, which enhances efficiency without\ncompromising performance. The ATFLRec framework is proposed to integrate audio\nand text modalities into a multimodal recommendation system, utilizing various\nLoRA configurations and modality fusion techniques. Results indicate that\nATFLRec outperforms baseline models, including traditional and graph neural\nnetwork-based approaches, achieving higher AUC scores. Furthermore, separate\nfine-tuning of audio and text data with distinct LoRA modules yields optimal\nperformance, with different pooling methods and Mel filter bank numbers\nsignificantly impacting performance. This research offers valuable insights\ninto optimizing multimodal recommender systems and advancing the integration of\ndiverse data modalities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender Systems (RS) play a pivotal role in boosting user satisfaction by\nproviding personalized product suggestions in domains such as e-commerce and\nentertainment. This study examines the integration of multimodal data text and\naudio into large language models (LLMs) with the aim of enhancing\nrecommendation performance. Traditional text and audio recommenders encounter\nlimitations such as the cold-start problem, and recent advancements in LLMs,\nwhile promising, are computationally expensive. To address these issues,\nLow-Rank Adaptation (LoRA) is introduced, which enhances efficiency without\ncompromising performance. The ATFLRec framework is proposed to integrate audio\nand text modalities into a multimodal recommendation system, utilizing various\nLoRA configurations and modality fusion techniques. Results indicate that\nATFLRec outperforms baseline models, including traditional and graph neural\nnetwork-based approaches, achieving higher AUC scores. Furthermore, separate\nfine-tuning of audio and text data with distinct LoRA modules yields optimal\nperformance, with different pooling methods and Mel filter bank numbers\nsignificantly impacting performance. This research offers valuable insights\ninto optimizing multimodal recommender systems and advancing the integration of\ndiverse data modalities in LLMs."
                },
                "authors": [
                    {
                        "name": "Zezheng Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zezheng Qin"
                },
                "author": "Zezheng Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04811v2",
                "updated": "2024-09-13T05:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    19,
                    32,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-09T01:45:39Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    1,
                    45,
                    39,
                    4,
                    222,
                    0
                ],
                "title": "h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM\n  Safety Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM\n  Safety Assessment"
                },
                "summary": "The safety of Large Language Models (LLMs) remains a critical concern due to\na lack of adequate benchmarks for systematically evaluating their ability to\nresist generating harmful content. Previous efforts towards automated red\nteaming involve static or templated sets of illicit requests and adversarial\nprompts which have limited utility given jailbreak attacks' evolving and\ncomposable nature. We propose a novel dynamic benchmark of composable jailbreak\nattacks to move beyond static datasets and taxonomies of attacks and harms. Our\napproach consists of three components collectively called h4rm3l: (1) a\ndomain-specific language that formally expresses jailbreak attacks as\ncompositions of parameterized prompt transformation primitives, (2)\nbandit-based few-shot program synthesis algorithms that generate novel attacks\noptimized to penetrate the safety filters of a target black box LLM, and (3)\nopen-source automated red-teaming software employing the previous two\ncomponents. We use h4rm3l to generate a dataset of 2656 successful novel\njailbreak attacks targeting 6 state-of-the-art (SOTA) open-source and\nproprietary LLMs. Several of our synthesized attacks are more effective than\npreviously reported ones, with Attack Success Rates exceeding 90% on SOTA\nclosed language models such as claude-3-haiku and GPT4-o. By generating\ndatasets of jailbreak attacks in a unified formal representation, h4rm3l\nenables reproducible benchmarking and automated red-teaming, contributes to\nunderstanding LLM safety limitations, and supports the development of robust\ndefenses in an increasingly LLM-integrated world.\n  Warning: This paper and related research artifacts contain offensive and\npotentially disturbing prompts and model-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety of Large Language Models (LLMs) remains a critical concern due to\na lack of adequate benchmarks for systematically evaluating their ability to\nresist generating harmful content. Previous efforts towards automated red\nteaming involve static or templated sets of illicit requests and adversarial\nprompts which have limited utility given jailbreak attacks' evolving and\ncomposable nature. We propose a novel dynamic benchmark of composable jailbreak\nattacks to move beyond static datasets and taxonomies of attacks and harms. Our\napproach consists of three components collectively called h4rm3l: (1) a\ndomain-specific language that formally expresses jailbreak attacks as\ncompositions of parameterized prompt transformation primitives, (2)\nbandit-based few-shot program synthesis algorithms that generate novel attacks\noptimized to penetrate the safety filters of a target black box LLM, and (3)\nopen-source automated red-teaming software employing the previous two\ncomponents. We use h4rm3l to generate a dataset of 2656 successful novel\njailbreak attacks targeting 6 state-of-the-art (SOTA) open-source and\nproprietary LLMs. Several of our synthesized attacks are more effective than\npreviously reported ones, with Attack Success Rates exceeding 90% on SOTA\nclosed language models such as claude-3-haiku and GPT4-o. By generating\ndatasets of jailbreak attacks in a unified formal representation, h4rm3l\nenables reproducible benchmarking and automated red-teaming, contributes to\nunderstanding LLM safety limitations, and supports the development of robust\ndefenses in an increasingly LLM-integrated world.\n  Warning: This paper and related research artifacts contain offensive and\npotentially disturbing prompts and model-generated content."
                },
                "authors": [
                    {
                        "name": "Moussa Koulako Bala Doumbouya"
                    },
                    {
                        "name": "Ananjan Nandi"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Davide Ghilardi"
                    },
                    {
                        "name": "Anna Goldie"
                    },
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.0; I.2.1; I.2.5; I.2.7; K.6.5; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v4",
                "updated": "2024-09-13T05:12:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    5,
                    12,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v2",
                "updated": "2024-09-13T04:54:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    54,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on nVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various LLMs and\ntoken lengths, with a particular focus on the bottleneck caused by CPU-GPU data\ntransfers via PCIe. Our results indicate that while there is minimal\ncomputational overhead within the GPU, the overall performance penalty is\nprimarily attributable to data transfer. For the majority of typical LLM\nqueries, the overhead remains below 5%, with larger models and longer sequences\nexperiencing nearly zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on nVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various LLMs and\ntoken lengths, with a particular focus on the bottleneck caused by CPU-GPU data\ntransfers via PCIe. Our results indicate that while there is minimal\ncomputational overhead within the GPU, the overall performance penalty is\nprimarily attributable to data transfer. For the majority of typical LLM\nqueries, the overhead remains below 5%, with larger models and longer sequences\nexperiencing nearly zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Peng Deng"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14515v2",
                "updated": "2024-09-13T04:25:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    25,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-25T11:33:52Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    11,
                    33,
                    52,
                    6,
                    238,
                    0
                ],
                "title": "A Joint Learning Model with Variational Interaction for Multilingual\n  Program Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Model with Variational Interaction for Multilingual\n  Program Translation"
                },
                "summary": "Programs implemented in various programming languages form the foundation of\nsoftware applications. To alleviate the burden of program migration and\nfacilitate the development of software systems, automated program translation\nacross languages has garnered significant attention. Previous approaches\nprimarily focus on pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel data is\ndifficult to collect for some language pairs, and the distribution of program\nsemantics across languages can shift, posing challenges for pairwise program\ntranslation. In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior to separately\nlearning from bilingual parallel data. We propose Variational Interaction for\nMultilingual Program Translation~(VIM-PT), a disentanglement-based generative\napproach that jointly trains a unified model for multilingual program\ntranslation across multiple languages. VIM-PT disentangles code into\nlanguage-shared and language-specific features, using variational inference and\ninteraction information with a novel lower bound, then achieves program\ntranslation through conditional generation. VIM-PT demonstrates four\nadvantages: 1) captures language-shared information more accurately from\nvarious implementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel data, 3)\naddresses the distribution shift of program semantics across languages, 4) and\nserves as a unified model, reducing deployment complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programs implemented in various programming languages form the foundation of\nsoftware applications. To alleviate the burden of program migration and\nfacilitate the development of software systems, automated program translation\nacross languages has garnered significant attention. Previous approaches\nprimarily focus on pairwise translation paradigms, learning translation between\npairs of languages using bilingual parallel data. However, parallel data is\ndifficult to collect for some language pairs, and the distribution of program\nsemantics across languages can shift, posing challenges for pairwise program\ntranslation. In this paper, we argue that jointly learning a unified model to\ntranslate code across multiple programming languages is superior to separately\nlearning from bilingual parallel data. We propose Variational Interaction for\nMultilingual Program Translation~(VIM-PT), a disentanglement-based generative\napproach that jointly trains a unified model for multilingual program\ntranslation across multiple languages. VIM-PT disentangles code into\nlanguage-shared and language-specific features, using variational inference and\ninteraction information with a novel lower bound, then achieves program\ntranslation through conditional generation. VIM-PT demonstrates four\nadvantages: 1) captures language-shared information more accurately from\nvarious implementations and improves the quality of multilingual program\ntranslation, 2) mines and leverages the capability of non-parallel data, 3)\naddresses the distribution shift of program semantics across languages, 4) and\nserves as a unified model, reducing deployment complexity."
                },
                "authors": [
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07276v2",
                "updated": "2024-09-13T04:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    16,
                    55,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-11T13:49:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM"
                },
                "summary": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08523v1",
                "updated": "2024-09-13T04:06:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    6,
                    0,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T04:06:00Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    6,
                    0,
                    4,
                    257,
                    0
                ],
                "title": "Eir: Thai Medical Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eir: Thai Medical Large Language Models"
                },
                "summary": "We present Eir Thai Medical LLM, a large language model with 8 billion\nparameters, specifically designed to enhance the accuracy of handling medical\ntasks in the Thai language. This model focuses on providing clear and\neasy-to-understand answers for both healthcare professionals and patients,\nthereby improving the efficiency of diagnosis and treatment processes. Human\nevaluation was conducted to ensure that the model adheres to care standards and\nprovides unbiased answers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop Eir\nThai Medical LLM. Our evaluation employed multiple questioning strategies,\nincluding zero-shot, few-shot, chain-of-thought reasoning, and\nensemble/self-consistency voting methods. Our model outperformed commercially\navailable Thai-language large language models by more than 10%. In addition, we\ndeveloped enhanced model testing tailored for clinical use in Thai across 18\nclinical tasks, where our model exceeded GPT-4o performance by more than 11%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Eir Thai Medical LLM, a large language model with 8 billion\nparameters, specifically designed to enhance the accuracy of handling medical\ntasks in the Thai language. This model focuses on providing clear and\neasy-to-understand answers for both healthcare professionals and patients,\nthereby improving the efficiency of diagnosis and treatment processes. Human\nevaluation was conducted to ensure that the model adheres to care standards and\nprovides unbiased answers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop Eir\nThai Medical LLM. Our evaluation employed multiple questioning strategies,\nincluding zero-shot, few-shot, chain-of-thought reasoning, and\nensemble/self-consistency voting methods. Our model outperformed commercially\navailable Thai-language large language models by more than 10%. In addition, we\ndeveloped enhanced model testing tailored for clinical use in Thai across 18\nclinical tasks, where our model exceeded GPT-4o performance by more than 11%"
                },
                "authors": [
                    {
                        "name": "Yutthakorn Thiprak"
                    },
                    {
                        "name": "Rungtam Ngodngamthaweesuk"
                    },
                    {
                        "name": "Songtam Ngodngamtaweesuk"
                    }
                ],
                "author_detail": {
                    "name": "Songtam Ngodngamtaweesuk"
                },
                "author": "Songtam Ngodngamtaweesuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07556v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07556v4",
                "updated": "2024-09-13T03:59:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    59,
                    49,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-12T11:40:44Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    11,
                    40,
                    44,
                    1,
                    72,
                    0
                ],
                "title": "Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information."
                },
                "authors": [
                    {
                        "name": "Tian Yu"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Accepted to ACL 2024 Findings. Code is available at:\n  https://github.com/ictnlp/TACS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07556v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07556v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06816v2",
                "updated": "2024-09-13T03:12:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    12,
                    52,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-10T18:52:40Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    52,
                    40,
                    1,
                    254,
                    0
                ],
                "title": "LLM-Enhanced Software Patch Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Software Patch Localization"
                },
                "summary": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source software (OSS) is integral to modern product development, and any\nvulnerability within it potentially compromises numerous products. While\ndevelopers strive to apply security patches, pinpointing these patches among\nextensive OSS updates remains a challenge. Security patch localization (SPL)\nrecommendation methods are leading approaches to address this. However,\nexisting SPL models often falter when a commit lacks a clear association with\nits corresponding CVE, and do not consider a scenario that a vulnerability has\nmultiple patches proposed over time before it has been fully resolved. To\naddress these challenges, we introduce LLM-SPL, a recommendation-based SPL\napproach that leverages the capabilities of the Large Language Model (LLM) to\nlocate the security patch commit for a given CVE. More specifically, we propose\na joint learning framework, in which the outputs of LLM serves as additional\nfeatures to aid our recommendation model in prioritizing security patches. Our\nevaluation on a dataset of 1,915 CVEs associated with 2,461 patches\ndemonstrates that LLM-SPL excels in ranking patch commits, surpassing the\nstate-of-the-art method in terms of Recall, while significantly reducing manual\neffort. Notably, for vulnerabilities requiring multiple patches, LLM-SPL\nsignificantly improves Recall by 22.83\\%, NDCG by 19.41\\%, and reduces manual\neffort by over 25\\% when checking up to the top 10 rankings. The dataset and\nsource code are available at\n\\url{https://anonymous.4open.science/r/LLM-SPL-91F8}."
                },
                "authors": [
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Di Tang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Haixu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Haixu Tang"
                },
                "author": "Haixu Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12665v2",
                "updated": "2024-09-13T03:07:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    3,
                    7,
                    37,
                    4,
                    257,
                    0
                ],
                "published": "2024-07-17T15:48:39Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    15,
                    48,
                    39,
                    2,
                    199,
                    0
                ],
                "title": "Patch-Level Training for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch-Level Training for Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}."
                },
                "authors": [
                    {
                        "name": "Chenze Shao"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06658v2",
                "updated": "2024-09-13T02:59:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    2,
                    59,
                    36,
                    4,
                    257,
                    0
                ],
                "published": "2024-08-13T06:15:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    6,
                    15,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "ComGPT: Detecting Local Community Structure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComGPT: Detecting Local Community Structure with Large Language Models"
                },
                "summary": "Large Language Models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks, such as node classification. Despite their strong\nabilities in graph reasoning tasks, they lack specific domain knowledge and\nhave a weaker understanding of community-related graph information, which\nhinders their capabilities in the community detection task. Moreover, local\ncommunity detection algorithms based on seed expansion, referred to as seed\nexpansion algorithms, often face the seed-dependent problem, community\ndiffusion, and free rider effect. To use LLMs to overcome the above\nshortcomings, we explore a GPT-guided seed expansion algorithm named ComGPT.\nComGPT iteratively selects potential nodes by local modularity M from the\ndetected community's neighbors, and subsequently employs LLMs to choose the\nnode to join the detected community from these selected potential nodes. To\naddress the above issues faced by LLMs, we improve graph encoding method,\ncalled Incident, by incorporating community knowledge to improve LLMs's\nunderstanding of community-related graph information. Additionally, we design\nthe NSG (Node Selection Guide) prompt to enhance LLMs' understanding of\ncommunity characteristics. Experimental results demonstrate that ComGPT\noutperforms the comparison methods, thereby confirming the effectiveness of the\nimproved graph encoding method and prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), like GPT, have demonstrated the ability to\nunderstand graph structures and have achieved excellent performance in various\ngraph reasoning tasks, such as node classification. Despite their strong\nabilities in graph reasoning tasks, they lack specific domain knowledge and\nhave a weaker understanding of community-related graph information, which\nhinders their capabilities in the community detection task. Moreover, local\ncommunity detection algorithms based on seed expansion, referred to as seed\nexpansion algorithms, often face the seed-dependent problem, community\ndiffusion, and free rider effect. To use LLMs to overcome the above\nshortcomings, we explore a GPT-guided seed expansion algorithm named ComGPT.\nComGPT iteratively selects potential nodes by local modularity M from the\ndetected community's neighbors, and subsequently employs LLMs to choose the\nnode to join the detected community from these selected potential nodes. To\naddress the above issues faced by LLMs, we improve graph encoding method,\ncalled Incident, by incorporating community knowledge to improve LLMs's\nunderstanding of community-related graph information. Additionally, we design\nthe NSG (Node Selection Guide) prompt to enhance LLMs' understanding of\ncommunity characteristics. Experimental results demonstrate that ComGPT\noutperforms the comparison methods, thereby confirming the effectiveness of the\nimproved graph encoding method and prompts."
                },
                "authors": [
                    {
                        "name": "Li Ni"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Lin Mu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Wenjian Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenjian Luo"
                },
                "author": "Wenjian Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08493v1",
                "updated": "2024-09-13T02:37:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    2,
                    37,
                    28,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T02:37:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    2,
                    37,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Intelligent LiDAR Navigation: Leveraging External Information and\n  Semantic Maps with LLM as Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent LiDAR Navigation: Leveraging External Information and\n  Semantic Maps with LLM as Copilot"
                },
                "summary": "Traditional robot navigation systems primarily utilize occupancy grid maps\nand laser-based sensing technologies, as demonstrated by the popular move_base\npackage in ROS. Unlike robots, humans navigate not only through spatial\nawareness and physical distances but also by integrating external information,\nsuch as elevator maintenance updates from public notification boards and\nexperiential knowledge, like the need for special access through certain doors.\nWith the development of Large Language Models (LLMs), which posses text\nunderstanding and intelligence close to human performance, there is now an\nopportunity to infuse robot navigation systems with a level of understanding\nakin to human cognition. In this study, we propose using osmAG (Area Graph in\nOpensStreetMap textual format), an innovative semantic topometric hierarchical\nmap representation, to bridge the gap between the capabilities of ROS move_base\nand the contextual understanding offered by LLMs. Our methodology employs LLMs\nas actual copilot in robot navigation, enabling the integration of a broader\nrange of informational inputs while maintaining the robustness of traditional\nrobotic navigation systems. Our code, demo, map, experiment results can be\naccessed at\nhttps://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional robot navigation systems primarily utilize occupancy grid maps\nand laser-based sensing technologies, as demonstrated by the popular move_base\npackage in ROS. Unlike robots, humans navigate not only through spatial\nawareness and physical distances but also by integrating external information,\nsuch as elevator maintenance updates from public notification boards and\nexperiential knowledge, like the need for special access through certain doors.\nWith the development of Large Language Models (LLMs), which posses text\nunderstanding and intelligence close to human performance, there is now an\nopportunity to infuse robot navigation systems with a level of understanding\nakin to human cognition. In this study, we propose using osmAG (Area Graph in\nOpensStreetMap textual format), an innovative semantic topometric hierarchical\nmap representation, to bridge the gap between the capabilities of ROS move_base\nand the contextual understanding offered by LLMs. Our methodology employs LLMs\nas actual copilot in robot navigation, enabling the integration of a broader\nrange of informational inputs while maintaining the robustness of traditional\nrobotic navigation systems. Our code, demo, map, experiment results can be\naccessed at\nhttps://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot."
                },
                "authors": [
                    {
                        "name": "Fujing Xie"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Sören Schwertfeger"
                    }
                ],
                "author_detail": {
                    "name": "Sören Schwertfeger"
                },
                "author": "Sören Schwertfeger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06223v2",
                "updated": "2024-09-13T02:17:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    2,
                    17,
                    58,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-10T05:26:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models"
                },
                "summary": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01332v2",
                "updated": "2024-09-13T00:56:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    56,
                    1,
                    4,
                    257,
                    0
                ],
                "published": "2024-03-29T22:49:43Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    22,
                    49,
                    43,
                    4,
                    89,
                    0
                ],
                "title": "Explaining Large Language Models Decisions with Shapley Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Large Language Models Decisions with Shapley Values"
                },
                "summary": "The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel's output. Through two applications - a discrete choice experiment and an\ninvestigation of cognitive biases - we demonstrate how the Shapley value method\ncan uncover what we term \"token noise\" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for practitioners and researchers\nto strategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin survey settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel's output. Through two applications - a discrete choice experiment and an\ninvestigation of cognitive biases - we demonstrate how the Shapley value method\ncan uncover what we term \"token noise\" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for practitioners and researchers\nto strategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin survey settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs."
                },
                "authors": [
                    {
                        "name": "Behnam Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Behnam Mohammadi"
                },
                "author": "Behnam Mohammadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08444v1",
                "updated": "2024-09-13T00:26:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    26,
                    9,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T00:26:09Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    26,
                    9,
                    4,
                    257,
                    0
                ],
                "title": "Towards Unified Facial Action Unit Recognition Framework by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unified Facial Action Unit Recognition Framework by Large\n  Language Models"
                },
                "summary": "Facial Action Units (AUs) are of great significance in the realm of affective\ncomputing. In this paper, we propose AU-LLaVA, the first unified AU recognition\nframework based on the Large Language Model (LLM). AU-LLaVA consists of a\nvisual encoder, a linear projector layer, and a pre-trained LLM. We\nmeticulously craft the text descriptions and fine-tune the model on various AU\ndatasets, allowing it to generate different formats of AU recognition results\nfor the same input image. On the BP4D and DISFA datasets, AU-LLaVA delivers the\nmost accurate recognition results for nearly half of the AUs. Our model\nachieves improvements of F1-score up to 11.4% in specific AU recognition\ncompared to previous benchmark results. On the FEAFA dataset, our method\nachieves significant improvements over all 24 AUs compared to previous\nbenchmark results. AU-LLaVA demonstrates exceptional performance and\nversatility in AU recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Action Units (AUs) are of great significance in the realm of affective\ncomputing. In this paper, we propose AU-LLaVA, the first unified AU recognition\nframework based on the Large Language Model (LLM). AU-LLaVA consists of a\nvisual encoder, a linear projector layer, and a pre-trained LLM. We\nmeticulously craft the text descriptions and fine-tune the model on various AU\ndatasets, allowing it to generate different formats of AU recognition results\nfor the same input image. On the BP4D and DISFA datasets, AU-LLaVA delivers the\nmost accurate recognition results for nearly half of the AUs. Our model\nachieves improvements of F1-score up to 11.4% in specific AU recognition\ncompared to previous benchmark results. On the FEAFA dataset, our method\nachieves significant improvements over all 24 AUs compared to previous\nbenchmark results. AU-LLaVA demonstrates exceptional performance and\nversatility in AU recognition."
                },
                "authors": [
                    {
                        "name": "Guohong Hu"
                    },
                    {
                        "name": "Xing Lan"
                    },
                    {
                        "name": "Hanyu Jiang"
                    },
                    {
                        "name": "Jiayi Lyu"
                    },
                    {
                        "name": "Jian Xue"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xue"
                },
                "author": "Jian Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08435v1",
                "updated": "2024-09-13T00:03:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    3,
                    19,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T00:03:19Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    3,
                    19,
                    4,
                    257,
                    0
                ],
                "title": "When Context Leads but Parametric Memory Follows in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Context Leads but Parametric Memory Follows in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance."
                },
                "authors": [
                    {
                        "name": "Yufei Tao"
                    },
                    {
                        "name": "Adam Hiatt"
                    },
                    {
                        "name": "Erik Haake"
                    },
                    {
                        "name": "Antonie J. Jetter"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11793v2",
                "updated": "2024-09-12T23:08:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    23,
                    8,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-03-18T13:50:50Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    13,
                    50,
                    50,
                    0,
                    78,
                    0
                ],
                "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the\n  Abstraction and Reasoning Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the\n  Abstraction and Reasoning Corpus"
                },
                "summary": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstraction and\nReasoning Corpus (ARC) dataset to evaluate the inference and contextual\nunderstanding abilities of large language models in a process-centric manner.\nARC demands rigorous logical structures for problem-solving, making it a\nbenchmark that facilitates the comparison of model inference abilities with\nhumans. Experimental results confirm that while large language models possess\nweak inference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstraction and\nReasoning Corpus (ARC) dataset to evaluate the inference and contextual\nunderstanding abilities of large language models in a process-centric manner.\nARC demands rigorous logical structures for problem-solving, making it a\nbenchmark that facilitates the comparison of model inference abilities with\nhumans. Experimental results confirm that while large language models possess\nweak inference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning."
                },
                "authors": [
                    {
                        "name": "Seungpil Lee"
                    },
                    {
                        "name": "Woochang Sim"
                    },
                    {
                        "name": "Donghyeon Shin"
                    },
                    {
                        "name": "Wongyu Seo"
                    },
                    {
                        "name": "Jiwon Park"
                    },
                    {
                        "name": "Seokki Lee"
                    },
                    {
                        "name": "Sanha Hwang"
                    },
                    {
                        "name": "Sejin Kim"
                    },
                    {
                        "name": "Sundong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sundong Kim"
                },
                "author": "Sundong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02764v2",
                "updated": "2024-09-12T22:18:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    22,
                    18,
                    3,
                    3,
                    256,
                    0
                ],
                "published": "2024-05-04T22:00:28Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    22,
                    0,
                    28,
                    5,
                    125,
                    0
                ],
                "title": "Assessing Adversarial Robustness of Large Language Models: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Adversarial Robustness of Large Language Models: An Empirical\n  Study"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their robustness against adversarial attacks remains a critical concern. We\npresents a novel white-box style attack approach that exposes vulnerabilities\nin leading open-source LLMs, including Llama, OPT, and T5. We assess the impact\nof model size, structure, and fine-tuning strategies on their resistance to\nadversarial perturbations. Our comprehensive evaluation across five diverse\ntext classification tasks establishes a new benchmark for LLM robustness. The\nfindings of this study have far-reaching implications for the reliable\ndeployment of LLMs in real-world applications and contribute to the advancement\nof trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their robustness against adversarial attacks remains a critical concern. We\npresents a novel white-box style attack approach that exposes vulnerabilities\nin leading open-source LLMs, including Llama, OPT, and T5. We assess the impact\nof model size, structure, and fine-tuning strategies on their resistance to\nadversarial perturbations. Our comprehensive evaluation across five diverse\ntext classification tasks establishes a new benchmark for LLM robustness. The\nfindings of this study have far-reaching implications for the reliable\ndeployment of LLMs in real-world applications and contribute to the advancement\nof trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Oral presentation at KDD 2024 GenAI Evaluation workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00898v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00898v4",
                "updated": "2024-09-12T22:09:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    22,
                    9,
                    25,
                    3,
                    256,
                    0
                ],
                "published": "2023-10-02T04:29:40Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    4,
                    29,
                    40,
                    0,
                    275,
                    0
                ],
                "title": "Enabling Language Models to Implicitly Learn Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Language Models to Implicitly Learn Self-Improvement"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods."
                },
                "authors": [
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Le Hou"
                    },
                    {
                        "name": "Tianjian Lu"
                    },
                    {
                        "name": "Yuexin Wu"
                    },
                    {
                        "name": "Yunxuan Li"
                    },
                    {
                        "name": "Hongkun Yu"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted at ICLR 2024. 28 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00898v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00898v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08413v1",
                "updated": "2024-09-12T22:00:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    22,
                    0,
                    54,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T22:00:54Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    22,
                    0,
                    54,
                    3,
                    256,
                    0
                ],
                "title": "Safety of Linear Systems under Severe Sensor Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety of Linear Systems under Severe Sensor Attacks"
                },
                "summary": "Cyber-physical systems can be subject to sensor attacks, e.g., sensor\nspoofing, leading to unsafe behaviors. This paper addresses this problem in the\ncontext of linear systems when an omniscient attacker can spoof several system\nsensors at will. In this adversarial environment, existing results have derived\nnecessary and sufficient conditions under which the state estimation problem\nhas a unique solution. In this work, we consider a severe attacking scenario\nwhen such conditions do not hold. To deal with potential state estimation\nuncertainty, we derive an exact characterization of the set of all possible\nstate estimates. Using the framework of control barrier functions, we propose\ndesign principles for system safety in offline and online phases. For the\noffline phase, we derive conditions on safe sets for all possible sensor\nattacks that may be encountered during system deployment. For the online phase,\nwith past system measurements collected, a quadratic program-based safety\nfilter is proposed to enforce system safety. A 2D-vehicle example is used to\nillustrate the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems can be subject to sensor attacks, e.g., sensor\nspoofing, leading to unsafe behaviors. This paper addresses this problem in the\ncontext of linear systems when an omniscient attacker can spoof several system\nsensors at will. In this adversarial environment, existing results have derived\nnecessary and sufficient conditions under which the state estimation problem\nhas a unique solution. In this work, we consider a severe attacking scenario\nwhen such conditions do not hold. To deal with potential state estimation\nuncertainty, we derive an exact characterization of the set of all possible\nstate estimates. Using the framework of control barrier functions, we propose\ndesign principles for system safety in offline and online phases. For the\noffline phase, we derive conditions on safe sets for all possible sensor\nattacks that may be encountered during system deployment. For the online phase,\nwith past system measurements collected, a quadratic program-based safety\nfilter is proposed to enforce system safety. A 2D-vehicle example is used to\nillustrate the theoretical results."
                },
                "authors": [
                    {
                        "name": "Xiao Tan"
                    },
                    {
                        "name": "Pio Ong"
                    },
                    {
                        "name": "Paulo Tabuada"
                    },
                    {
                        "name": "Aaron D. Ames"
                    }
                ],
                "author_detail": {
                    "name": "Aaron D. Ames"
                },
                "author": "Aaron D. Ames",
                "arxiv_comment": "To appear at CDC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08410v1",
                "updated": "2024-09-12T21:42:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    42,
                    28,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T21:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    42,
                    28,
                    3,
                    256,
                    0
                ],
                "title": "Sequential Discrete Action Selection via Blocking Conditions and\n  Resolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Discrete Action Selection via Blocking Conditions and\n  Resolutions"
                },
                "summary": "In this work, we introduce a strategy that frames the sequential action\nselection problem for robots in terms of resolving \\textit{blocking\nconditions}, i.e., situations that impede progress on an action en route to a\ngoal. This strategy allows a robot to make one-at-a-time decisions that take in\npertinent contextual information and swiftly adapt and react to current\nsituations. We present a first instantiation of this strategy that combines a\nstate-transition graph and a zero-shot Large Language Model (LLM). The\nstate-transition graph tracks which previously attempted actions are currently\nblocked and which candidate actions may resolve existing blocking conditions.\nThis information from the state-transition graph is used to automatically\ngenerate a prompt for the LLM, which then uses the given context and set of\npossible actions to select a single action to try next. This selection process\nis iterative, with each chosen and executed action further refining the\nstate-transition graph, continuing until the agent either fulfills the goal or\nencounters a termination condition. We demonstrate the effectiveness of our\napproach by comparing it to various LLM and traditional task-planning methods\nin a testbed of simulation experiments. We discuss the implications of our work\nbased on our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a strategy that frames the sequential action\nselection problem for robots in terms of resolving \\textit{blocking\nconditions}, i.e., situations that impede progress on an action en route to a\ngoal. This strategy allows a robot to make one-at-a-time decisions that take in\npertinent contextual information and swiftly adapt and react to current\nsituations. We present a first instantiation of this strategy that combines a\nstate-transition graph and a zero-shot Large Language Model (LLM). The\nstate-transition graph tracks which previously attempted actions are currently\nblocked and which candidate actions may resolve existing blocking conditions.\nThis information from the state-transition graph is used to automatically\ngenerate a prompt for the LLM, which then uses the given context and set of\npossible actions to select a single action to try next. This selection process\nis iterative, with each chosen and executed action further refining the\nstate-transition graph, continuing until the agent either fulfills the goal or\nencounters a termination condition. We demonstrate the effectiveness of our\napproach by comparing it to various LLM and traditional task-planning methods\nin a testbed of simulation experiments. We discuss the implications of our work\nbased on our results."
                },
                "authors": [
                    {
                        "name": "Liam Merz Hoffmeister"
                    },
                    {
                        "name": "Brian Scassellati"
                    },
                    {
                        "name": "Daniel Rakita"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Rakita"
                },
                "author": "Daniel Rakita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08406v1",
                "updated": "2024-09-12T21:39:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    39,
                    1,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T21:39:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    21,
                    39,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Tagging with Large Language Model based Multi-Agent System"
                },
                "summary": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Ethan Chang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08386v1",
                "updated": "2024-09-12T20:32:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    20,
                    32,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T20:32:07Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    20,
                    32,
                    7,
                    3,
                    256,
                    0
                ],
                "title": "Self-Supervised Inference of Agents in Trustless Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Inference of Agents in Trustless Environments"
                },
                "summary": "In this paper, we propose a novel approach where agents can form swarms to\nproduce high-quality responses effectively. This is accomplished by utilizing\nagents capable of data inference and ranking, which can be effectively\nimplemented using LLMs as response classifiers. We assess existing approaches\nfor trustless agent inference, define our methodology, estimate practical\nparameters, and model various types of malicious agent attacks. Our method\nleverages the collective intelligence of swarms, ensuring robust and efficient\ndecentralized AI inference with better accuracy, security, and reliability. We\nshow that our approach is an order of magnitude faster than other trustless\ninference strategies reaching less than 125 ms validation latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel approach where agents can form swarms to\nproduce high-quality responses effectively. This is accomplished by utilizing\nagents capable of data inference and ranking, which can be effectively\nimplemented using LLMs as response classifiers. We assess existing approaches\nfor trustless agent inference, define our methodology, estimate practical\nparameters, and model various types of malicious agent attacks. Our method\nleverages the collective intelligence of swarms, ensuring robust and efficient\ndecentralized AI inference with better accuracy, security, and reliability. We\nshow that our approach is an order of magnitude faster than other trustless\ninference strategies reaching less than 125 ms validation latency."
                },
                "authors": [
                    {
                        "name": "Vladyslav Larin"
                    },
                    {
                        "name": "Ivan Nikitin"
                    },
                    {
                        "name": "Alexander Firsov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Firsov"
                },
                "author": "Alexander Firsov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08379v1",
                "updated": "2024-09-12T19:59:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    59,
                    54,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T19:59:54Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    59,
                    54,
                    3,
                    256,
                    0
                ],
                "title": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models on Open-source Innovation: Evidence\n  from GitHub Copilot"
                },
                "summary": "Generative AI (GenAI) has been shown to enhance individual productivity in a\nguided setting. While it is also likely to transform processes in a\ncollaborative work setting, it is unclear what trajectory this transformation\nwill follow. Collaborative environment is characterized by a blend of\norigination tasks that involve building something from scratch and iteration\ntasks that involve refining on others' work. Whether GenAI affects these two\naspects of collaborative work and to what extent is an open empirical question.\nWe study this question within the open-source development landscape, a prime\nexample of collaborative innovation, where contributions are voluntary and\nunguided. Specifically, we focus on the launch of GitHub Copilot in October\n2021 and leverage a natural experiment in which GitHub Copilot (a\nprogramming-focused LLM) selectively rolled out support for Python, but not for\nR. We observe a significant jump in overall contributions, suggesting that\nGenAI effectively augments collaborative innovation in an unguided setting.\nInterestingly, Copilot's launch increased maintenance-related contributions,\nwhich are mostly iterative tasks involving building on others' work,\nsignificantly more than code-development contributions, which are mostly\norigination tasks involving standalone contributions. This disparity was\nexacerbated in active projects with extensive coding activity, raising concerns\nthat, as GenAI models improve to accommodate richer context, the gap between\norigination and iterative solutions may widen. We discuss practical and policy\nimplications to incentivize high-value innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has been shown to enhance individual productivity in a\nguided setting. While it is also likely to transform processes in a\ncollaborative work setting, it is unclear what trajectory this transformation\nwill follow. Collaborative environment is characterized by a blend of\norigination tasks that involve building something from scratch and iteration\ntasks that involve refining on others' work. Whether GenAI affects these two\naspects of collaborative work and to what extent is an open empirical question.\nWe study this question within the open-source development landscape, a prime\nexample of collaborative innovation, where contributions are voluntary and\nunguided. Specifically, we focus on the launch of GitHub Copilot in October\n2021 and leverage a natural experiment in which GitHub Copilot (a\nprogramming-focused LLM) selectively rolled out support for Python, but not for\nR. We observe a significant jump in overall contributions, suggesting that\nGenAI effectively augments collaborative innovation in an unguided setting.\nInterestingly, Copilot's launch increased maintenance-related contributions,\nwhich are mostly iterative tasks involving building on others' work,\nsignificantly more than code-development contributions, which are mostly\norigination tasks involving standalone contributions. This disparity was\nexacerbated in active projects with extensive coding activity, raising concerns\nthat, as GenAI models improve to accommodate richer context, the gap between\norigination and iterative solutions may widen. We discuss practical and policy\nimplications to incentivize high-value innovative solutions."
                },
                "authors": [
                    {
                        "name": "Doron Yeverechyahu"
                    },
                    {
                        "name": "Raveesh Mayya"
                    },
                    {
                        "name": "Gal Oestreicher-Singer"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oestreicher-Singer"
                },
                "author": "Gal Oestreicher-Singer",
                "arxiv_comment": "JEL Classification: O31, C88, J24, O35, L86",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08369v1",
                "updated": "2024-09-12T19:30:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    30,
                    22,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T19:30:22Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    30,
                    22,
                    3,
                    256,
                    0
                ],
                "title": "E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural\n  Networks for Resource-Optimized Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural\n  Networks for Resource-Optimized Learning"
                },
                "summary": "Ensemble learning is a meta-learning approach that combines the predictions\nof multiple learners, demonstrating improved accuracy and robustness.\nNevertheless, ensembling models like Convolutional Neural Networks (CNNs)\nresult in high memory and computing overhead, preventing their deployment in\nembedded systems. These devices are usually equipped with small batteries that\nprovide power supply and might include energy-harvesting modules that extract\nenergy from the environment. In this work, we propose E-QUARTIC, a novel Energy\nEfficient Edge Ensembling framework to build ensembles of CNNs targeting\nArtificial Intelligence (AI)-based embedded systems. Our design outperforms\nsingle-instance CNN baselines and state-of-the-art edge AI solutions, improving\naccuracy and adapting to varying energy conditions while maintaining similar\nmemory requirements. Then, we leverage the multi-CNN structure of the designed\nensemble to implement an energy-aware model selection policy in\nenergy-harvesting AI systems. We show that our solution outperforms the\nstate-of-the-art by reducing system failure rate by up to 40% while ensuring\nhigher average output qualities. Ultimately, we show that the proposed design\nenables concurrent on-device training and high-quality inference execution at\nthe edge, limiting the performance and energy overheads to less than 0.04%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble learning is a meta-learning approach that combines the predictions\nof multiple learners, demonstrating improved accuracy and robustness.\nNevertheless, ensembling models like Convolutional Neural Networks (CNNs)\nresult in high memory and computing overhead, preventing their deployment in\nembedded systems. These devices are usually equipped with small batteries that\nprovide power supply and might include energy-harvesting modules that extract\nenergy from the environment. In this work, we propose E-QUARTIC, a novel Energy\nEfficient Edge Ensembling framework to build ensembles of CNNs targeting\nArtificial Intelligence (AI)-based embedded systems. Our design outperforms\nsingle-instance CNN baselines and state-of-the-art edge AI solutions, improving\naccuracy and adapting to varying energy conditions while maintaining similar\nmemory requirements. Then, we leverage the multi-CNN structure of the designed\nensemble to implement an energy-aware model selection policy in\nenergy-harvesting AI systems. We show that our solution outperforms the\nstate-of-the-art by reducing system failure rate by up to 40% while ensuring\nhigher average output qualities. Ultimately, we show that the proposed design\nenables concurrent on-device training and high-quality inference execution at\nthe edge, limiting the performance and energy overheads to less than 0.04%."
                },
                "authors": [
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Onat Gungor"
                    },
                    {
                        "name": "Flavio Ponzina"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "arxiv_comment": "Accepted by the 30th Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04667v2",
                "updated": "2024-09-12T19:15:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    19,
                    15,
                    17,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-06T16:43:35Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    16,
                    43,
                    35,
                    1,
                    219,
                    0
                ],
                "title": "LLM Stability: A detailed analysis with some surprises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Stability: A detailed analysis with some surprises"
                },
                "summary": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs, but we have been unable to find work that evaluates\nLLM stability as the main objective. In our study of 6 deterministically\nconfigured LLMs across 8 common tasks with 5 identical runs, we see accuracy\nvariations up to 10\\%. In addition, no LLM consistently delivers repeatable\naccuracy across all tasks. We also show examples of variation that are not\nnormally distributed and compare configurations with zero-shot/few-shot\nprompting and fine-tuned examples. To better quantify what is going on, we\nintroduce metrics focused on stability: TARr@N for the total agreement rate at\nN runs over raw output, and TARa@N for total agreement over parsed-out answers.\nWe suggest that stability metrics be integrated into leader boards and research\nresults going forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs, but we have been unable to find work that evaluates\nLLM stability as the main objective. In our study of 6 deterministically\nconfigured LLMs across 8 common tasks with 5 identical runs, we see accuracy\nvariations up to 10\\%. In addition, no LLM consistently delivers repeatable\naccuracy across all tasks. We also show examples of variation that are not\nnormally distributed and compare configurations with zero-shot/few-shot\nprompting and fine-tuned examples. To better quantify what is going on, we\nintroduce metrics focused on stability: TARr@N for the total agreement rate at\nN runs over raw output, and TARa@N for total agreement over parsed-out answers.\nWe suggest that stability metrics be integrated into leader boards and research\nresults going forward."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Alexa Chittams"
                    },
                    {
                        "name": "Liseng Fu"
                    },
                    {
                        "name": "Ferhan Ture"
                    },
                    {
                        "name": "Lixinyu Xu"
                    },
                    {
                        "name": "Breck Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Breck Baldwin"
                },
                "author": "Breck Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08357v1",
                "updated": "2024-09-12T18:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    50,
                    13,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T18:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    50,
                    13,
                    3,
                    256,
                    0
                ],
                "title": "An Experimental Study of Competitive Market Behavior Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Experimental Study of Competitive Market Behavior Through LLMs"
                },
                "summary": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies."
                },
                "authors": [
                    {
                        "name": "Jingru Jia"
                    },
                    {
                        "name": "Zehua Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehua Yuan"
                },
                "author": "Zehua Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18659v3",
                "updated": "2024-09-12T18:30:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    30,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-02-28T19:09:08Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    9,
                    8,
                    2,
                    59,
                    0
                ],
                "title": "Large Language Models and Games: A Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Games: A Survey and Roadmap"
                },
                "summary": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field."
                },
                "authors": [
                    {
                        "name": "Roberto Gallotta"
                    },
                    {
                        "name": "Graham Todd"
                    },
                    {
                        "name": "Marvin Zammit"
                    },
                    {
                        "name": "Sam Earle"
                    },
                    {
                        "name": "Antonios Liapis"
                    },
                    {
                        "name": "Julian Togelius"
                    },
                    {
                        "name": "Georgios N. Yannakakis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios N. Yannakakis"
                },
                "author": "Georgios N. Yannakakis",
                "arxiv_comment": "Accepted for publication at the IEEE Transactions on Games (18 pages,\n  6 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10704v2",
                "updated": "2024-09-12T18:25:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    25,
                    16,
                    3,
                    256,
                    0
                ],
                "published": "2024-03-15T21:43:46Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    21,
                    43,
                    46,
                    4,
                    75,
                    0
                ],
                "title": "Parameter Efficient Reinforcement Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Efficient Reinforcement Learning from Human Feedback"
                },
                "summary": "While Reinforcement Learning from Human Feedback (RLHF) effectively aligns\npretrained Large Language and Vision-Language Models (LLMs, and VLMs) with\nhuman preferences, its computational cost and complexity hamper its wider\nadoption. To alleviate some of the computational burden of fine-tuning,\nparameter efficient methods, like LoRA were introduced. In this work, we\nempirically evaluate the setup of Parameter Efficient Reinforcement Learning\nfrom Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward\nModeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six\ndiverse datasets spanning summarization, harmless/helpful response generation,\nUI automation, and visual question answering in terms of effectiveness of the\ntrained models, and the training resources required. Our findings show, for the\nfirst time, that PE-RLHF achieves comparable performance to RLHF, while\nsignificantly reducing training time (up to 90% faster for reward models, and\n30% faster for RL), and memory footprint (up to 50% reduction for reward\nmodels, and 27% for RL). We provide comprehensive ablations across LoRA ranks,\nand model sizes for both reward modeling and reinforcement learning. By\nmitigating the computational burden associated with RLHF, we push for a broader\nadoption of PE-RLHF as an alignment technique for LLMs and VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Reinforcement Learning from Human Feedback (RLHF) effectively aligns\npretrained Large Language and Vision-Language Models (LLMs, and VLMs) with\nhuman preferences, its computational cost and complexity hamper its wider\nadoption. To alleviate some of the computational burden of fine-tuning,\nparameter efficient methods, like LoRA were introduced. In this work, we\nempirically evaluate the setup of Parameter Efficient Reinforcement Learning\nfrom Human Feedback (PE-RLHF) that leverages LoRA fine-tuning for Reward\nModeling, and Reinforcement Learning. We benchmark the PE-RLHF setup on six\ndiverse datasets spanning summarization, harmless/helpful response generation,\nUI automation, and visual question answering in terms of effectiveness of the\ntrained models, and the training resources required. Our findings show, for the\nfirst time, that PE-RLHF achieves comparable performance to RLHF, while\nsignificantly reducing training time (up to 90% faster for reward models, and\n30% faster for RL), and memory footprint (up to 50% reduction for reward\nmodels, and 27% for RL). We provide comprehensive ablations across LoRA ranks,\nand model sizes for both reward modeling and reinforcement learning. By\nmitigating the computational burden associated with RLHF, we push for a broader\nadoption of PE-RLHF as an alignment technique for LLMs and VLMs."
                },
                "authors": [
                    {
                        "name": "Hakim Sidahmed"
                    },
                    {
                        "name": "Samrat Phatale"
                    },
                    {
                        "name": "Alex Hutcheson"
                    },
                    {
                        "name": "Zhuonan Lin"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Zac Yu"
                    },
                    {
                        "name": "Jarvis Jin"
                    },
                    {
                        "name": "Simral Chaudhary"
                    },
                    {
                        "name": "Roman Komarytsia"
                    },
                    {
                        "name": "Christiane Ahlheim"
                    },
                    {
                        "name": "Yonghao Zhu"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Saravanan Ganesh"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Hassan Mansoor"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Abhinav Rastogi"
                    },
                    {
                        "name": "Lucas Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Dixon"
                },
                "author": "Lucas Dixon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06442v2",
                "updated": "2024-09-12T18:22:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    22,
                    51,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-10T11:48:05Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    11,
                    48,
                    5,
                    1,
                    254,
                    0
                ],
                "title": "Prompt2Fashion: An automatically generated fashion dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt2Fashion: An automatically generated fashion dataset"
                },
                "summary": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion."
                },
                "authors": [
                    {
                        "name": "Georgia Argyrou"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08330v1",
                "updated": "2024-09-12T18:00:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    0,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T18:00:18Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    0,
                    18,
                    3,
                    256,
                    0
                ],
                "title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of\n  Human Responses in Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of\n  Human Responses in Dialogue"
                },
                "summary": "Studying and building datasets for dialogue tasks is both expensive and\ntime-consuming due to the need to recruit, train, and collect data from study\nparticipants. In response, much recent work has sought to use large language\nmodels (LLMs) to simulate both human-human and human-LLM interactions, as they\nhave been shown to generate convincingly human-like text in many settings.\nHowever, to what extent do LLM-based simulations \\textit{actually} reflect\nhuman dialogues? In this work, we answer this question by generating a\nlarge-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the\nWildChat dataset and quantifying how well the LLM simulations align with their\nhuman counterparts. Overall, we find relatively low alignment between\nsimulations and human interactions, demonstrating a systematic divergence along\nthe multiple textual properties, including style and content. Further, in\ncomparisons of English, Chinese, and Russian dialogues, we find that models\nperform similarly. Our results suggest that LLMs generally perform better when\nthe human themself writes in a way that is more similar to the LLM's own style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying and building datasets for dialogue tasks is both expensive and\ntime-consuming due to the need to recruit, train, and collect data from study\nparticipants. In response, much recent work has sought to use large language\nmodels (LLMs) to simulate both human-human and human-LLM interactions, as they\nhave been shown to generate convincingly human-like text in many settings.\nHowever, to what extent do LLM-based simulations \\textit{actually} reflect\nhuman dialogues? In this work, we answer this question by generating a\nlarge-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the\nWildChat dataset and quantifying how well the LLM simulations align with their\nhuman counterparts. Overall, we find relatively low alignment between\nsimulations and human interactions, demonstrating a systematic divergence along\nthe multiple textual properties, including style and content. Further, in\ncomparisons of English, Chinese, and Russian dialogues, we find that models\nperform similarly. Our results suggest that LLMs generally perform better when\nthe human themself writes in a way that is more similar to the LLM's own style."
                },
                "authors": [
                    {
                        "name": "Johnathan Ivey"
                    },
                    {
                        "name": "Shivani Kumar"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Sushrita Rakshit"
                    },
                    {
                        "name": "Rohan Raju"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Aparna Ananthasubramaniam"
                    },
                    {
                        "name": "Junghwan Kim"
                    },
                    {
                        "name": "Bowen Yi"
                    },
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Abraham Israeli"
                    },
                    {
                        "name": "Anders Giovanni Møller"
                    },
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08264v2",
                "updated": "2024-09-13T20:17:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    20,
                    17,
                    13,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-12T17:56:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale"
                },
                "summary": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena"
                },
                "authors": [
                    {
                        "name": "Rogerio Bonatti"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Francesco Bonacci"
                    },
                    {
                        "name": "Dillon Dupont"
                    },
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Yadong Lu"
                    },
                    {
                        "name": "Justin Wagle"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    },
                    {
                        "name": "Arthur Bucker"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Zack Hui"
                    }
                ],
                "author_detail": {
                    "name": "Zack Hui"
                },
                "author": "Zack Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08250v1",
                "updated": "2024-09-12T17:48:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:48:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering"
                },
                "summary": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time."
                },
                "authors": [
                    {
                        "name": "Jiahao Nick Li"
                    },
                    {
                        "name": "Zhuohao Jerry Zhang"
                    },
                    {
                        "name": "Jiaju Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaju Ma"
                },
                "author": "Jiaju Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08239v1",
                "updated": "2024-09-12T17:39:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:39:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources"
                },
                "summary": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines."
                },
                "authors": [
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Carlos Gemmell"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Maria Lomeli"
                    }
                ],
                "author_detail": {
                    "name": "Maria Lomeli"
                },
                "author": "Maria Lomeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08234v1",
                "updated": "2024-09-12T17:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:33:06Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems"
                },
                "summary": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure."
                },
                "authors": [
                    {
                        "name": "Hakan T. Otal"
                    },
                    {
                        "name": "M. Abdullah Canbaz"
                    }
                ],
                "author_detail": {
                    "name": "M. Abdullah Canbaz"
                },
                "author": "M. Abdullah Canbaz",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.4.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09090v2",
                "updated": "2024-09-12T16:40:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    40,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-17T09:51:32Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    9,
                    51,
                    32,
                    2,
                    17,
                    0
                ],
                "title": "Understanding the concerns and choices of public when using large\n  language models for healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the concerns and choices of public when using large\n  language models for healthcare"
                },
                "summary": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Yunpeng Xiao"
                    },
                    {
                        "name": "Kyrie Zhixuan Zhou"
                    },
                    {
                        "name": "Yueqing Liang"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14573v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14573v5",
                "updated": "2024-09-12T16:22:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    22,
                    52,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-21T06:27:45Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    6,
                    27,
                    45,
                    6,
                    203,
                    0
                ],
                "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization"
                },
                "summary": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs."
                },
                "authors": [
                    {
                        "name": "Orson Mengara"
                    }
                ],
                "author_detail": {
                    "name": "Orson Mengara"
                },
                "author": "Orson Mengara",
                "arxiv_comment": "END (will never be modified again!!) :Jumps-Diffusion and stock\n  market: Better quantify uncertainty in financial simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14573v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14573v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08185v1",
                "updated": "2024-09-12T16:20:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T16:20:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Entity Matching"
                },
                "summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini."
                },
                "authors": [
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10551v3",
                "updated": "2024-09-12T15:43:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    59,
                    3,
                    256,
                    0
                ],
                "published": "2023-11-17T14:34:45Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    14,
                    34,
                    45,
                    4,
                    321,
                    0
                ],
                "title": "A Tutorial on 5G Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tutorial on 5G Positioning"
                },
                "summary": "The widespread adoption of the fifth generation (5G) of cellular networks has\nbrought new opportunities for the development of localization-based services.\nHigh-accuracy positioning use cases and functionalities defined by the\nstandards are drawing the interest of vertical industries. In the transition\ntowards the deployment, this paper aims to provide an in-depth tutorial on 5G\npositioning, summarizing the evolutionary path that led to the standardization\nof cellular-based positioning, describing the localization elements in current\nand forthcoming releases of the Third Generation Partnership Project (3GPP)\nstandard, and the major research trends. By providing fundamental notions on\nwireless localization, comprehensive definitions of measurements and\narchitectures, examples of algorithms, and details on simulation approaches,\nthis paper is intended to represent an exhaustive guide for researchers and\npractitioners. Our approach aims to merge practical aspects of enabled use\ncases and related requirements with theoretical methodologies and fundamental\nbounds, allowing to understand the trade-off between system complexity and\nachievable, i.e., tangible, benefits of 5G positioning services. We analyze the\nperformance of 3GPP Rel-16 positioning by standard-compliant simulations in\nrealistic outdoor and indoor propagation environments, investigating the impact\nof the system configuration and the limitations to be resolved for delivering\naccurate positioning solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of the fifth generation (5G) of cellular networks has\nbrought new opportunities for the development of localization-based services.\nHigh-accuracy positioning use cases and functionalities defined by the\nstandards are drawing the interest of vertical industries. In the transition\ntowards the deployment, this paper aims to provide an in-depth tutorial on 5G\npositioning, summarizing the evolutionary path that led to the standardization\nof cellular-based positioning, describing the localization elements in current\nand forthcoming releases of the Third Generation Partnership Project (3GPP)\nstandard, and the major research trends. By providing fundamental notions on\nwireless localization, comprehensive definitions of measurements and\narchitectures, examples of algorithms, and details on simulation approaches,\nthis paper is intended to represent an exhaustive guide for researchers and\npractitioners. Our approach aims to merge practical aspects of enabled use\ncases and related requirements with theoretical methodologies and fundamental\nbounds, allowing to understand the trade-off between system complexity and\nachievable, i.e., tangible, benefits of 5G positioning services. We analyze the\nperformance of 3GPP Rel-16 positioning by standard-compliant simulations in\nrealistic outdoor and indoor propagation environments, investigating the impact\nof the system configuration and the limitations to be resolved for delivering\naccurate positioning solutions."
                },
                "authors": [
                    {
                        "name": "Lorenzo Italiano"
                    },
                    {
                        "name": "Bernardo Camajori Tedeschini"
                    },
                    {
                        "name": "Mattia Brambilla"
                    },
                    {
                        "name": "Huiping Huang"
                    },
                    {
                        "name": "Monica Nicoli"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_doi": "10.1109/COMST.2024.3449031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COMST.2024.3449031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted in IEEE Communications Surveys &\n  Tutorials. Copyright may be transferred without notice, after which this\n  version may no longer be accessible",
                "arxiv_journal_ref": "IEEE Communications Surveys & Tutorials 2024",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08148v1",
                "updated": "2024-09-12T15:43:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:43:10Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "title": "Faster Speech-LLaMA Inference with Multi-token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Speech-LLaMA Inference with Multi-token Prediction"
                },
                "summary": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance."
                },
                "authors": [
                    {
                        "name": "Desh Raj"
                    },
                    {
                        "name": "Gil Keren"
                    },
                    {
                        "name": "Junteng Jia"
                    },
                    {
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "name": "Ozlem Kalinli"
                    }
                ],
                "author_detail": {
                    "name": "Ozlem Kalinli"
                },
                "author": "Ozlem Kalinli",
                "arxiv_comment": "Submitted to IEEE ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08147v1",
                "updated": "2024-09-12T15:40:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:40:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models"
                },
                "summary": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation."
                },
                "authors": [
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Oleksandra Zolotarevych"
                    },
                    {
                        "name": "Rongwei Yang"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07409v2",
                "updated": "2024-09-12T15:35:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    35,
                    49,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-11T16:50:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    50,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps"
                },
                "summary": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/"
                },
                "authors": [
                    {
                        "name": "Shaoting Zhu"
                    },
                    {
                        "name": "Runhan Huang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v2",
                "updated": "2024-09-12T15:04:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    4,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v3",
                "updated": "2024-09-12T14:56:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    56,
                    35,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08098v1",
                "updated": "2024-09-12T14:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:51:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal"
                },
                "summary": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution."
                },
                "authors": [
                    {
                        "name": "Huiyuan Xie"
                    },
                    {
                        "name": "Felix Steffek"
                    },
                    {
                        "name": "Joana Ribeiro de Faria"
                    },
                    {
                        "name": "Christine Carter"
                    },
                    {
                        "name": "Jonathan Rutherford"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Rutherford"
                },
                "author": "Jonathan Rutherford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08087v1",
                "updated": "2024-09-12T14:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:42:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field."
                },
                "authors": [
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Qian Niu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Niu"
                },
                "author": "Qian Niu",
                "arxiv_comment": "17 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08078v1",
                "updated": "2024-09-12T14:31:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    31,
                    2,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:31:02Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    31,
                    2,
                    3,
                    256,
                    0
                ],
                "title": "MosquitoMiner: A Light Weight Rover for Detecting and Eliminating\n  Mosquito Breeding Sites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MosquitoMiner: A Light Weight Rover for Detecting and Eliminating\n  Mosquito Breeding Sites"
                },
                "summary": "In this paper, we present a novel approach to the development and deployment\nof an autonomous mosquito breeding place detector rover with the object and\nobstacle detection capabilities to control mosquitoes. Mosquito-borne diseases\ncontinue to pose significant health threats globally, with conventional control\nmethods proving slow and inefficient. Amidst rising concerns over the rapid\nspread of these diseases, there is an urgent need for innovative and efficient\nstrategies to manage mosquito populations and prevent disease transmission. To\nmitigate the limitations of manual labor and traditional methods, our rover\nemploys autonomous control strategies. Leveraging our own custom dataset, the\nrover can autonomously navigate along a pre-defined path, identifying and\nmitigating potential breeding grounds with precision. It then proceeds to\neliminate these breeding grounds by spraying a chemical agent, effectively\neradicating mosquito habitats. Our project demonstrates the effectiveness that\nis absent in traditional ways of controlling and safeguarding public health.\nThe code for this project is available on GitHub at -\nhttps://github.com/faiyazabdullah/MosquitoMiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel approach to the development and deployment\nof an autonomous mosquito breeding place detector rover with the object and\nobstacle detection capabilities to control mosquitoes. Mosquito-borne diseases\ncontinue to pose significant health threats globally, with conventional control\nmethods proving slow and inefficient. Amidst rising concerns over the rapid\nspread of these diseases, there is an urgent need for innovative and efficient\nstrategies to manage mosquito populations and prevent disease transmission. To\nmitigate the limitations of manual labor and traditional methods, our rover\nemploys autonomous control strategies. Leveraging our own custom dataset, the\nrover can autonomously navigate along a pre-defined path, identifying and\nmitigating potential breeding grounds with precision. It then proceeds to\neliminate these breeding grounds by spraying a chemical agent, effectively\neradicating mosquito habitats. Our project demonstrates the effectiveness that\nis absent in traditional ways of controlling and safeguarding public health.\nThe code for this project is available on GitHub at -\nhttps://github.com/faiyazabdullah/MosquitoMiner"
                },
                "authors": [
                    {
                        "name": "Md. Adnanul Islam"
                    },
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Jannatul Ferdous Deepti"
                    },
                    {
                        "name": "Shahanur Rahman Bappy"
                    },
                    {
                        "name": "Safrin Sanzida Islam"
                    },
                    {
                        "name": "Fahim Hafiz"
                    }
                ],
                "author_detail": {
                    "name": "Fahim Hafiz"
                },
                "author": "Fahim Hafiz",
                "arxiv_comment": "Accepted - 2024 IEEE Region 10 Symposium (TENSYMP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08069v1",
                "updated": "2024-09-12T14:24:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:24:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelAgent: An AI Assistant for Personalized Travel Planning"
                },
                "summary": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Xuyang Ge"
                    },
                    {
                        "name": "Ziquan Fu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Jiangjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiangjie Chen"
                },
                "author": "Jiangjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03325v2",
                "updated": "2024-09-12T14:18:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    18,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-04T09:52:22Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    9,
                    52,
                    22,
                    3,
                    95,
                    0
                ],
                "title": "Embodied Neuromorphic Artificial Intelligence for Robotics:\n  Perspectives, Challenges, and Research Development Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Neuromorphic Artificial Intelligence for Robotics:\n  Perspectives, Challenges, and Research Development Stack"
                },
                "summary": "Robotic technologies have been an indispensable part for improving human\nproductivity since they have been helping humans in completing diverse,\ncomplex, and intensive tasks in a fast yet accurate and efficient way.\nTherefore, robotic technologies have been deployed in a wide range of\napplications, ranging from personal to industrial use-cases. However, current\nrobotic technologies and their computing paradigm still lack embodied\nintelligence to efficiently interact with operational environments, respond\nwith correct/expected actions, and adapt to changes in the environments. Toward\nthis, recent advances in neuromorphic computing with Spiking Neural Networks\n(SNN) have demonstrated the potential to enable the embodied intelligence for\nrobotics through bio-plausible computing paradigm that mimics how the\nbiological brain works, known as \"neuromorphic artificial intelligence (AI)\".\nHowever, the field of neuromorphic AI-based robotics is still at an early\nstage, therefore its development and deployment for solving real-world problems\nexpose new challenges in different design aspects, such as accuracy,\nadaptability, efficiency, reliability, and security. To address these\nchallenges, this paper will discuss how we can enable embodied neuromorphic AI\nfor robotic systems through our perspectives: (P1) Embodied intelligence based\non effective learning rule, training mechanism, and adaptability; (P2)\nCross-layer optimizations for energy-efficient neuromorphic computing; (P3)\nRepresentative and fair benchmarks; (P4) Low-cost reliability and safety\nenhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A\nsynergistic development for energy-efficient and robust neuromorphic-based\nrobotics. Furthermore, this paper identifies research challenges and\nopportunities, as well as elaborates our vision for future research development\ntoward embodied neuromorphic AI for robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic technologies have been an indispensable part for improving human\nproductivity since they have been helping humans in completing diverse,\ncomplex, and intensive tasks in a fast yet accurate and efficient way.\nTherefore, robotic technologies have been deployed in a wide range of\napplications, ranging from personal to industrial use-cases. However, current\nrobotic technologies and their computing paradigm still lack embodied\nintelligence to efficiently interact with operational environments, respond\nwith correct/expected actions, and adapt to changes in the environments. Toward\nthis, recent advances in neuromorphic computing with Spiking Neural Networks\n(SNN) have demonstrated the potential to enable the embodied intelligence for\nrobotics through bio-plausible computing paradigm that mimics how the\nbiological brain works, known as \"neuromorphic artificial intelligence (AI)\".\nHowever, the field of neuromorphic AI-based robotics is still at an early\nstage, therefore its development and deployment for solving real-world problems\nexpose new challenges in different design aspects, such as accuracy,\nadaptability, efficiency, reliability, and security. To address these\nchallenges, this paper will discuss how we can enable embodied neuromorphic AI\nfor robotic systems through our perspectives: (P1) Embodied intelligence based\non effective learning rule, training mechanism, and adaptability; (P2)\nCross-layer optimizations for energy-efficient neuromorphic computing; (P3)\nRepresentative and fair benchmarks; (P4) Low-cost reliability and safety\nenhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A\nsynergistic development for energy-efficient and robust neuromorphic-based\nrobotics. Furthermore, this paper identifies research challenges and\nopportunities, as well as elaborates our vision for future research development\ntoward embodied neuromorphic AI for robotics."
                },
                "authors": [
                    {
                        "name": "Rachmad Vidya Wicaksana Putra"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Fakhreddine Zayer"
                    },
                    {
                        "name": "Jorge Dias"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08043v1",
                "updated": "2024-09-12T13:47:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    47,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:47:18Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    47,
                    18,
                    3,
                    256,
                    0
                ],
                "title": "External Memories of PDP Switches for In-Network Implementable Functions\n  Placement: Deep Learning Based Reconfiguration of SFCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Memories of PDP Switches for In-Network Implementable Functions\n  Placement: Deep Learning Based Reconfiguration of SFCs"
                },
                "summary": "Network function virtualization leverages programmable data plane switches to\ndeploy in-network implementable functions, to improve QoS. The memories of\nswitches can be extended through remote direct memory access to access external\nmemories. This paper exploits the switches external memories to place VNFs at\ntime intervals with ultra-low latency and high bandwidth demands. The\nreconfiguration decision is modeled as an optimization to minimize the\ndeployment and reconfiguration cost, while meeting the SFCs deadlines. A DRL\nbased method is proposed to reconfigure service chains adoptable with dynamic\nnetwork and traffic characteristics. To deal with slow convergence due to the\ncomplexity of deployment scenarios, static and dynamic filters are used in\npolicy networks construction to diminish unfeasible placement exploration.\nResults illustrate improvement in convergence, acceptance ratio and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network function virtualization leverages programmable data plane switches to\ndeploy in-network implementable functions, to improve QoS. The memories of\nswitches can be extended through remote direct memory access to access external\nmemories. This paper exploits the switches external memories to place VNFs at\ntime intervals with ultra-low latency and high bandwidth demands. The\nreconfiguration decision is modeled as an optimization to minimize the\ndeployment and reconfiguration cost, while meeting the SFCs deadlines. A DRL\nbased method is proposed to reconfigure service chains adoptable with dynamic\nnetwork and traffic characteristics. To deal with slow convergence due to the\ncomplexity of deployment scenarios, static and dynamic filters are used in\npolicy networks construction to diminish unfeasible placement exploration.\nResults illustrate improvement in convergence, acceptance ratio and cost."
                },
                "authors": [
                    {
                        "name": "Somayeh Kianpisheh"
                    },
                    {
                        "name": "Tarik Taleb"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Taleb"
                },
                "author": "Tarik Taleb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08038v1",
                "updated": "2024-09-12T13:40:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    40,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:40:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    40,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Towards Scalable Quantum Key Distribution: A Machine Learning-Based\n  Cascade Protocol Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Quantum Key Distribution: A Machine Learning-Based\n  Cascade Protocol Approach"
                },
                "summary": "Quantum Key Distribution (QKD) is a pivotal technology in the quest for\nsecure communication, harnessing the power of quantum mechanics to ensure\nrobust data protection. However, scaling QKD to meet the demands of high-speed,\nreal-world applications remains a significant challenge. Traditional key rate\ndetermination methods, dependent on complex mathematical models, often fall\nshort in efficiency and scalability. In this paper, we propose an approach that\ninvolves integrating machine learning (ML) techniques with the Cascade error\ncorrection protocol to enhance the scalability and efficiency of QKD systems.\nOur ML-based approach utilizes an autoencoder framework to predict the Quantum\nBit Error Rate (QBER) and final key length with over 99\\% accuracy. This method\nsignificantly reduces error correction time, maintaining a consistently low\ncomputation time even with large input sizes, such as data rates up to 156\nMbps. In contrast, traditional methods exhibit exponentially increasing\ncomputation times as input sizes grow, highlighting the superior scalability of\nour ML-based solution. Through comprehensive simulations, we demonstrate that\nour method not only accelerates the error correction process but also optimizes\nresource utilization, making it more cost-effective and practical for\nreal-world deployment. The Cascade protocol's integration further enhances\nsystem security by dynamically adjusting error correction based on real-time\nQBER observations, providing robust protection against potential eavesdropping.\n  Our research establishes a new benchmark for scalable, high-throughput QKD\nsystems, proving that machine learning can significantly advance the field of\nquantum cryptography. This work continues the evolution towards truly scalable\nquantum communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) is a pivotal technology in the quest for\nsecure communication, harnessing the power of quantum mechanics to ensure\nrobust data protection. However, scaling QKD to meet the demands of high-speed,\nreal-world applications remains a significant challenge. Traditional key rate\ndetermination methods, dependent on complex mathematical models, often fall\nshort in efficiency and scalability. In this paper, we propose an approach that\ninvolves integrating machine learning (ML) techniques with the Cascade error\ncorrection protocol to enhance the scalability and efficiency of QKD systems.\nOur ML-based approach utilizes an autoencoder framework to predict the Quantum\nBit Error Rate (QBER) and final key length with over 99\\% accuracy. This method\nsignificantly reduces error correction time, maintaining a consistently low\ncomputation time even with large input sizes, such as data rates up to 156\nMbps. In contrast, traditional methods exhibit exponentially increasing\ncomputation times as input sizes grow, highlighting the superior scalability of\nour ML-based solution. Through comprehensive simulations, we demonstrate that\nour method not only accelerates the error correction process but also optimizes\nresource utilization, making it more cost-effective and practical for\nreal-world deployment. The Cascade protocol's integration further enhances\nsystem security by dynamically adjusting error correction based on real-time\nQBER observations, providing robust protection against potential eavesdropping.\n  Our research establishes a new benchmark for scalable, high-throughput QKD\nsystems, proving that machine learning can significantly advance the field of\nquantum cryptography. This work continues the evolution towards truly scalable\nquantum communication."
                },
                "authors": [
                    {
                        "name": "Hasan Abbas Al-Mohammed"
                    },
                    {
                        "name": "Saif Al-Kuwari"
                    },
                    {
                        "name": "Hashir Kuniyil"
                    },
                    {
                        "name": "Ahmed Farouk"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Farouk"
                },
                "author": "Ahmed Farouk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08027v1",
                "updated": "2024-09-12T13:18:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback"
                },
                "summary": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Bhargav Srinivasa Desikan"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08014v1",
                "updated": "2024-09-12T12:57:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:57:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models"
                },
                "summary": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers."
                },
                "authors": [
                    {
                        "name": "Hanane Djeddal"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Raouf Toukal"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Karen Pinel-Sauvagnat"
                    },
                    {
                        "name": "Sophia Katrenko"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "author": "Lynda Tamine",
                "arxiv_doi": "10.1145/3627673.3679172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07985v1",
                "updated": "2024-09-12T12:30:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:30:07Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    7,
                    3,
                    256,
                    0
                ],
                "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment\n  Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Games for AI Control: Models of Safety Evaluations of AI Deployment\n  Protocols"
                },
                "summary": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols."
                },
                "authors": [
                    {
                        "name": "Charlie Griffin"
                    },
                    {
                        "name": "Louis Thomson"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Alessandro Abate"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Abate"
                },
                "author": "Alessandro Abate",
                "arxiv_comment": "7 pages, with appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11386v2",
                "updated": "2024-09-12T12:08:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    8,
                    4,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-21T07:30:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management"
                },
                "summary": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not."
                },
                "authors": [
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Stephan A. Fahrenkrog-Petersen"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Luise Pufahl"
                    }
                ],
                "author_detail": {
                    "name": "Luise Pufahl"
                },
                "author": "Luise Pufahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05639v2",
                "updated": "2024-09-12T12:00:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    0,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-09T04:42:19Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    4,
                    42,
                    19,
                    6,
                    161,
                    0
                ],
                "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair"
                },
                "summary": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources."
                },
                "authors": [
                    {
                        "name": "Guochang Li"
                    },
                    {
                        "name": "Chen Zhi"
                    },
                    {
                        "name": "Jialiang Chen"
                    },
                    {
                        "name": "Junxiao Han"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03302v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03302v4",
                "updated": "2024-09-12T11:51:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    51,
                    51,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-04T08:52:30Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    8,
                    52,
                    30,
                    3,
                    95,
                    0
                ],
                "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?"
                },
                "summary": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."
                },
                "authors": [
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03302v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03302v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]