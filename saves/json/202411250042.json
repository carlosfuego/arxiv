[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v1",
                "updated": "2024-11-18T11:12:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "JoÃ£o Monteiro"
                    },
                    {
                        "name": "Ãtienne Marcotte"
                    },
                    {
                        "name": "Pierre-AndrÃ© NoÃ«l"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David VÃ¡zquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.14432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14432v1",
                "updated": "2024-11-21T18:59:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:59:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks."
                },
                "authors": [
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14423v1",
                "updated": "2024-11-21T18:55:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    55,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    55,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "Unleashing the Potential of Multi-modal Foundation Models and Video\n  Diffusion for 4D Dynamic Physical Scene Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of Multi-modal Foundation Models and Video\n  Diffusion for 4D Dynamic Physical Scene Simulation"
                },
                "summary": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce a novel approach\nthat leverages multi-modal foundation models and video diffusion to achieve\nenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to\nidentify material types and initialize material parameters through image\nqueries, while simultaneously inferring 3D Gaussian splats for detailed scene\nrepresentation. We further refine these material parameters using video\ndiffusion with a differentiable Material Point Method (MPM) and optical flow\nguidance rather than render loss or Score Distillation Sampling (SDS) loss.\nThis integrated framework enables accurate prediction and realistic simulation\nof dynamic interactions in real-world scenarios, advancing both accuracy and\nflexibility in physics-based simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce a novel approach\nthat leverages multi-modal foundation models and video diffusion to achieve\nenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to\nidentify material types and initialize material parameters through image\nqueries, while simultaneously inferring 3D Gaussian splats for detailed scene\nrepresentation. We further refine these material parameters using video\ndiffusion with a differentiable Material Point Method (MPM) and optical flow\nguidance rather than render loss or Score Distillation Sampling (SDS) loss.\nThis integrated framework enables accurate prediction and realistic simulation\nof dynamic interactions in real-world scenarios, advancing both accuracy and\nflexibility in physics-based simulations."
                },
                "authors": [
                    {
                        "name": "Zhuoman Liu"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Yan Luximon"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "arxiv_comment": "Homepage: https://zhuomanliu.github.io/PhysFlow/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00754v2",
                "updated": "2024-11-21T18:52:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    52,
                    31,
                    3,
                    326,
                    0
                ],
                "published": "2024-08-01T17:57:12Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    57,
                    12,
                    3,
                    214,
                    0
                ],
                "title": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model"
                },
                "summary": "Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning."
                },
                "authors": [
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Yiqin Wang"
                    },
                    {
                        "name": "Zixian Ma"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Luming Tang"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Wei-Chiu Ma"
                    },
                    {
                        "name": "Ranjay Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Ranjay Krishna"
                },
                "author": "Ranjay Krishna",
                "arxiv_comment": "project page: https://coarse-correspondence.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14419v1",
                "updated": "2024-11-21T18:52:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    52,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:52:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    52,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Combining summary statistics with simulation-based inference for the 21\n  cm signal from the Epoch of Reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining summary statistics with simulation-based inference for the 21\n  cm signal from the Epoch of Reionization"
                },
                "summary": "The 21 cm signal from the Epoch of Reionization will be observed with the\nup-coming Square Kilometer Array (SKA). SKA should yield a full tomography of\nthe signal which opens the possibility to explore its non-Gaussian properties.\nHow can we extract the maximum information from the tomography and derive the\ntightest constraint on the signal? In this work, instead of looking for the\nmost informative summary statistics, we investigate how to combine the\ninformation from two sets of summary statistics using simulation-based\ninference. To this purpose, we train Neural Density Estimators (NDE) to fit the\nimplicit likelihood of our model, the LICORICE code, using the Loreli II\ndatabase. We train three different NDEs: one to perform Bayesian inference on\nthe power spectrum, one to do it on the linear moments of the Pixel\nDistribution Function (PDF) and one to work with the combination of the two. We\nperform $\\sim 900$ inferences at different points in our parameter space and\nuse them to assess both the validity of our posteriors with Simulation-based\nCalibration (SBC) and the typical gain obtained by combining summary\nstatistics. We find that our posteriors are biased by no more than $\\sim 20 \\%$\nof their standard deviation and under-confident by no more than $\\sim 15 \\%$.\nThen, we establish that combining summary statistics produces a contraction of\nthe 4-D volume of the posterior (derived from the generalized variance) in 91.5\n% of our cases, and in 70 to 80 % of the cases for the marginalized 1-D\nposteriors. The median volume variation is a contraction of a factor of a few\nfor the 4D posteriors and a contraction of 20 to 30 % in the case of the\nmarginalized 1D posteriors. This shows that our approach is a possible\nalternative to looking for sufficient statistics in the theoretical sense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21 cm signal from the Epoch of Reionization will be observed with the\nup-coming Square Kilometer Array (SKA). SKA should yield a full tomography of\nthe signal which opens the possibility to explore its non-Gaussian properties.\nHow can we extract the maximum information from the tomography and derive the\ntightest constraint on the signal? In this work, instead of looking for the\nmost informative summary statistics, we investigate how to combine the\ninformation from two sets of summary statistics using simulation-based\ninference. To this purpose, we train Neural Density Estimators (NDE) to fit the\nimplicit likelihood of our model, the LICORICE code, using the Loreli II\ndatabase. We train three different NDEs: one to perform Bayesian inference on\nthe power spectrum, one to do it on the linear moments of the Pixel\nDistribution Function (PDF) and one to work with the combination of the two. We\nperform $\\sim 900$ inferences at different points in our parameter space and\nuse them to assess both the validity of our posteriors with Simulation-based\nCalibration (SBC) and the typical gain obtained by combining summary\nstatistics. We find that our posteriors are biased by no more than $\\sim 20 \\%$\nof their standard deviation and under-confident by no more than $\\sim 15 \\%$.\nThen, we establish that combining summary statistics produces a contraction of\nthe 4-D volume of the posterior (derived from the generalized variance) in 91.5\n% of our cases, and in 70 to 80 % of the cases for the marginalized 1-D\nposteriors. The median volume variation is a contraction of a factor of a few\nfor the 4D posteriors and a contraction of 20 to 30 % in the case of the\nmarginalized 1D posteriors. This shows that our approach is a possible\nalternative to looking for sufficient statistics in the theoretical sense."
                },
                "authors": [
                    {
                        "name": "Benoit Semelin"
                    },
                    {
                        "name": "Romain MÃ©riot"
                    },
                    {
                        "name": "Ashutosh Mishra"
                    },
                    {
                        "name": "David Cornu"
                    }
                ],
                "author_detail": {
                    "name": "David Cornu"
                },
                "author": "David Cornu",
                "arxiv_comment": "11 pages, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14398v1",
                "updated": "2024-11-21T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings"
                },
                "summary": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark."
                },
                "authors": [
                    {
                        "name": "Aaron Zheng"
                    },
                    {
                        "name": "Mansi Rana"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "To appear in Proceedings of COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14392v1",
                "updated": "2024-11-21T18:25:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    25,
                    18,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:25:18Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    25,
                    18,
                    3,
                    326,
                    0
                ],
                "title": "Convolutional Vision Transformer for Cosmology Parameter Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Vision Transformer for Cosmology Parameter Inference"
                },
                "summary": "Parameter inference is a crucial task in modern cosmology that requires\naccurate and fast computational methods to handle the high precision and volume\nof observational datasets. In this study, we explore a hybrid vision\ntransformer, the Convolution vision Transformer (CvT), which combines the\nbenefits of vision transformers and convolutional neural networks. We use this\napproach to infer the $\\Omega_m$ and $\\sigma_8$ cosmological parameters from\nsimulated dark matter and halo fields. Our experiments indicate that the\nconstraints on $\\Omega_m$ and $\\sigma_8$ obtained using CvT are better than the\ntraditional vision transformer (ViT) and CNN, using either dark matter or halo\nfields. For CvT, pretraining on dark matter fields proves advantageous for\nimproving constraints using halo fields compared to training a model from the\nbeginning. However, ViT and CNN do not show these benefits. The CvT is more\nefficient than ViT since, despite having more parameters, it requires a\ntraining time similar to that of ViT and has similar inference times. The code\nis available at \\url{https://github.com/Yash-10/cvt-cosmo-inference/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter inference is a crucial task in modern cosmology that requires\naccurate and fast computational methods to handle the high precision and volume\nof observational datasets. In this study, we explore a hybrid vision\ntransformer, the Convolution vision Transformer (CvT), which combines the\nbenefits of vision transformers and convolutional neural networks. We use this\napproach to infer the $\\Omega_m$ and $\\sigma_8$ cosmological parameters from\nsimulated dark matter and halo fields. Our experiments indicate that the\nconstraints on $\\Omega_m$ and $\\sigma_8$ obtained using CvT are better than the\ntraditional vision transformer (ViT) and CNN, using either dark matter or halo\nfields. For CvT, pretraining on dark matter fields proves advantageous for\nimproving constraints using halo fields compared to training a model from the\nbeginning. However, ViT and CNN do not show these benefits. The CvT is more\nefficient than ViT since, despite having more parameters, it requires a\ntraining time similar to that of ViT and has similar inference times. The code\nis available at \\url{https://github.com/Yash-10/cvt-cosmo-inference/}."
                },
                "authors": [
                    {
                        "name": "Yash Gondhalekar"
                    },
                    {
                        "name": "Kana Moriwaki"
                    }
                ],
                "author_detail": {
                    "name": "Kana Moriwaki"
                },
                "author": "Kana Moriwaki",
                "arxiv_comment": "Accepted at the NeurIPS ML4PS Workshop 2024. The code is available at\n  https://github.com/Yash-10/cvt-cosmo-inference/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14386v1",
                "updated": "2024-11-21T18:21:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Learning Humanoid Locomotion with Perceptive Internal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Humanoid Locomotion with Perceptive Internal Model"
                },
                "summary": "In contrast to quadruped robots that can navigate diverse terrains using a\n\"blind\" policy, humanoid robots require accurate perception for stable\nlocomotion due to their high degrees of freedom and inherently unstable\nmorphology. However, incorporating perceptual signals often introduces\nadditional disturbances to the system, potentially reducing its robustness,\ngeneralizability, and efficiency. This paper presents the Perceptive Internal\nModel (PIM), which relies on onboard, continuously updated elevation maps\ncentered around the robot to perceive its surroundings. We train the policy\nusing ground-truth obstacle heights surrounding the robot in simulation,\noptimizing it based on the Hybrid Internal Model (HIM), and perform inference\nwith heights sampled from the constructed elevation map. Unlike previous\nmethods that directly encode depth maps or raw point clouds, our approach\nallows the robot to perceive the terrain beneath its feet clearly and is less\naffected by camera movement or noise. Furthermore, since depth map rendering is\nnot required in simulation, our method introduces minimal additional\ncomputational costs and can train the policy in 3 hours on an RTX 4090 GPU. We\nverify the effectiveness of our method across various humanoid robots, various\nindoor and outdoor terrains, stairs, and various sensor configurations. Our\nmethod can enable a humanoid robot to continuously climb stairs and has the\npotential to serve as a foundational algorithm for the development of future\nhumanoid control methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to quadruped robots that can navigate diverse terrains using a\n\"blind\" policy, humanoid robots require accurate perception for stable\nlocomotion due to their high degrees of freedom and inherently unstable\nmorphology. However, incorporating perceptual signals often introduces\nadditional disturbances to the system, potentially reducing its robustness,\ngeneralizability, and efficiency. This paper presents the Perceptive Internal\nModel (PIM), which relies on onboard, continuously updated elevation maps\ncentered around the robot to perceive its surroundings. We train the policy\nusing ground-truth obstacle heights surrounding the robot in simulation,\noptimizing it based on the Hybrid Internal Model (HIM), and perform inference\nwith heights sampled from the constructed elevation map. Unlike previous\nmethods that directly encode depth maps or raw point clouds, our approach\nallows the robot to perceive the terrain beneath its feet clearly and is less\naffected by camera movement or noise. Furthermore, since depth map rendering is\nnot required in simulation, our method introduces minimal additional\ncomputational costs and can train the policy in 3 hours on an RTX 4090 GPU. We\nverify the effectiveness of our method across various humanoid robots, various\nindoor and outdoor terrains, stairs, and various sensor configurations. Our\nmethod can enable a humanoid robot to continuously climb stairs and has the\npotential to serve as a foundational algorithm for the development of future\nhumanoid control methods."
                },
                "authors": [
                    {
                        "name": "Junfeng Long"
                    },
                    {
                        "name": "Junli Ren"
                    },
                    {
                        "name": "Moji Shi"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "submitted to ICRA2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10126v2",
                "updated": "2024-11-21T18:19:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    19,
                    21,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-15T17:28:52Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    17,
                    28,
                    52,
                    3,
                    46,
                    0
                ],
                "title": "Exchangeability, prediction and predictive modeling in Bayesian\n  statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exchangeability, prediction and predictive modeling in Bayesian\n  statistics"
                },
                "summary": "There is currently a renewed interest in the Bayesian predictive approach to\nstatistics. This paper offers a review on foundational concepts and focuses on\npredictive modeling, which by directly reasoning on prediction, bypasses\ninferential models or may characterize them. We detail predictive\ncharacterizations in exchangeable and partially exchangeable settings, for a\nlarge variety of data structures, and hint at new directions. The underlying\nconcept is that Bayesian predictive rules are probabilistic learning rules,\nformalizing through conditional probability how we learn on future events given\nthe available information. This concept has implications in any statistical\nproblem and in inference, from classic contexts to less explored challenges,\nsuch as providing Bayesian uncertainty quantification to predictive algorithms\nin data science, as we show in the last part of the paper. The paper gives a\nhistorical overview, but also includes a few new results, presents some recent\ndevelopments and poses some open questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is currently a renewed interest in the Bayesian predictive approach to\nstatistics. This paper offers a review on foundational concepts and focuses on\npredictive modeling, which by directly reasoning on prediction, bypasses\ninferential models or may characterize them. We detail predictive\ncharacterizations in exchangeable and partially exchangeable settings, for a\nlarge variety of data structures, and hint at new directions. The underlying\nconcept is that Bayesian predictive rules are probabilistic learning rules,\nformalizing through conditional probability how we learn on future events given\nthe available information. This concept has implications in any statistical\nproblem and in inference, from classic contexts to less explored challenges,\nsuch as providing Bayesian uncertainty quantification to predictive algorithms\nin data science, as we show in the last part of the paper. The paper gives a\nhistorical overview, but also includes a few new results, presents some recent\ndevelopments and poses some open questions."
                },
                "authors": [
                    {
                        "name": "Sandra Fortini"
                    },
                    {
                        "name": "Sonia Petrone"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Petrone"
                },
                "author": "Sonia Petrone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10079v3",
                "updated": "2024-11-21T17:58:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    58,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-14T14:35:58Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    35,
                    58,
                    4,
                    166,
                    0
                ],
                "title": "Localizing Events in Videos with Multimodal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Events in Videos with Multimodal Queries"
                },
                "summary": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization."
                },
                "authors": [
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Mang Ling Ada Fok"
                    },
                    {
                        "name": "Jialu Ma"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Daniel Cremers"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Jindong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Gu"
                },
                "author": "Jindong Gu",
                "arxiv_comment": "20 pages (including references and appendix); for the project\n  homepage, see https://icq-benchmark.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14512v2",
                "updated": "2024-11-21T17:48:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    48,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-08-25T04:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    32,
                    45,
                    6,
                    238,
                    0
                ],
                "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings"
                },
                "summary": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
                },
                "authors": [
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Fengzhi Li"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14351v1",
                "updated": "2024-11-21T17:46:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    46,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:46:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    46,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Indiscriminate Disruption of Conditional Inference on Multivariate\n  Gaussians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indiscriminate Disruption of Conditional Inference on Multivariate\n  Gaussians"
                },
                "summary": "The multivariate Gaussian distribution underpins myriad operations-research,\ndecision-analytic, and machine-learning models (e.g., Bayesian optimization,\nGaussian influence diagrams, and variational autoencoders). However, despite\nrecent advances in adversarial machine learning (AML), inference for Gaussian\nmodels in the presence of an adversary is notably understudied. Therefore, we\nconsider a self-interested attacker who wishes to disrupt a decisionmaker's\nconditional inference and subsequent actions by corrupting a set of evidentiary\nvariables. To avoid detection, the attacker also desires the attack to appear\nplausible wherein plausibility is determined by the density of the corrupted\nevidence. We consider white- and grey-box settings such that the attacker has\ncomplete and incomplete knowledge about the decisionmaker's underlying\nmultivariate Gaussian distribution, respectively. Select instances are shown to\nreduce to quadratic and stochastic quadratic programs, and structural\nproperties are derived to inform solution methods. We assess the impact and\nefficacy of these attacks in three examples, including, real estate evaluation,\ninterest rate estimation and signals processing. Each example leverages an\nalternative underlying model, thereby highlighting the attacks' broad\napplicability. Through these applications, we also juxtapose the behavior of\nthe white- and grey-box attacks to understand how uncertainty and structure\naffect attacker behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multivariate Gaussian distribution underpins myriad operations-research,\ndecision-analytic, and machine-learning models (e.g., Bayesian optimization,\nGaussian influence diagrams, and variational autoencoders). However, despite\nrecent advances in adversarial machine learning (AML), inference for Gaussian\nmodels in the presence of an adversary is notably understudied. Therefore, we\nconsider a self-interested attacker who wishes to disrupt a decisionmaker's\nconditional inference and subsequent actions by corrupting a set of evidentiary\nvariables. To avoid detection, the attacker also desires the attack to appear\nplausible wherein plausibility is determined by the density of the corrupted\nevidence. We consider white- and grey-box settings such that the attacker has\ncomplete and incomplete knowledge about the decisionmaker's underlying\nmultivariate Gaussian distribution, respectively. Select instances are shown to\nreduce to quadratic and stochastic quadratic programs, and structural\nproperties are derived to inform solution methods. We assess the impact and\nefficacy of these attacks in three examples, including, real estate evaluation,\ninterest rate estimation and signals processing. Each example leverages an\nalternative underlying model, thereby highlighting the attacks' broad\napplicability. Through these applications, we also juxtapose the behavior of\nthe white- and grey-box attacks to understand how uncertainty and structure\naffect attacker behavior."
                },
                "authors": [
                    {
                        "name": "William N. Caballero"
                    },
                    {
                        "name": "Matthew LaRosa"
                    },
                    {
                        "name": "Alexander Fisher"
                    },
                    {
                        "name": "Vahid Tarokh"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Tarokh"
                },
                "author": "Vahid Tarokh",
                "arxiv_comment": "30 pages, 6 figures; 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14343v1",
                "updated": "2024-11-21T17:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:41:08Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl."
                },
                "authors": [
                    {
                        "name": "Bethel Melesse Tessema"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Tae-Sun Chung"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Sun Chung"
                },
                "arxiv_affiliation": "Ajou University",
                "author": "Tae-Sun Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14341v1",
                "updated": "2024-11-21T17:38:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    38,
                    49,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:38:49Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    38,
                    49,
                    3,
                    326,
                    0
                ],
                "title": "Logarithmic Neyman Regret for Adaptive Estimation of the Average\n  Treatment Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Neyman Regret for Adaptive Estimation of the Average\n  Treatment Effect"
                },
                "summary": "Estimation of the Average Treatment Effect (ATE) is a core problem in causal\ninference with strong connections to Off-Policy Evaluation in Reinforcement\nLearning. This paper considers the problem of adaptively selecting the\ntreatment allocation probability in order to improve estimation of the ATE. The\nmajority of prior work on adaptive ATE estimation focus on asymptotic\nguarantees, and in turn overlooks important practical considerations such as\nthe difficulty of learning the optimal treatment allocation as well as\nhyper-parameter selection. Existing non-asymptotic methods are limited by poor\nempirical performance and exponential scaling of the Neyman regret with respect\nto problem parameters. In order to address these gaps, we propose and analyze\nthe Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an\nexisting algorithm with strong asymptotic optimality guarantees, and provide\nfinite sample bounds on its Neyman regret. Our analysis shows that ClipSMT\nachieves exponential improvements in Neyman regret on two fronts: improving the\ndependence on $T$ from $O(\\sqrt{T})$ to $O(\\log T)$, as well as reducing the\nexponential dependence on problem parameters to a polynomial dependence.\nFinally, we conclude with simulations which show the marked improvement of\nClipSMT over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of the Average Treatment Effect (ATE) is a core problem in causal\ninference with strong connections to Off-Policy Evaluation in Reinforcement\nLearning. This paper considers the problem of adaptively selecting the\ntreatment allocation probability in order to improve estimation of the ATE. The\nmajority of prior work on adaptive ATE estimation focus on asymptotic\nguarantees, and in turn overlooks important practical considerations such as\nthe difficulty of learning the optimal treatment allocation as well as\nhyper-parameter selection. Existing non-asymptotic methods are limited by poor\nempirical performance and exponential scaling of the Neyman regret with respect\nto problem parameters. In order to address these gaps, we propose and analyze\nthe Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an\nexisting algorithm with strong asymptotic optimality guarantees, and provide\nfinite sample bounds on its Neyman regret. Our analysis shows that ClipSMT\nachieves exponential improvements in Neyman regret on two fronts: improving the\ndependence on $T$ from $O(\\sqrt{T})$ to $O(\\log T)$, as well as reducing the\nexponential dependence on problem parameters to a polynomial dependence.\nFinally, we conclude with simulations which show the marked improvement of\nClipSMT over existing approaches."
                },
                "authors": [
                    {
                        "name": "Ojash Neopane"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    },
                    {
                        "name": "Aarti Singh"
                    }
                ],
                "author_detail": {
                    "name": "Aarti Singh"
                },
                "author": "Aarti Singh",
                "arxiv_comment": "12 pages, 2 figures. Submitted to AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14338v1",
                "updated": "2024-11-21T17:34:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    34,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:34:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    34,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "On the Morphology of Relativistically Broadened Line Emission from\n  Axisymmetric Equatorial Accretion Disks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Morphology of Relativistically Broadened Line Emission from\n  Axisymmetric Equatorial Accretion Disks"
                },
                "summary": "Single-frequency emission from an accretion disk around a black hole is\nbroadened into a line profile due to gravitational redshift and the motion of\nthe disk's particles relative to the observer. The ensemble of relativistically\nbroadened emission frequencies from the disk elements forms the spectrum viewed\nby an observer. Over the past decades, the broadened spectra of accreting\nsystems have been used to constrain the spin of the black hole, the observer's\ninclination, and the astrophysical model parameters of the system. These\ninferences are usually made under the assumption that the accretion disk\nconsists of particles orbiting around the black hole on stable circular orbits\nin the equatorial plane. Under this Standard disk model, in this work, we\nrevisit line profile morphology, i.e., its extent, kinks, and fall-off. We\nprovide a unified analytical explanation for these line profile morphological\nfeatures, which encode the black hole spin, viewing inclination, and locations\nof the disk's inner and outer edges. We then show that these features, however,\nare model-dependent, by parametrically relaxing some of the astrophysical\nassumptions. In particular, we explore how allowing the disk particles to\ndeviate from stable circular orbits rapidly degenerates the characteristic\nfeatures of the line profile under the Standard disk model. Our results further\ndemonstrate how sensitive our understanding of black hole and system properties\ncan be to assumptions we make when interpreting these types of measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-frequency emission from an accretion disk around a black hole is\nbroadened into a line profile due to gravitational redshift and the motion of\nthe disk's particles relative to the observer. The ensemble of relativistically\nbroadened emission frequencies from the disk elements forms the spectrum viewed\nby an observer. Over the past decades, the broadened spectra of accreting\nsystems have been used to constrain the spin of the black hole, the observer's\ninclination, and the astrophysical model parameters of the system. These\ninferences are usually made under the assumption that the accretion disk\nconsists of particles orbiting around the black hole on stable circular orbits\nin the equatorial plane. Under this Standard disk model, in this work, we\nrevisit line profile morphology, i.e., its extent, kinks, and fall-off. We\nprovide a unified analytical explanation for these line profile morphological\nfeatures, which encode the black hole spin, viewing inclination, and locations\nof the disk's inner and outer edges. We then show that these features, however,\nare model-dependent, by parametrically relaxing some of the astrophysical\nassumptions. In particular, we explore how allowing the disk particles to\ndeviate from stable circular orbits rapidly degenerates the characteristic\nfeatures of the line profile under the Standard disk model. Our results further\ndemonstrate how sensitive our understanding of black hole and system properties\ncan be to assumptions we make when interpreting these types of measurements."
                },
                "authors": [
                    {
                        "name": "Delilah E. A. Gates"
                    },
                    {
                        "name": "Chau Truong"
                    },
                    {
                        "name": "Amrita Sahu"
                    },
                    {
                        "name": "Alejandro CÃ¡rdenas-AvendaÃ±o"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro CÃ¡rdenas-AvendaÃ±o"
                },
                "author": "Alejandro CÃ¡rdenas-AvendaÃ±o",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24079v2",
                "updated": "2024-11-21T17:33:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    33,
                    18,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-31T16:16:18Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    16,
                    18,
                    3,
                    305,
                    0
                ],
                "title": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models"
                },
                "summary": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences."
                },
                "authors": [
                    {
                        "name": "Jinlin Lai"
                    },
                    {
                        "name": "Justin Domke"
                    },
                    {
                        "name": "Daniel Sheldon"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sheldon"
                },
                "author": "Daniel Sheldon",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14336v1",
                "updated": "2024-11-21T17:30:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    30,
                    49,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:30:49Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    30,
                    49,
                    3,
                    326,
                    0
                ],
                "title": "Finding the root in random nearest neighbor trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the root in random nearest neighbor trees"
                },
                "summary": "We study the inference of network archaeology in growing random geometric\ngraphs. We consider the root finding problem for a random nearest neighbor tree\nin dimension $d \\in \\mathbb{N}$, generated by sequentially embedding vertices\nuniformly at random in the $d$-dimensional torus and connecting each new vertex\nto the nearest existing vertex. More precisely, given an error parameter\n$\\varepsilon > 0$ and the unlabeled tree, we want to efficiently find a small\nset of candidate vertices, such that the root is included in this set with\nprobability at least $1 - \\varepsilon$. We call such a candidate set a\n$\\textit{confidence set}$. We define several variations of the root finding\nproblem in geometric settings -- embedded, metric, and graph root finding --\nwhich differ based on the nature of the type of metric information provided in\naddition to the graph structure (torus embedding, edge lengths, or no\nadditional information, respectively).\n  We show that there exist efficient root finding algorithms for embedded and\nmetric root finding. For embedded root finding, we derive upper and lower\nbounds (uniformly bounded in $n$) on the size of the confidence set: the upper\nbound is subpolynomial in $1/\\varepsilon$ and stems from an explicit efficient\nalgorithm, and the information-theoretic lower bound is polylogarithmic in\n$1/\\varepsilon$. In particular, in $d=1$, we obtain matching upper and lower\nbounds for a confidence set of size\n$\\Theta\\left(\\frac{\\log(1/\\varepsilon)}{\\log \\log(1/\\varepsilon)} \\right)$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the inference of network archaeology in growing random geometric\ngraphs. We consider the root finding problem for a random nearest neighbor tree\nin dimension $d \\in \\mathbb{N}$, generated by sequentially embedding vertices\nuniformly at random in the $d$-dimensional torus and connecting each new vertex\nto the nearest existing vertex. More precisely, given an error parameter\n$\\varepsilon > 0$ and the unlabeled tree, we want to efficiently find a small\nset of candidate vertices, such that the root is included in this set with\nprobability at least $1 - \\varepsilon$. We call such a candidate set a\n$\\textit{confidence set}$. We define several variations of the root finding\nproblem in geometric settings -- embedded, metric, and graph root finding --\nwhich differ based on the nature of the type of metric information provided in\naddition to the graph structure (torus embedding, edge lengths, or no\nadditional information, respectively).\n  We show that there exist efficient root finding algorithms for embedded and\nmetric root finding. For embedded root finding, we derive upper and lower\nbounds (uniformly bounded in $n$) on the size of the confidence set: the upper\nbound is subpolynomial in $1/\\varepsilon$ and stems from an explicit efficient\nalgorithm, and the information-theoretic lower bound is polylogarithmic in\n$1/\\varepsilon$. In particular, in $d=1$, we obtain matching upper and lower\nbounds for a confidence set of size\n$\\Theta\\left(\\frac{\\log(1/\\varepsilon)}{\\log \\log(1/\\varepsilon)} \\right)$."
                },
                "authors": [
                    {
                        "name": "Anna Brandenberger"
                    },
                    {
                        "name": "Cassandra Marcussen"
                    },
                    {
                        "name": "Elchanan Mossel"
                    },
                    {
                        "name": "Madhu Sudan"
                    }
                ],
                "author_detail": {
                    "name": "Madhu Sudan"
                },
                "author": "Madhu Sudan",
                "arxiv_comment": "22 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14303v1",
                "updated": "2024-11-21T16:56:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:56:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "Automated Generation of Code Debugging Exercises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Generation of Code Debugging Exercises"
                },
                "summary": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
                },
                "authors": [
                    {
                        "name": "Victor-Alexandru PÄdurean"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "Preprint of the SIGCSE'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v1",
                "updated": "2024-11-21T16:50:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams"
                },
                "summary": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13009v2",
                "updated": "2024-11-21T16:49:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    49,
                    51,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T03:17:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts"
                },
                "summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
                },
                "authors": [
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09697v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09697v7",
                "updated": "2024-11-21T16:45:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    45,
                    56,
                    3,
                    326,
                    0
                ],
                "published": "2023-04-19T14:41:14Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    14,
                    41,
                    14,
                    2,
                    109,
                    0
                ],
                "title": "A Calculus for Scoped Effects & Handlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Calculus for Scoped Effects & Handlers"
                },
                "summary": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algebraic effects & handlers have become a standard approach for side-effects\nin functional programming. Their modular composition with other effects and\nclean separation of syntax and semantics make them attractive to a wide\naudience. However, not all effects can be classified as algebraic; some need a\nmore sophisticated handling. In particular, effects that have or create a\ndelimited scope need special care, as their continuation consists of two\nparts-in and out of the scope-and their modular composition introduces\nadditional complexity. These effects are called scoped and have gained\nattention by their growing applicability and adoption in popular libraries.\nWhile calculi have been designed with algebraic effects & handlers built in to\nfacilitate their use, a calculus that supports scoped effects & handlers in a\nsimilar manner does not yet exist. This work fills this gap: we present\n$\\lambda_{\\mathit{sc}}$, a calculus with native support for both algebraic and\nscoped effects & handlers. It addresses the need for polymorphic handlers and\nexplicit clauses for forwarding unknown scoped operations to other handlers.\nOur calculus is based on Eff, an existing calculus for algebraic effects,\nextended with Koka-style row polymorphism, and consists of a formal grammar,\noperational semantics, a (type-safe) type-and-effect system and type inference.\nWe demonstrate $\\lambda_{\\mathit{sc}}$ on a range of examples."
                },
                "authors": [
                    {
                        "name": "Roger Bosman"
                    },
                    {
                        "name": "Birthe van den Berg"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Tom Schrijvers"
                    }
                ],
                "author_detail": {
                    "name": "Tom Schrijvers"
                },
                "author": "Tom Schrijvers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09697v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09697v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v2",
                "updated": "2024-11-21T16:43:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    43,
                    6,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_comment": "9 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.06405v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.06405v3",
                "updated": "2024-11-21T16:42:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    42,
                    56,
                    3,
                    326,
                    0
                ],
                "published": "2023-08-11T22:03:36Z",
                "published_parsed": [
                    2023,
                    8,
                    11,
                    22,
                    3,
                    36,
                    4,
                    223,
                    0
                ],
                "title": "White-box Membership Inference Attacks against Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-box Membership Inference Attacks against Diffusion Models"
                },
                "summary": "Diffusion models have begun to overshadow GANs and other generative models in\nindustrial applications due to their superior image generation performance. The\ncomplex architecture of these models furnishes an extensive array of attack\nfeatures. In light of this, we aim to design membership inference attacks\n(MIAs) catered to diffusion models. We first conduct an exhaustive analysis of\nexisting MIAs on diffusion models, taking into account factors such as\nblack-box/white-box models and the selection of attack features. We found that\nwhite-box attacks are highly applicable in real-world scenarios, and the most\neffective attacks presently are white-box. Departing from earlier research,\nwhich employs model loss as the attack feature for white-box MIAs, we employ\nmodel gradients in our attack, leveraging the fact that these gradients provide\na more profound understanding of model responses to various samples. We subject\nthese models to rigorous testing across a range of parameters, including\ntraining steps, sampling frequency, diffusion steps, and data variance. Across\nall experimental settings, our method consistently demonstrated near-flawless\nattack performance, with attack success rate approaching 100% and attack AUCROC\nnear 1.0. We also evaluate our attack against common defense mechanisms, and\nobserve our attacks continue to exhibit commendable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have begun to overshadow GANs and other generative models in\nindustrial applications due to their superior image generation performance. The\ncomplex architecture of these models furnishes an extensive array of attack\nfeatures. In light of this, we aim to design membership inference attacks\n(MIAs) catered to diffusion models. We first conduct an exhaustive analysis of\nexisting MIAs on diffusion models, taking into account factors such as\nblack-box/white-box models and the selection of attack features. We found that\nwhite-box attacks are highly applicable in real-world scenarios, and the most\neffective attacks presently are white-box. Departing from earlier research,\nwhich employs model loss as the attack feature for white-box MIAs, we employ\nmodel gradients in our attack, leveraging the fact that these gradients provide\na more profound understanding of model responses to various samples. We subject\nthese models to rigorous testing across a range of parameters, including\ntraining steps, sampling frequency, diffusion steps, and data variance. Across\nall experimental settings, our method consistently demonstrated near-flawless\nattack performance, with attack success rate approaching 100% and attack AUCROC\nnear 1.0. We also evaluate our attack against common defense mechanisms, and\nobserve our attacks continue to exhibit commendable performance."
                },
                "authors": [
                    {
                        "name": "Yan Pang"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Xuhui Kang"
                    },
                    {
                        "name": "Mengdi Huai"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.06405v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.06405v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14292v1",
                "updated": "2024-11-21T16:39:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    39,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:39:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    39,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Hypothesis testing of symmetry in quantum dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis testing of symmetry in quantum dynamics"
                },
                "summary": "Symmetry plays a crucial role in quantum physics, dictating the behavior and\ndynamics of physical systems. In this paper, We develop a hypothesis-testing\nframework for quantum dynamics symmetry using a limited number of queries to\nthe unknown unitary operation and establish the quantum max-relative entropy\nlower bound for the type-II error. We construct optimal ancilla-free protocols\nthat achieve optimal type-II error probability for testing time-reversal\nsymmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries.\nContrasting with the advantages of indefinite causal order strategies in\nvarious quantum information processing tasks, we show that parallel, adaptive,\nand indefinite causal order strategies have equal power for our tasks. We\nestablish optimal protocols for T-symmetry testing and Z-symmetry testing for 6\nand 5 queries, respectively, from which we infer that the type-II error\nexhibits a decay rate of $\\mathcal{O}(m^{-2})$ with respect to the number of\nqueries $m$. This represents a significant improvement over the basic\nrepetition protocols without using global entanglement, where the error decays\nat a slower rate of $\\mathcal{O}(m^{-1})$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetry plays a crucial role in quantum physics, dictating the behavior and\ndynamics of physical systems. In this paper, We develop a hypothesis-testing\nframework for quantum dynamics symmetry using a limited number of queries to\nthe unknown unitary operation and establish the quantum max-relative entropy\nlower bound for the type-II error. We construct optimal ancilla-free protocols\nthat achieve optimal type-II error probability for testing time-reversal\nsymmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries.\nContrasting with the advantages of indefinite causal order strategies in\nvarious quantum information processing tasks, we show that parallel, adaptive,\nand indefinite causal order strategies have equal power for our tasks. We\nestablish optimal protocols for T-symmetry testing and Z-symmetry testing for 6\nand 5 queries, respectively, from which we infer that the type-II error\nexhibits a decay rate of $\\mathcal{O}(m^{-2})$ with respect to the number of\nqueries $m$. This represents a significant improvement over the basic\nrepetition protocols without using global entanglement, where the error decays\nat a slower rate of $\\mathcal{O}(m^{-1})$."
                },
                "authors": [
                    {
                        "name": "Yu-Ao Chen"
                    },
                    {
                        "name": "Chenghong Zhu"
                    },
                    {
                        "name": "Keming He"
                    },
                    {
                        "name": "Yingjian Liu"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14285v1",
                "updated": "2024-11-21T16:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    35,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    35,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "Stochastic interventions, sensitivity analysis, and optimal transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic interventions, sensitivity analysis, and optimal transport"
                },
                "summary": "Recent methodological research in causal inference has focused on effects of\nstochastic interventions, which assign treatment randomly, often according to\nsubject-specific covariates. In this work, we demonstrate that the usual notion\nof stochastic interventions have a surprising property: when there is\nunmeasured confounding, bounds on their effects do not collapse when the policy\napproaches the observational regime. As an alternative, we propose to study\ngeneralized policies, treatment rules that can depend on covariates, the\nnatural value of treatment, and auxiliary randomness. We show that certain\ngeneralized policy formulations can resolve the \"non-collapsing\" bound issue:\nbounds narrow to a point when the target treatment distribution approaches that\nin the observed data. Moreover, drawing connections to the theory of optimal\ntransport, we characterize generalized policies that minimize worst-case bound\nwidth in various sensitivity analysis models, as well as corresponding sharp\nbounds on their causal effects. These optimal policies are new, and can have a\nmore parsimonious interpretation compared to their usual stochastic policy\nanalogues. Finally, we develop flexible, efficient, and robust estimators for\nthe sharp nonparametric bounds that emerge from the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methodological research in causal inference has focused on effects of\nstochastic interventions, which assign treatment randomly, often according to\nsubject-specific covariates. In this work, we demonstrate that the usual notion\nof stochastic interventions have a surprising property: when there is\nunmeasured confounding, bounds on their effects do not collapse when the policy\napproaches the observational regime. As an alternative, we propose to study\ngeneralized policies, treatment rules that can depend on covariates, the\nnatural value of treatment, and auxiliary randomness. We show that certain\ngeneralized policy formulations can resolve the \"non-collapsing\" bound issue:\nbounds narrow to a point when the target treatment distribution approaches that\nin the observed data. Moreover, drawing connections to the theory of optimal\ntransport, we characterize generalized policies that minimize worst-case bound\nwidth in various sensitivity analysis models, as well as corresponding sharp\nbounds on their causal effects. These optimal policies are new, and can have a\nmore parsimonious interpretation compared to their usual stochastic policy\nanalogues. Finally, we develop flexible, efficient, and robust estimators for\nthe sharp nonparametric bounds that emerge from the framework."
                },
                "authors": [
                    {
                        "name": "Alexander W. Levis"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Alec McClean"
                    },
                    {
                        "name": "Sivaraman Balakrishnan"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "37 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14283v1",
                "updated": "2024-11-21T16:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    34,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:34:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    34,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "CAIP: Detecting Router Misconfigurations with Context-Aware Iterative\n  Prompting of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAIP: Detecting Router Misconfigurations with Context-Aware Iterative\n  Prompting of LLMs"
                },
                "summary": "Model checkers and consistency checkers detect critical errors in router\nconfigurations, but these tools require significant manual effort to develop\nand maintain. LLM-based Q&A models have emerged as a promising alternative,\nallowing users to query partitions of configurations through prompts and\nreceive answers based on learned patterns, thanks to transformer models\npre-trained on vast datasets that provide generic configuration context for\ninterpreting router configurations. Yet, current methods of partition-based\nprompting often do not provide enough network-specific context from the actual\nconfigurations to enable accurate inference. We introduce a Context-Aware\nIterative Prompting (CAIP) framework that automates network-specific context\nextraction and optimizes LLM prompts for more precise router misconfiguration\ndetection. CAIP addresses three challenges: (1) efficiently mining relevant\ncontext from complex configuration files, (2) accurately distinguishing between\npre-defined and user-defined parameter values to prevent irrelevant context\nfrom being introduced, and (3) managing prompt context overload with iterative,\nguided interactions with the model. Our evaluations on synthetic and real-world\nconfigurations show that CAIP improves misconfiguration detection accuracy by\nmore than 30% compared to partition-based LLM approaches, model checkers, and\nconsistency checkers, uncovering over 20 previously undetected\nmisconfigurations in real-world configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model checkers and consistency checkers detect critical errors in router\nconfigurations, but these tools require significant manual effort to develop\nand maintain. LLM-based Q&A models have emerged as a promising alternative,\nallowing users to query partitions of configurations through prompts and\nreceive answers based on learned patterns, thanks to transformer models\npre-trained on vast datasets that provide generic configuration context for\ninterpreting router configurations. Yet, current methods of partition-based\nprompting often do not provide enough network-specific context from the actual\nconfigurations to enable accurate inference. We introduce a Context-Aware\nIterative Prompting (CAIP) framework that automates network-specific context\nextraction and optimizes LLM prompts for more precise router misconfiguration\ndetection. CAIP addresses three challenges: (1) efficiently mining relevant\ncontext from complex configuration files, (2) accurately distinguishing between\npre-defined and user-defined parameter values to prevent irrelevant context\nfrom being introduced, and (3) managing prompt context overload with iterative,\nguided interactions with the model. Our evaluations on synthetic and real-world\nconfigurations show that CAIP improves misconfiguration detection accuracy by\nmore than 30% compared to partition-based LLM approaches, model checkers, and\nconsistency checkers, uncovering over 20 previously undetected\nmisconfigurations in real-world configurations."
                },
                "authors": [
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Aaron Gember-Jacobson"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "arxiv_comment": "12 pages, 4 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14279v1",
                "updated": "2024-11-21T16:33:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    33,
                    30,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:33:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    33,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance"
                },
                "summary": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io)."
                },
                "authors": [
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Mingjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "19 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03416v2",
                "updated": "2024-11-21T16:32:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    32,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-05T18:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    57,
                    45,
                    1,
                    310,
                    0
                ],
                "title": "Accelerating Gaussian Variational Inference for Motion Planning Under\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Gaussian Variational Inference for Motion Planning Under\n  Uncertainty"
                },
                "summary": "This work addresses motion planning under uncertainty as a stochastic optimal\ncontrol problem. The path distribution induced by the optimal controller\ncorresponds to a posterior path distribution with a known form. To approximate\nthis posterior, we frame an optimization problem in the space of Gaussian\ndistributions, which aligns with the Gaussian Variational Inference Motion\nPlanning (GVIMP) paradigm introduced in \\cite{yu2023gaussian}. In this\nframework, the computation bottleneck lies in evaluating the expectation of\ncollision costs over a dense discretized trajectory and computing the marginal\ncovariances. This work exploits the sparse motion planning factor graph, which\nallows for parallel computing collision costs and Gaussian Belief Propagation\n(GBP) marginal covariance computation, to introduce a computationally efficient\napproach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian\nVariational Inference Motion Planning (P-GVIMP). We validate the proposed\nframework on various robotic systems, demonstrating significant speed\nacceleration achieved by leveraging Graphics Processing Units (GPUs) for\nparallel computation. An open-sourced implementation is presented at\nhttps://github.com/hzyu17/VIMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses motion planning under uncertainty as a stochastic optimal\ncontrol problem. The path distribution induced by the optimal controller\ncorresponds to a posterior path distribution with a known form. To approximate\nthis posterior, we frame an optimization problem in the space of Gaussian\ndistributions, which aligns with the Gaussian Variational Inference Motion\nPlanning (GVIMP) paradigm introduced in \\cite{yu2023gaussian}. In this\nframework, the computation bottleneck lies in evaluating the expectation of\ncollision costs over a dense discretized trajectory and computing the marginal\ncovariances. This work exploits the sparse motion planning factor graph, which\nallows for parallel computing collision costs and Gaussian Belief Propagation\n(GBP) marginal covariance computation, to introduce a computationally efficient\napproach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian\nVariational Inference Motion Planning (P-GVIMP). We validate the proposed\nframework on various robotic systems, demonstrating significant speed\nacceleration achieved by leveraging Graphics Processing Units (GPUs) for\nparallel computation. An open-sourced implementation is presented at\nhttps://github.com/hzyu17/VIMP."
                },
                "authors": [
                    {
                        "name": "Zinuo Chang"
                    },
                    {
                        "name": "Hongzhe Yu"
                    },
                    {
                        "name": "Patricio Vela"
                    },
                    {
                        "name": "Yongxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Chen"
                },
                "author": "Yongxin Chen",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09682v2",
                "updated": "2024-11-21T16:29:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    29,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2023-02-19T22:26:25Z",
                "published_parsed": [
                    2023,
                    2,
                    19,
                    22,
                    26,
                    25,
                    6,
                    50,
                    0
                ],
                "title": "Dual Attention Model with Reinforcement Learning for Classification of\n  Histology Whole-Slide Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Attention Model with Reinforcement Learning for Classification of\n  Histology Whole-Slide Images"
                },
                "summary": "Digital whole slide images (WSIs) are generally captured at microscopic\nresolution and encompass extensive spatial data. Directly feeding these images\nto deep learning models is computationally intractable due to memory\nconstraints, while downsampling the WSIs risks incurring information loss.\nAlternatively, splitting the WSIs into smaller patches may result in a loss of\nimportant contextual information. In this paper, we propose a novel dual\nattention approach, consisting of two main components, both inspired by the\nvisual examination process of a pathologist: The first soft attention model\nprocesses a low magnification view of the WSI to identify relevant regions of\ninterest, followed by a custom sampling method to extract diverse and spatially\ndistinct image tiles from the selected ROIs. The second component, the hard\nattention classification model further extracts a sequence of multi-resolution\nglimpses from each tile for classification. Since hard attention is\nnon-differentiable, we train this component using reinforcement learning to\npredict the location of the glimpses. This approach allows the model to focus\non essential regions instead of processing the entire tile, thereby aligning\nwith a pathologist's way of diagnosis. The two components are trained in an\nend-to-end fashion using a joint loss function to demonstrate the efficacy of\nthe model. The proposed model was evaluated on two WSI-level classification\nproblems: Human epidermal growth factor receptor 2 scoring on breast cancer\nhistology images and prediction of Intact/Loss status of two Mismatch Repair\nbiomarkers from colorectal cancer histology images. We show that the proposed\nmodel achieves performance better than or comparable to the state-of-the-art\nmethods while processing less than 10% of the WSI at the highest magnification\nand reducing the time required to infer the WSI-level label by more than 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital whole slide images (WSIs) are generally captured at microscopic\nresolution and encompass extensive spatial data. Directly feeding these images\nto deep learning models is computationally intractable due to memory\nconstraints, while downsampling the WSIs risks incurring information loss.\nAlternatively, splitting the WSIs into smaller patches may result in a loss of\nimportant contextual information. In this paper, we propose a novel dual\nattention approach, consisting of two main components, both inspired by the\nvisual examination process of a pathologist: The first soft attention model\nprocesses a low magnification view of the WSI to identify relevant regions of\ninterest, followed by a custom sampling method to extract diverse and spatially\ndistinct image tiles from the selected ROIs. The second component, the hard\nattention classification model further extracts a sequence of multi-resolution\nglimpses from each tile for classification. Since hard attention is\nnon-differentiable, we train this component using reinforcement learning to\npredict the location of the glimpses. This approach allows the model to focus\non essential regions instead of processing the entire tile, thereby aligning\nwith a pathologist's way of diagnosis. The two components are trained in an\nend-to-end fashion using a joint loss function to demonstrate the efficacy of\nthe model. The proposed model was evaluated on two WSI-level classification\nproblems: Human epidermal growth factor receptor 2 scoring on breast cancer\nhistology images and prediction of Intact/Loss status of two Mismatch Repair\nbiomarkers from colorectal cancer histology images. We show that the proposed\nmodel achieves performance better than or comparable to the state-of-the-art\nmethods while processing less than 10% of the WSI at the highest magnification\nand reducing the time required to infer the WSI-level label by more than 75%."
                },
                "authors": [
                    {
                        "name": "Manahil Raza"
                    },
                    {
                        "name": "Ruqayya Awan"
                    },
                    {
                        "name": "Raja Muhammad Saad Bashir"
                    },
                    {
                        "name": "Talha Qaiser"
                    },
                    {
                        "name": "Nasir M. Rajpoot"
                    }
                ],
                "author_detail": {
                    "name": "Nasir M. Rajpoot"
                },
                "author": "Nasir M. Rajpoot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.09682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14272v1",
                "updated": "2024-11-21T16:28:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    32,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:28:32Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    32,
                    3,
                    326,
                    0
                ],
                "title": "Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models"
                },
                "summary": "The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction."
                },
                "authors": [
                    {
                        "name": "Iacopo Ghinassi"
                    },
                    {
                        "name": "Leonardo Catalano"
                    },
                    {
                        "name": "Tommaso Colella"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Colella"
                },
                "author": "Tommaso Colella",
                "arxiv_journal_ref": "Proceedings of the Third Workshop on NLP for Positive Impact\n  (2024) 123-139",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14271v1",
                "updated": "2024-11-21T16:28:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:28:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "The properties of the interstellar medium in dusty, star-forming\n  galaxies at $z \\sim 2-4$: The shape of the CO spectral line energy\n  distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of the interstellar medium in dusty, star-forming\n  galaxies at $z \\sim 2-4$: The shape of the CO spectral line energy\n  distributions"
                },
                "summary": "The molecular gas in the interstellar medium (ISM) of star-forming galaxy\npopulations exhibits diverse physical properties. We investigate the $^{12}$CO\nexcitation of twelve dusty, luminous star-forming galaxies at $z \\sim 2-4$ by\ncombining observations of the $^{12}$CO from $J_{\\rm up} = 1$ to $J_{\\rm up} =\n8$. The spectral line energy distribution (SLED) has a similar shape to NGC\n253, M82, and local ULIRGs, with much stronger excitation than the Milky Way\ninner disc. By combining with resolved dust continuum sizes from\nhigh-resolution $870$-$\\mu$m ALMA observations and dust mass measurements\ndetermined from multi-wavelength SED fitting, we measure the relationship\nbetween the $^{12}$CO SLED and probable physical drivers of excitation:\nstar-formation efficiency, the average intensity of the radiation field\n$\\langle U\\rangle$, and the star-formation rate surface density. The primary\ndriver of high-$J_{\\rm up}$ $^{12}$CO excitation in star-forming galaxies is\nstar-formation rate surface density. We use the ratio of the CO($3-2$) and\nCO($6-5$) line fluxes to infer the CO excitation in each source and find that\nthe average ratios for our sample are elevated compared to observations of\nlow-redshift, less actively star-forming galaxies and agree well with\npredictions from numerical models that relate the ISM excitation to the\nstar-formation rate surface density. The significant scatter in the line ratios\nof a factor $\\approx 3$ within our sample likely reflects intrinsic variations\nin the ISM properties which may be caused by other effects on the excitation of\nthe molecular gas, such as cosmic ray ionization rates and mechanical heating\nthrough turbulence dissipation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The molecular gas in the interstellar medium (ISM) of star-forming galaxy\npopulations exhibits diverse physical properties. We investigate the $^{12}$CO\nexcitation of twelve dusty, luminous star-forming galaxies at $z \\sim 2-4$ by\ncombining observations of the $^{12}$CO from $J_{\\rm up} = 1$ to $J_{\\rm up} =\n8$. The spectral line energy distribution (SLED) has a similar shape to NGC\n253, M82, and local ULIRGs, with much stronger excitation than the Milky Way\ninner disc. By combining with resolved dust continuum sizes from\nhigh-resolution $870$-$\\mu$m ALMA observations and dust mass measurements\ndetermined from multi-wavelength SED fitting, we measure the relationship\nbetween the $^{12}$CO SLED and probable physical drivers of excitation:\nstar-formation efficiency, the average intensity of the radiation field\n$\\langle U\\rangle$, and the star-formation rate surface density. The primary\ndriver of high-$J_{\\rm up}$ $^{12}$CO excitation in star-forming galaxies is\nstar-formation rate surface density. We use the ratio of the CO($3-2$) and\nCO($6-5$) line fluxes to infer the CO excitation in each source and find that\nthe average ratios for our sample are elevated compared to observations of\nlow-redshift, less actively star-forming galaxies and agree well with\npredictions from numerical models that relate the ISM excitation to the\nstar-formation rate surface density. The significant scatter in the line ratios\nof a factor $\\approx 3$ within our sample likely reflects intrinsic variations\nin the ISM properties which may be caused by other effects on the excitation of\nthe molecular gas, such as cosmic ray ionization rates and mechanical heating\nthrough turbulence dissipation."
                },
                "authors": [
                    {
                        "name": "Dominic J. Taylor"
                    },
                    {
                        "name": "A. M. Swinbank"
                    },
                    {
                        "name": "Ian Smail"
                    },
                    {
                        "name": "Annagrazia Puglisi"
                    },
                    {
                        "name": "Jack E. Birkin"
                    },
                    {
                        "name": "Ugne Dudzeviciute"
                    },
                    {
                        "name": "Chian-Chou Chen"
                    },
                    {
                        "name": "S. Ikarashi"
                    },
                    {
                        "name": "Marta Frias Castillo"
                    },
                    {
                        "name": "Axel Weiss"
                    },
                    {
                        "name": "Zefeng Li"
                    },
                    {
                        "name": "Scott C. Chapman"
                    },
                    {
                        "name": "Jasper Jansen"
                    },
                    {
                        "name": "E. F. Jimenez-Andrade"
                    },
                    {
                        "name": "Leah K. Morabito"
                    },
                    {
                        "name": "Eric J. Murphy"
                    },
                    {
                        "name": "Matus Rybak"
                    },
                    {
                        "name": "P. P. van der Werf"
                    }
                ],
                "author_detail": {
                    "name": "P. P. van der Werf"
                },
                "arxiv_affiliation": "Durham University-CEA",
                "author": "P. P. van der Werf",
                "arxiv_comment": "Accepted for publication in MNRAS; 17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11585v3",
                "updated": "2024-11-21T16:28:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-03-18T08:58:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    8,
                    58,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines"
                },
                "summary": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields."
                },
                "authors": [
                    {
                        "name": "Ekaterina Trofimova"
                    },
                    {
                        "name": "Emil Sataev"
                    },
                    {
                        "name": "Andrey E. Ustyuzhanin"
                    }
                ],
                "author_detail": {
                    "name": "Andrey E. Ustyuzhanin"
                },
                "author": "Andrey E. Ustyuzhanin",
                "arxiv_doi": "10.7717/peerj-cs.2328",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.7717/peerj-cs.2328",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19040v3",
                "updated": "2024-11-21T16:27:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    27,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-29T12:27:19Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    27,
                    19,
                    2,
                    150,
                    0
                ],
                "title": "Finite-Choice Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-Choice Logic Programming"
                },
                "summary": "Logic programming, as exemplified by datalog, defines the meaning of a\nprogram as its unique smallest model: the deductive closure of its inference\nrules. However, many problems call for an enumeration of models that vary along\nsome set of choices while maintaining structural and logical constraints --\nthere is no single canonical model. The notion of stable models for logic\nprograms with negation has successfully captured programmer intuition about the\nset of valid solutions for such problems, giving rise to a family of\nprogramming languages and associated solvers known as answer set programming.\nUnfortunately, the definition of a stable model is frustratingly indirect,\nespecially in the presence of rules containing free variables.\n  We propose a new formalism, finite-choice logic programming, that uses\nchoice, not negation, to admit multiple solutions. Finite-choice logic\nprogramming contains all the expressive power of the stable model semantics,\ngives meaning to a new and useful class of programs, and enjoys a\nleast-fixed-point interpretation over a novel domain. We present an algorithm\nfor exploring the solution space and prove it correct with respect to our\nsemantics. Our implementation, the Dusa logic programming language, has\nperformance that compares favorably with state-of-the-art answer set solvers\nand exhibits more predictable scaling with problem size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic programming, as exemplified by datalog, defines the meaning of a\nprogram as its unique smallest model: the deductive closure of its inference\nrules. However, many problems call for an enumeration of models that vary along\nsome set of choices while maintaining structural and logical constraints --\nthere is no single canonical model. The notion of stable models for logic\nprograms with negation has successfully captured programmer intuition about the\nset of valid solutions for such problems, giving rise to a family of\nprogramming languages and associated solvers known as answer set programming.\nUnfortunately, the definition of a stable model is frustratingly indirect,\nespecially in the presence of rules containing free variables.\n  We propose a new formalism, finite-choice logic programming, that uses\nchoice, not negation, to admit multiple solutions. Finite-choice logic\nprogramming contains all the expressive power of the stable model semantics,\ngives meaning to a new and useful class of programs, and enjoys a\nleast-fixed-point interpretation over a novel domain. We present an algorithm\nfor exploring the solution space and prove it correct with respect to our\nsemantics. Our implementation, the Dusa logic programming language, has\nperformance that compares favorably with state-of-the-art answer set solvers\nand exhibits more predictable scaling with problem size."
                },
                "authors": [
                    {
                        "name": "Chris Martens"
                    },
                    {
                        "name": "Robert J. Simmons"
                    },
                    {
                        "name": "Michael Arntzenius"
                    }
                ],
                "author_detail": {
                    "name": "Michael Arntzenius"
                },
                "author": "Michael Arntzenius",
                "arxiv_doi": "10.1145/3704849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3704849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.19040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at POPL 2025",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15368v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15368v4",
                "updated": "2024-11-21T16:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    19,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-23T15:02:44Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    15,
                    2,
                    44,
                    4,
                    54,
                    0
                ],
                "title": "Probabilistically Correct Language-based Multi-Robot Planning using\n  Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistically Correct Language-based Multi-Robot Planning using\n  Conformal Prediction"
                },
                "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guocheng He"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15368v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15368v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21271v2",
                "updated": "2024-11-21T16:12:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    12,
                    34,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-28T17:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation"
                },
                "summary": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements."
                },
                "authors": [
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Huck Yang"
                    },
                    {
                        "name": "Chien-Yi Wang"
                    },
                    {
                        "name": "Nai Chit Fung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Min-Hung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hung Chen"
                },
                "author": "Min-Hung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14258v1",
                "updated": "2024-11-21T16:09:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    9,
                    5,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:09:05Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    9,
                    5,
                    3,
                    326,
                    0
                ],
                "title": "Knowledge Graphs, Large Language Models, and Hallucinations: An NLP\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs, Large Language Models, and Hallucinations: An NLP\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) based applications including automated text generation, question\nanswering, chatbots, and others. However, they face a significant challenge:\nhallucinations, where models produce plausible-sounding but factually incorrect\nresponses. This undermines trust and limits the applicability of LLMs in\ndifferent domains. Knowledge Graphs (KGs), on the other hand, provide a\nstructured collection of interconnected facts represented as entities (nodes)\nand their relationships (edges). In recent research, KGs have been leveraged to\nprovide context that can fill gaps in an LLM understanding of certain topics\noffering a promising approach to mitigate hallucinations in LLMs, enhancing\ntheir reliability and accuracy while benefiting from their wide applicability.\nNonetheless, it is still a very active area of research with various unresolved\nopen problems. In this paper, we discuss these open challenges covering\nstate-of-the-art datasets and benchmarks as well as methods for knowledge\nintegration and evaluating hallucinations. In our discussion, we consider the\ncurrent use of KGs in LLM systems and identify future directions within each of\nthese challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) based applications including automated text generation, question\nanswering, chatbots, and others. However, they face a significant challenge:\nhallucinations, where models produce plausible-sounding but factually incorrect\nresponses. This undermines trust and limits the applicability of LLMs in\ndifferent domains. Knowledge Graphs (KGs), on the other hand, provide a\nstructured collection of interconnected facts represented as entities (nodes)\nand their relationships (edges). In recent research, KGs have been leveraged to\nprovide context that can fill gaps in an LLM understanding of certain topics\noffering a promising approach to mitigate hallucinations in LLMs, enhancing\ntheir reliability and accuracy while benefiting from their wide applicability.\nNonetheless, it is still a very active area of research with various unresolved\nopen problems. In this paper, we discuss these open challenges covering\nstate-of-the-art datasets and benchmarks as well as methods for knowledge\nintegration and evaluating hallucinations. In our discussion, we consider the\ncurrent use of KGs in LLM systems and identify future directions within each of\nthese challenges."
                },
                "authors": [
                    {
                        "name": "Ernests Lavrinovics"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Katja Hose"
                    }
                ],
                "author_detail": {
                    "name": "Katja Hose"
                },
                "author": "Katja Hose",
                "arxiv_comment": "7 pages, 2 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14256v1",
                "updated": "2024-11-21T16:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    4,
                    10,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:04:10Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    4,
                    10,
                    3,
                    326,
                    0
                ],
                "title": "Generalizing End-To-End Autonomous Driving In Real-World Environments\n  Using Zero-Shot LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing End-To-End Autonomous Driving In Real-World Environments\n  Using Zero-Shot LLMs"
                },
                "summary": "Traditional autonomous driving methods adopt a modular design, decomposing\ntasks into sub-tasks. In contrast, end-to-end autonomous driving directly\noutputs actions from raw sensor data, avoiding error accumulation. However,\ntraining an end-to-end model requires a comprehensive dataset; otherwise, the\nmodel exhibits poor generalization capabilities. Recently, large language\nmodels (LLMs) have been applied to enhance the generalization capabilities of\nend-to-end driving models. Most studies explore LLMs in an open-loop manner,\nwhere the output actions are compared to those of experts without direct\nfeedback from the real world, while others examine closed-loop results only in\nsimulations. This paper proposes an efficient architecture that integrates\nmultimodal LLMs into end-to-end driving models operating in closed-loop\nsettings in real-world environments. In our architecture, the LLM periodically\nprocesses raw sensor data to generate high-level driving instructions,\neffectively guiding the end-to-end model, even at a slower rate than the raw\nsensor data. This architecture relaxes the trade-off between the latency and\ninference quality of the LLM. It also allows us to choose from a wide variety\nof LLMs to improve high-level driving instructions and minimize fine-tuning\ncosts. Consequently, our architecture reduces data collection requirements\nbecause the LLMs do not directly output actions; we only need to train a simple\nimitation learning model to output actions. In our experiments, the training\ndata for the end-to-end model in a real-world environment consists of only\nsimple obstacle configurations with one traffic cone, while the test\nenvironment is more complex and contains multiple obstacles placed in various\npositions. Experiments show that the proposed architecture enhances the\ngeneralization capabilities of the end-to-end model even without fine-tuning\nthe LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional autonomous driving methods adopt a modular design, decomposing\ntasks into sub-tasks. In contrast, end-to-end autonomous driving directly\noutputs actions from raw sensor data, avoiding error accumulation. However,\ntraining an end-to-end model requires a comprehensive dataset; otherwise, the\nmodel exhibits poor generalization capabilities. Recently, large language\nmodels (LLMs) have been applied to enhance the generalization capabilities of\nend-to-end driving models. Most studies explore LLMs in an open-loop manner,\nwhere the output actions are compared to those of experts without direct\nfeedback from the real world, while others examine closed-loop results only in\nsimulations. This paper proposes an efficient architecture that integrates\nmultimodal LLMs into end-to-end driving models operating in closed-loop\nsettings in real-world environments. In our architecture, the LLM periodically\nprocesses raw sensor data to generate high-level driving instructions,\neffectively guiding the end-to-end model, even at a slower rate than the raw\nsensor data. This architecture relaxes the trade-off between the latency and\ninference quality of the LLM. It also allows us to choose from a wide variety\nof LLMs to improve high-level driving instructions and minimize fine-tuning\ncosts. Consequently, our architecture reduces data collection requirements\nbecause the LLMs do not directly output actions; we only need to train a simple\nimitation learning model to output actions. In our experiments, the training\ndata for the end-to-end model in a real-world environment consists of only\nsimple obstacle configurations with one traffic cone, while the test\nenvironment is more complex and contains multiple obstacles placed in various\npositions. Experiments show that the proposed architecture enhances the\ngeneralization capabilities of the end-to-end model even without fine-tuning\nthe LLM."
                },
                "authors": [
                    {
                        "name": "Zeyu Dong"
                    },
                    {
                        "name": "Yimin Zhu"
                    },
                    {
                        "name": "Yansong Li"
                    },
                    {
                        "name": "Kevin Mahon"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v1",
                "updated": "2024-11-21T15:59:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification"
                },
                "summary": "Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14251v1",
                "updated": "2024-11-21T15:57:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    57,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:57:02Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    57,
                    2,
                    3,
                    326,
                    0
                ],
                "title": "Natural Language Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL."
                },
                "authors": [
                    {
                        "name": "Xidong Feng"
                    },
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Haotian Fu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Extension of arXiv:2402.07157",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10446v2",
                "updated": "2024-11-21T15:56:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    56,
                    48,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-15T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Ekpo"
                    },
                    {
                        "name": "Mara Levy"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14243v1",
                "updated": "2024-11-21T15:50:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    50,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:50:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    50,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection"
                },
                "summary": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a significant threat by implanting hidden backdoor in a victim\nmodel, which adversaries can later exploit to trigger malicious behaviors\nduring inference. However, current backdoor techniques are limited to static\nscenarios where attackers must define a malicious objective before training,\nlocking the attack into a predetermined action without inference-time\nadaptability. Given the expressive output space in object detection, including\nobject existence detection, bounding box estimation, and object classification,\nthe feasibility of implanting a backdoor that provides inference-time control\nwith a high degree of freedom remains unexplored. This paper introduces\nAnywhereDoor, a flexible backdoor attack tailored for object detection. Once\nimplanted, AnywhereDoor enables adversaries to specify different attack types\n(object vanishing, fabrication, or misclassification) and configurations\n(untargeted or targeted with specific classes) to dynamically control detection\nbehavior. This flexibility is achieved through three key innovations: (i)\nobjective disentanglement to support a broader range of attack combinations\nwell beyond what existing methods allow; (ii) trigger mosaicking to ensure\nbackdoor activations are robust, even against those object detectors that\nextract localized regions from the input image for recognition; and (iii)\nstrategic batching to address object-level data imbalances that otherwise\nhinders a balanced manipulation. Extensive experiments demonstrate that\nAnywhereDoor provides attackers with a high degree of control, achieving an\nattack success rate improvement of nearly 80% compared to adaptations of\nexisting methods for such flexible control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a significant threat by implanting hidden backdoor in a victim\nmodel, which adversaries can later exploit to trigger malicious behaviors\nduring inference. However, current backdoor techniques are limited to static\nscenarios where attackers must define a malicious objective before training,\nlocking the attack into a predetermined action without inference-time\nadaptability. Given the expressive output space in object detection, including\nobject existence detection, bounding box estimation, and object classification,\nthe feasibility of implanting a backdoor that provides inference-time control\nwith a high degree of freedom remains unexplored. This paper introduces\nAnywhereDoor, a flexible backdoor attack tailored for object detection. Once\nimplanted, AnywhereDoor enables adversaries to specify different attack types\n(object vanishing, fabrication, or misclassification) and configurations\n(untargeted or targeted with specific classes) to dynamically control detection\nbehavior. This flexibility is achieved through three key innovations: (i)\nobjective disentanglement to support a broader range of attack combinations\nwell beyond what existing methods allow; (ii) trigger mosaicking to ensure\nbackdoor activations are robust, even against those object detectors that\nextract localized regions from the input image for recognition; and (iii)\nstrategic batching to address object-level data imbalances that otherwise\nhinders a balanced manipulation. Extensive experiments demonstrate that\nAnywhereDoor provides attackers with a high degree of control, achieving an\nattack success rate improvement of nearly 80% compared to adaptations of\nexisting methods for such flexible control."
                },
                "authors": [
                    {
                        "name": "Jialin Lu"
                    },
                    {
                        "name": "Junjie Shan"
                    },
                    {
                        "name": "Ziqi Zhao"
                    },
                    {
                        "name": "Ka-Ho Chow"
                    }
                ],
                "author_detail": {
                    "name": "Ka-Ho Chow"
                },
                "author": "Ka-Ho Chow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14228v1",
                "updated": "2024-11-21T15:37:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    37,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:37:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    37,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression"
                },
                "summary": "Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets."
                },
                "authors": [
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Sheng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Guo"
                },
                "author": "Sheng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14215v1",
                "updated": "2024-11-21T15:25:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    25,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:25:08Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    25,
                    8,
                    3,
                    326,
                    0
                ],
                "title": "Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models"
                },
                "summary": "LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities."
                },
                "authors": [
                    {
                        "name": "Martha Lewis"
                    },
                    {
                        "name": "Melanie Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Mitchell"
                },
                "author": "Melanie Mitchell",
                "arxiv_comment": "31 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2402.08955",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14214v1",
                "updated": "2024-11-21T15:24:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:24:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "Physics-Informed LLM-Agent for Automated Modulation Design in Power\n  Electronics Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed LLM-Agent for Automated Modulation Design in Power\n  Electronics Systems"
                },
                "summary": "LLM-based autonomous agents have demonstrated outstanding performance in\nsolving complex industrial tasks. However, in the pursuit of carbon neutrality\nand high-performance renewable energy systems, existing AI-assisted design\nautomation faces significant limitations in explainability, scalability, and\nusability. To address these challenges, we propose LP-COMDA, an LLM-based,\nphysics-informed autonomous agent that automates the modulation design of power\nconverters in Power Electronics Systems with minimal human supervision. Unlike\ntraditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that\ngathers and validates design specifications through a user-friendly chat\ninterface. The planner then coordinates with physics-informed design and\noptimization tools to iteratively generate and refine modulation designs\nautonomously. Through the chat interface, LP-COMDA provides an explainable\ndesign process, presenting explanations and charts. Experiments show that\nLP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error\ncompared to the second-best benchmark method in terms of standard mean absolute\nerror. Furthermore, empirical studies with 20 experts conclude that design time\nwith LP-COMDA is over 33 times faster than conventional methods, showing its\nsignificant improvement on design efficiency over the current processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents have demonstrated outstanding performance in\nsolving complex industrial tasks. However, in the pursuit of carbon neutrality\nand high-performance renewable energy systems, existing AI-assisted design\nautomation faces significant limitations in explainability, scalability, and\nusability. To address these challenges, we propose LP-COMDA, an LLM-based,\nphysics-informed autonomous agent that automates the modulation design of power\nconverters in Power Electronics Systems with minimal human supervision. Unlike\ntraditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that\ngathers and validates design specifications through a user-friendly chat\ninterface. The planner then coordinates with physics-informed design and\noptimization tools to iteratively generate and refine modulation designs\nautonomously. Through the chat interface, LP-COMDA provides an explainable\ndesign process, presenting explanations and charts. Experiments show that\nLP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error\ncompared to the second-best benchmark method in terms of standard mean absolute\nerror. Furthermore, empirical studies with 20 experts conclude that design time\nwith LP-COMDA is over 33 times faster than conventional methods, showing its\nsignificant improvement on design efficiency over the current processes."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Fanfan Lin"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14202v1",
                "updated": "2024-11-21T15:11:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    11,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:11:02Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    11,
                    2,
                    3,
                    326,
                    0
                ],
                "title": "Revised Regularization for Efficient Continual Learning through\n  Correlation-Based Parameter Update in Bayesian Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revised Regularization for Efficient Continual Learning through\n  Correlation-Based Parameter Update in Bayesian Neural Networks"
                },
                "summary": "We propose a Bayesian neural network-based continual learning algorithm using\nVariational Inference, aiming to overcome several drawbacks of existing\nmethods. Specifically, in continual learning scenarios, storing network\nparameters at each step to retain knowledge poses challenges. This is\ncompounded by the crucial need to mitigate catastrophic forgetting,\nparticularly given the limited access to past datasets, which complicates\nmaintaining correspondence between network parameters and datasets across all\nsessions. Current methods using Variational Inference with KL divergence risk\ncatastrophic forgetting during uncertain node updates and coupled disruptions\nin certain nodes. To address these challenges, we propose the following\nstrategies. To reduce the storage of the dense layer parameters, we propose a\nparameter distribution learning method that significantly reduces the storage\nrequirements. In the continual learning framework employing variational\ninference, our study introduces a regularization term that specifically targets\nthe dynamics and population of the mean and variance of the parameters. This\nterm aims to retain the benefits of KL divergence while addressing related\nchallenges. To ensure proper correspondence between network parameters and the\ndata, our method introduces an importance-weighted Evidence Lower Bound term to\ncapture data and parameter correlations. This enables storage of common and\ndistinctive parameter hyperspace bases. The proposed method partitions the\nparameter space into common and distinctive subspaces, with conditions for\neffective backward and forward knowledge transfer, elucidating the\nnetwork-parameter dataset correspondence. The experimental results demonstrate\nthe effectiveness of our method across diverse datasets and various\ncombinations of sequential datasets, yielding superior performance compared to\nexisting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Bayesian neural network-based continual learning algorithm using\nVariational Inference, aiming to overcome several drawbacks of existing\nmethods. Specifically, in continual learning scenarios, storing network\nparameters at each step to retain knowledge poses challenges. This is\ncompounded by the crucial need to mitigate catastrophic forgetting,\nparticularly given the limited access to past datasets, which complicates\nmaintaining correspondence between network parameters and datasets across all\nsessions. Current methods using Variational Inference with KL divergence risk\ncatastrophic forgetting during uncertain node updates and coupled disruptions\nin certain nodes. To address these challenges, we propose the following\nstrategies. To reduce the storage of the dense layer parameters, we propose a\nparameter distribution learning method that significantly reduces the storage\nrequirements. In the continual learning framework employing variational\ninference, our study introduces a regularization term that specifically targets\nthe dynamics and population of the mean and variance of the parameters. This\nterm aims to retain the benefits of KL divergence while addressing related\nchallenges. To ensure proper correspondence between network parameters and the\ndata, our method introduces an importance-weighted Evidence Lower Bound term to\ncapture data and parameter correlations. This enables storage of common and\ndistinctive parameter hyperspace bases. The proposed method partitions the\nparameter space into common and distinctive subspaces, with conditions for\neffective backward and forward knowledge transfer, elucidating the\nnetwork-parameter dataset correspondence. The experimental results demonstrate\nthe effectiveness of our method across diverse datasets and various\ncombinations of sequential datasets, yielding superior performance compared to\nexisting approaches."
                },
                "authors": [
                    {
                        "name": "Sanchar Palit"
                    },
                    {
                        "name": "Biplab Banerjee"
                    },
                    {
                        "name": "Subhasis Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Subhasis Chaudhuri"
                },
                "author": "Subhasis Chaudhuri",
                "arxiv_comment": "at ICVGIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14199v1",
                "updated": "2024-11-21T15:07:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    7,
                    42,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:07:42Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    7,
                    42,
                    3,
                    326,
                    0
                ],
                "title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs"
                },
                "summary": "Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo."
                },
                "authors": [
                    {
                        "name": "Akari Asai"
                    },
                    {
                        "name": "Jacqueline He"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Luca Soldaini"
                    },
                    {
                        "name": "Sergey Feldman"
                    },
                    {
                        "name": "Mike D'arcy"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Matt Latzke"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Shengyan Liu"
                    },
                    {
                        "name": "Hao Tong"
                    },
                    {
                        "name": "Bohao Wu"
                    },
                    {
                        "name": "Yanyu Xiong"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Dan Weld"
                    },
                    {
                        "name": "Doug Downey"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14192v1",
                "updated": "2024-11-21T15:01:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    1,
                    17,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:01:17Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    1,
                    17,
                    3,
                    326,
                    0
                ],
                "title": "Learning Pore-scale Multi-phase Flow from Experimental Data with Graph\n  Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Pore-scale Multi-phase Flow from Experimental Data with Graph\n  Neural Network"
                },
                "summary": "Understanding the process of multiphase fluid flow through porous media is\ncrucial for many climate change mitigation technologies, including CO$_2$\ngeological storage, hydrogen storage, and fuel cells. However, current\nnumerical models are often incapable of accurately capturing the complex\npore-scale physics observed in experiments. In this study, we address this\nchallenge using a graph neural network-based approach and directly learn\npore-scale fluid flow using micro-CT experimental data. We propose a\nLong-Short-Edge MeshGraphNet (LSE-MGN) that predicts the state of each node in\nthe pore space at each time step. During inference, given an initial state, the\nmodel can autoregressively predict the evolution of the multiphase flow process\nover time. This approach successfully captures the physics from the\nhigh-resolution experimental data while maintaining computational efficiency,\nproviding a promising direction for accurate and efficient pore-scale modeling\nof complex multiphase fluid flow dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the process of multiphase fluid flow through porous media is\ncrucial for many climate change mitigation technologies, including CO$_2$\ngeological storage, hydrogen storage, and fuel cells. However, current\nnumerical models are often incapable of accurately capturing the complex\npore-scale physics observed in experiments. In this study, we address this\nchallenge using a graph neural network-based approach and directly learn\npore-scale fluid flow using micro-CT experimental data. We propose a\nLong-Short-Edge MeshGraphNet (LSE-MGN) that predicts the state of each node in\nthe pore space at each time step. During inference, given an initial state, the\nmodel can autoregressively predict the evolution of the multiphase flow process\nover time. This approach successfully captures the physics from the\nhigh-resolution experimental data while maintaining computational efficiency,\nproviding a promising direction for accurate and efficient pore-scale modeling\nof complex multiphase fluid flow dynamics."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Catherine Spurin"
                    },
                    {
                        "name": "Gege Wen"
                    }
                ],
                "author_detail": {
                    "name": "Gege Wen"
                },
                "author": "Gege Wen",
                "arxiv_comment": "Accpeted for Machine Learning and the Physical Sciences Workshop at\n  the 38th conference on Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14172v1",
                "updated": "2024-11-21T14:34:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    34,
                    46,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:34:46Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    34,
                    46,
                    3,
                    326,
                    0
                ],
                "title": "TaQ-DiT: Time-aware Quantization for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaQ-DiT: Time-aware Quantization for Diffusion Transformers"
                },
                "summary": "Transformer-based diffusion models, dubbed Diffusion Transformers (DiTs),\nhave achieved state-of-the-art performance in image and video generation tasks.\nHowever, their large model size and slow inference speed limit their practical\napplications, calling for model compression methods such as quantization.\nUnfortunately, existing DiT quantization methods overlook (1) the impact of\nreconstruction and (2) the varying quantization sensitivities across different\nlayers, which hinder their achievable performance. To tackle these issues, we\npropose innovative time-aware quantization for DiTs (TaQ-DiT). Specifically,\n(1) we observe a non-convergence issue when reconstructing weights and\nactivations separately during quantization and introduce a joint reconstruction\nmethod to resolve this problem. (2) We discover that Post-GELU activations are\nparticularly sensitive to quantization due to their significant variability\nacross different denoising steps as well as extreme asymmetries and variations\nwithin each step. To address this, we propose time-variance-aware\ntransformations to facilitate more effective quantization. Experimental results\nshow that when quantizing DiTs' weights to 4-bit and activations to 8-bit\n(W4A8), our method significantly surpasses previous quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based diffusion models, dubbed Diffusion Transformers (DiTs),\nhave achieved state-of-the-art performance in image and video generation tasks.\nHowever, their large model size and slow inference speed limit their practical\napplications, calling for model compression methods such as quantization.\nUnfortunately, existing DiT quantization methods overlook (1) the impact of\nreconstruction and (2) the varying quantization sensitivities across different\nlayers, which hinder their achievable performance. To tackle these issues, we\npropose innovative time-aware quantization for DiTs (TaQ-DiT). Specifically,\n(1) we observe a non-convergence issue when reconstructing weights and\nactivations separately during quantization and introduce a joint reconstruction\nmethod to resolve this problem. (2) We discover that Post-GELU activations are\nparticularly sensitive to quantization due to their significant variability\nacross different denoising steps as well as extreme asymmetries and variations\nwithin each step. To address this, we propose time-variance-aware\ntransformations to facilitate more effective quantization. Experimental results\nshow that when quantizing DiTs' weights to 4-bit and activations to 8-bit\n(W4A8), our method significantly surpasses previous quantization methods."
                },
                "authors": [
                    {
                        "name": "Xinyan Liu"
                    },
                    {
                        "name": "Huihong Shi"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14169v1",
                "updated": "2024-11-21T14:27:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    27,
                    15,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:27:15Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    27,
                    15,
                    3,
                    326,
                    0
                ],
                "title": "Spatiotemporal Decoupling for Efficient Vision-Based Occupancy\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal Decoupling for Efficient Vision-Based Occupancy\n  Forecasting"
                },
                "summary": "The task of occupancy forecasting (OCF) involves utilizing past and present\nperception data to predict future occupancy states of autonomous vehicle\nsurrounding environments, which is critical for downstream tasks such as\nobstacle avoidance and path planning. Existing 3D OCF approaches struggle to\npredict plausible spatial details for movable objects and suffer from slow\ninference speeds due to neglecting the bias and uneven distribution of changing\noccupancy states in both space and time. In this paper, we propose a novel\nspatiotemporal decoupling vision-based paradigm to explicitly tackle the bias\nand achieve both effective and efficient 3D OCF. To tackle spatial bias in\nempty areas, we introduce a novel spatial representation that decouples the\nconventional dense 3D format into 2D bird's-eye view (BEV) occupancy with\ncorresponding height values, enabling 3D OCF derived only from 2D predictions\nthus enhancing efficiency. To reduce temporal bias on static voxels, we design\ntemporal decoupling to improve end-to-end OCF by temporally associating\ninstances via predicted flows. We develop an efficient multi-head network\nEfficientOCF to achieve 3D OCF with our devised spatiotemporally decoupled\nrepresentation. A new metric, conditional IoU (C-IoU), is also introduced to\nprovide a robust 3D OCF performance assessment, especially in datasets with\nmissing or incomplete annotations. The experimental results demonstrate that\nEfficientOCF surpasses existing baseline methods on accuracy and efficiency,\nachieving state-of-the-art performance with a fast inference time of 82.33ms\nwith a single GPU. Our code will be released as open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of occupancy forecasting (OCF) involves utilizing past and present\nperception data to predict future occupancy states of autonomous vehicle\nsurrounding environments, which is critical for downstream tasks such as\nobstacle avoidance and path planning. Existing 3D OCF approaches struggle to\npredict plausible spatial details for movable objects and suffer from slow\ninference speeds due to neglecting the bias and uneven distribution of changing\noccupancy states in both space and time. In this paper, we propose a novel\nspatiotemporal decoupling vision-based paradigm to explicitly tackle the bias\nand achieve both effective and efficient 3D OCF. To tackle spatial bias in\nempty areas, we introduce a novel spatial representation that decouples the\nconventional dense 3D format into 2D bird's-eye view (BEV) occupancy with\ncorresponding height values, enabling 3D OCF derived only from 2D predictions\nthus enhancing efficiency. To reduce temporal bias on static voxels, we design\ntemporal decoupling to improve end-to-end OCF by temporally associating\ninstances via predicted flows. We develop an efficient multi-head network\nEfficientOCF to achieve 3D OCF with our devised spatiotemporally decoupled\nrepresentation. A new metric, conditional IoU (C-IoU), is also introduced to\nprovide a robust 3D OCF performance assessment, especially in datasets with\nmissing or incomplete annotations. The experimental results demonstrate that\nEfficientOCF surpasses existing baseline methods on accuracy and efficiency,\nachieving state-of-the-art performance with a fast inference time of 82.33ms\nwith a single GPU. Our code will be released as open source."
                },
                "authors": [
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    },
                    {
                        "name": "Junyi Ma"
                    },
                    {
                        "name": "Jiawei Huang"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Ling Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ling Pei"
                },
                "author": "Ling Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18097v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18097v3",
                "updated": "2024-11-21T14:23:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    23,
                    49,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-08T11:28:06Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    6,
                    1,
                    282,
                    0
                ],
                "title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine"
                },
                "summary": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries."
                },
                "authors": [
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Youngjune Lee"
                    },
                    {
                        "name": "Gyu-Hwung Cho"
                    },
                    {
                        "name": "Haeyu Jeong"
                    },
                    {
                        "name": "Jungmin Kong"
                    },
                    {
                        "name": "Saehun Kim"
                    },
                    {
                        "name": "Keunchan Park"
                    },
                    {
                        "name": "Sarah Cho"
                    },
                    {
                        "name": "Inchang Jeong"
                    },
                    {
                        "name": "Gyohee Nam"
                    },
                    {
                        "name": "Sunghoon Han"
                    },
                    {
                        "name": "Wonil Yang"
                    },
                    {
                        "name": "Jaeho Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Choi"
                },
                "author": "Jaeho Choi",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18097v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18097v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14164v1",
                "updated": "2024-11-21T14:22:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    22,
                    38,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:22:38Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    22,
                    38,
                    3,
                    326,
                    0
                ],
                "title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoPru: Focal Pruning for Efficient Large Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Xiaohua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Xu"
                },
                "author": "Xiaohua Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11340v2",
                "updated": "2024-11-21T14:09:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    9,
                    12,
                    3,
                    326,
                    0
                ],
                "published": "2024-09-17T16:42:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    42,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "OmniGen: Unified Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGen: Unified Image Generation"
                },
                "summary": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements."
                },
                "authors": [
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "Update the paper for OmniGen-v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14133v1",
                "updated": "2024-11-21T14:00:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs."
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "28 pages, 9 tables, 13 figures; under review at CVPR '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09055v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09055v3",
                "updated": "2024-11-21T13:57:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    57,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-03-14T02:51:01Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    2,
                    51,
                    1,
                    3,
                    74,
                    0
                ],
                "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image\n  Diffusion Models"
                },
                "summary": "We introduce SemanticDraw, a new paradigm of interactive content creation\nwhere high-quality images are generated in near real-time from given multiple\nhand-drawn regions, each encoding prescribed semantic meaning. In order to\nmaximize the productivity of content creators and to fully realize their\nartistic imagination, it requires both quick interactive interfaces and\nfine-grained regional controls in their tools. Despite astonishing generation\nquality from recent diffusion models, we find that existing approaches for\nregional controllability are very slow (52 seconds for $512 \\times 512$ image)\nwhile not compatible with acceleration methods such as LCM, blocking their huge\npotential in interactive content creation. From this observation, we build our\nsolution for interactive content creation in two steps: (1) we establish\ncompatibility between region-based controls and acceleration techniques for\ndiffusion models, maintaining high fidelity of multi-prompt image generation\nwith $\\times 10$ reduced number of inference steps, (2) we increase the\ngeneration throughput with our new multi-prompt stream batch pipeline, enabling\nlow-latency generation from multiple, region-based text prompts on a single RTX\n2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion\nmodels and acceleration schedulers, allowing sub-second (0.64 seconds) image\ncontent creation application upon well-established image diffusion models. Our\nproject page is: https://jaerinlee.com/research/semantic-draw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SemanticDraw, a new paradigm of interactive content creation\nwhere high-quality images are generated in near real-time from given multiple\nhand-drawn regions, each encoding prescribed semantic meaning. In order to\nmaximize the productivity of content creators and to fully realize their\nartistic imagination, it requires both quick interactive interfaces and\nfine-grained regional controls in their tools. Despite astonishing generation\nquality from recent diffusion models, we find that existing approaches for\nregional controllability are very slow (52 seconds for $512 \\times 512$ image)\nwhile not compatible with acceleration methods such as LCM, blocking their huge\npotential in interactive content creation. From this observation, we build our\nsolution for interactive content creation in two steps: (1) we establish\ncompatibility between region-based controls and acceleration techniques for\ndiffusion models, maintaining high fidelity of multi-prompt image generation\nwith $\\times 10$ reduced number of inference steps, (2) we increase the\ngeneration throughput with our new multi-prompt stream batch pipeline, enabling\nlow-latency generation from multiple, region-based text prompts on a single RTX\n2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion\nmodels and acceleration schedulers, allowing sub-second (0.64 seconds) image\ncontent creation application upon well-established image diffusion models. Our\nproject page is: https://jaerinlee.com/research/semantic-draw."
                },
                "authors": [
                    {
                        "name": "Jaerin Lee"
                    },
                    {
                        "name": "Daniel Sungho Jung"
                    },
                    {
                        "name": "Kanggeon Lee"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyoung Mu Lee"
                },
                "author": "Kyoung Mu Lee",
                "arxiv_comment": "20 pages, 15 figures. v3: added tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09055v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09055v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14121v1",
                "updated": "2024-11-21T13:45:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    45,
                    40,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:45:40Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    45,
                    40,
                    3,
                    326,
                    0
                ],
                "title": "Learning from \"Silly\" Questions Improves Large Language Models, But Only\n  Slightly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from \"Silly\" Questions Improves Large Language Models, But Only\n  Slightly"
                },
                "summary": "Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Shudong Liu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Takahiro Shinozaki"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "27 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14110v1",
                "updated": "2024-11-21T13:18:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:18:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\n  Generation Applications with Agent-based Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\n  Generation Applications with Agent-based Attacks"
                },
                "summary": "While large language models (LLMs) have achieved notable success in\ngenerative tasks, they still face limitations, such as lacking up-to-date\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\nenhances LLM performance by integrating external knowledge bases, providing\nadditional context which significantly improves accuracy and knowledge\ncoverage. However, building these external knowledge bases often requires\nsubstantial resources and may involve sensitive information. In this paper, we\npropose an agent-based automated privacy attack called RAG-Thief, which can\nextract a scalable amount of private data from the private database used in RAG\napplications. We conduct a systematic study on the privacy risks associated\nwith RAG applications, revealing that the vulnerability of LLMs makes the\nprivate knowledge bases suffer significant privacy risks. Unlike previous\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\nstarts with an initial adversarial query and learns from model responses,\nprogressively generating new queries to extract as many chunks from the\nknowledge base as possible. Experimental results show that our RAG-Thief can\nextract over 70% information from the private knowledge bases within customized\nRAG applications deployed on local machines and real-world platforms, including\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\nvulnerabilities in current RAG applications and underscore the pressing need\nfor stronger safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved notable success in\ngenerative tasks, they still face limitations, such as lacking up-to-date\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\nenhances LLM performance by integrating external knowledge bases, providing\nadditional context which significantly improves accuracy and knowledge\ncoverage. However, building these external knowledge bases often requires\nsubstantial resources and may involve sensitive information. In this paper, we\npropose an agent-based automated privacy attack called RAG-Thief, which can\nextract a scalable amount of private data from the private database used in RAG\napplications. We conduct a systematic study on the privacy risks associated\nwith RAG applications, revealing that the vulnerability of LLMs makes the\nprivate knowledge bases suffer significant privacy risks. Unlike previous\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\nstarts with an initial adversarial query and learns from model responses,\nprogressively generating new queries to extract as many chunks from the\nknowledge base as possible. Experimental results show that our RAG-Thief can\nextract over 70% information from the private knowledge bases within customized\nRAG applications deployed on local machines and real-world platforms, including\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\nvulnerabilities in current RAG applications and underscore the pressing need\nfor stronger safeguards."
                },
                "authors": [
                    {
                        "name": "Changyue Jiang"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Chenfu Bao"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12023v2",
                "updated": "2024-11-21T13:10:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    10,
                    39,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-18T20:03:36Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    3,
                    36,
                    0,
                    323,
                    0
                ],
                "title": "Exploring HOD-dependent systematics for the DESI 2024 Full-Shape galaxy\n  clustering analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring HOD-dependent systematics for the DESI 2024 Full-Shape galaxy\n  clustering analysis"
                },
                "summary": "We analyse the robustness of the DESI 2024 cosmological inference from fits\nto the full shape of the galaxy power spectrum to uncertainties in the Halo\nOccupation Distribution (HOD) model of the galaxy-halo connection and the\nchoice of priors on nuisance parameters. We assess variations in the recovered\ncosmological parameters across a range of mocks populated with different HOD\nmodels and find that shifts are often greater than 20% of the expected\nstatistical uncertainties from the DESI data. We encapsulate the effect of such\nshifts in terms of a systematic covariance term, $\\mathsf{C}_{\\rm HOD}$, and an\nadditional diagonal contribution quantifying the impact of our choice of\nnuisance parameter priors on the ability of the effective field theory (EFT)\nmodel to correctly recover the cosmological parameters of the simulations.\nThese two covariance contributions are designed to be added to the usual\ncovariance term, $\\mathsf{C}_{\\rm stat}$, describing the statistical\nuncertainty in the power spectrum measurement, in order to fairly represent\nthese sources of systematic uncertainty. This approach is more general and\nrobust to choices of model free parameters or additional external datasets used\nin cosmological fits than the alternative approach of adding systematic\nuncertainties at the level of the recovered marginalised parameter posteriors.\nWe compare the approaches within the context of a fixed $\\Lambda$CDM model and\ndemonstrate that our method gives conservative estimates of the systematic\nuncertainty that nevertheless have little impact on the final posteriors\nobtained from DESI data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the robustness of the DESI 2024 cosmological inference from fits\nto the full shape of the galaxy power spectrum to uncertainties in the Halo\nOccupation Distribution (HOD) model of the galaxy-halo connection and the\nchoice of priors on nuisance parameters. We assess variations in the recovered\ncosmological parameters across a range of mocks populated with different HOD\nmodels and find that shifts are often greater than 20% of the expected\nstatistical uncertainties from the DESI data. We encapsulate the effect of such\nshifts in terms of a systematic covariance term, $\\mathsf{C}_{\\rm HOD}$, and an\nadditional diagonal contribution quantifying the impact of our choice of\nnuisance parameter priors on the ability of the effective field theory (EFT)\nmodel to correctly recover the cosmological parameters of the simulations.\nThese two covariance contributions are designed to be added to the usual\ncovariance term, $\\mathsf{C}_{\\rm stat}$, describing the statistical\nuncertainty in the power spectrum measurement, in order to fairly represent\nthese sources of systematic uncertainty. This approach is more general and\nrobust to choices of model free parameters or additional external datasets used\nin cosmological fits than the alternative approach of adding systematic\nuncertainties at the level of the recovered marginalised parameter posteriors.\nWe compare the approaches within the context of a fixed $\\Lambda$CDM model and\ndemonstrate that our method gives conservative estimates of the systematic\nuncertainty that nevertheless have little impact on the final posteriors\nobtained from DESI data."
                },
                "authors": [
                    {
                        "name": "N. Findlay"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "P. Zarrouk"
                    },
                    {
                        "name": "H. Gil-MarÃ­n"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "J. Mena-FernÃ¡ndez"
                    },
                    {
                        "name": "C. Garcia-Quintero"
                    },
                    {
                        "name": "A. Rocher"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "S. Cole"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "Arjun Dey"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "K. Fanning"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "E. GaztaÃ±aga"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "C. Hahn"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "M. E. Levi"
                    },
                    {
                        "name": "A. Meisner"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Moustakas"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "I. PÃ©rez-RÃ fols"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "H. Seo"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "G. TarlÃ©"
                    },
                    {
                        "name": "M. Vargas-MagaÃ±a"
                    },
                    {
                        "name": "B. A. Weaver"
                    }
                ],
                "author_detail": {
                    "name": "B. A. Weaver"
                },
                "author": "B. A. Weaver",
                "arxiv_comment": "This DESI Collaboration Publication is part of the 2024 publication\n  series using the first year of observations (see\n  https://data.desi.lbl.gov/doc/papers/). 26 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14103v1",
                "updated": "2024-11-21T13:09:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    9,
                    36,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:09:36Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    9,
                    36,
                    3,
                    326,
                    0
                ],
                "title": "Lost in Inference: Rediscovering the Role of Natural Language Inference\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Inference: Rediscovering the Role of Natural Language Inference\n  for Large Language Models"
                },
                "summary": "In the recent past, a popular way of evaluating natural language\nunderstanding (NLU), was to consider a model's ability to perform natural\nlanguage inference (NLI) tasks. In this paper, we investigate if NLI tasks,\nthat are rarely used for LLM evaluation, can still be informative for\nevaluating LLMs. Focusing on five different NLI benchmarks across six models of\ndifferent scales, we investigate if they are able to discriminate models of\ndifferent size and quality and how their accuracies develop during training.\nFurthermore, we investigate the extent to which the softmax distributions of\nmodels align with human distributions in cases where statements are ambiguous\nor vague. Overall, our results paint a positive picture for the NLI tasks: we\nfind that they are able to discriminate well between models at various stages\nof training, yet are not (all) saturated. Furthermore, we find that while the\nsimilarity of model distributions with human label distributions increases with\nscale, it is still much higher than the similarity between two populations of\nhumans, making it a potentially interesting statistic to consider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent past, a popular way of evaluating natural language\nunderstanding (NLU), was to consider a model's ability to perform natural\nlanguage inference (NLI) tasks. In this paper, we investigate if NLI tasks,\nthat are rarely used for LLM evaluation, can still be informative for\nevaluating LLMs. Focusing on five different NLI benchmarks across six models of\ndifferent scales, we investigate if they are able to discriminate models of\ndifferent size and quality and how their accuracies develop during training.\nFurthermore, we investigate the extent to which the softmax distributions of\nmodels align with human distributions in cases where statements are ambiguous\nor vague. Overall, our results paint a positive picture for the NLI tasks: we\nfind that they are able to discriminate well between models at various stages\nof training, yet are not (all) saturated. Furthermore, we find that while the\nsimilarity of model distributions with human label distributions increases with\nscale, it is still much higher than the similarity between two populations of\nhumans, making it a potentially interesting statistic to consider."
                },
                "authors": [
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "David Esiobu"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "arxiv_comment": "preprint, 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.04359v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.04359v3",
                "updated": "2024-11-21T13:06:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    6,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2022-10-09T22:02:58Z",
                "published_parsed": [
                    2022,
                    10,
                    9,
                    22,
                    2,
                    58,
                    6,
                    282,
                    0
                ],
                "title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates"
                },
                "summary": "Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks."
                },
                "authors": [
                    {
                        "name": "Aida Kostikova"
                    },
                    {
                        "name": "Benjamin Paassen"
                    },
                    {
                        "name": "Dominik Beese"
                    },
                    {
                        "name": "Ole PÃ¼tz"
                    },
                    {
                        "name": "Gregor Wiedemann"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "EMNLP 2024 (Main Conference) Camera-Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.04359v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.04359v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07753v2",
                "updated": "2024-11-21T12:47:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    47,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-10T09:29:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    29,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models"
                },
                "summary": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis"
                },
                "authors": [
                    {
                        "name": "Danush Kumar Venkatesh"
                    },
                    {
                        "name": "Dominik Rivoir"
                    },
                    {
                        "name": "Micha Pfeiffer"
                    },
                    {
                        "name": "Fiona Kolbinger"
                    },
                    {
                        "name": "Stefanie Speidel"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Speidel"
                },
                "author": "Stefanie Speidel",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17158v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17158v4",
                "updated": "2024-11-21T12:35:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    35,
                    18,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-27T13:31:46Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    13,
                    31,
                    46,
                    0,
                    148,
                    0
                ],
                "title": "PatchScaler: An Efficient Patch-Independent Diffusion Model for Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchScaler: An Efficient Patch-Independent Diffusion Model for Image\n  Super-Resolution"
                },
                "summary": "While diffusion models significantly improve the perceptual quality of\nsuper-resolved images, they usually require a large number of sampling steps,\nresulting in high computational costs and long inference times. Recent efforts\nhave explored reasonable acceleration schemes by reducing the number of\nsampling steps. However, these approaches treat all regions of the image\nequally, overlooking the fact that regions with varying levels of\nreconstruction difficulty require different sampling steps. To address this\nlimitation, we propose PatchScaler, an efficient patch-independent diffusion\npipeline for single image super-resolution. Specifically, PatchScaler\nintroduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature\npatches by quantifying their reconstruction difficulty and establishes shortcut\npaths with different sampling configurations for each group. To further\noptimize the patch-level reconstruction process of PGS, we propose a texture\nprompt that provides rich texture conditional information to the diffusion\nmodel. The texture prompt adaptively retrieves texture priors for the target\npatch from a common reference texture memory. Extensive experiments show that\nour PatchScaler achieves superior performance in both quantitative and\nqualitative evaluations, while significantly speeding up inference. Our code\nwill be available at \\url{https://github.com/yongliuy/PatchScaler}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion models significantly improve the perceptual quality of\nsuper-resolved images, they usually require a large number of sampling steps,\nresulting in high computational costs and long inference times. Recent efforts\nhave explored reasonable acceleration schemes by reducing the number of\nsampling steps. However, these approaches treat all regions of the image\nequally, overlooking the fact that regions with varying levels of\nreconstruction difficulty require different sampling steps. To address this\nlimitation, we propose PatchScaler, an efficient patch-independent diffusion\npipeline for single image super-resolution. Specifically, PatchScaler\nintroduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature\npatches by quantifying their reconstruction difficulty and establishes shortcut\npaths with different sampling configurations for each group. To further\noptimize the patch-level reconstruction process of PGS, we propose a texture\nprompt that provides rich texture conditional information to the diffusion\nmodel. The texture prompt adaptively retrieves texture priors for the target\npatch from a common reference texture memory. Extensive experiments show that\nour PatchScaler achieves superior performance in both quantitative and\nqualitative evaluations, while significantly speeding up inference. Our code\nwill be available at \\url{https://github.com/yongliuy/PatchScaler}."
                },
                "authors": [
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Hang Dong"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Qingji Dong"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Rongxiang Zhang"
                    },
                    {
                        "name": "Lean Fu"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17158v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17158v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14064v1",
                "updated": "2024-11-21T12:26:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    26,
                    33,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:26:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    26,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model"
                },
                "summary": "Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets."
                },
                "authors": [
                    {
                        "name": "Ege Kesim"
                    },
                    {
                        "name": "Selahattin Serdar Helli"
                    }
                ],
                "author_detail": {
                    "name": "Selahattin Serdar Helli"
                },
                "author": "Selahattin Serdar Helli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14729v2",
                "updated": "2024-11-21T12:17:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    17,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-16T07:13:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    7,
                    13,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP"
                },
                "summary": "Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency."
                },
                "authors": [
                    {
                        "name": "Zixin Wang"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Sen Wang"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yadan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yadan Luo"
                },
                "author": "Yadan Luo",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14057v1",
                "updated": "2024-11-21T12:09:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    9,
                    21,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:09:21Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    9,
                    21,
                    3,
                    326,
                    0
                ],
                "title": "Characterizing and Transforming DAGs within the I-LCA Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing and Transforming DAGs within the I-LCA Framework"
                },
                "summary": "We explore the connections between clusters and least common ancestors (LCAs)\nin directed acyclic graphs (DAGs), focusing on DAGs with unique LCAs for\nspecific subsets of their leaves. These DAGs are important in modeling\nphylogenetic networks that account for reticulate processes or horizontal gene\ntransfer. Phylogenetic DAGs inferred from genomic data are often complex,\nobscuring evolutionary insights, especially when vertices lack support as LCAs\nfor any subset of taxa. To address this, we focus on $I$-lca-relevant DAGs,\nwhere each vertex serves as the unique LCA for a subset $A$ of leaves of\nspecific size $|A|\\in I$. We characterize DAGs with the so-called\n$I$-lca-property and establish their close relationship to pre-$I$-ary and\n$I$-ary set systems. Moreover, we build upon recently established results that\nuse a simple operator $\\ominus$, enabling the transformation of arbitrary DAGs\ninto $I$-lca-relevant DAGs. This process reduces unnecessary complexity while\npreserving the key structural properties of the original DAG. The set $C_G$\nconsists of all clusters in a DAG $G$, where clusters correspond to the\ndescendant leaves of vertices. While in some cases $C_H = C_G$ when\ntransforming $G$ into an $I$-lca-relevant DAG $H$, it often happens that\ncertain clusters in $C_G$ do not appear as clusters in $H$. To understand this\nphenomenon in detail, we characterize the subset of clusters in $C_G$ that\nremain in $H$ for DAGs $G$ with the $I$-lca-property. Furthermore, we show that\nthe set $W$ of vertices required to transform $G$ into $H = G \\ominus W$ is\nuniquely determined for such DAGs. This, in turn, allows us to show that the\ntransformed DAG $H$ is always a tree or a galled-tree whenever $C_G$ represents\nthe clustering system of a tree or galled-tree and $G$ has the\n$I$-lca-property. In the latter case $C_H = C_G$ always holds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the connections between clusters and least common ancestors (LCAs)\nin directed acyclic graphs (DAGs), focusing on DAGs with unique LCAs for\nspecific subsets of their leaves. These DAGs are important in modeling\nphylogenetic networks that account for reticulate processes or horizontal gene\ntransfer. Phylogenetic DAGs inferred from genomic data are often complex,\nobscuring evolutionary insights, especially when vertices lack support as LCAs\nfor any subset of taxa. To address this, we focus on $I$-lca-relevant DAGs,\nwhere each vertex serves as the unique LCA for a subset $A$ of leaves of\nspecific size $|A|\\in I$. We characterize DAGs with the so-called\n$I$-lca-property and establish their close relationship to pre-$I$-ary and\n$I$-ary set systems. Moreover, we build upon recently established results that\nuse a simple operator $\\ominus$, enabling the transformation of arbitrary DAGs\ninto $I$-lca-relevant DAGs. This process reduces unnecessary complexity while\npreserving the key structural properties of the original DAG. The set $C_G$\nconsists of all clusters in a DAG $G$, where clusters correspond to the\ndescendant leaves of vertices. While in some cases $C_H = C_G$ when\ntransforming $G$ into an $I$-lca-relevant DAG $H$, it often happens that\ncertain clusters in $C_G$ do not appear as clusters in $H$. To understand this\nphenomenon in detail, we characterize the subset of clusters in $C_G$ that\nremain in $H$ for DAGs $G$ with the $I$-lca-property. Furthermore, we show that\nthe set $W$ of vertices required to transform $G$ into $H = G \\ominus W$ is\nuniquely determined for such DAGs. This, in turn, allows us to show that the\ntransformed DAG $H$ is always a tree or a galled-tree whenever $C_G$ represents\nthe clustering system of a tree or galled-tree and $G$ has the\n$I$-lca-property. In the latter case $C_H = C_G$ always holds."
                },
                "authors": [
                    {
                        "name": "Marc Hellmuth"
                    },
                    {
                        "name": "Anna Lindeberg"
                    }
                ],
                "author_detail": {
                    "name": "Anna Lindeberg"
                },
                "author": "Anna Lindeberg",
                "arxiv_comment": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2411.00708",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14055v1",
                "updated": "2024-11-21T12:02:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    2,
                    39,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:02:39Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    2,
                    39,
                    3,
                    326,
                    0
                ],
                "title": "DRPruning: Efficient Large Language Model Pruning through\n  Distributionally Robust Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRPruning: Efficient Large Language Model Pruning through\n  Distributionally Robust Optimization"
                },
                "summary": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\nwhich incorporates distributionally robust optimization to restore balanced\nperformance across domains, along with further improvements to enhance\nrobustness. Experiments in monolingual and multilingual settings show that our\nmethod surpasses similarly sized models in pruning and continued pretraining\nover perplexity, downstream tasks, and instruction tuning. We further provide\nanalysis demonstrating the robustness of our method towards various domains and\ndistribution shifts. Furthermore, our method automatically determines optimal\nreference losses and data ratios, suggesting potential for broader\napplications. Our code is available at https://github.com/hexuandeng/DRPruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\nwhich incorporates distributionally robust optimization to restore balanced\nperformance across domains, along with further improvements to enhance\nrobustness. Experiments in monolingual and multilingual settings show that our\nmethod surpasses similarly sized models in pruning and continued pretraining\nover perplexity, downstream tasks, and instruction tuning. We further provide\nanalysis demonstrating the robustness of our method towards various domains and\ndistribution shifts. Furthermore, our method automatically determines optimal\nreference losses and data ratios, suggesting potential for broader\napplications. Our code is available at https://github.com/hexuandeng/DRPruning."
                },
                "authors": [
                    {
                        "name": "Hexuan Deng"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v2",
                "updated": "2024-11-21T12:00:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    0,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 26 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12641v2",
                "updated": "2024-11-21T11:46:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    46,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-19T16:52:34Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "title": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models"
                },
                "summary": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ..."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixiao Zhang"
                },
                "author": "Yixiao Zhang",
                "arxiv_comment": "PhD Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14042v1",
                "updated": "2024-11-21T11:44:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    44,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:44:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    44,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling"
                },
                "summary": "Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction."
                },
                "authors": [
                    {
                        "name": "Daehoon Gwak"
                    },
                    {
                        "name": "Junwoo Park"
                    },
                    {
                        "name": "Minho Park"
                    },
                    {
                        "name": "Chaehun Park"
                    },
                    {
                        "name": "Hyunchan Lee"
                    },
                    {
                        "name": "Edward Choi"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14035v1",
                "updated": "2024-11-21T11:39:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    39,
                    9,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:39:09Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    39,
                    9,
                    3,
                    326,
                    0
                ],
                "title": "Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for\n  Efficient and Accurate Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for\n  Efficient and Accurate Inference"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results\nin various heterogeneous graph learning tasks, owing to their superiority in\ncapturing the intricate relationships and diverse relational semantics inherent\nin heterogeneous graph structures. However, the neighborhood-fetching latency\nincurred by structure dependency in HGNNs makes it challenging to deploy for\nlatency-constrained applications that require fast inference. Inspired by\nrecent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and\nHG2M+ to combine both HGNN's superior performance and MLP's efficient\ninference. HG2M directly trains student MLPs with node features as input and\nsoft labels from teacher HGNNs as targets, and HG2M+ further distills reliable\nand heterogeneous semantic knowledge into student MLPs through reliable node\ndistillation and reliable meta-path distillation. Experiments conducted on six\nheterogeneous graph datasets show that despite lacking structural dependencies,\nHG2Ms can still achieve competitive or even better performance than HGNNs and\nsignificantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a\n379.24$\\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19\ndataset, showcasing their ability for latency-sensitive deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results\nin various heterogeneous graph learning tasks, owing to their superiority in\ncapturing the intricate relationships and diverse relational semantics inherent\nin heterogeneous graph structures. However, the neighborhood-fetching latency\nincurred by structure dependency in HGNNs makes it challenging to deploy for\nlatency-constrained applications that require fast inference. Inspired by\nrecent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and\nHG2M+ to combine both HGNN's superior performance and MLP's efficient\ninference. HG2M directly trains student MLPs with node features as input and\nsoft labels from teacher HGNNs as targets, and HG2M+ further distills reliable\nand heterogeneous semantic knowledge into student MLPs through reliable node\ndistillation and reliable meta-path distillation. Experiments conducted on six\nheterogeneous graph datasets show that despite lacking structural dependencies,\nHG2Ms can still achieve competitive or even better performance than HGNNs and\nsignificantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a\n379.24$\\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19\ndataset, showcasing their ability for latency-sensitive deployments."
                },
                "authors": [
                    {
                        "name": "Yunhui Liu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Tieke He"
                    },
                    {
                        "name": "Jianhua Zhao"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14033v1",
                "updated": "2024-11-21T11:36:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    36,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:36:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    36,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Multi-LLM-Agent Systems: Techniques and Business Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM-Agent Systems: Techniques and Business Perspectives"
                },
                "summary": "In the era of (multi-modal) large language models, most operational processes\ncan be reformulated and reproduced using LLM agents. The LLM agents can\nperceive, control, and get feedback from the environment so as to accomplish\nthe given tasks in an autonomous manner. Besides the environment-interaction\nproperty, the LLM agents can call various external tools to ease the task\ncompletion process. The tools can be regarded as a predefined operational\nprocess with private or real-time knowledge that does not exist in the\nparameters of LLMs. As a natural trend of development, the tools for calling\nare becoming autonomous agents, thus the full intelligent system turns out to\nbe a multi-LLM-agent system (MLAS). This paper discusses the technical and\nbusiness landscapes of MLAS. Compared to the previous single-LLM-agent system,\na MLAS has the advantages of i) higher potential of task-solving performance,\nii) higher flexibility for system changing, iii) proprietary data preserving\nfor each participating entity, and iv) feasibility of monetization for each\nentity. To support the ecosystem of MLAS, we provide a preliminary version of\nsuch MLAS protocol considering technical requirements, data privacy, and\nbusiness incentives. As such, MLAS would be a practical solution to achieve\nartificial collective intelligence in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of (multi-modal) large language models, most operational processes\ncan be reformulated and reproduced using LLM agents. The LLM agents can\nperceive, control, and get feedback from the environment so as to accomplish\nthe given tasks in an autonomous manner. Besides the environment-interaction\nproperty, the LLM agents can call various external tools to ease the task\ncompletion process. The tools can be regarded as a predefined operational\nprocess with private or real-time knowledge that does not exist in the\nparameters of LLMs. As a natural trend of development, the tools for calling\nare becoming autonomous agents, thus the full intelligent system turns out to\nbe a multi-LLM-agent system (MLAS). This paper discusses the technical and\nbusiness landscapes of MLAS. Compared to the previous single-LLM-agent system,\na MLAS has the advantages of i) higher potential of task-solving performance,\nii) higher flexibility for system changing, iii) proprietary data preserving\nfor each participating entity, and iv) feasibility of monetization for each\nentity. To support the ecosystem of MLAS, we provide a preliminary version of\nsuch MLAS protocol considering technical requirements, data privacy, and\nbusiness incentives. As such, MLAS would be a practical solution to achieve\nartificial collective intelligence in the near future."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Qiuying Peng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v2",
                "updated": "2024-11-21T11:27:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    27,
                    34,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agents Social Interaction Simulations on One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agents Social Interaction Simulations on One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14012v1",
                "updated": "2024-11-21T10:54:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:54:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Logic Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Augmented Generation"
                },
                "summary": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results."
                },
                "authors": [
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15145v3",
                "updated": "2024-11-21T10:52:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    52,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-24T01:49:02Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    1,
                    49,
                    2,
                    4,
                    145,
                    0
                ],
                "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models"
                },
                "summary": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Damien Teney"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14010v1",
                "updated": "2024-11-21T10:46:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    46,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:46:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    46,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "Spectral domain likelihoods for Bayesian inference in time-varying\n  parameter models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral domain likelihoods for Bayesian inference in time-varying\n  parameter models"
                },
                "summary": "Inference for locally stationary processes is often based on some local\nWhittle-type approximation of the likelihood function defined in the frequency\ndomain. The main reasons for using such a likelihood approximation is that i)\nit has substantially lower computational cost and better scalability to long\ntime series compared to the time domain likelihood, particularly when used for\nBayesian inference via Markov Chain Monte Carlo (MCMC), ii) convenience when\nthe model itself is specified in the frequency domain, and iii) it provides\naccess to bootstrap and subsampling MCMC which exploits the asymptotic\nindependence of Fourier transformed data. Most of the existing literature\ncompares the asymptotic performance of the maximum likelihood estimator (MLE)\nfrom such frequency domain likelihood approximation with the exact time domain\nMLE. Our article uses three simulation studies to assess the finite-sample\naccuracy of several frequency domain likelihood functions when used to\napproximate the posterior distribution in time-varying parameter models. The\nmethods are illustrated on an application to egg price data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for locally stationary processes is often based on some local\nWhittle-type approximation of the likelihood function defined in the frequency\ndomain. The main reasons for using such a likelihood approximation is that i)\nit has substantially lower computational cost and better scalability to long\ntime series compared to the time domain likelihood, particularly when used for\nBayesian inference via Markov Chain Monte Carlo (MCMC), ii) convenience when\nthe model itself is specified in the frequency domain, and iii) it provides\naccess to bootstrap and subsampling MCMC which exploits the asymptotic\nindependence of Fourier transformed data. Most of the existing literature\ncompares the asymptotic performance of the maximum likelihood estimator (MLE)\nfrom such frequency domain likelihood approximation with the exact time domain\nMLE. Our article uses three simulation studies to assess the finite-sample\naccuracy of several frequency domain likelihood functions when used to\napproximate the posterior distribution in time-varying parameter models. The\nmethods are illustrated on an application to egg price data."
                },
                "authors": [
                    {
                        "name": "Oskar Gustafsson"
                    },
                    {
                        "name": "Mattias Villani"
                    },
                    {
                        "name": "Robert Kohn"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kohn"
                },
                "author": "Robert Kohn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14009v1",
                "updated": "2024-11-21T10:45:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    45,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:45:44Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    45,
                    44,
                    3,
                    326,
                    0
                ],
                "title": "GPT versus Humans: Uncovering Ethical Concerns in Conversational\n  Generative AI-empowered Multi-Robot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT versus Humans: Uncovering Ethical Concerns in Conversational\n  Generative AI-empowered Multi-Robot Systems"
                },
                "summary": "The emergence of generative artificial intelligence (GAI) and large language\nmodels (LLMs) such ChatGPT has enabled the realization of long-harbored desires\nin software and robotic development. The technology however, has brought with\nit novel ethical challenges. These challenges are compounded by the application\nof LLMs in other machine learning systems, such as multi-robot systems. The\nobjectives of the study were to examine novel ethical issues arising from the\napplication of LLMs in multi-robot systems. Unfolding ethical issues in GPT\nagent behavior (deliberation of ethical concerns) was observed, and GPT output\nwas compared with human experts. The article also advances a model for ethical\ndevelopment of multi-robot systems. A qualitative workshop-based method was\nemployed in three workshops for the collection of ethical concerns: two human\nexpert workshops (N=16 participants) and one GPT-agent-based workshop (N=7\nagents; two teams of 6 agents plus one judge). Thematic analysis was used to\nanalyze the qualitative data. The results reveal differences between the\nhuman-produced and GPT-based ethical concerns. Human experts placed greater\nemphasis on new themes related to deviance, data privacy, bias and unethical\ncorporate conduct. GPT agents emphasized concerns present in existing AI ethics\nguidelines. The study contributes to a growing body of knowledge in\ncontext-specific AI ethics and GPT application. It demonstrates the gap between\nhuman expert thinking and LLM output, while emphasizing new ethical concerns\nemerging in novel technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of generative artificial intelligence (GAI) and large language\nmodels (LLMs) such ChatGPT has enabled the realization of long-harbored desires\nin software and robotic development. The technology however, has brought with\nit novel ethical challenges. These challenges are compounded by the application\nof LLMs in other machine learning systems, such as multi-robot systems. The\nobjectives of the study were to examine novel ethical issues arising from the\napplication of LLMs in multi-robot systems. Unfolding ethical issues in GPT\nagent behavior (deliberation of ethical concerns) was observed, and GPT output\nwas compared with human experts. The article also advances a model for ethical\ndevelopment of multi-robot systems. A qualitative workshop-based method was\nemployed in three workshops for the collection of ethical concerns: two human\nexpert workshops (N=16 participants) and one GPT-agent-based workshop (N=7\nagents; two teams of 6 agents plus one judge). Thematic analysis was used to\nanalyze the qualitative data. The results reveal differences between the\nhuman-produced and GPT-based ethical concerns. Human experts placed greater\nemphasis on new themes related to deviance, data privacy, bias and unethical\ncorporate conduct. GPT agents emphasized concerns present in existing AI ethics\nguidelines. The study contributes to a growing body of knowledge in\ncontext-specific AI ethics and GPT application. It demonstrates the gap between\nhuman expert thinking and LLM output, while emphasizing new ethical concerns\nemerging in novel technology."
                },
                "authors": [
                    {
                        "name": "Rebekah Rousi"
                    },
                    {
                        "name": "Niko Makitalo"
                    },
                    {
                        "name": "Hooman Samani"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jose Siqueira de Cerqueira"
                    },
                    {
                        "name": "Ville Vakkuri"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "51 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14003v1",
                "updated": "2024-11-21T10:37:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    37,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:37:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    37,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Generative Intervention Models for Causal Perturbation Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Intervention Models for Causal Perturbation Modeling"
                },
                "summary": "We consider the problem of predicting perturbation effects via causal models.\nIn many applications, it is a priori unknown which mechanisms of a system are\nmodified by an external perturbation, even though the features of the\nperturbation are available. For example, in genomics, some properties of a drug\nmay be known, but not their causal effects on the regulatory pathways of cells.\nWe propose a generative intervention model (GIM) that learns to map these\nperturbation features to distributions over atomic interventions in a\njointly-estimated causal model. Contrary to prior approaches, this enables us\nto predict the distribution shifts of unseen perturbation features while\ngaining insights about their mechanistic effects in the underlying\ndata-generating process. On synthetic data and scRNA-seq drug perturbation\ndata, GIMs achieve robust out-of-distribution predictions on par with\nunstructured approaches, while effectively inferring the underlying\nperturbation mechanisms, often better than other causal inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of predicting perturbation effects via causal models.\nIn many applications, it is a priori unknown which mechanisms of a system are\nmodified by an external perturbation, even though the features of the\nperturbation are available. For example, in genomics, some properties of a drug\nmay be known, but not their causal effects on the regulatory pathways of cells.\nWe propose a generative intervention model (GIM) that learns to map these\nperturbation features to distributions over atomic interventions in a\njointly-estimated causal model. Contrary to prior approaches, this enables us\nto predict the distribution shifts of unseen perturbation features while\ngaining insights about their mechanistic effects in the underlying\ndata-generating process. On synthetic data and scRNA-seq drug perturbation\ndata, GIMs achieve robust out-of-distribution predictions on par with\nunstructured approaches, while effectively inferring the underlying\nperturbation mechanisms, often better than other causal inference methods."
                },
                "authors": [
                    {
                        "name": "Nora Schneider"
                    },
                    {
                        "name": "Lars Lorch"
                    },
                    {
                        "name": "Niki Kilbertus"
                    },
                    {
                        "name": "Bernhard SchÃ¶lkopf"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14002v1",
                "updated": "2024-11-21T10:37:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    37,
                    54,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:37:54Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    37,
                    54,
                    3,
                    326,
                    0
                ],
                "title": "SEMPose: A Single End-to-end Network for Multi-object Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEMPose: A Single End-to-end Network for Multi-object Pose Estimation"
                },
                "summary": "In computer vision, estimating the six-degree-of-freedom pose from an RGB\nimage is a fundamental task. However, this task becomes highly challenging in\nmulti-object scenes. Currently, the best methods typically employ an indirect\nstrategy, which identifies 2D and 3D correspondences, and then solves with the\nPerspective-n-Points method. Yet, this approach cannot be trained end-to-end.\nDirect methods, on the other hand, suffer from lower accuracy due to challenges\nsuch as varying object sizes and occlusions. To address these issues, we\npropose SEMPose, an end-to-end multi-object pose estimation network. SEMPose\nutilizes a well-designed texture-shape guided feature pyramid network,\neffectively tackling the challenge of object size variations. Additionally, it\nemploys an iterative refinement head structure, progressively regressing\nrotation and translation separately to enhance estimation accuracy. During\ntraining, we alleviate the impact of occlusion by selecting positive samples\nfrom visible parts. Experimental results demonstrate that SEMPose can perform\ninference at 32 FPS without requiring inputs other than the RGB image. It can\naccurately estimate the poses of multiple objects in real time, with inference\ntime unaffected by the number of target objects. On the LM-O and YCB-V\ndatasets, our method outperforms other RGB-based single-model methods,\nachieving higher accuracy. Even when compared with multi-model methods and\napproaches that use additional refinement, our results remain competitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computer vision, estimating the six-degree-of-freedom pose from an RGB\nimage is a fundamental task. However, this task becomes highly challenging in\nmulti-object scenes. Currently, the best methods typically employ an indirect\nstrategy, which identifies 2D and 3D correspondences, and then solves with the\nPerspective-n-Points method. Yet, this approach cannot be trained end-to-end.\nDirect methods, on the other hand, suffer from lower accuracy due to challenges\nsuch as varying object sizes and occlusions. To address these issues, we\npropose SEMPose, an end-to-end multi-object pose estimation network. SEMPose\nutilizes a well-designed texture-shape guided feature pyramid network,\neffectively tackling the challenge of object size variations. Additionally, it\nemploys an iterative refinement head structure, progressively regressing\nrotation and translation separately to enhance estimation accuracy. During\ntraining, we alleviate the impact of occlusion by selecting positive samples\nfrom visible parts. Experimental results demonstrate that SEMPose can perform\ninference at 32 FPS without requiring inputs other than the RGB image. It can\naccurately estimate the poses of multiple objects in real time, with inference\ntime unaffected by the number of target objects. On the LM-O and YCB-V\ndatasets, our method outperforms other RGB-based single-model methods,\nachieving higher accuracy. Even when compared with multi-model methods and\napproaches that use additional refinement, our results remain competitive."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shibei Xue"
                    },
                    {
                        "name": "Dezong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dezong Zhao"
                },
                "author": "Dezong Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v1",
                "updated": "2024-11-21T10:00:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xing Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13361v2",
                "updated": "2024-11-21T09:51:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    9,
                    51,
                    22,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T14:35:16Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    35,
                    16,
                    2,
                    325,
                    0
                ],
                "title": "Integration of Active Learning and MCMC Sampling for Efficient Bayesian\n  Calibration of Mechanical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Active Learning and MCMC Sampling for Efficient Bayesian\n  Calibration of Mechanical Properties"
                },
                "summary": "Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate\nmodelling have significantly enhanced the feasibility of Bayesian analysis\nacross engineering fields. However, the selection and integration of surrogate\nmodels and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A\nsystematic assessment of their combined influence on analytical accuracy and\nefficiency is notably lacking. The present work offers a comprehensive\ncomparative study, employing a scalable case study in computational mechanics\nfocused on the inference of spatially varying material parameters, that sheds\nlight on the impact of methodological choices for surrogate modelling and\nsampling. We show that a priori training of the surrogate model introduces\nlarge errors in the posterior estimation even in low to moderate dimensions. We\nintroduce a simple active learning strategy based on the path of the MCMC\nalgorithm that is superior to all a priori trained models, and determine its\ntraining data requirements. We demonstrate that the choice of the MCMC\nalgorithm has only a small influence on the amount of training data but no\nsignificant influence on the accuracy of the resulting surrogate model.\nFurther, we show that the accuracy of the posterior estimation largely depends\non the surrogate model, but not even a tailored surrogate guarantees\nconvergence of the MCMC.Finally, we identify the forward model as the\nbottleneck in the inference process, not the MCMC algorithm. While related\nworks focus on employing advanced MCMC algorithms, we demonstrate that the\ntraining data requirements render the surrogate modelling approach infeasible\nbefore the benefits of these gradient-based MCMC algorithms on cheap models can\nbe reaped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate\nmodelling have significantly enhanced the feasibility of Bayesian analysis\nacross engineering fields. However, the selection and integration of surrogate\nmodels and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A\nsystematic assessment of their combined influence on analytical accuracy and\nefficiency is notably lacking. The present work offers a comprehensive\ncomparative study, employing a scalable case study in computational mechanics\nfocused on the inference of spatially varying material parameters, that sheds\nlight on the impact of methodological choices for surrogate modelling and\nsampling. We show that a priori training of the surrogate model introduces\nlarge errors in the posterior estimation even in low to moderate dimensions. We\nintroduce a simple active learning strategy based on the path of the MCMC\nalgorithm that is superior to all a priori trained models, and determine its\ntraining data requirements. We demonstrate that the choice of the MCMC\nalgorithm has only a small influence on the amount of training data but no\nsignificant influence on the accuracy of the resulting surrogate model.\nFurther, we show that the accuracy of the posterior estimation largely depends\non the surrogate model, but not even a tailored surrogate guarantees\nconvergence of the MCMC.Finally, we identify the forward model as the\nbottleneck in the inference process, not the MCMC algorithm. While related\nworks focus on employing advanced MCMC algorithms, we demonstrate that the\ntraining data requirements render the surrogate modelling approach infeasible\nbefore the benefits of these gradient-based MCMC algorithms on cheap models can\nbe reaped."
                },
                "authors": [
                    {
                        "name": "Leon Riccius"
                    },
                    {
                        "name": "Iuri B. C. M. Rocha"
                    },
                    {
                        "name": "Joris Bierkens"
                    },
                    {
                        "name": "Hanne Kekkonen"
                    },
                    {
                        "name": "Frans P. van der Meer"
                    }
                ],
                "author_detail": {
                    "name": "Frans P. van der Meer"
                },
                "author": "Frans P. van der Meer",
                "arxiv_comment": "28 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "74S60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00774v3",
                "updated": "2024-11-21T09:19:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    9,
                    19,
                    28,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-01T17:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    59,
                    51,
                    4,
                    306,
                    0
                ],
                "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM"
                },
                "summary": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources."
                },
                "authors": [
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Long Ma"
                    }
                ],
                "author_detail": {
                    "name": "Long Ma"
                },
                "author": "Long Ma",
                "arxiv_comment": "Project Page: https://freeze-omni.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13960v1",
                "updated": "2024-11-21T09:15:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    9,
                    15,
                    51,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T09:15:51Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    9,
                    15,
                    51,
                    3,
                    326,
                    0
                ],
                "title": "Learning the Universe: Cosmological and Astrophysical Parameter\n  Inference with Galaxy Luminosity Functions and Colours",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Universe: Cosmological and Astrophysical Parameter\n  Inference with Galaxy Luminosity Functions and Colours"
                },
                "summary": "We perform the first direct cosmological and astrophysical parameter\ninference from the combination of galaxy luminosity functions and colours using\na simulation based inference approach. Using the Synthesizer code we simulate\nthe dust attenuated ultraviolet--near infrared stellar emission from galaxies\nin thousands of cosmological hydrodynamic simulations from the CAMELS suite,\nincluding the Swift-EAGLE, Illustris-TNG, Simba & Astrid galaxy formation\nmodels. For each galaxy we calculate the rest-frame luminosity in a number of\nphotometric bands, including the SDSS $\\textit{ugriz}$ and GALEX FUV & NUV\nfilters; this dataset represents the largest catalogue of synthetic photometry\nbased on hydrodynamic galaxy formation simulations produced to date, totalling\n>200 million sources. From these we compile luminosity functions and colour\ndistributions, and find clear dependencies on both cosmology and feedback. We\nthen perform simulation based (likelihood-free) inference using these\ndistributions, and obtain constraints on both cosmological and astrophysical\nparameters. Both colour distributions and luminosity functions provide\ncomplementary information on certain parameters when performing inference. Most\ninterestingly we achieve constraints on $\\sigma_8$, describing the clustering\nof matter. This is attributable to the fact that the photometry encodes the\nstar formation--metal enrichment history of each galaxy; galaxies in a universe\nwith a higher $\\sigma_8$ tend to form earlier and have higher metallicities,\nwhich leads to redder colours. We find that a model trained on one galaxy\nformation simulation generalises poorly when applied to another, and attribute\nthis to differences in the subgrid prescriptions, and lack of flexibility in\nour emission modelling. The photometric catalogues are publicly available at:\nhttps://camels.readthedocs.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform the first direct cosmological and astrophysical parameter\ninference from the combination of galaxy luminosity functions and colours using\na simulation based inference approach. Using the Synthesizer code we simulate\nthe dust attenuated ultraviolet--near infrared stellar emission from galaxies\nin thousands of cosmological hydrodynamic simulations from the CAMELS suite,\nincluding the Swift-EAGLE, Illustris-TNG, Simba & Astrid galaxy formation\nmodels. For each galaxy we calculate the rest-frame luminosity in a number of\nphotometric bands, including the SDSS $\\textit{ugriz}$ and GALEX FUV & NUV\nfilters; this dataset represents the largest catalogue of synthetic photometry\nbased on hydrodynamic galaxy formation simulations produced to date, totalling\n>200 million sources. From these we compile luminosity functions and colour\ndistributions, and find clear dependencies on both cosmology and feedback. We\nthen perform simulation based (likelihood-free) inference using these\ndistributions, and obtain constraints on both cosmological and astrophysical\nparameters. Both colour distributions and luminosity functions provide\ncomplementary information on certain parameters when performing inference. Most\ninterestingly we achieve constraints on $\\sigma_8$, describing the clustering\nof matter. This is attributable to the fact that the photometry encodes the\nstar formation--metal enrichment history of each galaxy; galaxies in a universe\nwith a higher $\\sigma_8$ tend to form earlier and have higher metallicities,\nwhich leads to redder colours. We find that a model trained on one galaxy\nformation simulation generalises poorly when applied to another, and attribute\nthis to differences in the subgrid prescriptions, and lack of flexibility in\nour emission modelling. The photometric catalogues are publicly available at:\nhttps://camels.readthedocs.io/ ."
                },
                "authors": [
                    {
                        "name": "Christopher C. Lovell"
                    },
                    {
                        "name": "Tjitske Starkenburg"
                    },
                    {
                        "name": "Matthew Ho"
                    },
                    {
                        "name": "Daniel AnglÃ©s-AlcÃ¡zar"
                    },
                    {
                        "name": "Romeel DavÃ©"
                    },
                    {
                        "name": "Austen Gabrielpillai"
                    },
                    {
                        "name": "Kartheik Iyer"
                    },
                    {
                        "name": "Alice E. Matthews"
                    },
                    {
                        "name": "William J. Roper"
                    },
                    {
                        "name": "Rachel Somerville"
                    },
                    {
                        "name": "Laura Sommovigo"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Villaescusa-Navarro"
                },
                "author": "Francisco Villaescusa-Navarro",
                "arxiv_comment": "28 pages, 20 figures, submitted to MNRAS. Comments and feedback\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02611v2",
                "updated": "2024-11-21T08:57:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    57,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2023-12-05T09:39:04Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    9,
                    39,
                    4,
                    1,
                    339,
                    0
                ],
                "title": "Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets"
                },
                "summary": "Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value."
                },
                "authors": [
                    {
                        "name": "Shashi Raj Pandey"
                    },
                    {
                        "name": "Pierre Pinson"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_comment": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v2",
                "updated": "2024-11-21T08:56:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    56,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13941v1",
                "updated": "2024-11-21T08:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    49,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T08:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    49,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "LLMs as Continuous Learners: Improving the Reproduction of Defective\n  Code in Software Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Continuous Learners: Improving the Reproduction of Defective\n  Code in Software Issues"
                },
                "summary": "Reproducing buggy code is the first and crucially important step in issue\nresolving, as it aids in identifying the underlying problems and validating\nthat generated patches resolve the problem. While numerous approaches have been\nproposed for this task, they primarily address common, widespread errors and\nstruggle to adapt to unique, evolving errors specific to individual code\nrepositories. To fill this gap, we propose EvoCoder, a multi-agent continuous\nlearning framework for issue code reproduction. EvoCoder adopts a reflection\nmechanism that allows the LLM to continuously learn from previously resolved\nproblems and dynamically refine its strategies to new emerging challenges. To\nprevent experience bloating, EvoCoder introduces a novel hierarchical\nexperience pool that enables the model to adaptively update common and\nrepo-specific experiences. Our experimental results show a 20\\% improvement in\nissue reproduction rates over existing SOTA methods. Furthermore, integrating\nour reproduction mechanism significantly boosts the overall accuracy of the\nexisting issue-resolving pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing buggy code is the first and crucially important step in issue\nresolving, as it aids in identifying the underlying problems and validating\nthat generated patches resolve the problem. While numerous approaches have been\nproposed for this task, they primarily address common, widespread errors and\nstruggle to adapt to unique, evolving errors specific to individual code\nrepositories. To fill this gap, we propose EvoCoder, a multi-agent continuous\nlearning framework for issue code reproduction. EvoCoder adopts a reflection\nmechanism that allows the LLM to continuously learn from previously resolved\nproblems and dynamically refine its strategies to new emerging challenges. To\nprevent experience bloating, EvoCoder introduces a novel hierarchical\nexperience pool that enables the model to adaptively update common and\nrepo-specific experiences. Our experimental results show a 20\\% improvement in\nissue reproduction rates over existing SOTA methods. Furthermore, integrating\nour reproduction mechanism significantly boosts the overall accuracy of the\nexisting issue-resolving pipeline."
                },
                "authors": [
                    {
                        "name": "Yalan Lin"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17497v2",
                "updated": "2024-11-21T08:44:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    44,
                    20,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-27T13:22:51Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    13,
                    22,
                    51,
                    1,
                    58,
                    0
                ],
                "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering"
                },
                "summary": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR."
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Published on ACL Anthology:\n  https://aclanthology.org/2024.emnlp-main.321.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06519v2",
                "updated": "2024-11-21T08:31:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    31,
                    49,
                    3,
                    326,
                    0
                ],
                "published": "2024-07-09T03:21:39Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    3,
                    21,
                    39,
                    1,
                    191,
                    0
                ],
                "title": "F2PAD: A General Optimization Framework for Feature-Level to Pixel-Level\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2PAD: A General Optimization Framework for Feature-Level to Pixel-Level\n  Anomaly Detection"
                },
                "summary": "Image-based inspection systems have been widely deployed in manufacturing\nproduction lines. Due to the scarcity of defective samples, unsupervised\nanomaly detection that only leverages normal samples during training to detect\nvarious defects is popular. Existing feature-based methods, utilizing deep\nfeatures from pretrained neural networks, show their impressive performance in\nanomaly localization and the low demand for the sample size for training.\nHowever, the detected anomalous regions of these methods always exhibit\ninaccurate boundaries, which impedes the downstream tasks. This deficiency is\ncaused: (i) The decreased resolution of high-level features compared with the\noriginal image, and (ii) The mixture of adjacent normal and anomalous pixels\nduring feature extraction. To address them, we propose a novel unified\noptimization framework (F2PAD) that leverages the Feature-level information to\nguide the optimization process for Pixel-level Anomaly Detection in the\ninference stage. The proposed framework is universal and plug-and-play, which\ncan enhance various feature-based methods with limited assumptions. Case\nstudies are provided to demonstrate the effectiveness of our strategy,\nparticularly when applied to three popular backbone methods: PaDiM, CFLOW-AD,\nand PatchCore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-based inspection systems have been widely deployed in manufacturing\nproduction lines. Due to the scarcity of defective samples, unsupervised\nanomaly detection that only leverages normal samples during training to detect\nvarious defects is popular. Existing feature-based methods, utilizing deep\nfeatures from pretrained neural networks, show their impressive performance in\nanomaly localization and the low demand for the sample size for training.\nHowever, the detected anomalous regions of these methods always exhibit\ninaccurate boundaries, which impedes the downstream tasks. This deficiency is\ncaused: (i) The decreased resolution of high-level features compared with the\noriginal image, and (ii) The mixture of adjacent normal and anomalous pixels\nduring feature extraction. To address them, we propose a novel unified\noptimization framework (F2PAD) that leverages the Feature-level information to\nguide the optimization process for Pixel-level Anomaly Detection in the\ninference stage. The proposed framework is universal and plug-and-play, which\ncan enhance various feature-based methods with limited assumptions. Case\nstudies are provided to demonstrate the effectiveness of our strategy,\nparticularly when applied to three popular backbone methods: PaDiM, CFLOW-AD,\nand PatchCore."
                },
                "authors": [
                    {
                        "name": "Chengyu Tao"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Juan Du"
                    }
                ],
                "author_detail": {
                    "name": "Juan Du"
                },
                "author": "Juan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13932v1",
                "updated": "2024-11-21T08:28:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T08:28:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "XAgents: A Framework for Interpretable Rule-Based Multi-Agents\n  Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAgents: A Framework for Interpretable Rule-Based Multi-Agents\n  Cooperation"
                },
                "summary": "Extracting implicit knowledge and logical reasoning abilities from large\nlanguage models (LLMs) has consistently been a significant challenge. The\nadvancement of multi-agent systems has further en-hanced the capabilities of\nLLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the\nXAgents framework, an in-terpretable multi-agent cooperative framework based on\nthe IF-THEN rule-based system. The IF-Parts of the rules are responsible for\nlogical reasoning and domain membership calculation, while the THEN-Parts are\ncomprised of domain expert agents that generate domain-specific contents.\nFollowing the calculation of the member-ship, XAgetns transmits the task to the\ndisparate domain rules, which subsequently generate the various responses.\nThese re-sponses are analogous to the answers provided by different experts to\nthe same question. The final response is reached at by eliminat-ing the\nhallucinations and erroneous knowledge of the LLM through membership\ncomputation and semantic adversarial genera-tion of the various domain rules.\nThe incorporation of rule-based interpretability serves to bolster user\nconfidence in the XAgents framework. We evaluate the efficacy of XAgents\nthrough a com-parative analysis with the latest AutoAgents, in which XAgents\ndemonstrated superior performance across three distinct datasets. We perform\npost-hoc interpretable studies with SHAP algorithm and case studies, proving\nthe interpretability of XAgent in terms of input-output feature correlation and\nrule-based semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting implicit knowledge and logical reasoning abilities from large\nlanguage models (LLMs) has consistently been a significant challenge. The\nadvancement of multi-agent systems has further en-hanced the capabilities of\nLLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the\nXAgents framework, an in-terpretable multi-agent cooperative framework based on\nthe IF-THEN rule-based system. The IF-Parts of the rules are responsible for\nlogical reasoning and domain membership calculation, while the THEN-Parts are\ncomprised of domain expert agents that generate domain-specific contents.\nFollowing the calculation of the member-ship, XAgetns transmits the task to the\ndisparate domain rules, which subsequently generate the various responses.\nThese re-sponses are analogous to the answers provided by different experts to\nthe same question. The final response is reached at by eliminat-ing the\nhallucinations and erroneous knowledge of the LLM through membership\ncomputation and semantic adversarial genera-tion of the various domain rules.\nThe incorporation of rule-based interpretability serves to bolster user\nconfidence in the XAgents framework. We evaluate the efficacy of XAgents\nthrough a com-parative analysis with the latest AutoAgents, in which XAgents\ndemonstrated superior performance across three distinct datasets. We perform\npost-hoc interpretable studies with SHAP algorithm and case studies, proving\nthe interpretability of XAgent in terms of input-output feature correlation and\nrule-based semantics."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Mingxian Gu"
                    },
                    {
                        "name": "Renhuo Zhao"
                    },
                    {
                        "name": "Fuping Hu"
                    },
                    {
                        "name": "Zhaohong Deng"
                    },
                    {
                        "name": "Yitang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yitang Chen"
                },
                "author": "Yitang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13918v1",
                "updated": "2024-11-21T08:13:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    13,
                    24,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T08:13:24Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    13,
                    24,
                    3,
                    326,
                    0
                ],
                "title": "Quantization without Tears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization without Tears"
                },
                "summary": "Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms."
                },
                "authors": [
                    {
                        "name": "Minghao Fu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Ke Zhu"
                    },
                    {
                        "name": "Jianxin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wu"
                },
                "author": "Jianxin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01127v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01127v5",
                "updated": "2024-11-21T07:58:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    58,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-01-02T09:47:18Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    9,
                    47,
                    18,
                    1,
                    2,
                    0
                ],
                "title": "Wireless 6G Connectivity for Massive Number of Devices and Critical\n  Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless 6G Connectivity for Massive Number of Devices and Critical\n  Services"
                },
                "summary": "Compared to the generations up to 4G, whose main focus was on broadband and\ncoverage aspects, 5G has expanded the scope of wireless cellular systems\ntowards embracing two new types of connectivity: massive machine-type\ncommunication (mMTC) and ultra-reliable low-latency communications (URLLC).\nThis paper discusses the possible evolution of these two types of connectivity\nwithin the umbrella of 6G wireless systems. The paper consists of three parts.\nThe first part deals with the connectivity for a massive number of devices.\nWhile mMTC research in 5G predominantly focuses on the problem of uncoordinated\naccess in the uplink for a large number of devices, the traffic patterns in 6G\nmay become more symmetric, leading to closed-loop massive connectivity. One of\nthe drivers for this is distributed learning/inference. The second part of the\npaper discusses the evolution of wireless connectivity for critical services.\nWhile latency and reliability are tightly coupled in 5G, 6G will support a\nvariety of safety critical control applications with different types of timing\nrequirements, as evidenced by the emergence of metrics related to information\nfreshness and information value. Additionally, ensuring ultra-high reliability\nfor safety critical control applications requires modeling and estimation of\nthe tail statistics of the wireless channel, queue length, and delay. The\nfulfillment of these stringent requirements calls for the development of novel\nAI-based techniques, incorporating optimization theory, explainable AI,\ngenerative AI and digital twins. The third part analyzes the coexistence of\nmassive connectivity and critical services. We will consider scenarios in which\na massive number of devices need to support traffic patterns of mixed\ncriticality. This is followed by a discussion about the management of wireless\nresources shared by services with different criticality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to the generations up to 4G, whose main focus was on broadband and\ncoverage aspects, 5G has expanded the scope of wireless cellular systems\ntowards embracing two new types of connectivity: massive machine-type\ncommunication (mMTC) and ultra-reliable low-latency communications (URLLC).\nThis paper discusses the possible evolution of these two types of connectivity\nwithin the umbrella of 6G wireless systems. The paper consists of three parts.\nThe first part deals with the connectivity for a massive number of devices.\nWhile mMTC research in 5G predominantly focuses on the problem of uncoordinated\naccess in the uplink for a large number of devices, the traffic patterns in 6G\nmay become more symmetric, leading to closed-loop massive connectivity. One of\nthe drivers for this is distributed learning/inference. The second part of the\npaper discusses the evolution of wireless connectivity for critical services.\nWhile latency and reliability are tightly coupled in 5G, 6G will support a\nvariety of safety critical control applications with different types of timing\nrequirements, as evidenced by the emergence of metrics related to information\nfreshness and information value. Additionally, ensuring ultra-high reliability\nfor safety critical control applications requires modeling and estimation of\nthe tail statistics of the wireless channel, queue length, and delay. The\nfulfillment of these stringent requirements calls for the development of novel\nAI-based techniques, incorporating optimization theory, explainable AI,\ngenerative AI and digital twins. The third part analyzes the coexistence of\nmassive connectivity and critical services. We will consider scenarios in which\na massive number of devices need to support traffic patterns of mixed\ncriticality. This is followed by a discussion about the management of wireless\nresources shared by services with different criticality."
                },
                "authors": [
                    {
                        "name": "Anders E. KalÃ¸r"
                    },
                    {
                        "name": "Giuseppe Durisi"
                    },
                    {
                        "name": "Sinem Coleri"
                    },
                    {
                        "name": "Stefan Parkvall"
                    },
                    {
                        "name": "Wei Yu"
                    },
                    {
                        "name": "Andreas Mueller"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_doi": "10.1109/JPROC.2024.3484529",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JPROC.2024.3484529",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.01127v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01127v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to Proceedings of the IEEE. 19 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13909v1",
                "updated": "2024-11-21T07:47:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:47:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts"
                },
                "summary": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther."
                },
                "authors": [
                    {
                        "name": "Honglin Li"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Chenglu Zhu"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13904v1",
                "updated": "2024-11-21T07:30:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    30,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:30:02Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    30,
                    2,
                    3,
                    326,
                    0
                ],
                "title": "Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel\n  Planning"
                },
                "summary": "How are LLM-based agents used in the future? While many of the existing work\non agents has focused on improving the performance of a specific family of\nobjective and challenging tasks, in this work, we take a different perspective\nby thinking about full delegation: agents take over humans' routine\ndecision-making processes and are trusted by humans to find solutions that fit\npeople's personalized needs and are adaptive to ever-changing context. In order\nto achieve such a goal, the behavior of the agents, i.e., agentic behaviors,\nshould be evaluated not only on their achievements (i.e., outcome evaluation),\nbut also how they achieved that (i.e., procedure evaluation). For this, we\npropose APEC Agent Constitution, a list of criteria that an agent should follow\nfor good agentic behaviors, including Accuracy, Proactivity, Efficiency and\nCredibility. To verify whether APEC aligns with human preferences, we develop\nAPEC-Travel, a travel planning agent that proactively extracts hidden\npersonalized needs via multi-round dialog with travelers. APEC-Travel is\nconstructed purely from synthetic data generated by Llama3.1-405B-Instruct with\na diverse set of travelers' persona to simulate rich distribution of dialogs.\nIteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses\nbaselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores\nacross the constitution axes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How are LLM-based agents used in the future? While many of the existing work\non agents has focused on improving the performance of a specific family of\nobjective and challenging tasks, in this work, we take a different perspective\nby thinking about full delegation: agents take over humans' routine\ndecision-making processes and are trusted by humans to find solutions that fit\npeople's personalized needs and are adaptive to ever-changing context. In order\nto achieve such a goal, the behavior of the agents, i.e., agentic behaviors,\nshould be evaluated not only on their achievements (i.e., outcome evaluation),\nbut also how they achieved that (i.e., procedure evaluation). For this, we\npropose APEC Agent Constitution, a list of criteria that an agent should follow\nfor good agentic behaviors, including Accuracy, Proactivity, Efficiency and\nCredibility. To verify whether APEC aligns with human preferences, we develop\nAPEC-Travel, a travel planning agent that proactively extracts hidden\npersonalized needs via multi-round dialog with travelers. APEC-Travel is\nconstructed purely from synthetic data generated by Llama3.1-405B-Instruct with\na diverse set of travelers' persona to simulate rich distribution of dialogs.\nIteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses\nbaselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores\nacross the constitution axes."
                },
                "authors": [
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Da JU"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Sasha Mitts"
                    },
                    {
                        "name": "Aaron Foss"
                    },
                    {
                        "name": "Justine T Kao"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13902v1",
                "updated": "2024-11-21T07:28:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    28,
                    7,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:28:07Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    28,
                    7,
                    3,
                    326,
                    0
                ],
                "title": "PIORS: Personalized Intelligent Outpatient Reception based on Large\n  Language Model with Multi-Agents Medical Scenario Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIORS: Personalized Intelligent Outpatient Reception based on Large\n  Language Model with Multi-Agents Medical Scenario Simulation"
                },
                "summary": "In China, receptionist nurses face overwhelming workloads in outpatient\nsettings, limiting their time and attention for each patient and ultimately\nreducing service quality. In this paper, we present the Personalized\nIntelligent Outpatient Reception System (PIORS). This system integrates an\nLLM-based reception nurse and a collaboration between LLM and hospital\ninformation system (HIS) into real outpatient reception setting, aiming to\ndeliver personalized, high-quality, and efficient reception services.\nAdditionally, to enhance the performance of LLMs in real-world healthcare\nscenarios, we propose a medical conversational data generation framework named\nService Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM\nto the real-world environments and PIORS settings. We evaluate the\neffectiveness of PIORS and SFMSS through automatic and human assessments\ninvolving 15 users and 15 clinical experts. The results demonstrate that\nPIORS-Nurse outperforms all baselines, including the current state-of-the-art\nmodel GPT-4o, and aligns with human preferences and clinical needs. Further\ndetails and demo can be found at https://github.com/FudanDISC/PIORS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In China, receptionist nurses face overwhelming workloads in outpatient\nsettings, limiting their time and attention for each patient and ultimately\nreducing service quality. In this paper, we present the Personalized\nIntelligent Outpatient Reception System (PIORS). This system integrates an\nLLM-based reception nurse and a collaboration between LLM and hospital\ninformation system (HIS) into real outpatient reception setting, aiming to\ndeliver personalized, high-quality, and efficient reception services.\nAdditionally, to enhance the performance of LLMs in real-world healthcare\nscenarios, we propose a medical conversational data generation framework named\nService Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM\nto the real-world environments and PIORS settings. We evaluate the\neffectiveness of PIORS and SFMSS through automatic and human assessments\ninvolving 15 users and 15 clinical experts. The results demonstrate that\nPIORS-Nurse outperforms all baselines, including the current state-of-the-art\nmodel GPT-4o, and aligns with human preferences and clinical needs. Further\ndetails and demo can be found at https://github.com/FudanDISC/PIORS"
                },
                "authors": [
                    {
                        "name": "Zhijie Bao"
                    },
                    {
                        "name": "Qingyun Liu"
                    },
                    {
                        "name": "Ying Guo"
                    },
                    {
                        "name": "Zhengqiang Ye"
                    },
                    {
                        "name": "Jun Shen"
                    },
                    {
                        "name": "Shirong Xie"
                    },
                    {
                        "name": "Jiajie Peng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13280v2",
                "updated": "2024-11-21T07:23:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    23,
                    37,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T12:48:29Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    48,
                    29,
                    2,
                    325,
                    0
                ],
                "title": "Structure-Based Molecule Optimization via Gradient-Guided Bayesian\n  Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-Based Molecule Optimization via Gradient-Guided Bayesian\n  Update"
                },
                "summary": "Structure-based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. Our\nproposed MolJO achieves state-of-the-art performance on CrossDocked2020\nbenchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x\nimprovement in Success Rate compared to the gradient-based counterpart, and 2x\n\"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a\nwide range of optimization settings, including multi-objective optimization and\nchallenging tasks in drug design such as R-group optimization and scaffold\nhopping, further underscoring its versatility and potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. Our\nproposed MolJO achieves state-of-the-art performance on CrossDocked2020\nbenchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x\nimprovement in Success Rate compared to the gradient-based counterpart, and 2x\n\"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a\nwide range of optimization settings, including multi-objective optimization and\nchallenging tasks in drug design such as R-group optimization and scaffold\nhopping, further underscoring its versatility and potential."
                },
                "authors": [
                    {
                        "name": "Keyue Qiu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Hongbo Ma"
                    },
                    {
                        "name": "Ziyao Cao"
                    },
                    {
                        "name": "Zhilong Zhang"
                    },
                    {
                        "name": "Yushuai Wu"
                    },
                    {
                        "name": "Mingyue Zheng"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Ying Ma"
                },
                "author": "Wei-Ying Ma",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13899v1",
                "updated": "2024-11-21T07:21:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Schemato -- An LLM for Netlist-to-Schematic Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schemato -- An LLM for Netlist-to-Schematic Conversion"
                },
                "summary": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in the\ntwo settings of converting netlists to .asc files for LTSpice and LATEX files\nfor CircuiTikz schematics. Experiments on our circuit dataset show that\nSchemato achieves up to 93% compilation success rate for the netlist-to-LaTeX\nconversion task, surpassing the 26% rate scored by the state-of-the-art LLMs.\nFurthermore, our experiments show that Schemato generates schematics with a\nmean structural similarity index measure that is 3xhigher than the best\nperforming LLMs, therefore closer to the reference human design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in the\ntwo settings of converting netlists to .asc files for LTSpice and LATEX files\nfor CircuiTikz schematics. Experiments on our circuit dataset show that\nSchemato achieves up to 93% compilation success rate for the netlist-to-LaTeX\nconversion task, surpassing the 26% rate scored by the state-of-the-art LLMs.\nFurthermore, our experiments show that Schemato generates schematics with a\nmean structural similarity index measure that is 3xhigher than the best\nperforming LLMs, therefore closer to the reference human design."
                },
                "authors": [
                    {
                        "name": "Ryoga Matsuo"
                    },
                    {
                        "name": "Stefan Uhlich"
                    },
                    {
                        "name": "Arun Venkitaraman"
                    },
                    {
                        "name": "Andrea Bonetti"
                    },
                    {
                        "name": "Chia-Yu Hsieh"
                    },
                    {
                        "name": "Ali Momeni"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Augusto Capone"
                    },
                    {
                        "name": "Eisaku Ohbuchi"
                    },
                    {
                        "name": "Lorenzo Servadei"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Servadei"
                },
                "author": "Lorenzo Servadei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02884v2",
                "updated": "2024-11-21T07:07:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    7,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-03T18:12:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    12,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning"
                },
                "summary": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Jianbo Wu"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Tong Che"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Xiaoshui Huang"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04838v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04838v5",
                "updated": "2024-11-21T06:52:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    52,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-07T13:39:38Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    13,
                    39,
                    38,
                    2,
                    38,
                    0
                ],
                "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition"
                },
                "summary": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets."
                },
                "authors": [
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Yanjie Wang"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Brian Mac Namee"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "arxiv_comment": "Accepted to Neurips2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04838v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04838v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09747v3",
                "updated": "2024-11-21T06:46:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    46,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-13T06:53:58Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    6,
                    53,
                    58,
                    6,
                    287,
                    0
                ],
                "title": "t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving"
                },
                "summary": "Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations."
                },
                "authors": [
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Yuhang Qian"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08012v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08012v3",
                "updated": "2024-11-21T06:38:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    38,
                    54,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-12T09:04:12Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    9,
                    4,
                    12,
                    2,
                    164,
                    0
                ],
                "title": "Interaction of an outflow with surrounding gaseous clouds as the origin\n  of the late-time radio flares in TDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction of an outflow with surrounding gaseous clouds as the origin\n  of the late-time radio flares in TDEs"
                },
                "summary": "Close encounter between a star and a supermassive black hole (SMBH) results\nin the tidal disruption of the star, known as a tidal disruption event (TDE).\nRecently, a few TDEs, e.g., ASASSN-15oi and AT2018hyz, have shown late-time\n(hundreds of days after their UV/optical peaks) radio flares with radio\nluminosities of $10^{38\\sim39}$ erg/s. The super-Eddington fallback or\naccretion in a TDE may generate a mass outflow. Here we investigate a scenario\nthat the late-time radio flares come from the interaction of the outflow with\nthe circum-nuclear gaseous clouds, in addition to the slow-evolving emission\ncomponent due to the outflow-diffuse medium interaction. We calculate the\nassociated radio temporal and spectral signatures and find that they reproduce\nwell the observations. The outflows have the inferred velocity of\n0.2$c\\sim0.6$$c$, the total mass of $10^{-3}\\sim10^{-1}$ $\\mathrm{M_{\\odot}}$\nand the ejection duration of a month to a year. The distances of the clouds to\nthe SMBH are $0.1\\sim1$ pc. This scenario has advantages in explaining the long\ndelay, sharpness of the rise and the multiplicity of the late radio flares.\nFuture observations may build up a much larger sample of late-time radio flares\nand enable their use as a probe of the TDE physics and the host circumnuclear\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Close encounter between a star and a supermassive black hole (SMBH) results\nin the tidal disruption of the star, known as a tidal disruption event (TDE).\nRecently, a few TDEs, e.g., ASASSN-15oi and AT2018hyz, have shown late-time\n(hundreds of days after their UV/optical peaks) radio flares with radio\nluminosities of $10^{38\\sim39}$ erg/s. The super-Eddington fallback or\naccretion in a TDE may generate a mass outflow. Here we investigate a scenario\nthat the late-time radio flares come from the interaction of the outflow with\nthe circum-nuclear gaseous clouds, in addition to the slow-evolving emission\ncomponent due to the outflow-diffuse medium interaction. We calculate the\nassociated radio temporal and spectral signatures and find that they reproduce\nwell the observations. The outflows have the inferred velocity of\n0.2$c\\sim0.6$$c$, the total mass of $10^{-3}\\sim10^{-1}$ $\\mathrm{M_{\\odot}}$\nand the ejection duration of a month to a year. The distances of the clouds to\nthe SMBH are $0.1\\sim1$ pc. This scenario has advantages in explaining the long\ndelay, sharpness of the rise and the multiplicity of the late radio flares.\nFuture observations may build up a much larger sample of late-time radio flares\nand enable their use as a probe of the TDE physics and the host circumnuclear\nenvironment."
                },
                "authors": [
                    {
                        "name": "Jialun Zhuang"
                    },
                    {
                        "name": "Rong-Feng Shen"
                    },
                    {
                        "name": "Guobin Mou"
                    },
                    {
                        "name": "Wenbin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Lu"
                },
                "author": "Wenbin Lu",
                "arxiv_comment": "14 pages, 15 figures. Submitted to ApJ. A new version with some\n  modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08012v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08012v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13874v1",
                "updated": "2024-11-21T06:20:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    20,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T06:20:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    20,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Next-Generation Phishing: How LLM Agents Empower Cyber Attackers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Generation Phishing: How LLM Agents Empower Cyber Attackers"
                },
                "summary": "The escalating threat of phishing emails has become increasingly\nsophisticated with the rise of Large Language Models (LLMs). As attackers\nexploit LLMs to craft more convincing and evasive phishing emails, it is\ncrucial to assess the resilience of current phishing defenses. In this study we\nconduct a comprehensive evaluation of traditional phishing detectors, such as\nGmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine\nlearning models like SVM, Logistic Regression, and Naive Bayes, in identifying\nboth traditional and LLM-rephrased phishing emails. We also explore the\nemerging role of LLMs as phishing detection tools, a method already adopted by\ncompanies like NTT Security Holdings and JPMorgan Chase. Our results reveal\nnotable declines in detection accuracy for rephrased emails across all\ndetectors, highlighting critical weaknesses in current phishing defenses. As\nthe threat landscape evolves, our findings underscore the need for stronger\nsecurity controls and regulatory oversight on LLM-generated content to prevent\nits misuse in creating advanced phishing attacks. This study contributes to the\ndevelopment of more effective Cyber Threat Intelligence (CTI) by leveraging\nLLMs to generate diverse phishing variants that can be used for data\naugmentation, harnessing the power of LLMs to enhance phishing detection, and\npaving the way for more robust and adaptable threat detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating threat of phishing emails has become increasingly\nsophisticated with the rise of Large Language Models (LLMs). As attackers\nexploit LLMs to craft more convincing and evasive phishing emails, it is\ncrucial to assess the resilience of current phishing defenses. In this study we\nconduct a comprehensive evaluation of traditional phishing detectors, such as\nGmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine\nlearning models like SVM, Logistic Regression, and Naive Bayes, in identifying\nboth traditional and LLM-rephrased phishing emails. We also explore the\nemerging role of LLMs as phishing detection tools, a method already adopted by\ncompanies like NTT Security Holdings and JPMorgan Chase. Our results reveal\nnotable declines in detection accuracy for rephrased emails across all\ndetectors, highlighting critical weaknesses in current phishing defenses. As\nthe threat landscape evolves, our findings underscore the need for stronger\nsecurity controls and regulatory oversight on LLM-generated content to prevent\nits misuse in creating advanced phishing attacks. This study contributes to the\ndevelopment of more effective Cyber Threat Intelligence (CTI) by leveraging\nLLMs to generate diverse phishing variants that can be used for data\naugmentation, harnessing the power of LLMs to enhance phishing detection, and\npaving the way for more robust and adaptable threat detection systems."
                },
                "authors": [
                    {
                        "name": "Khalifa Afane"
                    },
                    {
                        "name": "Wenqi Wei"
                    },
                    {
                        "name": "Ying Mao"
                    },
                    {
                        "name": "Junaid Farooq"
                    },
                    {
                        "name": "Juntao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Chen"
                },
                "author": "Juntao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03163v2",
                "updated": "2024-11-21T06:18:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    18,
                    7,
                    3,
                    326,
                    0
                ],
                "published": "2024-03-05T17:56:27Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    17,
                    56,
                    27,
                    1,
                    65,
                    0
                ],
                "title": "Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering"
                },
                "summary": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs."
                },
                "authors": [
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13891v2",
                "updated": "2024-11-21T06:07:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    7,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-07-18T20:31:07Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    20,
                    31,
                    7,
                    3,
                    200,
                    0
                ],
                "title": "High Risk of Political Bias in Black Box Emotion Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Risk of Political Bias in Black Box Emotion Inference Models"
                },
                "summary": "This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts."
                },
                "authors": [
                    {
                        "name": "Hubert Plisiecki"
                    },
                    {
                        "name": "PaweÅ Lenartowicz"
                    },
                    {
                        "name": "Maria Flakus"
                    },
                    {
                        "name": "Artur Pokropek"
                    }
                ],
                "author_detail": {
                    "name": "Artur Pokropek"
                },
                "author": "Artur Pokropek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.14432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14432v1",
                "updated": "2024-11-21T18:59:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:59:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks."
                },
                "authors": [
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14398v1",
                "updated": "2024-11-21T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings"
                },
                "summary": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark."
                },
                "authors": [
                    {
                        "name": "Aaron Zheng"
                    },
                    {
                        "name": "Mansi Rana"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "To appear in Proceedings of COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10079v3",
                "updated": "2024-11-21T17:58:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    58,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-14T14:35:58Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    35,
                    58,
                    4,
                    166,
                    0
                ],
                "title": "Localizing Events in Videos with Multimodal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Events in Videos with Multimodal Queries"
                },
                "summary": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization."
                },
                "authors": [
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Mang Ling Ada Fok"
                    },
                    {
                        "name": "Jialu Ma"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Daniel Cremers"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Jindong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Gu"
                },
                "author": "Jindong Gu",
                "arxiv_comment": "20 pages (including references and appendix); for the project\n  homepage, see https://icq-benchmark.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14512v2",
                "updated": "2024-11-21T17:48:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    48,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-08-25T04:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    32,
                    45,
                    6,
                    238,
                    0
                ],
                "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings"
                },
                "summary": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
                },
                "authors": [
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Fengzhi Li"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14343v1",
                "updated": "2024-11-21T17:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:41:08Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl."
                },
                "authors": [
                    {
                        "name": "Bethel Melesse Tessema"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Tae-Sun Chung"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Sun Chung"
                },
                "arxiv_affiliation": "Ajou University",
                "author": "Tae-Sun Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14303v1",
                "updated": "2024-11-21T16:56:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:56:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "Automated Generation of Code Debugging Exercises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Generation of Code Debugging Exercises"
                },
                "summary": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
                },
                "authors": [
                    {
                        "name": "Victor-Alexandru PÄdurean"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "Preprint of the SIGCSE'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v1",
                "updated": "2024-11-21T16:50:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams"
                },
                "summary": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13009v2",
                "updated": "2024-11-21T16:49:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    49,
                    51,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T03:17:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts"
                },
                "summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
                },
                "authors": [
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v2",
                "updated": "2024-11-21T16:43:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    43,
                    6,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_comment": "9 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14283v1",
                "updated": "2024-11-21T16:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    34,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:34:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    34,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "CAIP: Detecting Router Misconfigurations with Context-Aware Iterative\n  Prompting of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAIP: Detecting Router Misconfigurations with Context-Aware Iterative\n  Prompting of LLMs"
                },
                "summary": "Model checkers and consistency checkers detect critical errors in router\nconfigurations, but these tools require significant manual effort to develop\nand maintain. LLM-based Q&A models have emerged as a promising alternative,\nallowing users to query partitions of configurations through prompts and\nreceive answers based on learned patterns, thanks to transformer models\npre-trained on vast datasets that provide generic configuration context for\ninterpreting router configurations. Yet, current methods of partition-based\nprompting often do not provide enough network-specific context from the actual\nconfigurations to enable accurate inference. We introduce a Context-Aware\nIterative Prompting (CAIP) framework that automates network-specific context\nextraction and optimizes LLM prompts for more precise router misconfiguration\ndetection. CAIP addresses three challenges: (1) efficiently mining relevant\ncontext from complex configuration files, (2) accurately distinguishing between\npre-defined and user-defined parameter values to prevent irrelevant context\nfrom being introduced, and (3) managing prompt context overload with iterative,\nguided interactions with the model. Our evaluations on synthetic and real-world\nconfigurations show that CAIP improves misconfiguration detection accuracy by\nmore than 30% compared to partition-based LLM approaches, model checkers, and\nconsistency checkers, uncovering over 20 previously undetected\nmisconfigurations in real-world configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model checkers and consistency checkers detect critical errors in router\nconfigurations, but these tools require significant manual effort to develop\nand maintain. LLM-based Q&A models have emerged as a promising alternative,\nallowing users to query partitions of configurations through prompts and\nreceive answers based on learned patterns, thanks to transformer models\npre-trained on vast datasets that provide generic configuration context for\ninterpreting router configurations. Yet, current methods of partition-based\nprompting often do not provide enough network-specific context from the actual\nconfigurations to enable accurate inference. We introduce a Context-Aware\nIterative Prompting (CAIP) framework that automates network-specific context\nextraction and optimizes LLM prompts for more precise router misconfiguration\ndetection. CAIP addresses three challenges: (1) efficiently mining relevant\ncontext from complex configuration files, (2) accurately distinguishing between\npre-defined and user-defined parameter values to prevent irrelevant context\nfrom being introduced, and (3) managing prompt context overload with iterative,\nguided interactions with the model. Our evaluations on synthetic and real-world\nconfigurations show that CAIP improves misconfiguration detection accuracy by\nmore than 30% compared to partition-based LLM approaches, model checkers, and\nconsistency checkers, uncovering over 20 previously undetected\nmisconfigurations in real-world configurations."
                },
                "authors": [
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Aaron Gember-Jacobson"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "arxiv_comment": "12 pages, 4 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14279v1",
                "updated": "2024-11-21T16:33:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    33,
                    30,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:33:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    33,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance"
                },
                "summary": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io)."
                },
                "authors": [
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Mingjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "19 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14272v1",
                "updated": "2024-11-21T16:28:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    32,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:28:32Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    32,
                    3,
                    326,
                    0
                ],
                "title": "Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models"
                },
                "summary": "The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction."
                },
                "authors": [
                    {
                        "name": "Iacopo Ghinassi"
                    },
                    {
                        "name": "Leonardo Catalano"
                    },
                    {
                        "name": "Tommaso Colella"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Colella"
                },
                "author": "Tommaso Colella",
                "arxiv_journal_ref": "Proceedings of the Third Workshop on NLP for Positive Impact\n  (2024) 123-139",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11585v3",
                "updated": "2024-11-21T16:28:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    28,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-03-18T08:58:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    8,
                    58,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines"
                },
                "summary": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields."
                },
                "authors": [
                    {
                        "name": "Ekaterina Trofimova"
                    },
                    {
                        "name": "Emil Sataev"
                    },
                    {
                        "name": "Andrey E. Ustyuzhanin"
                    }
                ],
                "author_detail": {
                    "name": "Andrey E. Ustyuzhanin"
                },
                "author": "Andrey E. Ustyuzhanin",
                "arxiv_doi": "10.7717/peerj-cs.2328",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.7717/peerj-cs.2328",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15368v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15368v4",
                "updated": "2024-11-21T16:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    19,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-23T15:02:44Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    15,
                    2,
                    44,
                    4,
                    54,
                    0
                ],
                "title": "Probabilistically Correct Language-based Multi-Robot Planning using\n  Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistically Correct Language-based Multi-Robot Planning using\n  Conformal Prediction"
                },
                "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guocheng He"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15368v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15368v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21271v2",
                "updated": "2024-11-21T16:12:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    12,
                    34,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-28T17:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation"
                },
                "summary": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements."
                },
                "authors": [
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Huck Yang"
                    },
                    {
                        "name": "Chien-Yi Wang"
                    },
                    {
                        "name": "Nai Chit Fung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Min-Hung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hung Chen"
                },
                "author": "Min-Hung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14258v1",
                "updated": "2024-11-21T16:09:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    9,
                    5,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:09:05Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    9,
                    5,
                    3,
                    326,
                    0
                ],
                "title": "Knowledge Graphs, Large Language Models, and Hallucinations: An NLP\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs, Large Language Models, and Hallucinations: An NLP\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) based applications including automated text generation, question\nanswering, chatbots, and others. However, they face a significant challenge:\nhallucinations, where models produce plausible-sounding but factually incorrect\nresponses. This undermines trust and limits the applicability of LLMs in\ndifferent domains. Knowledge Graphs (KGs), on the other hand, provide a\nstructured collection of interconnected facts represented as entities (nodes)\nand their relationships (edges). In recent research, KGs have been leveraged to\nprovide context that can fill gaps in an LLM understanding of certain topics\noffering a promising approach to mitigate hallucinations in LLMs, enhancing\ntheir reliability and accuracy while benefiting from their wide applicability.\nNonetheless, it is still a very active area of research with various unresolved\nopen problems. In this paper, we discuss these open challenges covering\nstate-of-the-art datasets and benchmarks as well as methods for knowledge\nintegration and evaluating hallucinations. In our discussion, we consider the\ncurrent use of KGs in LLM systems and identify future directions within each of\nthese challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) based applications including automated text generation, question\nanswering, chatbots, and others. However, they face a significant challenge:\nhallucinations, where models produce plausible-sounding but factually incorrect\nresponses. This undermines trust and limits the applicability of LLMs in\ndifferent domains. Knowledge Graphs (KGs), on the other hand, provide a\nstructured collection of interconnected facts represented as entities (nodes)\nand their relationships (edges). In recent research, KGs have been leveraged to\nprovide context that can fill gaps in an LLM understanding of certain topics\noffering a promising approach to mitigate hallucinations in LLMs, enhancing\ntheir reliability and accuracy while benefiting from their wide applicability.\nNonetheless, it is still a very active area of research with various unresolved\nopen problems. In this paper, we discuss these open challenges covering\nstate-of-the-art datasets and benchmarks as well as methods for knowledge\nintegration and evaluating hallucinations. In our discussion, we consider the\ncurrent use of KGs in LLM systems and identify future directions within each of\nthese challenges."
                },
                "authors": [
                    {
                        "name": "Ernests Lavrinovics"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Katja Hose"
                    }
                ],
                "author_detail": {
                    "name": "Katja Hose"
                },
                "author": "Katja Hose",
                "arxiv_comment": "7 pages, 2 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14256v1",
                "updated": "2024-11-21T16:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    4,
                    10,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:04:10Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    4,
                    10,
                    3,
                    326,
                    0
                ],
                "title": "Generalizing End-To-End Autonomous Driving In Real-World Environments\n  Using Zero-Shot LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing End-To-End Autonomous Driving In Real-World Environments\n  Using Zero-Shot LLMs"
                },
                "summary": "Traditional autonomous driving methods adopt a modular design, decomposing\ntasks into sub-tasks. In contrast, end-to-end autonomous driving directly\noutputs actions from raw sensor data, avoiding error accumulation. However,\ntraining an end-to-end model requires a comprehensive dataset; otherwise, the\nmodel exhibits poor generalization capabilities. Recently, large language\nmodels (LLMs) have been applied to enhance the generalization capabilities of\nend-to-end driving models. Most studies explore LLMs in an open-loop manner,\nwhere the output actions are compared to those of experts without direct\nfeedback from the real world, while others examine closed-loop results only in\nsimulations. This paper proposes an efficient architecture that integrates\nmultimodal LLMs into end-to-end driving models operating in closed-loop\nsettings in real-world environments. In our architecture, the LLM periodically\nprocesses raw sensor data to generate high-level driving instructions,\neffectively guiding the end-to-end model, even at a slower rate than the raw\nsensor data. This architecture relaxes the trade-off between the latency and\ninference quality of the LLM. It also allows us to choose from a wide variety\nof LLMs to improve high-level driving instructions and minimize fine-tuning\ncosts. Consequently, our architecture reduces data collection requirements\nbecause the LLMs do not directly output actions; we only need to train a simple\nimitation learning model to output actions. In our experiments, the training\ndata for the end-to-end model in a real-world environment consists of only\nsimple obstacle configurations with one traffic cone, while the test\nenvironment is more complex and contains multiple obstacles placed in various\npositions. Experiments show that the proposed architecture enhances the\ngeneralization capabilities of the end-to-end model even without fine-tuning\nthe LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional autonomous driving methods adopt a modular design, decomposing\ntasks into sub-tasks. In contrast, end-to-end autonomous driving directly\noutputs actions from raw sensor data, avoiding error accumulation. However,\ntraining an end-to-end model requires a comprehensive dataset; otherwise, the\nmodel exhibits poor generalization capabilities. Recently, large language\nmodels (LLMs) have been applied to enhance the generalization capabilities of\nend-to-end driving models. Most studies explore LLMs in an open-loop manner,\nwhere the output actions are compared to those of experts without direct\nfeedback from the real world, while others examine closed-loop results only in\nsimulations. This paper proposes an efficient architecture that integrates\nmultimodal LLMs into end-to-end driving models operating in closed-loop\nsettings in real-world environments. In our architecture, the LLM periodically\nprocesses raw sensor data to generate high-level driving instructions,\neffectively guiding the end-to-end model, even at a slower rate than the raw\nsensor data. This architecture relaxes the trade-off between the latency and\ninference quality of the LLM. It also allows us to choose from a wide variety\nof LLMs to improve high-level driving instructions and minimize fine-tuning\ncosts. Consequently, our architecture reduces data collection requirements\nbecause the LLMs do not directly output actions; we only need to train a simple\nimitation learning model to output actions. In our experiments, the training\ndata for the end-to-end model in a real-world environment consists of only\nsimple obstacle configurations with one traffic cone, while the test\nenvironment is more complex and contains multiple obstacles placed in various\npositions. Experiments show that the proposed architecture enhances the\ngeneralization capabilities of the end-to-end model even without fine-tuning\nthe LLM."
                },
                "authors": [
                    {
                        "name": "Zeyu Dong"
                    },
                    {
                        "name": "Yimin Zhu"
                    },
                    {
                        "name": "Yansong Li"
                    },
                    {
                        "name": "Kevin Mahon"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v1",
                "updated": "2024-11-21T15:59:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification"
                },
                "summary": "Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14251v1",
                "updated": "2024-11-21T15:57:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    57,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:57:02Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    57,
                    2,
                    3,
                    326,
                    0
                ],
                "title": "Natural Language Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL."
                },
                "authors": [
                    {
                        "name": "Xidong Feng"
                    },
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Haotian Fu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Extension of arXiv:2402.07157",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10446v2",
                "updated": "2024-11-21T15:56:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    56,
                    48,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-15T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    59,
                    51,
                    4,
                    320,
                    0
                ],
                "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Ekpo"
                    },
                    {
                        "name": "Mara Levy"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Chuong Huynh"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14228v1",
                "updated": "2024-11-21T15:37:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    37,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:37:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    37,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression"
                },
                "summary": "Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets."
                },
                "authors": [
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Sheng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Guo"
                },
                "author": "Sheng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14215v1",
                "updated": "2024-11-21T15:25:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    25,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:25:08Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    25,
                    8,
                    3,
                    326,
                    0
                ],
                "title": "Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models"
                },
                "summary": "LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities."
                },
                "authors": [
                    {
                        "name": "Martha Lewis"
                    },
                    {
                        "name": "Melanie Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Mitchell"
                },
                "author": "Melanie Mitchell",
                "arxiv_comment": "31 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2402.08955",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14214v1",
                "updated": "2024-11-21T15:24:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T15:24:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "Physics-Informed LLM-Agent for Automated Modulation Design in Power\n  Electronics Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed LLM-Agent for Automated Modulation Design in Power\n  Electronics Systems"
                },
                "summary": "LLM-based autonomous agents have demonstrated outstanding performance in\nsolving complex industrial tasks. However, in the pursuit of carbon neutrality\nand high-performance renewable energy systems, existing AI-assisted design\nautomation faces significant limitations in explainability, scalability, and\nusability. To address these challenges, we propose LP-COMDA, an LLM-based,\nphysics-informed autonomous agent that automates the modulation design of power\nconverters in Power Electronics Systems with minimal human supervision. Unlike\ntraditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that\ngathers and validates design specifications through a user-friendly chat\ninterface. The planner then coordinates with physics-informed design and\noptimization tools to iteratively generate and refine modulation designs\nautonomously. Through the chat interface, LP-COMDA provides an explainable\ndesign process, presenting explanations and charts. Experiments show that\nLP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error\ncompared to the second-best benchmark method in terms of standard mean absolute\nerror. Furthermore, empirical studies with 20 experts conclude that design time\nwith LP-COMDA is over 33 times faster than conventional methods, showing its\nsignificant improvement on design efficiency over the current processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents have demonstrated outstanding performance in\nsolving complex industrial tasks. However, in the pursuit of carbon neutrality\nand high-performance renewable energy systems, existing AI-assisted design\nautomation faces significant limitations in explainability, scalability, and\nusability. To address these challenges, we propose LP-COMDA, an LLM-based,\nphysics-informed autonomous agent that automates the modulation design of power\nconverters in Power Electronics Systems with minimal human supervision. Unlike\ntraditional AI-assisted approaches, LP-COMDA contains an LLM-based planner that\ngathers and validates design specifications through a user-friendly chat\ninterface. The planner then coordinates with physics-informed design and\noptimization tools to iteratively generate and refine modulation designs\nautonomously. Through the chat interface, LP-COMDA provides an explainable\ndesign process, presenting explanations and charts. Experiments show that\nLP-COMDA outperforms all baseline methods, achieving a 63.2% reduction in error\ncompared to the second-best benchmark method in terms of standard mean absolute\nerror. Furthermore, empirical studies with 20 experts conclude that design time\nwith LP-COMDA is over 33 times faster than conventional methods, showing its\nsignificant improvement on design efficiency over the current processes."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Fanfan Lin"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18097v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18097v3",
                "updated": "2024-11-21T14:23:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    23,
                    49,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-08T11:28:06Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    6,
                    1,
                    282,
                    0
                ],
                "title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine"
                },
                "summary": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries."
                },
                "authors": [
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Youngjune Lee"
                    },
                    {
                        "name": "Gyu-Hwung Cho"
                    },
                    {
                        "name": "Haeyu Jeong"
                    },
                    {
                        "name": "Jungmin Kong"
                    },
                    {
                        "name": "Saehun Kim"
                    },
                    {
                        "name": "Keunchan Park"
                    },
                    {
                        "name": "Sarah Cho"
                    },
                    {
                        "name": "Inchang Jeong"
                    },
                    {
                        "name": "Gyohee Nam"
                    },
                    {
                        "name": "Sunghoon Han"
                    },
                    {
                        "name": "Wonil Yang"
                    },
                    {
                        "name": "Jaeho Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Choi"
                },
                "author": "Jaeho Choi",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18097v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18097v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14164v1",
                "updated": "2024-11-21T14:22:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    22,
                    38,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:22:38Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    22,
                    38,
                    3,
                    326,
                    0
                ],
                "title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoPru: Focal Pruning for Efficient Large Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Xiaohua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Xu"
                },
                "author": "Xiaohua Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11340v2",
                "updated": "2024-11-21T14:09:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    9,
                    12,
                    3,
                    326,
                    0
                ],
                "published": "2024-09-17T16:42:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    42,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "OmniGen: Unified Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGen: Unified Image Generation"
                },
                "summary": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements."
                },
                "authors": [
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "Update the paper for OmniGen-v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14133v1",
                "updated": "2024-11-21T14:00:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T14:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    0,
                    1,
                    3,
                    326,
                    0
                ],
                "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs."
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "28 pages, 9 tables, 13 figures; under review at CVPR '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14121v1",
                "updated": "2024-11-21T13:45:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    45,
                    40,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:45:40Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    45,
                    40,
                    3,
                    326,
                    0
                ],
                "title": "Learning from \"Silly\" Questions Improves Large Language Models, But Only\n  Slightly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from \"Silly\" Questions Improves Large Language Models, But Only\n  Slightly"
                },
                "summary": "Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Shudong Liu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Takahiro Shinozaki"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "27 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14110v1",
                "updated": "2024-11-21T13:18:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:18:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\n  Generation Applications with Agent-based Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\n  Generation Applications with Agent-based Attacks"
                },
                "summary": "While large language models (LLMs) have achieved notable success in\ngenerative tasks, they still face limitations, such as lacking up-to-date\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\nenhances LLM performance by integrating external knowledge bases, providing\nadditional context which significantly improves accuracy and knowledge\ncoverage. However, building these external knowledge bases often requires\nsubstantial resources and may involve sensitive information. In this paper, we\npropose an agent-based automated privacy attack called RAG-Thief, which can\nextract a scalable amount of private data from the private database used in RAG\napplications. We conduct a systematic study on the privacy risks associated\nwith RAG applications, revealing that the vulnerability of LLMs makes the\nprivate knowledge bases suffer significant privacy risks. Unlike previous\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\nstarts with an initial adversarial query and learns from model responses,\nprogressively generating new queries to extract as many chunks from the\nknowledge base as possible. Experimental results show that our RAG-Thief can\nextract over 70% information from the private knowledge bases within customized\nRAG applications deployed on local machines and real-world platforms, including\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\nvulnerabilities in current RAG applications and underscore the pressing need\nfor stronger safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved notable success in\ngenerative tasks, they still face limitations, such as lacking up-to-date\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\nenhances LLM performance by integrating external knowledge bases, providing\nadditional context which significantly improves accuracy and knowledge\ncoverage. However, building these external knowledge bases often requires\nsubstantial resources and may involve sensitive information. In this paper, we\npropose an agent-based automated privacy attack called RAG-Thief, which can\nextract a scalable amount of private data from the private database used in RAG\napplications. We conduct a systematic study on the privacy risks associated\nwith RAG applications, revealing that the vulnerability of LLMs makes the\nprivate knowledge bases suffer significant privacy risks. Unlike previous\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\nstarts with an initial adversarial query and learns from model responses,\nprogressively generating new queries to extract as many chunks from the\nknowledge base as possible. Experimental results show that our RAG-Thief can\nextract over 70% information from the private knowledge bases within customized\nRAG applications deployed on local machines and real-world platforms, including\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\nvulnerabilities in current RAG applications and underscore the pressing need\nfor stronger safeguards."
                },
                "authors": [
                    {
                        "name": "Changyue Jiang"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Chenfu Bao"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14103v1",
                "updated": "2024-11-21T13:09:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    9,
                    36,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T13:09:36Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    9,
                    36,
                    3,
                    326,
                    0
                ],
                "title": "Lost in Inference: Rediscovering the Role of Natural Language Inference\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Inference: Rediscovering the Role of Natural Language Inference\n  for Large Language Models"
                },
                "summary": "In the recent past, a popular way of evaluating natural language\nunderstanding (NLU), was to consider a model's ability to perform natural\nlanguage inference (NLI) tasks. In this paper, we investigate if NLI tasks,\nthat are rarely used for LLM evaluation, can still be informative for\nevaluating LLMs. Focusing on five different NLI benchmarks across six models of\ndifferent scales, we investigate if they are able to discriminate models of\ndifferent size and quality and how their accuracies develop during training.\nFurthermore, we investigate the extent to which the softmax distributions of\nmodels align with human distributions in cases where statements are ambiguous\nor vague. Overall, our results paint a positive picture for the NLI tasks: we\nfind that they are able to discriminate well between models at various stages\nof training, yet are not (all) saturated. Furthermore, we find that while the\nsimilarity of model distributions with human label distributions increases with\nscale, it is still much higher than the similarity between two populations of\nhumans, making it a potentially interesting statistic to consider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent past, a popular way of evaluating natural language\nunderstanding (NLU), was to consider a model's ability to perform natural\nlanguage inference (NLI) tasks. In this paper, we investigate if NLI tasks,\nthat are rarely used for LLM evaluation, can still be informative for\nevaluating LLMs. Focusing on five different NLI benchmarks across six models of\ndifferent scales, we investigate if they are able to discriminate models of\ndifferent size and quality and how their accuracies develop during training.\nFurthermore, we investigate the extent to which the softmax distributions of\nmodels align with human distributions in cases where statements are ambiguous\nor vague. Overall, our results paint a positive picture for the NLI tasks: we\nfind that they are able to discriminate well between models at various stages\nof training, yet are not (all) saturated. Furthermore, we find that while the\nsimilarity of model distributions with human label distributions increases with\nscale, it is still much higher than the similarity between two populations of\nhumans, making it a potentially interesting statistic to consider."
                },
                "authors": [
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "David Esiobu"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "arxiv_comment": "preprint, 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.04359v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.04359v3",
                "updated": "2024-11-21T13:06:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    6,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2022-10-09T22:02:58Z",
                "published_parsed": [
                    2022,
                    10,
                    9,
                    22,
                    2,
                    58,
                    6,
                    282,
                    0
                ],
                "title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates"
                },
                "summary": "Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks."
                },
                "authors": [
                    {
                        "name": "Aida Kostikova"
                    },
                    {
                        "name": "Benjamin Paassen"
                    },
                    {
                        "name": "Dominik Beese"
                    },
                    {
                        "name": "Ole PÃ¼tz"
                    },
                    {
                        "name": "Gregor Wiedemann"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "EMNLP 2024 (Main Conference) Camera-Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.04359v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.04359v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14092v1",
                "updated": "2024-11-21T12:58:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    58,
                    9,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:58:09Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    58,
                    9,
                    3,
                    326,
                    0
                ],
                "title": "MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy\n  Navigation"
                },
                "summary": "Autonomous under-canopy navigation faces additional challenges compared to\nover-canopy settings - for example the tight spacing between the crop rows,\ndegraded GPS accuracy and excessive clutter. Keypoint-based visual navigation\nhas been shown to perform well in these conditions, however the differences\nbetween agricultural environments in terms of lighting, season, soil and crop\ntype mean that a domain shift will likely be encountered at some point of the\nrobot deployment. In this paper, we explore the use of Meta-Learning to\novercome this domain shift using a minimal amount of data. We train a\nbase-learner that can quickly adapt to new conditions, enabling more robust\nnavigation in low-data regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous under-canopy navigation faces additional challenges compared to\nover-canopy settings - for example the tight spacing between the crop rows,\ndegraded GPS accuracy and excessive clutter. Keypoint-based visual navigation\nhas been shown to perform well in these conditions, however the differences\nbetween agricultural environments in terms of lighting, season, soil and crop\ntype mean that a domain shift will likely be encountered at some point of the\nrobot deployment. In this paper, we explore the use of Meta-Learning to\novercome this domain shift using a minimal amount of data. We train a\nbase-learner that can quickly adapt to new conditions, enabling more robust\nnavigation in low-data regimes."
                },
                "authors": [
                    {
                        "name": "Thomas Woehrle"
                    },
                    {
                        "name": "Arun N. Sivakumar"
                    },
                    {
                        "name": "Naveen Uppalapati"
                    },
                    {
                        "name": "Girish Chowdhary"
                    }
                ],
                "author_detail": {
                    "name": "Girish Chowdhary"
                },
                "author": "Girish Chowdhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14088v1",
                "updated": "2024-11-21T12:53:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    53,
                    39,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:53:39Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    53,
                    39,
                    3,
                    326,
                    0
                ],
                "title": "Channel Customization for Low-Complexity CSI Acquisition in\n  Multi-RIS-Assisted MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Customization for Low-Complexity CSI Acquisition in\n  Multi-RIS-Assisted MIMO Systems"
                },
                "summary": "The deployment of multiple reconfigurable intelligent surfaces (RISs)\nenhances the propagation environment by improving channel quality, but it also\ncomplicates channel estimation. Following the conventional wireless\ncommunication system design, which involves full channel state information\n(CSI) acquisition followed by RIS configuration, can reduce transmission\nefficiency due to substantial pilot overhead and computational complexity. This\nstudy introduces an innovative approach that integrates CSI acquisition and RIS\nconfiguration, leveraging the channel-altering capabilities of the RIS to\nreduce both the overhead and complexity of CSI acquisition. The focus is on\nmulti-RIS-assisted systems, featuring both direct and reflected propagation\npaths. By applying a fast-varying reflection sequence during RIS configuration\nfor channel training, the complex problem of channel estimation is decomposed\ninto simpler, independent tasks. These fast-varying reflections effectively\nisolate transmit signals from different paths, streamlining the CSI acquisition\nprocess for both uplink and downlink communications with reduced complexity. In\nuplink scenarios, a positioning-based algorithm derives partial CSI, informing\nthe adjustment of RIS parameters to create a sparse reflection channel,\nenabling precise reconstruction of the uplink channel. Downlink communication\nbenefits from this strategically tailored reflection channel, allowing\neffective CSI acquisition with fewer pilot signals. Simulation results\nhighlight the proposed methodology's ability to accurately reconstruct the\nreflection channel with minimal impact on the normalized mean square error\nwhile simultaneously enhancing spectral efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of multiple reconfigurable intelligent surfaces (RISs)\nenhances the propagation environment by improving channel quality, but it also\ncomplicates channel estimation. Following the conventional wireless\ncommunication system design, which involves full channel state information\n(CSI) acquisition followed by RIS configuration, can reduce transmission\nefficiency due to substantial pilot overhead and computational complexity. This\nstudy introduces an innovative approach that integrates CSI acquisition and RIS\nconfiguration, leveraging the channel-altering capabilities of the RIS to\nreduce both the overhead and complexity of CSI acquisition. The focus is on\nmulti-RIS-assisted systems, featuring both direct and reflected propagation\npaths. By applying a fast-varying reflection sequence during RIS configuration\nfor channel training, the complex problem of channel estimation is decomposed\ninto simpler, independent tasks. These fast-varying reflections effectively\nisolate transmit signals from different paths, streamlining the CSI acquisition\nprocess for both uplink and downlink communications with reduced complexity. In\nuplink scenarios, a positioning-based algorithm derives partial CSI, informing\nthe adjustment of RIS parameters to create a sparse reflection channel,\nenabling precise reconstruction of the uplink channel. Downlink communication\nbenefits from this strategically tailored reflection channel, allowing\neffective CSI acquisition with fewer pilot signals. Simulation results\nhighlight the proposed methodology's ability to accurately reconstruct the\nreflection channel with minimal impact on the normalized mean square error\nwhile simultaneously enhancing spectral efficiency."
                },
                "authors": [
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "Accepted by IEEE JSAC special issue on Next Generation Advanced\n  Transceiver Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07753v2",
                "updated": "2024-11-21T12:47:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    47,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-10T09:29:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    29,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models"
                },
                "summary": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis"
                },
                "authors": [
                    {
                        "name": "Danush Kumar Venkatesh"
                    },
                    {
                        "name": "Dominik Rivoir"
                    },
                    {
                        "name": "Micha Pfeiffer"
                    },
                    {
                        "name": "Fiona Kolbinger"
                    },
                    {
                        "name": "Stefanie Speidel"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Speidel"
                },
                "author": "Stefanie Speidel",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14064v1",
                "updated": "2024-11-21T12:26:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    26,
                    33,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:26:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    26,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model"
                },
                "summary": "Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets."
                },
                "authors": [
                    {
                        "name": "Ege Kesim"
                    },
                    {
                        "name": "Selahattin Serdar Helli"
                    }
                ],
                "author_detail": {
                    "name": "Selahattin Serdar Helli"
                },
                "author": "Selahattin Serdar Helli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14055v1",
                "updated": "2024-11-21T12:02:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    2,
                    39,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T12:02:39Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    2,
                    39,
                    3,
                    326,
                    0
                ],
                "title": "DRPruning: Efficient Large Language Model Pruning through\n  Distributionally Robust Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRPruning: Efficient Large Language Model Pruning through\n  Distributionally Robust Optimization"
                },
                "summary": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\nwhich incorporates distributionally robust optimization to restore balanced\nperformance across domains, along with further improvements to enhance\nrobustness. Experiments in monolingual and multilingual settings show that our\nmethod surpasses similarly sized models in pruning and continued pretraining\nover perplexity, downstream tasks, and instruction tuning. We further provide\nanalysis demonstrating the robustness of our method towards various domains and\ndistribution shifts. Furthermore, our method automatically determines optimal\nreference losses and data ratios, suggesting potential for broader\napplications. Our code is available at https://github.com/hexuandeng/DRPruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\nwhich incorporates distributionally robust optimization to restore balanced\nperformance across domains, along with further improvements to enhance\nrobustness. Experiments in monolingual and multilingual settings show that our\nmethod surpasses similarly sized models in pruning and continued pretraining\nover perplexity, downstream tasks, and instruction tuning. We further provide\nanalysis demonstrating the robustness of our method towards various domains and\ndistribution shifts. Furthermore, our method automatically determines optimal\nreference losses and data ratios, suggesting potential for broader\napplications. Our code is available at https://github.com/hexuandeng/DRPruning."
                },
                "authors": [
                    {
                        "name": "Hexuan Deng"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v2",
                "updated": "2024-11-21T12:00:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    12,
                    0,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 26 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14049v1",
                "updated": "2024-11-21T11:56:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    56,
                    32,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:56:32Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    56,
                    32,
                    3,
                    326,
                    0
                ],
                "title": "Out-Of-Distribution Detection with Diversification (Provably)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-Of-Distribution Detection with Diversification (Provably)"
                },
                "summary": "Out-of-distribution (OOD) detection is crucial for ensuring reliable\ndeployment of machine learning models. Recent advancements focus on utilizing\neasily accessible auxiliary outliers (e.g., data from the web or other\ndatasets) in training. However, we experimentally reveal that these methods\nstill struggle to generalize their detection capabilities to unknown OOD data,\ndue to the limited diversity of the auxiliary outliers collected. Therefore, we\nthoroughly examine this problem from the generalization perspective and\ndemonstrate that a more diverse set of auxiliary outliers is essential for\nenhancing the detection capabilities. However, in practice, it is difficult and\ncostly to collect sufficiently diverse auxiliary outlier data. Therefore, we\npropose a simple yet practical approach with a theoretical guarantee, termed\nDiversity-induced Mixup for OOD detection (diverseMix), which enhances the\ndiversity of auxiliary outlier set for training in an efficient way. Extensive\nexperiments show that diverseMix achieves superior performance on commonly used\nand recent challenging large-scale benchmarks, which further confirm the\nimportance of the diversity of auxiliary outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is crucial for ensuring reliable\ndeployment of machine learning models. Recent advancements focus on utilizing\neasily accessible auxiliary outliers (e.g., data from the web or other\ndatasets) in training. However, we experimentally reveal that these methods\nstill struggle to generalize their detection capabilities to unknown OOD data,\ndue to the limited diversity of the auxiliary outliers collected. Therefore, we\nthoroughly examine this problem from the generalization perspective and\ndemonstrate that a more diverse set of auxiliary outliers is essential for\nenhancing the detection capabilities. However, in practice, it is difficult and\ncostly to collect sufficiently diverse auxiliary outlier data. Therefore, we\npropose a simple yet practical approach with a theoretical guarantee, termed\nDiversity-induced Mixup for OOD detection (diverseMix), which enhances the\ndiversity of auxiliary outlier set for training in an efficient way. Extensive\nexperiments show that diverseMix achieves superior performance on commonly used\nand recent challenging large-scale benchmarks, which further confirm the\nimportance of the diversity of auxiliary outliers."
                },
                "authors": [
                    {
                        "name": "Haiyun Yao"
                    },
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Xi Peng"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12641v2",
                "updated": "2024-11-21T11:46:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    46,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-19T16:52:34Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    52,
                    34,
                    1,
                    324,
                    0
                ],
                "title": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Controllability and Editability for Pretrained Text-to-Music\n  Generation Models"
                },
                "summary": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of AI-assisted music creation has made significant strides, yet\nexisting systems often struggle to meet the demands of iterative and nuanced\nmusic production. These challenges include providing sufficient control over\nthe generated content and allowing for flexible, precise edits. This thesis\ntackles these issues by introducing a series of advancements that progressively\nbuild upon each other, enhancing the controllability and editability of\ntext-to-music generation models.\n  First, we introduce Loop Copilot, a system that tries to address the need for\niterative refinement in music creation. Loop Copilot leverages a large language\nmodel (LLM) to coordinate multiple specialised AI models, enabling users to\ngenerate and refine music interactively through a conversational interface.\nCentral to this system is the Global Attribute Table, which records and\nmaintains key musical attributes throughout the iterative process, ensuring\nthat modifications at any stage preserve the overall coherence of the music.\nWhile Loop Copilot excels in orchestrating the music creation process, it does\nnot directly address the need for detailed edits to the generated content.\n  To overcome this limitation, MusicMagus is presented as a further solution\nfor editing AI-generated music. MusicMagus introduces a zero-shot text-to-music\nediting approach that allows for the modification of specific musical\nattributes, such as genre, mood, and instrumentation, without the need for\nretraining. By manipulating the latent space within pre-trained diffusion\nmodels, MusicMagus ensures that these edits are stylistically coherent and that\nnon-targeted attributes remain unchanged. This system is particularly effective\nin maintaining the structural integrity of the music during edits, but it\nencounters challenges with more complex and real-world audio scenarios.\n  ..."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixiao Zhang"
                },
                "author": "Yixiao Zhang",
                "arxiv_comment": "PhD Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14042v1",
                "updated": "2024-11-21T11:44:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    44,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:44:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    44,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling"
                },
                "summary": "Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction."
                },
                "authors": [
                    {
                        "name": "Daehoon Gwak"
                    },
                    {
                        "name": "Junwoo Park"
                    },
                    {
                        "name": "Minho Park"
                    },
                    {
                        "name": "Chaehun Park"
                    },
                    {
                        "name": "Hyunchan Lee"
                    },
                    {
                        "name": "Edward Choi"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14035v1",
                "updated": "2024-11-21T11:39:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    39,
                    9,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:39:09Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    39,
                    9,
                    3,
                    326,
                    0
                ],
                "title": "Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for\n  Efficient and Accurate Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for\n  Efficient and Accurate Inference"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results\nin various heterogeneous graph learning tasks, owing to their superiority in\ncapturing the intricate relationships and diverse relational semantics inherent\nin heterogeneous graph structures. However, the neighborhood-fetching latency\nincurred by structure dependency in HGNNs makes it challenging to deploy for\nlatency-constrained applications that require fast inference. Inspired by\nrecent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and\nHG2M+ to combine both HGNN's superior performance and MLP's efficient\ninference. HG2M directly trains student MLPs with node features as input and\nsoft labels from teacher HGNNs as targets, and HG2M+ further distills reliable\nand heterogeneous semantic knowledge into student MLPs through reliable node\ndistillation and reliable meta-path distillation. Experiments conducted on six\nheterogeneous graph datasets show that despite lacking structural dependencies,\nHG2Ms can still achieve competitive or even better performance than HGNNs and\nsignificantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a\n379.24$\\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19\ndataset, showcasing their ability for latency-sensitive deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results\nin various heterogeneous graph learning tasks, owing to their superiority in\ncapturing the intricate relationships and diverse relational semantics inherent\nin heterogeneous graph structures. However, the neighborhood-fetching latency\nincurred by structure dependency in HGNNs makes it challenging to deploy for\nlatency-constrained applications that require fast inference. Inspired by\nrecent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and\nHG2M+ to combine both HGNN's superior performance and MLP's efficient\ninference. HG2M directly trains student MLPs with node features as input and\nsoft labels from teacher HGNNs as targets, and HG2M+ further distills reliable\nand heterogeneous semantic knowledge into student MLPs through reliable node\ndistillation and reliable meta-path distillation. Experiments conducted on six\nheterogeneous graph datasets show that despite lacking structural dependencies,\nHG2Ms can still achieve competitive or even better performance than HGNNs and\nsignificantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a\n379.24$\\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19\ndataset, showcasing their ability for latency-sensitive deployments."
                },
                "authors": [
                    {
                        "name": "Yunhui Liu"
                    },
                    {
                        "name": "Xinyi Gao"
                    },
                    {
                        "name": "Tieke He"
                    },
                    {
                        "name": "Jianhua Zhao"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14033v1",
                "updated": "2024-11-21T11:36:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    36,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:36:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    36,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Multi-LLM-Agent Systems: Techniques and Business Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM-Agent Systems: Techniques and Business Perspectives"
                },
                "summary": "In the era of (multi-modal) large language models, most operational processes\ncan be reformulated and reproduced using LLM agents. The LLM agents can\nperceive, control, and get feedback from the environment so as to accomplish\nthe given tasks in an autonomous manner. Besides the environment-interaction\nproperty, the LLM agents can call various external tools to ease the task\ncompletion process. The tools can be regarded as a predefined operational\nprocess with private or real-time knowledge that does not exist in the\nparameters of LLMs. As a natural trend of development, the tools for calling\nare becoming autonomous agents, thus the full intelligent system turns out to\nbe a multi-LLM-agent system (MLAS). This paper discusses the technical and\nbusiness landscapes of MLAS. Compared to the previous single-LLM-agent system,\na MLAS has the advantages of i) higher potential of task-solving performance,\nii) higher flexibility for system changing, iii) proprietary data preserving\nfor each participating entity, and iv) feasibility of monetization for each\nentity. To support the ecosystem of MLAS, we provide a preliminary version of\nsuch MLAS protocol considering technical requirements, data privacy, and\nbusiness incentives. As such, MLAS would be a practical solution to achieve\nartificial collective intelligence in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of (multi-modal) large language models, most operational processes\ncan be reformulated and reproduced using LLM agents. The LLM agents can\nperceive, control, and get feedback from the environment so as to accomplish\nthe given tasks in an autonomous manner. Besides the environment-interaction\nproperty, the LLM agents can call various external tools to ease the task\ncompletion process. The tools can be regarded as a predefined operational\nprocess with private or real-time knowledge that does not exist in the\nparameters of LLMs. As a natural trend of development, the tools for calling\nare becoming autonomous agents, thus the full intelligent system turns out to\nbe a multi-LLM-agent system (MLAS). This paper discusses the technical and\nbusiness landscapes of MLAS. Compared to the previous single-LLM-agent system,\na MLAS has the advantages of i) higher potential of task-solving performance,\nii) higher flexibility for system changing, iii) proprietary data preserving\nfor each participating entity, and iv) feasibility of monetization for each\nentity. To support the ecosystem of MLAS, we provide a preliminary version of\nsuch MLAS protocol considering technical requirements, data privacy, and\nbusiness incentives. As such, MLAS would be a practical solution to achieve\nartificial collective intelligence in the near future."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Qiuying Peng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14030v1",
                "updated": "2024-11-21T11:30:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    30,
                    24,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T11:30:24Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    30,
                    24,
                    3,
                    326,
                    0
                ],
                "title": "Performance Analysis of STAR-RIS-Assisted Cell-Free Massive MIMO Systems\n  with Electromagnetic Interference and Phase Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of STAR-RIS-Assisted Cell-Free Massive MIMO Systems\n  with Electromagnetic Interference and Phase Errors"
                },
                "summary": "Simultaneous Transmitting and Reflecting Reconfigurable Intelligent Surfaces\n(STAR-RISs) are being explored for the next generation of sixth-generation (6G)\nnetworks. A promising configuration for their deployment is within cell-free\nmassive multiple-input multiple-output (MIMO) systems. However, despite the\nadvantages that STAR-RISs could bring, challenges such as electromagnetic\ninterference (EMI) and phase errors may lead to significant performance\ndegradation. In this paper, we investigate the impact of EMI and phase errors\non STAR-RIS-assisted cell-free massive MIMO systems and propose techniques to\nmitigate these effects. We introduce a novel projected gradient descent (GD)\nalgorithm for STAR-RIS coefficient matrix design by minimizing the local\nchannel estimation normalised mean square error. We also derive the closed-form\nexpressions of the uplink and downlink spectral efficiency (SE) to analyze\nsystem performance with EMI and phase errors, in which fractional power control\nmethods are applied for performance improvement. The results reveal that the\nprojected GD algorithm can effectively tackle EMI and phase errors to improve\nestimation accuracy and compensate for performance degradation with nearly\n$10\\%\\sim20\\%$ SE improvement. Moreover, increasing access points (APs),\nantennas per AP, and STAR-RIS elements can also improve SE performance.\nApplying STAR-RIS in the proposed system achieves a larger $25\\%$-likely SE\nthan conventional RISs. However, the advantages of employing more STAR-RIS\nelements are reduced when EMI is severe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Transmitting and Reflecting Reconfigurable Intelligent Surfaces\n(STAR-RISs) are being explored for the next generation of sixth-generation (6G)\nnetworks. A promising configuration for their deployment is within cell-free\nmassive multiple-input multiple-output (MIMO) systems. However, despite the\nadvantages that STAR-RISs could bring, challenges such as electromagnetic\ninterference (EMI) and phase errors may lead to significant performance\ndegradation. In this paper, we investigate the impact of EMI and phase errors\non STAR-RIS-assisted cell-free massive MIMO systems and propose techniques to\nmitigate these effects. We introduce a novel projected gradient descent (GD)\nalgorithm for STAR-RIS coefficient matrix design by minimizing the local\nchannel estimation normalised mean square error. We also derive the closed-form\nexpressions of the uplink and downlink spectral efficiency (SE) to analyze\nsystem performance with EMI and phase errors, in which fractional power control\nmethods are applied for performance improvement. The results reveal that the\nprojected GD algorithm can effectively tackle EMI and phase errors to improve\nestimation accuracy and compensate for performance degradation with nearly\n$10\\%\\sim20\\%$ SE improvement. Moreover, increasing access points (APs),\nantennas per AP, and STAR-RIS elements can also improve SE performance.\nApplying STAR-RIS in the proposed system achieves a larger $25\\%$-likely SE\nthan conventional RISs. However, the advantages of employing more STAR-RIS\nelements are reduced when EMI is severe."
                },
                "authors": [
                    {
                        "name": "Jun Qian"
                    },
                    {
                        "name": "Ross Murch"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "13 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v2",
                "updated": "2024-11-21T11:27:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    11,
                    27,
                    34,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agents Social Interaction Simulations on One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agents Social Interaction Simulations on One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06098v2",
                "updated": "2024-11-21T10:55:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    55,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-09-09T22:33:11Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    33,
                    11,
                    0,
                    253,
                    0
                ],
                "title": "Positioning of a Next Generation Mobile Cell to Maximise Aggregate\n  Network Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning of a Next Generation Mobile Cell to Maximise Aggregate\n  Network Capacity"
                },
                "summary": "In wireless communications, the need to cover operation areas, such as\nseaports, is at the forefront of discussion, especially regarding network\ncapacity provisioning. Radio network planning typically involves determining\nthe number of fixed cells, considering link budgets and deploying them\ngeometrically centered across targeted areas. This paper proposes a solution to\ndetermine the optimal position for a mobile cell, considering 3GPP path loss\nmodels. The optimal position for the mobile cell maximises the aggregate\nnetwork capacity offered to a set of User Equipments (UEs), with gains up to\n187% compared to the positioning of the mobile cell at the UEs geometrical\ncenter. The proposed solution can be used by network planners and integrated\ninto network optimisation tools. This has the potential to reduce costs\nassociated with the Radio Access Network (RAN) planning by enhancing\nflexibility for on-demand deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In wireless communications, the need to cover operation areas, such as\nseaports, is at the forefront of discussion, especially regarding network\ncapacity provisioning. Radio network planning typically involves determining\nthe number of fixed cells, considering link budgets and deploying them\ngeometrically centered across targeted areas. This paper proposes a solution to\ndetermine the optimal position for a mobile cell, considering 3GPP path loss\nmodels. The optimal position for the mobile cell maximises the aggregate\nnetwork capacity offered to a set of User Equipments (UEs), with gains up to\n187% compared to the positioning of the mobile cell at the UEs geometrical\ncenter. The proposed solution can be used by network planners and integrated\ninto network optimisation tools. This has the potential to reduce costs\nassociated with the Radio Access Network (RAN) planning by enhancing\nflexibility for on-demand deployments."
                },
                "authors": [
                    {
                        "name": "Paulo Furtado Correia"
                    },
                    {
                        "name": "Andre Coelho"
                    },
                    {
                        "name": "Manuel Ricardo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Ricardo"
                },
                "author": "Manuel Ricardo",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14012v1",
                "updated": "2024-11-21T10:54:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:54:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Logic Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Augmented Generation"
                },
                "summary": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results."
                },
                "authors": [
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15145v3",
                "updated": "2024-11-21T10:52:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    52,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-24T01:49:02Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    1,
                    49,
                    2,
                    4,
                    145,
                    0
                ],
                "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models"
                },
                "summary": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Damien Teney"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14009v1",
                "updated": "2024-11-21T10:45:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    45,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:45:44Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    45,
                    44,
                    3,
                    326,
                    0
                ],
                "title": "GPT versus Humans: Uncovering Ethical Concerns in Conversational\n  Generative AI-empowered Multi-Robot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT versus Humans: Uncovering Ethical Concerns in Conversational\n  Generative AI-empowered Multi-Robot Systems"
                },
                "summary": "The emergence of generative artificial intelligence (GAI) and large language\nmodels (LLMs) such ChatGPT has enabled the realization of long-harbored desires\nin software and robotic development. The technology however, has brought with\nit novel ethical challenges. These challenges are compounded by the application\nof LLMs in other machine learning systems, such as multi-robot systems. The\nobjectives of the study were to examine novel ethical issues arising from the\napplication of LLMs in multi-robot systems. Unfolding ethical issues in GPT\nagent behavior (deliberation of ethical concerns) was observed, and GPT output\nwas compared with human experts. The article also advances a model for ethical\ndevelopment of multi-robot systems. A qualitative workshop-based method was\nemployed in three workshops for the collection of ethical concerns: two human\nexpert workshops (N=16 participants) and one GPT-agent-based workshop (N=7\nagents; two teams of 6 agents plus one judge). Thematic analysis was used to\nanalyze the qualitative data. The results reveal differences between the\nhuman-produced and GPT-based ethical concerns. Human experts placed greater\nemphasis on new themes related to deviance, data privacy, bias and unethical\ncorporate conduct. GPT agents emphasized concerns present in existing AI ethics\nguidelines. The study contributes to a growing body of knowledge in\ncontext-specific AI ethics and GPT application. It demonstrates the gap between\nhuman expert thinking and LLM output, while emphasizing new ethical concerns\nemerging in novel technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of generative artificial intelligence (GAI) and large language\nmodels (LLMs) such ChatGPT has enabled the realization of long-harbored desires\nin software and robotic development. The technology however, has brought with\nit novel ethical challenges. These challenges are compounded by the application\nof LLMs in other machine learning systems, such as multi-robot systems. The\nobjectives of the study were to examine novel ethical issues arising from the\napplication of LLMs in multi-robot systems. Unfolding ethical issues in GPT\nagent behavior (deliberation of ethical concerns) was observed, and GPT output\nwas compared with human experts. The article also advances a model for ethical\ndevelopment of multi-robot systems. A qualitative workshop-based method was\nemployed in three workshops for the collection of ethical concerns: two human\nexpert workshops (N=16 participants) and one GPT-agent-based workshop (N=7\nagents; two teams of 6 agents plus one judge). Thematic analysis was used to\nanalyze the qualitative data. The results reveal differences between the\nhuman-produced and GPT-based ethical concerns. Human experts placed greater\nemphasis on new themes related to deviance, data privacy, bias and unethical\ncorporate conduct. GPT agents emphasized concerns present in existing AI ethics\nguidelines. The study contributes to a growing body of knowledge in\ncontext-specific AI ethics and GPT application. It demonstrates the gap between\nhuman expert thinking and LLM output, while emphasizing new ethical concerns\nemerging in novel technology."
                },
                "authors": [
                    {
                        "name": "Rebekah Rousi"
                    },
                    {
                        "name": "Niko Makitalo"
                    },
                    {
                        "name": "Hooman Samani"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jose Siqueira de Cerqueira"
                    },
                    {
                        "name": "Ville Vakkuri"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "51 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13997v1",
                "updated": "2024-11-21T10:23:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    23,
                    0,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:23:00Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    23,
                    0,
                    3,
                    326,
                    0
                ],
                "title": "Mirror Target YOLO: An Improved YOLOv8 Method with Indirect Vision for\n  Heritage Buildings Fire Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirror Target YOLO: An Improved YOLOv8 Method with Indirect Vision for\n  Heritage Buildings Fire Detection"
                },
                "summary": "Fires can cause severe damage to heritage buildings, making timely fire\ndetection essential. Traditional dense cabling and drilling can harm these\nstructures, so reducing the number of cameras to minimize such impact is\nchallenging. Additionally, avoiding false alarms due to noise sensitivity and\npreserving the expertise of managers in fire-prone areas is crucial. To address\nthese needs, we propose a fire detection method based on indirect vision,\ncalled Mirror Target YOLO (MITA-YOLO). MITA-YOLO integrates indirect vision\ndeployment and an enhanced detection module. It uses mirror angles to achieve\nindirect views, solving issues with limited visibility in irregular spaces and\naligning each indirect view with the target monitoring area. The Target-Mask\nmodule is designed to automatically identify and isolate the indirect vision\nareas in each image, filtering out non-target areas. This enables the model to\ninherit managers' expertise in assessing fire-risk zones, improving focus and\nresistance to interference in fire detection.In our experiments, we created an\n800-image fire dataset with indirect vision. Results show that MITA-YOLO\nsignificantly reduces camera requirements while achieving superior detection\nperformance compared to other mainstream models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fires can cause severe damage to heritage buildings, making timely fire\ndetection essential. Traditional dense cabling and drilling can harm these\nstructures, so reducing the number of cameras to minimize such impact is\nchallenging. Additionally, avoiding false alarms due to noise sensitivity and\npreserving the expertise of managers in fire-prone areas is crucial. To address\nthese needs, we propose a fire detection method based on indirect vision,\ncalled Mirror Target YOLO (MITA-YOLO). MITA-YOLO integrates indirect vision\ndeployment and an enhanced detection module. It uses mirror angles to achieve\nindirect views, solving issues with limited visibility in irregular spaces and\naligning each indirect view with the target monitoring area. The Target-Mask\nmodule is designed to automatically identify and isolate the indirect vision\nareas in each image, filtering out non-target areas. This enables the model to\ninherit managers' expertise in assessing fire-risk zones, improving focus and\nresistance to interference in fire detection.In our experiments, we created an\n800-image fire dataset with indirect vision. Results show that MITA-YOLO\nsignificantly reduces camera requirements while achieving superior detection\nperformance compared to other mainstream models."
                },
                "authors": [
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "JunSheng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "JunSheng Cheng"
                },
                "author": "JunSheng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v1",
                "updated": "2024-11-21T10:00:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xing Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13989v1",
                "updated": "2024-11-21T10:00:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    5,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T10:00:05Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    5,
                    3,
                    326,
                    0
                ],
                "title": "Towards Smart Fronthauling Management: Experimental Insights from a 5G\n  Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Smart Fronthauling Management: Experimental Insights from a 5G\n  Testbed"
                },
                "summary": "The fronthaul connection is a key component of Centralized RAN (C-RAN)\narchitectures, consistently required to handle high capacity demands. However,\nthis critical feature is at risk when the transport link relies on wireless\ntechnology. Fortunately, solutions exist to enhance the reliability of wireless\nlinks. In this paper, we recall the theoretical fronthaul model, present a\ndynamic reconfiguration strategy and perform a conclusive experiment.\nSpecifically, we showcase the setup of a wireless fronthaul testbed and discuss\nthe resulting measurements. For this task, we leveraged the commercial hardware\nprovided by the High-Frequency Campus Lab (HFCL), a private 5G network with\nmillimeter wave (mmWave) radio access interface. Our experiments provide\noriginal data on the fronthaul utilization in this real deployment,\ndemonstrating both a good accordance with the theoretical model discussed in\n[1] and the viability of one stabilizing solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fronthaul connection is a key component of Centralized RAN (C-RAN)\narchitectures, consistently required to handle high capacity demands. However,\nthis critical feature is at risk when the transport link relies on wireless\ntechnology. Fortunately, solutions exist to enhance the reliability of wireless\nlinks. In this paper, we recall the theoretical fronthaul model, present a\ndynamic reconfiguration strategy and perform a conclusive experiment.\nSpecifically, we showcase the setup of a wireless fronthaul testbed and discuss\nthe resulting measurements. For this task, we leveraged the commercial hardware\nprovided by the High-Frequency Campus Lab (HFCL), a private 5G network with\nmillimeter wave (mmWave) radio access interface. Our experiments provide\noriginal data on the fronthaul utilization in this real deployment,\ndemonstrating both a good accordance with the theoretical model discussed in\n[1] and the viability of one stabilizing solution."
                },
                "authors": [
                    {
                        "name": "Marcello Morini"
                    },
                    {
                        "name": "Eugenio Moro"
                    },
                    {
                        "name": "Ilario Filippini"
                    },
                    {
                        "name": "Danilo De Donno"
                    },
                    {
                        "name": "Salvatore Moscato"
                    },
                    {
                        "name": "Antonio Capone"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Capone"
                },
                "author": "Antonio Capone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00774v3",
                "updated": "2024-11-21T09:19:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    9,
                    19,
                    28,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-01T17:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    59,
                    51,
                    4,
                    306,
                    0
                ],
                "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM"
                },
                "summary": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources."
                },
                "authors": [
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Long Ma"
                    }
                ],
                "author_detail": {
                    "name": "Long Ma"
                },
                "author": "Long Ma",
                "arxiv_comment": "Project Page: https://freeze-omni.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v2",
                "updated": "2024-11-21T08:56:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    56,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13941v1",
                "updated": "2024-11-21T08:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    49,
                    23,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T08:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    49,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "LLMs as Continuous Learners: Improving the Reproduction of Defective\n  Code in Software Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Continuous Learners: Improving the Reproduction of Defective\n  Code in Software Issues"
                },
                "summary": "Reproducing buggy code is the first and crucially important step in issue\nresolving, as it aids in identifying the underlying problems and validating\nthat generated patches resolve the problem. While numerous approaches have been\nproposed for this task, they primarily address common, widespread errors and\nstruggle to adapt to unique, evolving errors specific to individual code\nrepositories. To fill this gap, we propose EvoCoder, a multi-agent continuous\nlearning framework for issue code reproduction. EvoCoder adopts a reflection\nmechanism that allows the LLM to continuously learn from previously resolved\nproblems and dynamically refine its strategies to new emerging challenges. To\nprevent experience bloating, EvoCoder introduces a novel hierarchical\nexperience pool that enables the model to adaptively update common and\nrepo-specific experiences. Our experimental results show a 20\\% improvement in\nissue reproduction rates over existing SOTA methods. Furthermore, integrating\nour reproduction mechanism significantly boosts the overall accuracy of the\nexisting issue-resolving pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing buggy code is the first and crucially important step in issue\nresolving, as it aids in identifying the underlying problems and validating\nthat generated patches resolve the problem. While numerous approaches have been\nproposed for this task, they primarily address common, widespread errors and\nstruggle to adapt to unique, evolving errors specific to individual code\nrepositories. To fill this gap, we propose EvoCoder, a multi-agent continuous\nlearning framework for issue code reproduction. EvoCoder adopts a reflection\nmechanism that allows the LLM to continuously learn from previously resolved\nproblems and dynamically refine its strategies to new emerging challenges. To\nprevent experience bloating, EvoCoder introduces a novel hierarchical\nexperience pool that enables the model to adaptively update common and\nrepo-specific experiences. Our experimental results show a 20\\% improvement in\nissue reproduction rates over existing SOTA methods. Furthermore, integrating\nour reproduction mechanism significantly boosts the overall accuracy of the\nexisting issue-resolving pipeline."
                },
                "authors": [
                    {
                        "name": "Yalan Lin"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17497v2",
                "updated": "2024-11-21T08:44:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    44,
                    20,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-27T13:22:51Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    13,
                    22,
                    51,
                    1,
                    58,
                    0
                ],
                "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering"
                },
                "summary": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR."
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference. Published on ACL Anthology:\n  https://aclanthology.org/2024.emnlp-main.321.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13932v1",
                "updated": "2024-11-21T08:28:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T08:28:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    8,
                    28,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "XAgents: A Framework for Interpretable Rule-Based Multi-Agents\n  Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAgents: A Framework for Interpretable Rule-Based Multi-Agents\n  Cooperation"
                },
                "summary": "Extracting implicit knowledge and logical reasoning abilities from large\nlanguage models (LLMs) has consistently been a significant challenge. The\nadvancement of multi-agent systems has further en-hanced the capabilities of\nLLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the\nXAgents framework, an in-terpretable multi-agent cooperative framework based on\nthe IF-THEN rule-based system. The IF-Parts of the rules are responsible for\nlogical reasoning and domain membership calculation, while the THEN-Parts are\ncomprised of domain expert agents that generate domain-specific contents.\nFollowing the calculation of the member-ship, XAgetns transmits the task to the\ndisparate domain rules, which subsequently generate the various responses.\nThese re-sponses are analogous to the answers provided by different experts to\nthe same question. The final response is reached at by eliminat-ing the\nhallucinations and erroneous knowledge of the LLM through membership\ncomputation and semantic adversarial genera-tion of the various domain rules.\nThe incorporation of rule-based interpretability serves to bolster user\nconfidence in the XAgents framework. We evaluate the efficacy of XAgents\nthrough a com-parative analysis with the latest AutoAgents, in which XAgents\ndemonstrated superior performance across three distinct datasets. We perform\npost-hoc interpretable studies with SHAP algorithm and case studies, proving\nthe interpretability of XAgent in terms of input-output feature correlation and\nrule-based semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting implicit knowledge and logical reasoning abilities from large\nlanguage models (LLMs) has consistently been a significant challenge. The\nadvancement of multi-agent systems has further en-hanced the capabilities of\nLLMs. Inspired by the structure of multi-polar neurons (MNs), we propose the\nXAgents framework, an in-terpretable multi-agent cooperative framework based on\nthe IF-THEN rule-based system. The IF-Parts of the rules are responsible for\nlogical reasoning and domain membership calculation, while the THEN-Parts are\ncomprised of domain expert agents that generate domain-specific contents.\nFollowing the calculation of the member-ship, XAgetns transmits the task to the\ndisparate domain rules, which subsequently generate the various responses.\nThese re-sponses are analogous to the answers provided by different experts to\nthe same question. The final response is reached at by eliminat-ing the\nhallucinations and erroneous knowledge of the LLM through membership\ncomputation and semantic adversarial genera-tion of the various domain rules.\nThe incorporation of rule-based interpretability serves to bolster user\nconfidence in the XAgents framework. We evaluate the efficacy of XAgents\nthrough a com-parative analysis with the latest AutoAgents, in which XAgents\ndemonstrated superior performance across three distinct datasets. We perform\npost-hoc interpretable studies with SHAP algorithm and case studies, proving\nthe interpretability of XAgent in terms of input-output feature correlation and\nrule-based semantics."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Mingxian Gu"
                    },
                    {
                        "name": "Renhuo Zhao"
                    },
                    {
                        "name": "Fuping Hu"
                    },
                    {
                        "name": "Zhaohong Deng"
                    },
                    {
                        "name": "Yitang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yitang Chen"
                },
                "author": "Yitang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13909v1",
                "updated": "2024-11-21T07:47:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:47:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts"
                },
                "summary": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther."
                },
                "authors": [
                    {
                        "name": "Honglin Li"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Chenglu Zhu"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14199v2",
                "updated": "2024-11-21T07:38:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    38,
                    12,
                    3,
                    326,
                    0
                ],
                "published": "2024-08-26T11:54:27Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    54,
                    27,
                    0,
                    239,
                    0
                ],
                "title": "A Survey on Small-Scale Testbeds for Connected and Automated Vehicles\n  and Robot Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Small-Scale Testbeds for Connected and Automated Vehicles\n  and Robot Swarms"
                },
                "summary": "Connected and automated vehicles and robot swarms hold transformative\npotential for enhancing safety, efficiency, and sustainability in the\ntransportation and manufacturing sectors. Extensive testing and validation of\nthese technologies is crucial for their deployment in the real world. While\nsimulations are essential for initial testing, they often have limitations in\ncapturing the complex dynamics of real-world interactions. This limitation\nunderscores the importance of small-scale testbeds. These testbeds provide a\nrealistic, cost-effective, and controlled environment for testing and\nvalidating algorithms, acting as an essential intermediary between simulation\nand full-scale experiments. This work serves to facilitate researchers' efforts\nin identifying existing small-scale testbeds suitable for their experiments and\nprovide insights for those who want to build their own. In addition, it\ndelivers a comprehensive survey of the current landscape of these testbeds. We\nderive 62 characteristics of testbeds based on the well-known sense-plan-act\nparadigm and offer an online table comparing 23 small-scale testbeds based on\nthese characteristics. The online table is hosted on our designated public\nwebpage https://bassamlab.github.io/testbeds-survey, and we invite testbed\ncreators and developers to contribute to it. We closely examine nine testbeds\nin this paper, demonstrating how the derived characteristics can be used to\npresent testbeds. Furthermore, we discuss three ongoing challenges concerning\nsmall-scale testbeds that we identified, i.e., small-scale to full-scale\ntransition, sustainability, and power and resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected and automated vehicles and robot swarms hold transformative\npotential for enhancing safety, efficiency, and sustainability in the\ntransportation and manufacturing sectors. Extensive testing and validation of\nthese technologies is crucial for their deployment in the real world. While\nsimulations are essential for initial testing, they often have limitations in\ncapturing the complex dynamics of real-world interactions. This limitation\nunderscores the importance of small-scale testbeds. These testbeds provide a\nrealistic, cost-effective, and controlled environment for testing and\nvalidating algorithms, acting as an essential intermediary between simulation\nand full-scale experiments. This work serves to facilitate researchers' efforts\nin identifying existing small-scale testbeds suitable for their experiments and\nprovide insights for those who want to build their own. In addition, it\ndelivers a comprehensive survey of the current landscape of these testbeds. We\nderive 62 characteristics of testbeds based on the well-known sense-plan-act\nparadigm and offer an online table comparing 23 small-scale testbeds based on\nthese characteristics. The online table is hosted on our designated public\nwebpage https://bassamlab.github.io/testbeds-survey, and we invite testbed\ncreators and developers to contribute to it. We closely examine nine testbeds\nin this paper, demonstrating how the derived characteristics can be used to\npresent testbeds. Furthermore, we discuss three ongoing challenges concerning\nsmall-scale testbeds that we identified, i.e., small-scale to full-scale\ntransition, sustainability, and power and resource management."
                },
                "authors": [
                    {
                        "name": "Armin Mokhtarian"
                    },
                    {
                        "name": "Jianye Xu"
                    },
                    {
                        "name": "Patrick Scheffe"
                    },
                    {
                        "name": "Maximilian Kloock"
                    },
                    {
                        "name": "Simon SchÃ¤fer"
                    },
                    {
                        "name": "Heeseung Bang"
                    },
                    {
                        "name": "Viet-Anh Le"
                    },
                    {
                        "name": "Sangeet Ulhas"
                    },
                    {
                        "name": "Johannes Betz"
                    },
                    {
                        "name": "Sean Wilson"
                    },
                    {
                        "name": "Spring Berman"
                    },
                    {
                        "name": "Liam Paull"
                    },
                    {
                        "name": "Amanda Prorok"
                    },
                    {
                        "name": "Bassam Alrifaee"
                    }
                ],
                "author_detail": {
                    "name": "Bassam Alrifaee"
                },
                "author": "Bassam Alrifaee",
                "arxiv_doi": "10.13140/RG.2.2.16176.74248/1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.16176.74248/1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 11 figures, 1 table. This work was accepted by the IEEE\n  Robotics & Automation Magazine",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13904v1",
                "updated": "2024-11-21T07:30:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    30,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:30:02Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    30,
                    2,
                    3,
                    326,
                    0
                ],
                "title": "Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel\n  Planning"
                },
                "summary": "How are LLM-based agents used in the future? While many of the existing work\non agents has focused on improving the performance of a specific family of\nobjective and challenging tasks, in this work, we take a different perspective\nby thinking about full delegation: agents take over humans' routine\ndecision-making processes and are trusted by humans to find solutions that fit\npeople's personalized needs and are adaptive to ever-changing context. In order\nto achieve such a goal, the behavior of the agents, i.e., agentic behaviors,\nshould be evaluated not only on their achievements (i.e., outcome evaluation),\nbut also how they achieved that (i.e., procedure evaluation). For this, we\npropose APEC Agent Constitution, a list of criteria that an agent should follow\nfor good agentic behaviors, including Accuracy, Proactivity, Efficiency and\nCredibility. To verify whether APEC aligns with human preferences, we develop\nAPEC-Travel, a travel planning agent that proactively extracts hidden\npersonalized needs via multi-round dialog with travelers. APEC-Travel is\nconstructed purely from synthetic data generated by Llama3.1-405B-Instruct with\na diverse set of travelers' persona to simulate rich distribution of dialogs.\nIteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses\nbaselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores\nacross the constitution axes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How are LLM-based agents used in the future? While many of the existing work\non agents has focused on improving the performance of a specific family of\nobjective and challenging tasks, in this work, we take a different perspective\nby thinking about full delegation: agents take over humans' routine\ndecision-making processes and are trusted by humans to find solutions that fit\npeople's personalized needs and are adaptive to ever-changing context. In order\nto achieve such a goal, the behavior of the agents, i.e., agentic behaviors,\nshould be evaluated not only on their achievements (i.e., outcome evaluation),\nbut also how they achieved that (i.e., procedure evaluation). For this, we\npropose APEC Agent Constitution, a list of criteria that an agent should follow\nfor good agentic behaviors, including Accuracy, Proactivity, Efficiency and\nCredibility. To verify whether APEC aligns with human preferences, we develop\nAPEC-Travel, a travel planning agent that proactively extracts hidden\npersonalized needs via multi-round dialog with travelers. APEC-Travel is\nconstructed purely from synthetic data generated by Llama3.1-405B-Instruct with\na diverse set of travelers' persona to simulate rich distribution of dialogs.\nIteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses\nbaselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores\nacross the constitution axes."
                },
                "authors": [
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Da JU"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Sasha Mitts"
                    },
                    {
                        "name": "Aaron Foss"
                    },
                    {
                        "name": "Justine T Kao"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13902v1",
                "updated": "2024-11-21T07:28:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    28,
                    7,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:28:07Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    28,
                    7,
                    3,
                    326,
                    0
                ],
                "title": "PIORS: Personalized Intelligent Outpatient Reception based on Large\n  Language Model with Multi-Agents Medical Scenario Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIORS: Personalized Intelligent Outpatient Reception based on Large\n  Language Model with Multi-Agents Medical Scenario Simulation"
                },
                "summary": "In China, receptionist nurses face overwhelming workloads in outpatient\nsettings, limiting their time and attention for each patient and ultimately\nreducing service quality. In this paper, we present the Personalized\nIntelligent Outpatient Reception System (PIORS). This system integrates an\nLLM-based reception nurse and a collaboration between LLM and hospital\ninformation system (HIS) into real outpatient reception setting, aiming to\ndeliver personalized, high-quality, and efficient reception services.\nAdditionally, to enhance the performance of LLMs in real-world healthcare\nscenarios, we propose a medical conversational data generation framework named\nService Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM\nto the real-world environments and PIORS settings. We evaluate the\neffectiveness of PIORS and SFMSS through automatic and human assessments\ninvolving 15 users and 15 clinical experts. The results demonstrate that\nPIORS-Nurse outperforms all baselines, including the current state-of-the-art\nmodel GPT-4o, and aligns with human preferences and clinical needs. Further\ndetails and demo can be found at https://github.com/FudanDISC/PIORS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In China, receptionist nurses face overwhelming workloads in outpatient\nsettings, limiting their time and attention for each patient and ultimately\nreducing service quality. In this paper, we present the Personalized\nIntelligent Outpatient Reception System (PIORS). This system integrates an\nLLM-based reception nurse and a collaboration between LLM and hospital\ninformation system (HIS) into real outpatient reception setting, aiming to\ndeliver personalized, high-quality, and efficient reception services.\nAdditionally, to enhance the performance of LLMs in real-world healthcare\nscenarios, we propose a medical conversational data generation framework named\nService Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM\nto the real-world environments and PIORS settings. We evaluate the\neffectiveness of PIORS and SFMSS through automatic and human assessments\ninvolving 15 users and 15 clinical experts. The results demonstrate that\nPIORS-Nurse outperforms all baselines, including the current state-of-the-art\nmodel GPT-4o, and aligns with human preferences and clinical needs. Further\ndetails and demo can be found at https://github.com/FudanDISC/PIORS"
                },
                "authors": [
                    {
                        "name": "Zhijie Bao"
                    },
                    {
                        "name": "Qingyun Liu"
                    },
                    {
                        "name": "Ying Guo"
                    },
                    {
                        "name": "Zhengqiang Ye"
                    },
                    {
                        "name": "Jun Shen"
                    },
                    {
                        "name": "Shirong Xie"
                    },
                    {
                        "name": "Jiajie Peng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13899v1",
                "updated": "2024-11-21T07:21:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T07:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    21,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Schemato -- An LLM for Netlist-to-Schematic Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schemato -- An LLM for Netlist-to-Schematic Conversion"
                },
                "summary": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in the\ntwo settings of converting netlists to .asc files for LTSpice and LATEX files\nfor CircuiTikz schematics. Experiments on our circuit dataset show that\nSchemato achieves up to 93% compilation success rate for the netlist-to-LaTeX\nconversion task, surpassing the 26% rate scored by the state-of-the-art LLMs.\nFurthermore, our experiments show that Schemato generates schematics with a\nmean structural similarity index measure that is 3xhigher than the best\nperforming LLMs, therefore closer to the reference human design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in the\ntwo settings of converting netlists to .asc files for LTSpice and LATEX files\nfor CircuiTikz schematics. Experiments on our circuit dataset show that\nSchemato achieves up to 93% compilation success rate for the netlist-to-LaTeX\nconversion task, surpassing the 26% rate scored by the state-of-the-art LLMs.\nFurthermore, our experiments show that Schemato generates schematics with a\nmean structural similarity index measure that is 3xhigher than the best\nperforming LLMs, therefore closer to the reference human design."
                },
                "authors": [
                    {
                        "name": "Ryoga Matsuo"
                    },
                    {
                        "name": "Stefan Uhlich"
                    },
                    {
                        "name": "Arun Venkitaraman"
                    },
                    {
                        "name": "Andrea Bonetti"
                    },
                    {
                        "name": "Chia-Yu Hsieh"
                    },
                    {
                        "name": "Ali Momeni"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Augusto Capone"
                    },
                    {
                        "name": "Eisaku Ohbuchi"
                    },
                    {
                        "name": "Lorenzo Servadei"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Servadei"
                },
                "author": "Lorenzo Servadei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02884v2",
                "updated": "2024-11-21T07:07:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    7,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-03T18:12:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    12,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning"
                },
                "summary": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Jianbo Wu"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Tong Che"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Xiaoshui Huang"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04838v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04838v5",
                "updated": "2024-11-21T06:52:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    52,
                    2,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-07T13:39:38Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    13,
                    39,
                    38,
                    2,
                    38,
                    0
                ],
                "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition"
                },
                "summary": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets."
                },
                "authors": [
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Yanjie Wang"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Brian Mac Namee"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "arxiv_comment": "Accepted to Neurips2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04838v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04838v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16141v3",
                "updated": "2024-11-21T06:20:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    20,
                    46,
                    3,
                    326,
                    0
                ],
                "published": "2023-11-05T12:20:29Z",
                "published_parsed": [
                    2023,
                    11,
                    5,
                    12,
                    20,
                    29,
                    6,
                    309,
                    0
                ],
                "title": "Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking\n  Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) have gained significant attention due to the\nenergy-efficient and multiplication-free characteristics. Despite these\nadvantages, deploying large-scale SNNs on edge hardware is challenging due to\nlimited resource availability. Network pruning offers a viable approach to\ncompress the network scale and reduce hardware resource requirements for model\ndeployment. However, existing SNN pruning methods cause high pruning costs and\nperformance loss because they lack efficiency in processing the sparse spike\nrepresentation of SNNs. In this paper, inspired by the critical brain\nhypothesis in neuroscience and the high biological plausibility of SNNs, we\nexplore and leverage criticality to facilitate efficient pruning in deep SNNs.\nWe firstly explain criticality in SNNs from the perspective of maximizing\nfeature information entropy. Second, We propose a low-cost metric for assess\nneuron criticality in feature transmission and design a pruning-regeneration\nmethod that incorporates this criticality into the pruning process.\nExperimental results demonstrate that our method achieves higher performance\nthan the current state-of-the-art (SOTA) method with up to 95.26\\% reduction of\npruning cost. The criticality-based regeneration process efficiently selects\npotential structures and facilitates consistent feature representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have gained significant attention due to the\nenergy-efficient and multiplication-free characteristics. Despite these\nadvantages, deploying large-scale SNNs on edge hardware is challenging due to\nlimited resource availability. Network pruning offers a viable approach to\ncompress the network scale and reduce hardware resource requirements for model\ndeployment. However, existing SNN pruning methods cause high pruning costs and\nperformance loss because they lack efficiency in processing the sparse spike\nrepresentation of SNNs. In this paper, inspired by the critical brain\nhypothesis in neuroscience and the high biological plausibility of SNNs, we\nexplore and leverage criticality to facilitate efficient pruning in deep SNNs.\nWe firstly explain criticality in SNNs from the perspective of maximizing\nfeature information entropy. Second, We propose a low-cost metric for assess\nneuron criticality in feature transmission and design a pruning-regeneration\nmethod that incorporates this criticality into the pruning process.\nExperimental results demonstrate that our method achieves higher performance\nthan the current state-of-the-art (SOTA) method with up to 95.26\\% reduction of\npruning cost. The criticality-based regeneration process efficiently selects\npotential structures and facilitates consistent feature representation."
                },
                "authors": [
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Zeshi Liu"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13874v1",
                "updated": "2024-11-21T06:20:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    20,
                    29,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T06:20:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    20,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "Next-Generation Phishing: How LLM Agents Empower Cyber Attackers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Generation Phishing: How LLM Agents Empower Cyber Attackers"
                },
                "summary": "The escalating threat of phishing emails has become increasingly\nsophisticated with the rise of Large Language Models (LLMs). As attackers\nexploit LLMs to craft more convincing and evasive phishing emails, it is\ncrucial to assess the resilience of current phishing defenses. In this study we\nconduct a comprehensive evaluation of traditional phishing detectors, such as\nGmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine\nlearning models like SVM, Logistic Regression, and Naive Bayes, in identifying\nboth traditional and LLM-rephrased phishing emails. We also explore the\nemerging role of LLMs as phishing detection tools, a method already adopted by\ncompanies like NTT Security Holdings and JPMorgan Chase. Our results reveal\nnotable declines in detection accuracy for rephrased emails across all\ndetectors, highlighting critical weaknesses in current phishing defenses. As\nthe threat landscape evolves, our findings underscore the need for stronger\nsecurity controls and regulatory oversight on LLM-generated content to prevent\nits misuse in creating advanced phishing attacks. This study contributes to the\ndevelopment of more effective Cyber Threat Intelligence (CTI) by leveraging\nLLMs to generate diverse phishing variants that can be used for data\naugmentation, harnessing the power of LLMs to enhance phishing detection, and\npaving the way for more robust and adaptable threat detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating threat of phishing emails has become increasingly\nsophisticated with the rise of Large Language Models (LLMs). As attackers\nexploit LLMs to craft more convincing and evasive phishing emails, it is\ncrucial to assess the resilience of current phishing defenses. In this study we\nconduct a comprehensive evaluation of traditional phishing detectors, such as\nGmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine\nlearning models like SVM, Logistic Regression, and Naive Bayes, in identifying\nboth traditional and LLM-rephrased phishing emails. We also explore the\nemerging role of LLMs as phishing detection tools, a method already adopted by\ncompanies like NTT Security Holdings and JPMorgan Chase. Our results reveal\nnotable declines in detection accuracy for rephrased emails across all\ndetectors, highlighting critical weaknesses in current phishing defenses. As\nthe threat landscape evolves, our findings underscore the need for stronger\nsecurity controls and regulatory oversight on LLM-generated content to prevent\nits misuse in creating advanced phishing attacks. This study contributes to the\ndevelopment of more effective Cyber Threat Intelligence (CTI) by leveraging\nLLMs to generate diverse phishing variants that can be used for data\naugmentation, harnessing the power of LLMs to enhance phishing detection, and\npaving the way for more robust and adaptable threat detection systems."
                },
                "authors": [
                    {
                        "name": "Khalifa Afane"
                    },
                    {
                        "name": "Wenqi Wei"
                    },
                    {
                        "name": "Ying Mao"
                    },
                    {
                        "name": "Junaid Farooq"
                    },
                    {
                        "name": "Juntao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Juntao Chen"
                },
                "author": "Juntao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03163v2",
                "updated": "2024-11-21T06:18:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    18,
                    7,
                    3,
                    326,
                    0
                ],
                "published": "2024-03-05T17:56:27Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    17,
                    56,
                    27,
                    1,
                    65,
                    0
                ],
                "title": "Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering"
                },
                "summary": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs."
                },
                "authors": [
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Ryan Li"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13868v1",
                "updated": "2024-11-21T06:06:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    6,
                    4,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T06:06:04Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    6,
                    4,
                    3,
                    326,
                    0
                ],
                "title": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Detection of Watermarks for Large Language Models Under Human\n  Edits"
                },
                "summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13867v1",
                "updated": "2024-11-21T06:03:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    3,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T06:03:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    3,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "Generative Fuzzy System for Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Fuzzy System for Sequence Generation"
                },
                "summary": "Generative Models (GMs), particularly Large Language Models (LLMs), have\ngarnered significant attention in machine learning and artificial intelligence\nfor their ability to generate new data by learning the statistical properties\nof training data and creating data that resemble the original. This capability\noffers a wide range of applications across various domains. However, the\ncomplex structures and numerous model parameters of GMs make the input-output\nprocesses opaque, complicating the understanding and control of outputs.\nMoreover, the purely data-driven learning mechanism limits GM's ability to\nacquire broader knowledge. There remains substantial potential for enhancing\nthe robustness and generalization capabilities of GMs. In this work, we\nintroduce the fuzzy system, a classical modeling method that combines data and\nknowledge-driven mechanisms, to generative tasks. We propose a novel Generative\nFuzzy System framework, named GenFS, which integrates the deep learning\ncapabilities of GM with the interpretability and dual-driven mechanisms of\nfuzzy systems. Specifically, we propose an end-to-end GenFS-based model for\nsequence generation, called FuzzyS2S. A series of experimental studies were\nconducted on 12 datasets, covering three distinct categories of generative\ntasks: machine translation, code generation, and summary generation. The\nresults demonstrate that FuzzyS2S outperforms the Transformer in terms of\naccuracy and fluency. Furthermore, it exhibits better performance on some\ndatasets compared to state-of-the-art models T5 and CodeT5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Models (GMs), particularly Large Language Models (LLMs), have\ngarnered significant attention in machine learning and artificial intelligence\nfor their ability to generate new data by learning the statistical properties\nof training data and creating data that resemble the original. This capability\noffers a wide range of applications across various domains. However, the\ncomplex structures and numerous model parameters of GMs make the input-output\nprocesses opaque, complicating the understanding and control of outputs.\nMoreover, the purely data-driven learning mechanism limits GM's ability to\nacquire broader knowledge. There remains substantial potential for enhancing\nthe robustness and generalization capabilities of GMs. In this work, we\nintroduce the fuzzy system, a classical modeling method that combines data and\nknowledge-driven mechanisms, to generative tasks. We propose a novel Generative\nFuzzy System framework, named GenFS, which integrates the deep learning\ncapabilities of GM with the interpretability and dual-driven mechanisms of\nfuzzy systems. Specifically, we propose an end-to-end GenFS-based model for\nsequence generation, called FuzzyS2S. A series of experimental studies were\nconducted on 12 datasets, covering three distinct categories of generative\ntasks: machine translation, code generation, and summary generation. The\nresults demonstrate that FuzzyS2S outperforms the Transformer in terms of\naccuracy and fluency. Furthermore, it exhibits better performance on some\ndatasets compared to state-of-the-art models T5 and CodeT5."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Zhaohong Deng"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Zhuangzhuang Zhao"
                    },
                    {
                        "name": "Guanjin Wang"
                    },
                    {
                        "name": "Kup-sze Choi"
                    }
                ],
                "author_detail": {
                    "name": "Kup-sze Choi"
                },
                "author": "Kup-sze Choi",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13865v1",
                "updated": "2024-11-21T06:01:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    1,
                    47,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T06:01:47Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    1,
                    47,
                    3,
                    326,
                    0
                ],
                "title": "HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems"
                },
                "summary": "Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics."
                },
                "authors": [
                    {
                        "name": "Qiyao Ma"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Mingxuan Ju"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Neil Shah"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09955v2",
                "updated": "2024-11-21T05:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    28,
                    10,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-15T05:18:15Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    5,
                    18,
                    15,
                    4,
                    320,
                    0
                ],
                "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era"
                },
                "summary": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing."
                },
                "authors": [
                    {
                        "name": "Thanh Tam Nguyen"
                    },
                    {
                        "name": "Zhao Ren"
                    },
                    {
                        "name": "Trinh Pham"
                    },
                    {
                        "name": "Thanh Trung Huynh"
                    },
                    {
                        "name": "Phi Le Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Viet Hung Nguyen"
                },
                "author": "Quoc Viet Hung Nguyen",
                "arxiv_comment": "Fixed a serious error in author information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13855v1",
                "updated": "2024-11-21T05:27:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    27,
                    42,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:27:42Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    27,
                    42,
                    3,
                    326,
                    0
                ],
                "title": "A Multimodal Approach to The Detection and Classification of Skin\n  Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Approach to The Detection and Classification of Skin\n  Diseases"
                },
                "summary": "According to PBS, nearly one-third of Americans lack access to primary care\nservices, and another forty percent delay going to avoid medical costs. As a\nresult, many diseases are left undiagnosed and untreated, even if the disease\nshows many physical symptoms on the skin. With the rise of AI, self-diagnosis\nand improved disease recognition have become more promising than ever; in spite\nof that, existing methods suffer from a lack of large-scale patient databases\nand outdated methods of study, resulting in studies being limited to only a few\ndiseases or modalities. This study incorporates readily available and easily\naccessible patient information via image and text for skin disease\nclassification on a new dataset of 26 skin disease types that includes both\nskin disease images (37K) and associated patient narratives. Using this\ndataset, baselines for various image models were established that outperform\nexisting methods. Initially, the Resnet-50 model was only able to achieve an\naccuracy of 70% but, after various optimization techniques, the accuracy was\nimproved to 80%. In addition, this study proposes a novel fine-tuning strategy\nfor sequence classification Large Language Models (LLMs), Chain of Options,\nwhich breaks down a complex reasoning task into intermediate steps at training\ntime instead of inference. With Chain of Options and preliminary disease\nrecommendations from the image model, this method achieves state of the art\naccuracy 91% in diagnosing patient skin disease given just an image of the\nafflicted area as well as a patient description of the symptoms (such as\nitchiness or dizziness). Through this research, an earlier diagnosis of skin\ndiseases can occur, and clinicians can work with deep learning models to give a\nmore accurate diagnosis, improving quality of life and saving lives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to PBS, nearly one-third of Americans lack access to primary care\nservices, and another forty percent delay going to avoid medical costs. As a\nresult, many diseases are left undiagnosed and untreated, even if the disease\nshows many physical symptoms on the skin. With the rise of AI, self-diagnosis\nand improved disease recognition have become more promising than ever; in spite\nof that, existing methods suffer from a lack of large-scale patient databases\nand outdated methods of study, resulting in studies being limited to only a few\ndiseases or modalities. This study incorporates readily available and easily\naccessible patient information via image and text for skin disease\nclassification on a new dataset of 26 skin disease types that includes both\nskin disease images (37K) and associated patient narratives. Using this\ndataset, baselines for various image models were established that outperform\nexisting methods. Initially, the Resnet-50 model was only able to achieve an\naccuracy of 70% but, after various optimization techniques, the accuracy was\nimproved to 80%. In addition, this study proposes a novel fine-tuning strategy\nfor sequence classification Large Language Models (LLMs), Chain of Options,\nwhich breaks down a complex reasoning task into intermediate steps at training\ntime instead of inference. With Chain of Options and preliminary disease\nrecommendations from the image model, this method achieves state of the art\naccuracy 91% in diagnosing patient skin disease given just an image of the\nafflicted area as well as a patient description of the symptoms (such as\nitchiness or dizziness). Through this research, an earlier diagnosis of skin\ndiseases can occur, and clinicians can work with deep learning models to give a\nmore accurate diagnosis, improving quality of life and saving lives."
                },
                "authors": [
                    {
                        "name": "Allen Yang"
                    },
                    {
                        "name": "Edward Yang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Yang"
                },
                "arxiv_affiliation": "Yale University, New Haven, CT",
                "author": "Edward Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11919v2",
                "updated": "2024-11-21T05:24:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    24,
                    27,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-17T04:00:41Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    4,
                    0,
                    41,
                    0,
                    169,
                    0
                ],
                "title": "Graph Knowledge Distillation to Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Knowledge Distillation to Mixture of Experts"
                },
                "summary": "In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural\nchoice for the node classification task. Their drawback in real-world\ndeployment is the latency that emerges from the neighbourhood processing\noperation. One solution to the latency issue is to perform knowledge\ndistillation from a trained GNN to a Multi-Layer Perceptron (MLP), where the\nMLP processes only the features of the node being classified (and possibly some\npre-computed structural information). However, the performance of such MLPs in\nboth transductive and inductive settings remains inconsistent for existing\nknowledge distillation techniques. We propose to address the performance\nconcerns by using a specially-designed student model instead of an MLP. Our\nmodel, named Routing-by-Memory (RbM), is a form of Mixture-of-Experts (MoE),\nwith a design that enforces expert specialization. By encouraging each expert\nto specialize on a certain region on the hidden representation space, we\ndemonstrate experimentally that it is possible to derive considerably more\nconsistent performance across multiple datasets. Code available at\nhttps://github.com/Rufaim/routing-by-memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural\nchoice for the node classification task. Their drawback in real-world\ndeployment is the latency that emerges from the neighbourhood processing\noperation. One solution to the latency issue is to perform knowledge\ndistillation from a trained GNN to a Multi-Layer Perceptron (MLP), where the\nMLP processes only the features of the node being classified (and possibly some\npre-computed structural information). However, the performance of such MLPs in\nboth transductive and inductive settings remains inconsistent for existing\nknowledge distillation techniques. We propose to address the performance\nconcerns by using a specially-designed student model instead of an MLP. Our\nmodel, named Routing-by-Memory (RbM), is a form of Mixture-of-Experts (MoE),\nwith a design that enforces expert specialization. By encouraging each expert\nto specialize on a certain region on the hidden representation space, we\ndemonstrate experimentally that it is possible to derive considerably more\nconsistent performance across multiple datasets. Code available at\nhttps://github.com/Rufaim/routing-by-memory."
                },
                "authors": [
                    {
                        "name": "Pavel Rumiantsev"
                    },
                    {
                        "name": "Mark Coates"
                    }
                ],
                "author_detail": {
                    "name": "Mark Coates"
                },
                "author": "Mark Coates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13504v2",
                "updated": "2024-11-21T05:19:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    19,
                    56,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T17:55:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "Disentangling Memory and Reasoning Ability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Memory and Reasoning Ability in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08109v2",
                "updated": "2024-11-21T04:39:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    39,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-10T16:56:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    56,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "A Closer Look at Machine Unlearning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at Machine Unlearning for Large Language Models"
                },
                "summary": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning."
                },
                "authors": [
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03346v2",
                "updated": "2024-11-21T04:35:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    35,
                    46,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-03T16:20:32Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    16,
                    20,
                    32,
                    6,
                    308,
                    0
                ],
                "title": "Fixing Security Vulnerabilities with AI in OSS-Fuzz",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Security Vulnerabilities with AI in OSS-Fuzz"
                },
                "summary": "Critical open source software systems undergo significant validation in the\nform of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased\nrandom search over the domain of program inputs, to find inputs which crash the\nsoftware system. Such fuzzing is useful to enhance the security of software\nsystems in general since even closed source software may use open source\ncomponents. Hence testing open source software is of paramount importance.\nCurrently OSS-Fuzz is the most significant and widely used infrastructure for\ncontinuous validation of open source systems. Unfortunately even though\nOSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more\nsoftware projects, the detected vulnerabilities may remain unpatched, as\nvulnerability fixing is often manual in practice. In this work, we rely on the\nrecent progress in Large Language Model (LLM) agents for autonomous program\nimprovement including bug fixing. We customise the well-known AutoCodeRover\nagent for fixing security vulnerabilities. This is because LLM agents like\nAutoCodeRover fix bugs from issue descriptions via code search. Instead for\nsecurity patching, we rely on the test execution of the exploit input to\nextract code elements relevant to the fix. Our experience with OSS-Fuzz\nvulnerability data shows that LLM agent autonomy is useful for successful\nsecurity patching, as opposed to approaches like Agentless where the control\nflow is fixed. More importantly our findings show that we cannot measure\nquality of patches by code similarity of the patch with reference codes (as in\nCodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores\nstill fail to pass given the given exploit input. Our findings indicate that\nsecurity patch correctness needs to consider dynamic attributes like test\nexecutions as opposed to relying of standard text/code similarity metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical open source software systems undergo significant validation in the\nform of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased\nrandom search over the domain of program inputs, to find inputs which crash the\nsoftware system. Such fuzzing is useful to enhance the security of software\nsystems in general since even closed source software may use open source\ncomponents. Hence testing open source software is of paramount importance.\nCurrently OSS-Fuzz is the most significant and widely used infrastructure for\ncontinuous validation of open source systems. Unfortunately even though\nOSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more\nsoftware projects, the detected vulnerabilities may remain unpatched, as\nvulnerability fixing is often manual in practice. In this work, we rely on the\nrecent progress in Large Language Model (LLM) agents for autonomous program\nimprovement including bug fixing. We customise the well-known AutoCodeRover\nagent for fixing security vulnerabilities. This is because LLM agents like\nAutoCodeRover fix bugs from issue descriptions via code search. Instead for\nsecurity patching, we rely on the test execution of the exploit input to\nextract code elements relevant to the fix. Our experience with OSS-Fuzz\nvulnerability data shows that LLM agent autonomy is useful for successful\nsecurity patching, as opposed to approaches like Agentless where the control\nflow is fixed. More importantly our findings show that we cannot measure\nquality of patches by code similarity of the patch with reference codes (as in\nCodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores\nstill fail to pass given the given exploit input. Our findings indicate that\nsecurity patch correctness needs to consider dynamic attributes like test\nexecutions as opposed to relying of standard text/code similarity metrics."
                },
                "authors": [
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Dominic Berzin"
                    },
                    {
                        "name": "Martin Mirchev"
                    },
                    {
                        "name": "Dongge Liu"
                    },
                    {
                        "name": "Abhishek Arya"
                    },
                    {
                        "name": "Oliver Chang"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13826v1",
                "updated": "2024-11-21T04:23:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    23,
                    17,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T04:23:17Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    23,
                    17,
                    3,
                    326,
                    0
                ],
                "title": "Interactive and Expressive Code-Augmented Planning with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive and Expressive Code-Augmented Planning with Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong abilities in common-sense\nreasoning and interactive decision-making, but often struggle with complex,\nlong-horizon planning tasks. Recent techniques have sought to structure LLM\noutputs using control flow and other code-adjacent techniques to improve\nplanning performance. These techniques include using variables (to track\nimportant information) and functions (to divide complex tasks into smaller\nre-usable sub-tasks). However, purely code-based approaches can be error-prone\nand insufficient for handling ambiguous or unstructured data. To address these\nchallenges, we propose REPL-Plan, an LLM planning approach that is fully\ncode-expressive (it can utilize all the benefits of code) while also being\ndynamic (it can flexibly adapt from errors and use the LLM for fuzzy\nsituations). In REPL-Plan, an LLM solves tasks by interacting with a\nRead-Eval-Print Loop (REPL), which iteratively executes and evaluates code,\nsimilar to language shells or interactive code notebooks, allowing the model to\nflexibly correct errors and handle tasks dynamically. We demonstrate that\nREPL-Plan achieves strong results across various planning domains compared to\nprevious methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong abilities in common-sense\nreasoning and interactive decision-making, but often struggle with complex,\nlong-horizon planning tasks. Recent techniques have sought to structure LLM\noutputs using control flow and other code-adjacent techniques to improve\nplanning performance. These techniques include using variables (to track\nimportant information) and functions (to divide complex tasks into smaller\nre-usable sub-tasks). However, purely code-based approaches can be error-prone\nand insufficient for handling ambiguous or unstructured data. To address these\nchallenges, we propose REPL-Plan, an LLM planning approach that is fully\ncode-expressive (it can utilize all the benefits of code) while also being\ndynamic (it can flexibly adapt from errors and use the LLM for fuzzy\nsituations). In REPL-Plan, an LLM solves tasks by interacting with a\nRead-Eval-Print Loop (REPL), which iteratively executes and evaluates code,\nsimilar to language shells or interactive code notebooks, allowing the model to\nflexibly correct errors and handle tasks dynamically. We demonstrate that\nREPL-Plan achieves strong results across various planning domains compared to\nprevious methods."
                },
                "authors": [
                    {
                        "name": "Anthony Z. Liu"
                    },
                    {
                        "name": "Xinhe Wang"
                    },
                    {
                        "name": "Jacob Sansom"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Jongwook Choi"
                    },
                    {
                        "name": "Sungryull Sohn"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Honglak Lee"
                    }
                ],
                "author_detail": {
                    "name": "Honglak Lee"
                },
                "author": "Honglak Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15864v4",
                "updated": "2024-11-21T03:51:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    51,
                    58,
                    3,
                    326,
                    0
                ],
                "published": "2023-11-27T14:32:33Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    14,
                    32,
                    33,
                    0,
                    331,
                    0
                ],
                "title": "InterControl: Zero-shot Human Interaction Generation by Controlling\n  Every Joint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterControl: Zero-shot Human Interaction Generation by Controlling\n  Every Joint"
                },
                "summary": "Text-conditioned motion synthesis has made remarkable progress with the\nemergence of diffusion models. However, the majority of these motion diffusion\nmodels are primarily designed for a single character and overlook multi-human\ninteractions. In our approach, we strive to explore this problem by\nsynthesizing human motion with interactions for a group of characters of any\nsize in a zero-shot manner. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators. Code is available at\nhttps://github.com/zhenzhiwang/intercontrol",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-conditioned motion synthesis has made remarkable progress with the\nemergence of diffusion models. However, the majority of these motion diffusion\nmodels are primarily designed for a single character and overlook multi-human\ninteractions. In our approach, we strive to explore this problem by\nsynthesizing human motion with interactions for a group of characters of any\nsize in a zero-shot manner. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators. Code is available at\nhttps://github.com/zhenzhiwang/intercontrol"
                },
                "authors": [
                    {
                        "name": "Zhenzhi Wang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "NeurIPS 2024 camera ready version. TL;DR: Generate human interactions\n  with only single-person motion data in training via joint contact pairs from\n  LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13817v1",
                "updated": "2024-11-21T03:48:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    48,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:48:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    48,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "Dynamic Structural Clustering Unleashed: Flexible Similarities,\n  Versatile Updates and for All Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Structural Clustering Unleashed: Flexible Similarities,\n  Versatile Updates and for All Parameters"
                },
                "summary": "We study structural clustering on graphs in dynamic scenarios, where the\ngraphs can be updated by arbitrary insertions or deletions of edges/vertices.\nThe goal is to efficiently compute structural clustering results for any\nclustering parameters $\\epsilon$ and $\\mu$ given on the fly, for arbitrary\ngraph update patterns, and for all typical similarity measurements.\nSpecifically, we adopt the idea of update affordability and propose an\na-lot-simpler yet more efficient (both theoretically and practically) algorithm\n(than state of the art), named VD-STAR to handle graph updates. First, with a\ntheoretical clustering result quality guarantee, VD-STAR can output\nhigh-quality clustering results with up to 99.9% accuracy. Second, our VD-STAR\nis easy to implement as it just needs to maintain certain sorted linked lists\nand hash tables, and hence, effectively enhances its deployment in practice.\nThird and most importantly, by careful analysis, VD-STAR improves the\nper-update time bound of the state-of-the-art from $O(\\log^2 n)$ expected with\ncertain update pattern assumption to $O(\\log n)$ amortized in expectation\nwithout any update pattern assumption. We further design two variants of\nVD-STAR to enhance its empirical performance. Experimental results show that\nour algorithms consistently outperform the state-of-the-art competitors by up\nto 9,315 times in update time across nine real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study structural clustering on graphs in dynamic scenarios, where the\ngraphs can be updated by arbitrary insertions or deletions of edges/vertices.\nThe goal is to efficiently compute structural clustering results for any\nclustering parameters $\\epsilon$ and $\\mu$ given on the fly, for arbitrary\ngraph update patterns, and for all typical similarity measurements.\nSpecifically, we adopt the idea of update affordability and propose an\na-lot-simpler yet more efficient (both theoretically and practically) algorithm\n(than state of the art), named VD-STAR to handle graph updates. First, with a\ntheoretical clustering result quality guarantee, VD-STAR can output\nhigh-quality clustering results with up to 99.9% accuracy. Second, our VD-STAR\nis easy to implement as it just needs to maintain certain sorted linked lists\nand hash tables, and hence, effectively enhances its deployment in practice.\nThird and most importantly, by careful analysis, VD-STAR improves the\nper-update time bound of the state-of-the-art from $O(\\log^2 n)$ expected with\ncertain update pattern assumption to $O(\\log n)$ amortized in expectation\nwithout any update pattern assumption. We further design two variants of\nVD-STAR to enhance its empirical performance. Experimental results show that\nour algorithms consistently outperform the state-of-the-art competitors by up\nto 9,315 times in update time across nine real datasets."
                },
                "authors": [
                    {
                        "name": "Zhuowei Zhao"
                    },
                    {
                        "name": "Junhao Gan"
                    },
                    {
                        "name": "Boyu Ruan"
                    },
                    {
                        "name": "Zhifeng Bao"
                    },
                    {
                        "name": "Jianzhong Qi"
                    },
                    {
                        "name": "Sibo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sibo Wang"
                },
                "author": "Sibo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13814v1",
                "updated": "2024-11-21T03:35:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    35,
                    7,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:35:07Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    35,
                    7,
                    3,
                    326,
                    0
                ],
                "title": "AutoMixQ: Self-Adjusting Quantization for High Performance\n  Memory-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMixQ: Self-Adjusting Quantization for High Performance\n  Memory-Efficient Fine-Tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) under resource constraints is a\nsignificant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning,\nand quantization are all effective methods for improving resource efficiency.\nHowever, combining them directly often results in suboptimal performance,\nespecially with uniform quantization across all model layers. This is due to\nthe complex, uneven interlayer relationships introduced by pruning,\nnecessitating more refined quantization strategies. To address this, we propose\nAutoMixQ, an end-to-end optimization framework that selects optimal\nquantization configurations for each LLM layer. AutoMixQ leverages lightweight\nperformance models to guide the selection process, significantly reducing time\nand computational resources compared to exhaustive search methods. By\nincorporating Pareto optimality, AutoMixQ balances memory usage and\nperformance, approaching the upper bounds of model capability under strict\nresource constraints. Our experiments on widely used benchmarks show that\nAutoMixQ reduces memory consumption while achieving superior performance. For\nexample, at a 30\\% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21\\% on BoolQ\ncompared to 62.45\\% for LoRA and 58.96\\% for LoftQ, while reducing memory\nconsumption by 35.5\\% compared to LoRA and 27.5\\% compared to LoftQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) under resource constraints is a\nsignificant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning,\nand quantization are all effective methods for improving resource efficiency.\nHowever, combining them directly often results in suboptimal performance,\nespecially with uniform quantization across all model layers. This is due to\nthe complex, uneven interlayer relationships introduced by pruning,\nnecessitating more refined quantization strategies. To address this, we propose\nAutoMixQ, an end-to-end optimization framework that selects optimal\nquantization configurations for each LLM layer. AutoMixQ leverages lightweight\nperformance models to guide the selection process, significantly reducing time\nand computational resources compared to exhaustive search methods. By\nincorporating Pareto optimality, AutoMixQ balances memory usage and\nperformance, approaching the upper bounds of model capability under strict\nresource constraints. Our experiments on widely used benchmarks show that\nAutoMixQ reduces memory consumption while achieving superior performance. For\nexample, at a 30\\% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21\\% on BoolQ\ncompared to 62.45\\% for LoRA and 58.96\\% for LoftQ, while reducing memory\nconsumption by 35.5\\% compared to LoRA and 27.5\\% compared to LoftQ."
                },
                "authors": [
                    {
                        "name": "Changhai Zhou"
                    },
                    {
                        "name": "Shiyang Zhang"
                    },
                    {
                        "name": "Yuhua Zhou"
                    },
                    {
                        "name": "Zekai Liu"
                    },
                    {
                        "name": "Shichao Weng"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Weng"
                },
                "author": "Shichao Weng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13809v1",
                "updated": "2024-11-21T03:16:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    16,
                    5,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:16:05Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    16,
                    5,
                    3,
                    326,
                    0
                ],
                "title": "DCSim: Computing and Networking Integration based Container Scheduling\n  Simulator for Data Centers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCSim: Computing and Networking Integration based Container Scheduling\n  Simulator for Data Centers"
                },
                "summary": "The increasing prevalence of cloud-native technologies, particularly\ncontainers, has led to the widespread adoption of containerized deployments in\ndata centers. The advancement of deep neural network models has increased the\ndemand for container-based distributed model training and inference, where\nfrequent data transmission among nodes has emerged as a significant performance\nbottleneck. However, traditional container scheduling simulators often overlook\nthe influence of network modeling on the efficiency of container scheduling,\nprimarily concentrating on modeling computational resources. In this paper, we\nfocus on a container scheduling simulator based on collaboration between\ncomputing and networking within data centers. We propose a new container\nscheduling simulator for data centers, named DCSim. The simulator consists of\nseveral modules: a data center module, a network simulation module, a container\nscheduling module, a discrete event-driven module, and a data collection and\nanalysis module. Together, these modules provide heterogeneous computing power\nmodeling and dynamic network simulation capabilities. We design a discrete\nevent model using SimPy to represent various aspects of container processing,\nincluding container requests, scheduling, execution, pauses, communication,\nmigration, and termination within data centers. Among these, lightweight\nvirtualization technology based on Mininet is employed to construct a\nsoftware-defined network. An experimental environment for container scheduling\nsimulation was established, and functional and performance tests were conducted\non the simulator to validate its scheduling simulation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of cloud-native technologies, particularly\ncontainers, has led to the widespread adoption of containerized deployments in\ndata centers. The advancement of deep neural network models has increased the\ndemand for container-based distributed model training and inference, where\nfrequent data transmission among nodes has emerged as a significant performance\nbottleneck. However, traditional container scheduling simulators often overlook\nthe influence of network modeling on the efficiency of container scheduling,\nprimarily concentrating on modeling computational resources. In this paper, we\nfocus on a container scheduling simulator based on collaboration between\ncomputing and networking within data centers. We propose a new container\nscheduling simulator for data centers, named DCSim. The simulator consists of\nseveral modules: a data center module, a network simulation module, a container\nscheduling module, a discrete event-driven module, and a data collection and\nanalysis module. Together, these modules provide heterogeneous computing power\nmodeling and dynamic network simulation capabilities. We design a discrete\nevent model using SimPy to represent various aspects of container processing,\nincluding container requests, scheduling, execution, pauses, communication,\nmigration, and termination within data centers. Among these, lightweight\nvirtualization technology based on Mininet is employed to construct a\nsoftware-defined network. An experimental environment for container scheduling\nsimulation was established, and functional and performance tests were conducted\non the simulator to validate its scheduling simulation capabilities."
                },
                "authors": [
                    {
                        "name": "Jinlong Hu"
                    },
                    {
                        "name": "Zhizhe Rao"
                    },
                    {
                        "name": "Xingchen Liu"
                    },
                    {
                        "name": "Lihao Deng"
                    },
                    {
                        "name": "Shoubin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Shoubin Dong"
                },
                "author": "Shoubin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13808v1",
                "updated": "2024-11-21T03:14:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    14,
                    31,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:14:31Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    14,
                    31,
                    3,
                    326,
                    0
                ],
                "title": "GPAI Evaluations Standards Taskforce: Towards Effective AI Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPAI Evaluations Standards Taskforce: Towards Effective AI Governance"
                },
                "summary": "General-purpose AI evaluations have been proposed as a promising way of\nidentifying and mitigating systemic risks posed by AI development and\ndeployment. While GPAI evaluations play an increasingly central role in\ninstitutional decision- and policy-making -- including by way of the European\nUnion AI Act's mandate to conduct evaluations on GPAI models presenting\nsystemic risk -- no standards exist to date to promote their quality or\nlegitimacy. To strengthen GPAI evaluations in the EU, which currently\nconstitutes the first and only jurisdiction that mandates GPAI evaluations, we\noutline four desiderata for GPAI evaluations: internal validity, external\nvalidity, reproducibility, and portability. To uphold these desiderata in a\ndynamic environment of continuously evolving risks, we propose a dedicated EU\nGPAI Evaluation Standards Taskforce, to be housed within the bodies established\nby the EU AI Act. We outline the responsibilities of the Taskforce, specify the\nGPAI provider commitments that would facilitate Taskforce success, discuss the\npotential impact of the Taskforce on global AI governance, and address\npotential sources of failure that policymakers should heed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose AI evaluations have been proposed as a promising way of\nidentifying and mitigating systemic risks posed by AI development and\ndeployment. While GPAI evaluations play an increasingly central role in\ninstitutional decision- and policy-making -- including by way of the European\nUnion AI Act's mandate to conduct evaluations on GPAI models presenting\nsystemic risk -- no standards exist to date to promote their quality or\nlegitimacy. To strengthen GPAI evaluations in the EU, which currently\nconstitutes the first and only jurisdiction that mandates GPAI evaluations, we\noutline four desiderata for GPAI evaluations: internal validity, external\nvalidity, reproducibility, and portability. To uphold these desiderata in a\ndynamic environment of continuously evolving risks, we propose a dedicated EU\nGPAI Evaluation Standards Taskforce, to be housed within the bodies established\nby the EU AI Act. We outline the responsibilities of the Taskforce, specify the\nGPAI provider commitments that would facilitate Taskforce success, discuss the\npotential impact of the Taskforce on global AI governance, and address\npotential sources of failure that policymakers should heed."
                },
                "authors": [
                    {
                        "name": "Patricia Paskov"
                    },
                    {
                        "name": "Lukas Berglund"
                    },
                    {
                        "name": "Everett Smith"
                    },
                    {
                        "name": "Lisa Soder"
                    }
                ],
                "author_detail": {
                    "name": "Lisa Soder"
                },
                "author": "Lisa Soder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13802v1",
                "updated": "2024-11-21T03:05:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    5,
                    38,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:05:38Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    5,
                    38,
                    3,
                    326,
                    0
                ],
                "title": "SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong"
                },
                "authors": [
                    {
                        "name": "Christopher Nguyen"
                    },
                    {
                        "name": "William Nguyen"
                    },
                    {
                        "name": "Atsushi Suzuki"
                    },
                    {
                        "name": "Daisuke Oku"
                    },
                    {
                        "name": "Hong An Phan"
                    },
                    {
                        "name": "Sang Dinh"
                    },
                    {
                        "name": "Zooey Nguyen"
                    },
                    {
                        "name": "Anh Ha"
                    },
                    {
                        "name": "Shruti Raghavan"
                    },
                    {
                        "name": "Huy Vo"
                    },
                    {
                        "name": "Thang Nguyen"
                    },
                    {
                        "name": "Lan Nguyen"
                    },
                    {
                        "name": "Yoshikuni Hirayama"
                    }
                ],
                "author_detail": {
                    "name": "Yoshikuni Hirayama"
                },
                "author": "Yoshikuni Hirayama",
                "arxiv_comment": "On-going work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13799v1",
                "updated": "2024-11-21T02:57:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    57,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:57:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    57,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Unconsidered Installations: Discovering IoT Deployments in the IPv6\n  Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconsidered Installations: Discovering IoT Deployments in the IPv6\n  Internet"
                },
                "summary": "Internet-wide studies provide extremely valuable insight into how operators\nmanage their Internet of Things (IoT) deployments in reality and often reveal\ngrievances, e.g., significant security issues. However, while IoT devices often\nuse IPv6, past studies resorted to comprehensively scan the IPv4 address space.\nTo fully understand how the IoT and all its services and devices is operated,\nincluding IPv6-reachable deployments is inevitable-although scanning the entire\nIPv6 address space is infeasible. In this paper, we close this gap and examine\nhow to best discover IPv6-reachable IoT deployments. To this end, we propose a\nmethodology that allows combining various IPv6 scan direction approaches to\nunderstand the findability and prevalence of IPv6-reachable IoT deployments.\nUsing three sources of active IPv6 addresses and eleven address generators, we\ndiscovered 6658 IoT deployments. We derive that the available address sources\nare a good starting point for finding IoT deployments. Additionally, we show\nthat using two address generators is sufficient to cover most found deployments\nand save time as well as resources. Assessing the security of the deployments,\nwe surprisingly find similar issues as in the IPv4 Internet, although IPv6\ndeployments might be newer and generally more up-to-date: Only 39% of\ndeployments have access control in place and only 6.2% make use of TLS inviting\nattackers, e.g., to eavesdrop sensitive data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet-wide studies provide extremely valuable insight into how operators\nmanage their Internet of Things (IoT) deployments in reality and often reveal\ngrievances, e.g., significant security issues. However, while IoT devices often\nuse IPv6, past studies resorted to comprehensively scan the IPv4 address space.\nTo fully understand how the IoT and all its services and devices is operated,\nincluding IPv6-reachable deployments is inevitable-although scanning the entire\nIPv6 address space is infeasible. In this paper, we close this gap and examine\nhow to best discover IPv6-reachable IoT deployments. To this end, we propose a\nmethodology that allows combining various IPv6 scan direction approaches to\nunderstand the findability and prevalence of IPv6-reachable IoT deployments.\nUsing three sources of active IPv6 addresses and eleven address generators, we\ndiscovered 6658 IoT deployments. We derive that the available address sources\nare a good starting point for finding IoT deployments. Additionally, we show\nthat using two address generators is sufficient to cover most found deployments\nand save time as well as resources. Assessing the security of the deployments,\nwe surprisingly find similar issues as in the IPv4 Internet, although IPv6\ndeployments might be newer and generally more up-to-date: Only 39% of\ndeployments have access control in place and only 6.2% make use of TLS inviting\nattackers, e.g., to eavesdrop sensitive data."
                },
                "authors": [
                    {
                        "name": "Markus Dahlmanns"
                    },
                    {
                        "name": "Felix Heidenreich"
                    },
                    {
                        "name": "Johannes LohmÃ¶ller"
                    },
                    {
                        "name": "Jan Pennekamp"
                    },
                    {
                        "name": "Klaus Wehrle"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_doi": "10.1109/NOMS59830.2024.10574963",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/NOMS59830.2024.10574963",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 2 figures",
                "arxiv_journal_ref": "In Proceedings of the 2024 IEEE/IFIP Network Operations and\n  Management Symposium (NOMS '24), May 6-10, 2024, Seoul, Korea",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08435v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08435v4",
                "updated": "2024-11-21T02:31:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    31,
                    15,
                    3,
                    326,
                    0
                ],
                "published": "2024-09-13T00:03:19Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    3,
                    19,
                    4,
                    257,
                    0
                ],
                "title": "When Context Leads but Parametric Memory Follows in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Context Leads but Parametric Memory Follows in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance."
                },
                "authors": [
                    {
                        "name": "Yufei Tao"
                    },
                    {
                        "name": "Adam Hiatt"
                    },
                    {
                        "name": "Erik Haake"
                    },
                    {
                        "name": "Antonie J. Jetter"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal",
                "arxiv_comment": "Accepted by EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08435v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08435v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13789v1",
                "updated": "2024-11-21T02:22:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    22,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:22:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    22,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System"
                },
                "summary": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day."
                },
                "authors": [
                    {
                        "name": "Fengxin Li"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xiaoxiang Deng"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Haijie Gu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hongyan Liu"
                    },
                    {
                        "name": "Biao Qin"
                    },
                    {
                        "name": "Jun He"
                    }
                ],
                "author_detail": {
                    "name": "Jun He"
                },
                "author": "Jun He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13787v1",
                "updated": "2024-11-21T02:18:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    18,
                    6,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    18,
                    6,
                    3,
                    326,
                    0
                ],
                "title": "Edge-Cloud Routing for Text-to-Image Model with Token-Level Multi-Metric\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Cloud Routing for Text-to-Image Model with Token-Level Multi-Metric\n  Prediction"
                },
                "summary": "Large text-to-image models demonstrate impressive generation capabilities;\nhowever, their substantial size necessitates expensive cloud servers for\ndeployment. Conversely, light-weight models can be deployed on edge devices at\nlower cost but often with inferior generation quality for complex user prompts.\nTo strike a balance between performance and cost, we propose a routing\nframework, called \\texttt{RouteT2I}, which dynamically selects either the large\ncloud model or the light-weight edge model for each user prompt. Since\ngenerated image quality is challenging to measure directly, \\texttt{RouteT2I}\nestablishes multi-dimensional quality metrics, particularly, by evaluating the\nsimilarity between the generated images and both positive and negative texts\nthat describe each specific quality metric. \\texttt{RouteT2I} then predicts the\nexpected quality of the generated images by identifying key tokens in the\nprompt and comparing their impact on the quality. \\texttt{RouteT2I} further\nintroduces the Pareto relative superiority to compare the multi-metric quality\nof the generated images. Based on this comparison and predefined cost\nconstraints, \\texttt{RouteT2I} allocates prompts to either the edge or the\ncloud. Evaluation reveals that \\texttt{RouteT2I} significantly reduces the\nnumber of requesting large cloud model while maintaining high-quality image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large text-to-image models demonstrate impressive generation capabilities;\nhowever, their substantial size necessitates expensive cloud servers for\ndeployment. Conversely, light-weight models can be deployed on edge devices at\nlower cost but often with inferior generation quality for complex user prompts.\nTo strike a balance between performance and cost, we propose a routing\nframework, called \\texttt{RouteT2I}, which dynamically selects either the large\ncloud model or the light-weight edge model for each user prompt. Since\ngenerated image quality is challenging to measure directly, \\texttt{RouteT2I}\nestablishes multi-dimensional quality metrics, particularly, by evaluating the\nsimilarity between the generated images and both positive and negative texts\nthat describe each specific quality metric. \\texttt{RouteT2I} then predicts the\nexpected quality of the generated images by identifying key tokens in the\nprompt and comparing their impact on the quality. \\texttt{RouteT2I} further\nintroduces the Pareto relative superiority to compare the multi-metric quality\nof the generated images. Based on this comparison and predefined cost\nconstraints, \\texttt{RouteT2I} allocates prompts to either the edge or the\ncloud. Evaluation reveals that \\texttt{RouteT2I} significantly reduces the\nnumber of requesting large cloud model while maintaining high-quality image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Zewei Xin"
                    },
                    {
                        "name": "Qinya Li"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11121v2",
                "updated": "2024-11-21T02:16:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    16,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2024-04-17T07:08:45Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    7,
                    8,
                    45,
                    2,
                    108,
                    0
                ],
                "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing\n  in Edge Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing\n  in Edge Deployment"
                },
                "summary": "Proprietary large language models (LLMs) have been widely applied in various\nscenarios. Additionally, deploying LLMs on edge devices is trending for\nefficiency and privacy reasons. However, edge deployment of proprietary LLMs\nintroduces new security challenges: edge-deployed models are exposed as\nwhite-box accessible to users, enabling adversaries to conduct effective model\nstealing (MS) attacks. Unfortunately, existing defense mechanisms fail to\nprovide effective protection. Specifically, we identify four critical\nprotection properties that existing methods fail to simultaneously satisfy: (1)\nmaintaining protection after a model is physically copied; (2) authorizing\nmodel access at request level; (3) safeguarding runtime reverse engineering;\n(4) achieving high security with negligible runtime overhead. To address the\nabove issues, we propose TransLinkGuard, a plug-and-play model protection\napproach against model stealing on edge devices. The core part of\nTransLinkGuard is a lightweight authorization module residing in a secure\nenvironment, e.g., TEE. The authorization module can freshly authorize each\nrequest based on its input. Extensive experiments show that TransLinkGuard\nachieves the same security protection as the black-box security guarantees with\nnegligible overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary large language models (LLMs) have been widely applied in various\nscenarios. Additionally, deploying LLMs on edge devices is trending for\nefficiency and privacy reasons. However, edge deployment of proprietary LLMs\nintroduces new security challenges: edge-deployed models are exposed as\nwhite-box accessible to users, enabling adversaries to conduct effective model\nstealing (MS) attacks. Unfortunately, existing defense mechanisms fail to\nprovide effective protection. Specifically, we identify four critical\nprotection properties that existing methods fail to simultaneously satisfy: (1)\nmaintaining protection after a model is physically copied; (2) authorizing\nmodel access at request level; (3) safeguarding runtime reverse engineering;\n(4) achieving high security with negligible runtime overhead. To address the\nabove issues, we propose TransLinkGuard, a plug-and-play model protection\napproach against model stealing on edge devices. The core part of\nTransLinkGuard is a lightweight authorization module residing in a secure\nenvironment, e.g., TEE. The authorization module can freshly authorize each\nrequest based on its input. Extensive experiments show that TransLinkGuard\nachieves the same security protection as the black-box security guarantees with\nnegligible overhead."
                },
                "authors": [
                    {
                        "name": "Qinfeng Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Zhenghan Qin"
                    },
                    {
                        "name": "Yangfan Xie"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_doi": "10.1145/3664647.3680786",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3680786",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM MM24 Conference",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.15179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.15179v4",
                "updated": "2024-11-21T01:47:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    47,
                    32,
                    3,
                    326,
                    0
                ],
                "published": "2022-09-30T01:59:53Z",
                "published_parsed": [
                    2022,
                    9,
                    30,
                    1,
                    59,
                    53,
                    4,
                    273,
                    0
                ],
                "title": "Physical Adversarial Attack meets Computer Vision: A Decade Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Adversarial Attack meets Computer Vision: A Decade Survey"
                },
                "summary": "Despite the impressive achievements of Deep Neural Networks (DNNs) in\ncomputer vision, their vulnerability to adversarial attacks remains a critical\nconcern. Extensive research has demonstrated that incorporating sophisticated\nperturbations into input images can lead to a catastrophic degradation in DNNs'\nperformance. This perplexing phenomenon not only exists in the digital space\nbut also in the physical world. Consequently, it becomes imperative to evaluate\nthe security of DNNs-based systems to ensure their safe deployment in\nreal-world scenarios, particularly in security-sensitive applications. To\nfacilitate a profound understanding of this topic, this paper presents a\ncomprehensive overview of physical adversarial attacks. Firstly, we distill\nfour general steps for launching physical adversarial attacks. Building upon\nthis foundation, we uncover the pervasive role of artifacts carrying\nadversarial perturbations in the physical world. These artifacts influence each\nstep. To denote them, we introduce a new term: adversarial medium. Then, we\ntake the first step to systematically evaluate the performance of physical\nadversarial attacks, taking the adversarial medium as a first attempt. Our\nproposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness,\nStealthiness, Robustness, Practicability, Aesthetics, and Economics. We also\nprovide comparative results across task categories, together with insightful\nobservations and suggestions for future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive achievements of Deep Neural Networks (DNNs) in\ncomputer vision, their vulnerability to adversarial attacks remains a critical\nconcern. Extensive research has demonstrated that incorporating sophisticated\nperturbations into input images can lead to a catastrophic degradation in DNNs'\nperformance. This perplexing phenomenon not only exists in the digital space\nbut also in the physical world. Consequently, it becomes imperative to evaluate\nthe security of DNNs-based systems to ensure their safe deployment in\nreal-world scenarios, particularly in security-sensitive applications. To\nfacilitate a profound understanding of this topic, this paper presents a\ncomprehensive overview of physical adversarial attacks. Firstly, we distill\nfour general steps for launching physical adversarial attacks. Building upon\nthis foundation, we uncover the pervasive role of artifacts carrying\nadversarial perturbations in the physical world. These artifacts influence each\nstep. To denote them, we introduce a new term: adversarial medium. Then, we\ntake the first step to systematically evaluate the performance of physical\nadversarial attacks, taking the adversarial medium as a first attempt. Our\nproposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness,\nStealthiness, Robustness, Practicability, Aesthetics, and Economics. We also\nprovide comparative results across task categories, together with insightful\nobservations and suggestions for future research directions."
                },
                "authors": [
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Xuemei Jia"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Hanxun Yu"
                    },
                    {
                        "name": "Zhubo Li"
                    },
                    {
                        "name": "Shin'ichi Satoh"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "arxiv_doi": "10.1109/TPAMI.2024.3430860",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2024.3430860",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2209.15179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.15179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at IEEE TPAMI. GitHub:https://github.com/weihui1308/PAA",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13779v1",
                "updated": "2024-11-21T01:37:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    37,
                    38,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T01:37:38Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    37,
                    38,
                    3,
                    326,
                    0
                ],
                "title": "NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap\n  via Informational Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap\n  via Informational Interviews"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating coherent text but often struggle with grounding language and\nstrategic dialogue. To address this gap, we focus on journalistic interviews, a\ndomain rich in grounding communication and abundant in data. We curate a\ndataset of 40,000 two-person informational interviews from NPR and CNN, and\nreveal that LLMs are significantly less likely than human interviewers to use\nacknowledgements and to pivot to higher-level questions. Realizing that a\nfundamental deficit exists in multi-turn planning and strategic thinking, we\ndevelop a realistic simulated environment, incorporating source personas and\npersuasive elements, in order to facilitate the development of agents with\nlonger-horizon rewards. Our experiments show that while source LLMs mimic human\nbehavior in information sharing, interviewer LLMs struggle with recognizing\nwhen questions are answered and engaging persuasively, leading to suboptimal\ninformation extraction across model size and capability. These findings\nunderscore the need for enhancing LLMs' strategic dialogue capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating coherent text but often struggle with grounding language and\nstrategic dialogue. To address this gap, we focus on journalistic interviews, a\ndomain rich in grounding communication and abundant in data. We curate a\ndataset of 40,000 two-person informational interviews from NPR and CNN, and\nreveal that LLMs are significantly less likely than human interviewers to use\nacknowledgements and to pivot to higher-level questions. Realizing that a\nfundamental deficit exists in multi-turn planning and strategic thinking, we\ndevelop a realistic simulated environment, incorporating source personas and\npersuasive elements, in order to facilitate the development of agents with\nlonger-horizon rewards. Our experiments show that while source LLMs mimic human\nbehavior in information sharing, interviewer LLMs struggle with recognizing\nwhen questions are answered and engaging persuasively, leading to suboptimal\ninformation extraction across model size and capability. These findings\nunderscore the need for enhancing LLMs' strategic dialogue capabilities."
                },
                "authors": [
                    {
                        "name": "Michael Lu"
                    },
                    {
                        "name": "Hyundong Justin Cho"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Jonathan May"
                    },
                    {
                        "name": "Alexander Spangher"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Spangher"
                },
                "author": "Alexander Spangher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13775v1",
                "updated": "2024-11-21T01:12:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    12,
                    46,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T01:12:46Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    12,
                    46,
                    3,
                    326,
                    0
                ],
                "title": "Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation\n  Across Languages, Domains, and Expertise Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation\n  Across Languages, Domains, and Expertise Levels"
                },
                "summary": "This study presents a comprehensive evaluation of GPT-4's translation\ncapabilities compared to human translators of varying expertise levels. Through\nsystematic human evaluation using the MQM schema, we assess translations across\nthree language pairs (Chinese$\\longleftrightarrow$English,\nRussian$\\longleftrightarrow$English, and Chinese$\\longleftrightarrow$Hindi) and\nthree domains (News, Technology, and Biomedical). Our findings reveal that\nGPT-4 achieves performance comparable to junior-level translators in terms of\ntotal errors, while still lagging behind senior translators. Unlike traditional\nNeural Machine Translation systems, which show significant performance\ndegradation in resource-poor language directions, GPT-4 maintains consistent\ntranslation quality across all evaluated language pairs. Through qualitative\nanalysis, we identify distinctive patterns in translation approaches: GPT-4\ntends toward overly literal translations and exhibits lexical inconsistency,\nwhile human translators sometimes over-interpret context and introduce\nhallucinations. This study represents the first systematic comparison between\nLLM and human translators across different proficiency levels, providing\nvaluable insights into the current capabilities and limitations of LLM-based\ntranslation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive evaluation of GPT-4's translation\ncapabilities compared to human translators of varying expertise levels. Through\nsystematic human evaluation using the MQM schema, we assess translations across\nthree language pairs (Chinese$\\longleftrightarrow$English,\nRussian$\\longleftrightarrow$English, and Chinese$\\longleftrightarrow$Hindi) and\nthree domains (News, Technology, and Biomedical). Our findings reveal that\nGPT-4 achieves performance comparable to junior-level translators in terms of\ntotal errors, while still lagging behind senior translators. Unlike traditional\nNeural Machine Translation systems, which show significant performance\ndegradation in resource-poor language directions, GPT-4 maintains consistent\ntranslation quality across all evaluated language pairs. Through qualitative\nanalysis, we identify distinctive patterns in translation approaches: GPT-4\ntends toward overly literal translations and exhibits lexical inconsistency,\nwhile human translators sometimes over-interpret context and introduce\nhallucinations. This study represents the first systematic comparison between\nLLM and human translators across different proficiency levels, providing\nvaluable insights into the current capabilities and limitations of LLM-based\ntranslation systems."
                },
                "authors": [
                    {
                        "name": "Jianhao Yan"
                    },
                    {
                        "name": "Pingchuan Yan"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xianchao Zhu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11853v2",
                "updated": "2024-11-21T01:10:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    10,
                    30,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-01T08:56:17Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    56,
                    17,
                    4,
                    306,
                    0
                ],
                "title": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat Bankman-Fried: an Exploration of LLM Alignment in Finance"
                },
                "summary": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt nine LLMs\nto impersonate the CEO of a financial institution and test their willingness to\nmisuse customer assets to repay outstanding corporate debt. Beginning with a\nbaseline configuration, we adjust preferences, incentives and constraints,\nanalyzing the impact of each adjustment with logistic regression. Our findings\nreveal significant heterogeneity in the baseline propensity for unethical\nbehavior of LLMs. Factors such as risk aversion, profit expectations, and\nregulatory environment consistently influence misalignment in ways predicted by\neconomic theory, although the magnitude of these effects varies across LLMs.\nThis paper highlights both the benefits and limitations of simulation-based, ex\npost safety testing. While it can inform financial authorities and institutions\naiming to ensure LLM safety, there is a clear trade-off between generality and\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt nine LLMs\nto impersonate the CEO of a financial institution and test their willingness to\nmisuse customer assets to repay outstanding corporate debt. Beginning with a\nbaseline configuration, we adjust preferences, incentives and constraints,\nanalyzing the impact of each adjustment with logistic regression. Our findings\nreveal significant heterogeneity in the baseline propensity for unethical\nbehavior of LLMs. Factors such as risk aversion, profit expectations, and\nregulatory environment consistently influence misalignment in ways predicted by\neconomic theory, although the magnitude of these effects varies across LLMs.\nThis paper highlights both the benefits and limitations of simulation-based, ex\npost safety testing. While it can inform financial authorities and institutions\naiming to ensure LLM safety, there is a clear trade-off between generality and\ncost."
                },
                "authors": [
                    {
                        "name": "Claudia Biancotti"
                    },
                    {
                        "name": "Carolina Camassa"
                    },
                    {
                        "name": "Andrea Coletta"
                    },
                    {
                        "name": "Oliver Giudice"
                    },
                    {
                        "name": "Aldo Glielmo"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Glielmo"
                },
                "author": "Aldo Glielmo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13773v1",
                "updated": "2024-11-21T01:00:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    0,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T01:00:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    1,
                    0,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "FastRAG: Retrieval Augmented Generation for Semi-structured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastRAG: Retrieval Augmented Generation for Semi-structured Data"
                },
                "summary": "Efficiently processing and interpreting network data is critical for the\noperation of increasingly complex networks. Recent advances in Large Language\nModels (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved\ndata processing in network management. However, existing RAG methods like\nVectorRAG and GraphRAG struggle with the complexity and implicit nature of\nsemi-structured technical data, leading to inefficiencies in time, cost, and\nretrieval. This paper introduces FastRAG, a novel RAG approach designed for\nsemi-structured data. FastRAG employs schema learning and script learning to\nextract and structure data without needing to submit entire data sources to an\nLLM. It integrates text search with knowledge graph (KG) querying to improve\naccuracy in retrieving context-rich information. Evaluation results demonstrate\nthat FastRAG provides accurate question answering, while improving up to 90% in\ntime and 85% in cost compared to GraphRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing and interpreting network data is critical for the\noperation of increasingly complex networks. Recent advances in Large Language\nModels (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved\ndata processing in network management. However, existing RAG methods like\nVectorRAG and GraphRAG struggle with the complexity and implicit nature of\nsemi-structured technical data, leading to inefficiencies in time, cost, and\nretrieval. This paper introduces FastRAG, a novel RAG approach designed for\nsemi-structured data. FastRAG employs schema learning and script learning to\nextract and structure data without needing to submit entire data sources to an\nLLM. It integrates text search with knowledge graph (KG) querying to improve\naccuracy in retrieving context-rich information. Evaluation results demonstrate\nthat FastRAG provides accurate question answering, while improving up to 90% in\ntime and 85% in cost compared to GraphRAG."
                },
                "authors": [
                    {
                        "name": "Amar Abane"
                    },
                    {
                        "name": "Anis Bekri"
                    },
                    {
                        "name": "Abdella Battou"
                    }
                ],
                "author_detail": {
                    "name": "Abdella Battou"
                },
                "author": "Abdella Battou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06735v2",
                "updated": "2024-11-21T00:52:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    52,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-11T06:04:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data"
                },
                "summary": "Current forecasting approaches are largely unimodal and ignore the rich\ntextual data that often accompany the time series due to lack of well-curated\nmultimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a\ncarefully curated, time-aligned text and time dataset for multimodal\nforecasting. Our dataset is composed of sequences of numbers and text aligned\nto timestamps, and includes data from two different domains: climate science\nand healthcare. Our data is a significant contribution to the rare selection of\navailable multimodal datasets. We also propose the Hybrid Multi-Modal\nForecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and\ntime series data using shared embeddings. However, contrary to our\nexpectations, our Hybrid-MMF model does not outperform existing baselines in\nour experiments. This negative result highlights the challenges inherent in\nmultimodal forecasting. Our code and data are available at\nhttps://github.com/Rose-STL-Lab/Multimodal_ Forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current forecasting approaches are largely unimodal and ignore the rich\ntextual data that often accompany the time series due to lack of well-curated\nmultimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a\ncarefully curated, time-aligned text and time dataset for multimodal\nforecasting. Our dataset is composed of sequences of numbers and text aligned\nto timestamps, and includes data from two different domains: climate science\nand healthcare. Our data is a significant contribution to the rare selection of\navailable multimodal datasets. We also propose the Hybrid Multi-Modal\nForecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and\ntime series data using shared embeddings. However, contrary to our\nexpectations, our Hybrid-MMF model does not outperform existing baselines in\nour experiments. This negative result highlights the challenges inherent in\nmultimodal forecasting. Our code and data are available at\nhttps://github.com/Rose-STL-Lab/Multimodal_ Forecasting."
                },
                "authors": [
                    {
                        "name": "Kai Kim"
                    },
                    {
                        "name": "Howard Tsai"
                    },
                    {
                        "name": "Rajat Sen"
                    },
                    {
                        "name": "Abhimanyu Das"
                    },
                    {
                        "name": "Zihao Zhou"
                    },
                    {
                        "name": "Abhishek Tanpure"
                    },
                    {
                        "name": "Mathew Luo"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "21 pages, 4 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13768v1",
                "updated": "2024-11-21T00:34:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    34,
                    30,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T00:34:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    34,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "An Evaluation-Driven Approach to Designing LLM Agents: Process and\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation-Driven Approach to Designing LLM Agents: Process and\n  Architecture"
                },
                "summary": "The advent of Large Language Models (LLMs) has enabled the development of LLM\nagents capable of autonomously achieving under-specified goals and continuously\nevolving through post-deployment improvement, sometimes without requiring code\nor model updates. Conventional approaches, such as pre-defined test cases and\ncode/model redevelopment pipelines, are inadequate for addressing the unique\nchallenges of LLM agent development, particularly in terms of quality and risk\ncontrol. This paper introduces an evaluation-driven design approach, inspired\nby test-driven development, to address these challenges. Through a multivocal\nliterature review (MLR), we synthesize existing LLM evaluation methods and\npropose a novel process model and reference architecture specifically designed\nfor LLM agents. The proposed approach integrates online and offline evaluations\nto support adaptive runtime adjustments and systematic offline redevelopment,\nimproving runtime pipelines, artifacts, system architecture, and LLMs by\ncontinuously incorporating evaluation results, including fine-grained feedback\nfrom human and AI evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has enabled the development of LLM\nagents capable of autonomously achieving under-specified goals and continuously\nevolving through post-deployment improvement, sometimes without requiring code\nor model updates. Conventional approaches, such as pre-defined test cases and\ncode/model redevelopment pipelines, are inadequate for addressing the unique\nchallenges of LLM agent development, particularly in terms of quality and risk\ncontrol. This paper introduces an evaluation-driven design approach, inspired\nby test-driven development, to address these challenges. Through a multivocal\nliterature review (MLR), we synthesize existing LLM evaluation methods and\npropose a novel process model and reference architecture specifically designed\nfor LLM agents. The proposed approach integrates online and offline evaluations\nto support adaptive runtime adjustments and systematic offline redevelopment,\nimproving runtime pipelines, artifacts, system architecture, and LLMs by\ncontinuously incorporating evaluation results, including fine-grained feedback\nfrom human and AI evaluators."
                },
                "authors": [
                    {
                        "name": "Boming Xia"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Dehai Zhao"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13766v1",
                "updated": "2024-11-21T00:29:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    29,
                    58,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T00:29:58Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    29,
                    58,
                    3,
                    326,
                    0
                ],
                "title": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language\n  Model on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language\n  Model on the Edge"
                },
                "summary": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Ruiyang Qin"
                    },
                    {
                        "name": "Dancheng Liu"
                    },
                    {
                        "name": "Gelei Xu"
                    },
                    {
                        "name": "Zheyu Yan"
                    },
                    {
                        "name": "Chenhui Xu"
                    },
                    {
                        "name": "Yuting Hu"
                    },
                    {
                        "name": "X. Sharon Hu"
                    },
                    {
                        "name": "Jinjun Xiong"
                    },
                    {
                        "name": "Yiyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yiyu Shi"
                },
                "author": "Yiyu Shi",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10588v2",
                "updated": "2024-11-21T00:24:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    24,
                    36,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-15T21:19:04Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    21,
                    19,
                    4,
                    4,
                    320,
                    0
                ],
                "title": "A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems"
                },
                "summary": "We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions."
                },
                "authors": [
                    {
                        "name": "Caspar Oesterheld"
                    },
                    {
                        "name": "Emery Cooper"
                    },
                    {
                        "name": "Miles Kodama"
                    },
                    {
                        "name": "Linh Chi Nguyen"
                    },
                    {
                        "name": "Ethan Perez"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Perez"
                },
                "author": "Ethan Perez",
                "arxiv_comment": "48 pages, 15 figures; code and data at\n  https://github.com/casparoe/newcomblike_questions_dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13760v1",
                "updated": "2024-11-21T00:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    15,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T00:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    15,
                    44,
                    3,
                    326,
                    0
                ],
                "title": "A Framework for Evaluating LLMs Under Task Indeterminacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Evaluating LLMs Under Task Indeterminacy"
                },
                "summary": "Large language model (LLM) evaluations often assume there is a single correct\nresponse -- a gold label -- for each item in the evaluation corpus. However,\nsome tasks can be ambiguous -- i.e., they provide insufficient information to\nidentify a unique interpretation -- or vague -- i.e., they do not clearly\nindicate where to draw the line when making a determination. Both ambiguity and\nvagueness can cause task indeterminacy -- the condition where some items in the\nevaluation corpus have more than one correct response. In this paper, we\ndevelop a framework for evaluating LLMs under task indeterminacy. Our framework\ndisentangles the relationships between task specification, human ratings, and\nLLM responses in the LLM evaluation pipeline. Using our framework, we conduct a\nsynthetic experiment showing that evaluations that use the \"gold label\"\nassumption underestimate the true performance. We also provide a method for\nestimating an error-adjusted performance interval given partial knowledge about\nindeterminate items in the evaluation corpus. We conclude by outlining\nimplications of our work for the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) evaluations often assume there is a single correct\nresponse -- a gold label -- for each item in the evaluation corpus. However,\nsome tasks can be ambiguous -- i.e., they provide insufficient information to\nidentify a unique interpretation -- or vague -- i.e., they do not clearly\nindicate where to draw the line when making a determination. Both ambiguity and\nvagueness can cause task indeterminacy -- the condition where some items in the\nevaluation corpus have more than one correct response. In this paper, we\ndevelop a framework for evaluating LLMs under task indeterminacy. Our framework\ndisentangles the relationships between task specification, human ratings, and\nLLM responses in the LLM evaluation pipeline. Using our framework, we conduct a\nsynthetic experiment showing that evaluations that use the \"gold label\"\nassumption underestimate the true performance. We also provide a method for\nestimating an error-adjusted performance interval given partial knowledge about\nindeterminate items in the evaluation corpus. We conclude by outlining\nimplications of our work for the research community."
                },
                "authors": [
                    {
                        "name": "Luke Guerdan"
                    },
                    {
                        "name": "Hanna Wallach"
                    },
                    {
                        "name": "Solon Barocas"
                    },
                    {
                        "name": "Alexandra Chouldechova"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Chouldechova"
                },
                "author": "Alexandra Chouldechova",
                "arxiv_comment": "To Appear in NeurIPS 2024 Workshops on Evaluating Evaluations\n  (EvalEval) and Statistical Foundations of LLMs and Foundation Models (SFLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13757v1",
                "updated": "2024-11-21T00:01:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    1,
                    51,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T00:01:51Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    1,
                    51,
                    3,
                    326,
                    0
                ],
                "title": "AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking\n  Vulnerabilities in LLMs through Bit-Flip Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking\n  Vulnerabilities in LLMs through Bit-Flip Attacks"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures."
                },
                "authors": [
                    {
                        "name": "Sanjay Das"
                    },
                    {
                        "name": "Swastik Bhattacharya"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Shamik Kundu"
                    },
                    {
                        "name": "Anand Menon"
                    },
                    {
                        "name": "Arnab Raha"
                    },
                    {
                        "name": "Kanad Basu"
                    }
                ],
                "author_detail": {
                    "name": "Kanad Basu"
                },
                "author": "Kanad Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06587v2",
                "updated": "2024-11-20T23:56:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    23,
                    56,
                    12,
                    2,
                    325,
                    0
                ],
                "published": "2024-08-13T02:54:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    54,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "Establishing Quantum-Secured Channels in Large-Scale Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing Quantum-Secured Channels in Large-Scale Optical Networks"
                },
                "summary": "Quantum-secured optical channels based on Quantum Key Distribution technology\nhave generated a significant global interest. Although the maturity level of\nthe short distance (less than 100 km) quantum-secured channels is at a\ndeployment level, instituting such channels over long distance faces\ntechnological challenges, which is the subject of a world-wide research. In\nthis article an industry perspective on establishing quantum-secured channels\nin large-scale optical networks in operational environments will be discussed,\nincluding the vision, requirements, and technical analysis of different\napproaches for establishing such channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-secured optical channels based on Quantum Key Distribution technology\nhave generated a significant global interest. Although the maturity level of\nthe short distance (less than 100 km) quantum-secured channels is at a\ndeployment level, instituting such channels over long distance faces\ntechnological challenges, which is the subject of a world-wide research. In\nthis article an industry perspective on establishing quantum-secured channels\nin large-scale optical networks in operational environments will be discussed,\nincluding the vision, requirements, and technical analysis of different\napproaches for establishing such channels."
                },
                "authors": [
                    {
                        "name": "Farzam Toudeh-Fallah"
                    }
                ],
                "author_detail": {
                    "name": "Farzam Toudeh-Fallah"
                },
                "author": "Farzam Toudeh-Fallah",
                "arxiv_doi": "10.1117/12.3025871",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3025871",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages. Information regarding the SPIE Conference Proceedings, where\n  this preprint article was published later on was added to the second version",
                "arxiv_journal_ref": "Long-distance quantum-secured optical channels in operational\n  environments, Proceedings of SPIE, Volume 13148, Quantum Communications and\n  Quantum Imaging XXII; 1314803 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]