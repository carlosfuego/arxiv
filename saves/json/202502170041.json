[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v1",
                "updated": "2025-02-13T14:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyzis to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyzis to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_doi": "10.1109/TPAMI.2025.3540542",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3540542",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernndez"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivankovi Nizi"
                    },
                    {
                        "name": "Milko Jaksi"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.09623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09623v1",
                "updated": "2025-02-13T18:59:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures"
                },
                "summary": "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights."
                },
                "authors": [
                    {
                        "name": "Francesco Ballerini"
                    },
                    {
                        "name": "Pierluigi Zama Ramirez"
                    },
                    {
                        "name": "Samuele Salti"
                    },
                    {
                        "name": "Luigi Di Stefano"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Di Stefano"
                },
                "author": "Luigi Di Stefano",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09621v1",
                "updated": "2025-02-13T18:59:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    46,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:46Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    46,
                    3,
                    44,
                    0
                ],
                "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency"
                },
                "summary": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/"
                },
                "authors": [
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Yu Qi"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Liuhui Wang"
                    },
                    {
                        "name": "Jianhan Jin"
                    },
                    {
                        "name": "Claire Guo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mmecot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09620v1",
                "updated": "2025-02-13T18:59:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:45Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    45,
                    3,
                    44,
                    0
                ],
                "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Encoder-free Architectures in 3D LMMs"
                },
                "summary": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL"
                },
                "authors": [
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Zoey Guo"
                    },
                    {
                        "name": "Zhuhao Wang"
                    },
                    {
                        "name": "Ray Zhang"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Junli Liu"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "arxiv_comment": "The code is released at https://github.com/Ivan-Tang-3D/ENEL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09617v1",
                "updated": "2025-02-13T18:59:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    19,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:19Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    19,
                    3,
                    44,
                    0
                ],
                "title": "LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh"
                },
                "summary": "Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality."
                },
                "authors": [
                    {
                        "name": "Jing Wen"
                    },
                    {
                        "name": "Alexander G. Schwing"
                    },
                    {
                        "name": "Shenlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shenlong Wang"
                },
                "author": "Shenlong Wang",
                "arxiv_comment": "ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09616v1",
                "updated": "2025-02-13T18:59:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "Variational Rectified Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Rectified Flow Matching"
                },
                "summary": "We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results."
                },
                "authors": [
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Alexander G. Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alexander G. Schwing"
                },
                "author": "Alexander G. Schwing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v3",
                "updated": "2025-02-13T18:58:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    58,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09606v1",
                "updated": "2025-02-13T18:55:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    56,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:55:56Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    56,
                    3,
                    44,
                    0
                ],
                "title": "Human-LLM Coevolution: Evidence from Academic Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM Coevolution: Evidence from Academic Writing"
                },
                "summary": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency."
                },
                "authors": [
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09604v1",
                "updated": "2025-02-13T18:55:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    13,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:55:13Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    13,
                    3,
                    44,
                    0
                ],
                "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models"
                },
                "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Benjamin Cohen-Wang"
                    },
                    {
                        "name": "Shannon Zejiang Shen"
                    },
                    {
                        "name": "Zhaofeng Wu"
                    },
                    {
                        "name": "Hu Xu"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Shang-Wen Li"
                    },
                    {
                        "name": "Wen-tau Yih"
                    }
                ],
                "author_detail": {
                    "name": "Wen-tau Yih"
                },
                "author": "Wen-tau Yih",
                "arxiv_comment": "Implementation available at https://github.com/voidism/SelfCite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09601v1",
                "updated": "2025-02-13T18:52:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    36,
                    3,
                    44,
                    0
                ],
                "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning"
                },
                "summary": "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Guangnian Wan"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09597v1",
                "updated": "2025-02-13T18:52:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    3,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:52:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Devamanyu Hazarika"
                    },
                    {
                        "name": "Kaixiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Kaixiang Lin"
                },
                "author": "Kaixiang Lin",
                "arxiv_comment": "Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09596v1",
                "updated": "2025-02-13T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    51,
                    12,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    51,
                    12,
                    3,
                    44,
                    0
                ],
                "title": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System"
                },
                "summary": "Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance."
                },
                "authors": [
                    {
                        "name": "Zitao Li"
                    },
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Weirui Kuang"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Bingchen Qian"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09591v1",
                "updated": "2025-02-13T18:48:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    48,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:48:04Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    48,
                    4,
                    3,
                    44,
                    0
                ],
                "title": "Censor Dependent Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Censor Dependent Variational Inference"
                },
                "summary": "This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions."
                },
                "authors": [
                    {
                        "name": "Chuanhui Liu"
                    },
                    {
                        "name": "Xiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Wang"
                },
                "author": "Xiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09589v1",
                "updated": "2025-02-13T18:46:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    46,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:46:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    46,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "Logical forms complement probability in understanding language model\n  (and human) performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical forms complement probability in understanding language model\n  (and human) performance"
                },
                "summary": "With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Freda Shi"
                    }
                ],
                "author_detail": {
                    "name": "Freda Shi"
                },
                "author": "Freda Shi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09582v1",
                "updated": "2025-02-13T18:40:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    40,
                    57,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:40:57Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    40,
                    57,
                    3,
                    44,
                    0
                ],
                "title": "Star-crossed Clusters: Asteroseismic Ages for Individual Stars are in\n  Tension with the Ages of their Host Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star-crossed Clusters: Asteroseismic Ages for Individual Stars are in\n  Tension with the Ages of their Host Clusters"
                },
                "summary": "A meta-analysis of seismic ages determined for individual stars in the\nwell-studied open and globular clusters NGC 6819, NGC 6791, M67, M4, M19, M80,\nand M9 reveals both high variance across measurements and significant\ndiscrepancy with independent, isochrone-based age determinations for the\nclusters in which these stars reside. The scatter among asteroseismic ages for\nindividual stars in any one of these clusters far surpasses both the absolute\nage uncertainty computed for reference cluster M92 (5.4\\%) and the\nmodel-to-model systematic uncertainties in isochrones (roughly 10\\%). This\nsuggests that either binary processes are significantly altering the masses of\nstars in these clusters, or some additional corrections, perhaps as a function\nof mass, metallicity, or surface gravity, are required to bring the\nasteroseismic age scale into concordance with ages inferred from isochrone or\nsimilar model fitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A meta-analysis of seismic ages determined for individual stars in the\nwell-studied open and globular clusters NGC 6819, NGC 6791, M67, M4, M19, M80,\nand M9 reveals both high variance across measurements and significant\ndiscrepancy with independent, isochrone-based age determinations for the\nclusters in which these stars reside. The scatter among asteroseismic ages for\nindividual stars in any one of these clusters far surpasses both the absolute\nage uncertainty computed for reference cluster M92 (5.4\\%) and the\nmodel-to-model systematic uncertainties in isochrones (roughly 10\\%). This\nsuggests that either binary processes are significantly altering the masses of\nstars in these clusters, or some additional corrections, perhaps as a function\nof mass, metallicity, or surface gravity, are required to bring the\nasteroseismic age scale into concordance with ages inferred from isochrone or\nsimilar model fitting."
                },
                "authors": [
                    {
                        "name": "Jamie Tayar"
                    },
                    {
                        "name": "Meridith Joyce"
                    }
                ],
                "author_detail": {
                    "name": "Meridith Joyce"
                },
                "author": "Meridith Joyce",
                "arxiv_comment": "13 pages, 2 figures, submitted to AAS journals. Comments Welcome.\n  Happy Valentine's/Galentine's Day!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09577v1",
                "updated": "2025-02-13T18:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks"
                },
                "summary": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting."
                },
                "authors": [
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Jiannan Li"
                    },
                    {
                        "name": "Huanchen Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "arxiv_comment": "Accepted to CSCW 2025 with minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09567v1",
                "updated": "2025-02-13T18:22:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    22,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:22:31Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    22,
                    31,
                    3,
                    44,
                    0
                ],
                "title": "MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing"
                },
                "summary": "We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label."
                },
                "authors": [
                    {
                        "name": "Vlad Andrei Negru"
                    },
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Camelia Lemnaru"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rodica Potolea"
                    }
                ],
                "author_detail": {
                    "name": "Rodica Potolea"
                },
                "author": "Rodica Potolea",
                "arxiv_comment": "16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09566v1",
                "updated": "2025-02-13T18:21:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    21,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:21:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    21,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "Zero-shot generation of synthetic neurosurgical data with large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot generation of synthetic neurosurgical data with large language\n  models"
                },
                "summary": "Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance."
                },
                "authors": [
                    {
                        "name": "Austin A. Barr"
                    },
                    {
                        "name": "Eddie Guo"
                    },
                    {
                        "name": "Emre Sezgin"
                    }
                ],
                "author_detail": {
                    "name": "Emre Sezgin"
                },
                "author": "Emre Sezgin",
                "arxiv_comment": "13 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09101v2",
                "updated": "2025-02-13T18:20:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    20,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-14T00:18:04Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    18,
                    4,
                    3,
                    319,
                    0
                ],
                "title": "Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery"
                },
                "summary": "Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation."
                },
                "authors": [
                    {
                        "name": "Ashim Dahal"
                    },
                    {
                        "name": "Saydul Akbar Murad"
                    },
                    {
                        "name": "Nick Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Nick Rahimi"
                },
                "author": "Nick Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09565v1",
                "updated": "2025-02-13T18:19:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    19,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:19:20Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    19,
                    20,
                    3,
                    44,
                    0
                ],
                "title": "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models"
                },
                "summary": "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models."
                },
                "authors": [
                    {
                        "name": "Quintina Campbell"
                    },
                    {
                        "name": "Sam Cox"
                    },
                    {
                        "name": "Jorge Medina"
                    },
                    {
                        "name": "Brittany Watterson"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04001v2",
                "updated": "2025-02-13T18:14:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    14,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-07T18:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos"
                },
                "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project page: https://lxtgh.github.io/project/sa2va",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05925v2",
                "updated": "2025-02-13T18:02:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    2,
                    34,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-09T21:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    21,
                    58,
                    32,
                    6,
                    161,
                    0
                ],
                "title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue"
                },
                "summary": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04103v2",
                "updated": "2025-02-13T17:57:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-06T14:27:54Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases."
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Aprille Xi"
                    },
                    {
                        "name": "Canwen Wang"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Kenneth R Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R Koedinger"
                },
                "author": "Kenneth R Koedinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2105.02379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2105.02379v3",
                "updated": "2025-02-13T17:56:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    56,
                    19,
                    3,
                    44,
                    0
                ],
                "published": "2021-05-06T00:12:58Z",
                "published_parsed": [
                    2021,
                    5,
                    6,
                    0,
                    12,
                    58,
                    3,
                    126,
                    0
                ],
                "title": "Targeted Quality Measurement of Health Care Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Quality Measurement of Health Care Providers"
                },
                "summary": "Assessing the quality of cancer care administered by US health providers\nposes numerous challenges due to meaningful heterogeneity in patient\npopulations. Patients undergoing oncology treatment exhibit substantial\nvariation in disease presentation among other crucial characteristics. In this\npaper, we present a framework for institutional quality measurement that\naddresses this patient heterogeneity. Our framework follows recent advancements\nin health outcomes research, conceptualizing quality measurement as a causal\ninference problem. This approach enables us to use flexible covariate profiles\nto target specific patient populations of interest. We use different clinically\nrelevant covariate profiles and evaluate methods for case-mix adjustments.\nThese adjustments integrate weighting and regression modeling approaches in a\nprogressive manner in order to reduce model extrapolation and allow for\nprovider effect modification. We evaluate these methods in an extensive\nsimulation study, comparing their performance in terms of point estimates and\nestimated rankings. We highlight the practical utility of weighting methods\nthat can generate stable weights when covariate overlap is limited and alert\ninvestigators when case-mix adjustments are infeasible without some form of\nextrapolation that goes beyond the support of the observed data. In our study\nof cancer-care outcomes, we assess the performance of oncology practices for\ndifferent profiles that correspond to important types of patients who may\nreceive cancer care. We describe how the methods examined may be particularly\nimportant for high-stakes quality measurement, such as public reporting or\nperformance-based payments. These methods have the potential to help inform\nindividual patient health care decisions and contribute to progress toward more\npersonalized quality measurement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the quality of cancer care administered by US health providers\nposes numerous challenges due to meaningful heterogeneity in patient\npopulations. Patients undergoing oncology treatment exhibit substantial\nvariation in disease presentation among other crucial characteristics. In this\npaper, we present a framework for institutional quality measurement that\naddresses this patient heterogeneity. Our framework follows recent advancements\nin health outcomes research, conceptualizing quality measurement as a causal\ninference problem. This approach enables us to use flexible covariate profiles\nto target specific patient populations of interest. We use different clinically\nrelevant covariate profiles and evaluate methods for case-mix adjustments.\nThese adjustments integrate weighting and regression modeling approaches in a\nprogressive manner in order to reduce model extrapolation and allow for\nprovider effect modification. We evaluate these methods in an extensive\nsimulation study, comparing their performance in terms of point estimates and\nestimated rankings. We highlight the practical utility of weighting methods\nthat can generate stable weights when covariate overlap is limited and alert\ninvestigators when case-mix adjustments are infeasible without some form of\nextrapolation that goes beyond the support of the observed data. In our study\nof cancer-care outcomes, we assess the performance of oncology practices for\ndifferent profiles that correspond to important types of patients who may\nreceive cancer care. We describe how the methods examined may be particularly\nimportant for high-stakes quality measurement, such as public reporting or\nperformance-based payments. These methods have the potential to help inform\nindividual patient health care decisions and contribute to progress toward more\npersonalized quality measurement."
                },
                "authors": [
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Nancy L. Keating"
                    },
                    {
                        "name": "Mary Beth Landrum"
                    },
                    {
                        "name": "Jose R. Zubizarreta"
                    }
                ],
                "author_detail": {
                    "name": "Jose R. Zubizarreta"
                },
                "author": "Jose R. Zubizarreta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2105.02379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2105.02379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06773v2",
                "updated": "2025-02-13T17:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    50,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-10T20:19:55Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    20,
                    19,
                    55,
                    0,
                    162,
                    0
                ],
                "title": "Evaluating Zero-Shot Long-Context LLM Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Zero-Shot Long-Context LLM Compression"
                },
                "summary": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Kai Li"
                    }
                ],
                "author_detail": {
                    "name": "Kai Li"
                },
                "author": "Kai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09532v1",
                "updated": "2025-02-13T17:49:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:49:30Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages"
                },
                "summary": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks."
                },
                "authors": [
                    {
                        "name": "Shreyan Biswas"
                    },
                    {
                        "name": "Alexander Erlei"
                    },
                    {
                        "name": "Ujwal Gadiraju"
                    }
                ],
                "author_detail": {
                    "name": "Ujwal Gadiraju"
                },
                "author": "Ujwal Gadiraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09528v1",
                "updated": "2025-02-13T17:39:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    39,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    39,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest"
                },
                "summary": "Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC."
                },
                "authors": [
                    {
                        "name": "Jack Erhardt"
                    },
                    {
                        "name": "Ziang Li"
                    },
                    {
                        "name": "Reid Pinkham"
                    },
                    {
                        "name": "Andrew Berkovich"
                    },
                    {
                        "name": "Zhengya Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengya Zhang"
                },
                "author": "Zhengya Zhang",
                "arxiv_comment": "Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05331v2",
                "updated": "2025-02-13T17:27:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    27,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-07T21:13:27Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    21,
                    13,
                    27,
                    4,
                    38,
                    0
                ],
                "title": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books"
                },
                "summary": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch."
                },
                "authors": [
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Robert Morabito"
                    },
                    {
                        "name": "Skye Reid"
                    },
                    {
                        "name": "Nikta Gohari Sadr"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "9 pages (excluding references), accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07414v3",
                "updated": "2025-02-13T17:27:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    27,
                    11,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-09T20:29:04Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    20,
                    29,
                    4,
                    2,
                    283,
                    0
                ],
                "title": "Bayes-Nash Generative Privacy Against Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayes-Nash Generative Privacy Against Membership Inference Attacks"
                },
                "summary": "Membership inference attacks (MIAs) expose significant privacy risks by\ndetermining whether an individual's data is in a dataset. While differential\nprivacy (DP) mitigates such risks, it has several limitations in achieving an\noptimal balance between utility and privacy, include limited resolution in\nexpressing this tradeoff in only a few privacy parameters, and intractable\nsensitivity calculations that may be necessary to provide tight privacy\nguarantees. We propose a game-theoretic framework that models privacy\nprotection from MIA as a Bayesian game between a defender and an attacker. In\nthis game, a dataset is the defender's private information, with privacy loss\nto the defender (which is gain to the attacker) captured in terms of the\nattacker's ability to infer membership of individuals in the dataset. To\naddress the strategic complexity of this game, we represent the mixed strategy\nof the defender as a neural network generator which maps a private dataset to\nits public representation (for example, noisy summary statistics), while the\nmixed strategy of the attacker is captured by a discriminator which makes\nmembership inference claims. We refer to the resulting computational approach\nas a general-sum Generative Adversarial Network, which is trained iteratively\nby alternating generator and discriminator updates akin to conventional GANs.\nWe call the defender's data sharing policy thereby obtained Bayes-Nash\nGenerative Privacy (BNGP). The BNGP strategy avoids sensitivity calculations,\nsupports compositions of correlated mechanisms, is robust to the attacker's\nheterogeneous preferences over true and false positives, and yields provable\ndifferential privacy guarantees, albeit in an idealized setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) expose significant privacy risks by\ndetermining whether an individual's data is in a dataset. While differential\nprivacy (DP) mitigates such risks, it has several limitations in achieving an\noptimal balance between utility and privacy, include limited resolution in\nexpressing this tradeoff in only a few privacy parameters, and intractable\nsensitivity calculations that may be necessary to provide tight privacy\nguarantees. We propose a game-theoretic framework that models privacy\nprotection from MIA as a Bayesian game between a defender and an attacker. In\nthis game, a dataset is the defender's private information, with privacy loss\nto the defender (which is gain to the attacker) captured in terms of the\nattacker's ability to infer membership of individuals in the dataset. To\naddress the strategic complexity of this game, we represent the mixed strategy\nof the defender as a neural network generator which maps a private dataset to\nits public representation (for example, noisy summary statistics), while the\nmixed strategy of the attacker is captured by a discriminator which makes\nmembership inference claims. We refer to the resulting computational approach\nas a general-sum Generative Adversarial Network, which is trained iteratively\nby alternating generator and discriminator updates akin to conventional GANs.\nWe call the defender's data sharing policy thereby obtained Bayes-Nash\nGenerative Privacy (BNGP). The BNGP strategy avoids sensitivity calculations,\nsupports compositions of correlated mechanisms, is robust to the attacker's\nheterogeneous preferences over true and false positives, and yields provable\ndifferential privacy guarantees, albeit in an idealized setting."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Rajagopal Venkatesaraman"
                    },
                    {
                        "name": "Rajat K. De"
                    },
                    {
                        "name": "Bradley A. Malin"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeniy Vorobeychik"
                },
                "author": "Yevgeniy Vorobeychik",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2406.01811",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09500v1",
                "updated": "2025-02-13T17:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    10,
                    43,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    10,
                    43,
                    3,
                    44,
                    0
                ],
                "title": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting"
                },
                "summary": "Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}."
                },
                "authors": [
                    {
                        "name": "Nicholas Dronen"
                    },
                    {
                        "name": "Randall Balestriero"
                    }
                ],
                "author_detail": {
                    "name": "Randall Balestriero"
                },
                "author": "Randall Balestriero",
                "arxiv_comment": "16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09497v1",
                "updated": "2025-02-13T17:09:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    9,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:09:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    9,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Improve LLM-based Automatic Essay Scoring with Linguistic Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improve LLM-based Automatic Essay Scoring with Linguistic Features"
                },
                "summary": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Joey Hou"
                    },
                    {
                        "name": "Alejandro Ciuba"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Lorraine Li"
                },
                "author": "Xiang Lorraine Li",
                "arxiv_comment": "To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v9",
                "updated": "2025-02-13T17:06:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    6,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09487v1",
                "updated": "2025-02-13T16:52:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T16:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "title": "Objective quantification of mood states using large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective quantification of mood states using large language models"
                },
                "summary": "Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings."
                },
                "authors": [
                    {
                        "name": "Jakub Onysk"
                    },
                    {
                        "name": "Quentin Huys"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Huys"
                },
                "author": "Quentin Huys",
                "arxiv_comment": "main text - 9 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14679v3",
                "updated": "2025-02-13T16:29:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    29,
                    16,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-24T17:57:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    57,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation"
                },
                "summary": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba."
                },
                "authors": [
                    {
                        "name": "Rongzhao He"
                    },
                    {
                        "name": "Weihao Zheng"
                    },
                    {
                        "name": "Leilei Zhao"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Dalin Zhu"
                    },
                    {
                        "name": "Dan Wu"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09447v1",
                "updated": "2025-02-13T16:16:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    16,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T16:16:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    16,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations"
                },
                "summary": "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST."
                },
                "authors": [
                    {
                        "name": "Dexian Cai"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01404v2",
                "updated": "2025-02-13T16:06:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    6,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-02T10:31:10Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    10,
                    31,
                    10,
                    2,
                    276,
                    0
                ],
                "title": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection"
                },
                "summary": "Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall."
                },
                "authors": [
                    {
                        "name": "Hongru Yan"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09419v1",
                "updated": "2025-02-13T15:42:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    42,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:42:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    42,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "On multi-token prediction for efficient LLM inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On multi-token prediction for efficient LLM inference"
                },
                "summary": "We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction."
                },
                "authors": [
                    {
                        "name": "Somesh Mehra"
                    },
                    {
                        "name": "Javier Alonso Garcia"
                    },
                    {
                        "name": "Lukas Mauch"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Mauch"
                },
                "author": "Lukas Mauch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v2",
                "updated": "2025-02-13T15:36:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    36,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "17 pages, 8 figures; figures corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19347v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19347v4",
                "updated": "2025-02-13T15:25:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    25,
                    2,
                    3,
                    44,
                    0
                ],
                "published": "2023-10-30T08:40:16Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    8,
                    40,
                    16,
                    0,
                    303,
                    0
                ],
                "title": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization"
                },
                "summary": "Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Yan Fan"
                    },
                    {
                        "name": "Xiong Liu"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Zekun Yao"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19347v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19347v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02280v2",
                "updated": "2025-02-13T15:21:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    21,
                    43,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-04T17:09:10Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain."
                },
                "authors": [
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Schrimpf"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrimpf"
                },
                "author": "Martin Schrimpf",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09396v1",
                "updated": "2025-02-13T15:16:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    16,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:16:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    16,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack"
                },
                "summary": "Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy."
                },
                "authors": [
                    {
                        "name": "Richard J. Preen"
                    },
                    {
                        "name": "Jim Smith"
                    }
                ],
                "author_detail": {
                    "name": "Jim Smith"
                },
                "author": "Jim Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17395v2",
                "updated": "2025-02-13T15:11:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    11,
                    24,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-23T08:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models"
                },
                "summary": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09390v1",
                "updated": "2025-02-13T15:07:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    7,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:07:20Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    7,
                    20,
                    3,
                    44,
                    0
                ],
                "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models"
                },
                "summary": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square."
                },
                "authors": [
                    {
                        "name": "Daniel Fleischer"
                    },
                    {
                        "name": "Moshe Berchansky"
                    },
                    {
                        "name": "Gad Markovits"
                    },
                    {
                        "name": "Moshe Wasserblat"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Wasserblat"
                },
                "author": "Moshe Wasserblat",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09387v1",
                "updated": "2025-02-13T15:04:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    4,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:04:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    4,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "Truth Knows No Language: Evaluating Truthfulness Beyond English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth Knows No Language: Evaluating Truthfulness Beyond English"
                },
                "summary": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses."
                },
                "authors": [
                    {
                        "name": "Blanca Calvo Figueras"
                    },
                    {
                        "name": "Eneko Sagarzazu"
                    },
                    {
                        "name": "Julen Etxaniz"
                    },
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Pablo Gamallo"
                    },
                    {
                        "name": "Iria De Dios Flores"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "13 pages, 5 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09385v1",
                "updated": "2025-02-13T15:01:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    1,
                    18,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:01:18Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    1,
                    18,
                    3,
                    44,
                    0
                ],
                "title": "APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent\n  Threats Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent\n  Threats Using Large Language Models"
                },
                "summary": "Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due\nto their stealth and ability to mimic normal system behavior, making detection\nparticularly difficult in highly imbalanced datasets. Traditional anomaly\ndetection methods struggle to effectively differentiate APT-related activities\nfrom benign processes, limiting their applicability in real-world scenarios.\nThis paper introduces APT-LLM, a novel embedding-based anomaly detection\nframework that integrates large language models (LLMs) -- BERT, ALBERT,\nDistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.\nUnlike prior approaches, which rely on manually engineered features or\nconventional anomaly detection models, APT-LLM leverages LLMs to encode\nprocess-action provenance traces into semantically rich embeddings, capturing\nnuanced behavioral patterns. These embeddings are analyzed using three\nautoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder\n(VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and\nidentify anomalies. The best-performing model is selected for comparison\nagainst traditional methods. The framework is evaluated on real-world, highly\nimbalanced provenance trace datasets from the DARPA Transparent Computing\nprogram, where APT-like attacks constitute as little as 0.004\\% of the data\nacross multiple operating systems (Android, Linux, BSD, and Windows) and attack\nscenarios. Results demonstrate that APT-LLM significantly improves detection\nperformance under extreme imbalance conditions, outperforming existing anomaly\ndetection methods and highlighting the effectiveness of LLM-based feature\nextraction in cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due\nto their stealth and ability to mimic normal system behavior, making detection\nparticularly difficult in highly imbalanced datasets. Traditional anomaly\ndetection methods struggle to effectively differentiate APT-related activities\nfrom benign processes, limiting their applicability in real-world scenarios.\nThis paper introduces APT-LLM, a novel embedding-based anomaly detection\nframework that integrates large language models (LLMs) -- BERT, ALBERT,\nDistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.\nUnlike prior approaches, which rely on manually engineered features or\nconventional anomaly detection models, APT-LLM leverages LLMs to encode\nprocess-action provenance traces into semantically rich embeddings, capturing\nnuanced behavioral patterns. These embeddings are analyzed using three\nautoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder\n(VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and\nidentify anomalies. The best-performing model is selected for comparison\nagainst traditional methods. The framework is evaluated on real-world, highly\nimbalanced provenance trace datasets from the DARPA Transparent Computing\nprogram, where APT-like attacks constitute as little as 0.004\\% of the data\nacross multiple operating systems (Android, Linux, BSD, and Windows) and attack\nscenarios. Results demonstrate that APT-LLM significantly improves detection\nperformance under extreme imbalance conditions, outperforming existing anomaly\ndetection methods and highlighting the effectiveness of LLM-based feature\nextraction in cybersecurity."
                },
                "authors": [
                    {
                        "name": "Sidahmed Benabderrahmane"
                    },
                    {
                        "name": "Petko Valtchev"
                    },
                    {
                        "name": "James Cheney"
                    },
                    {
                        "name": "Talal Rahwan"
                    }
                ],
                "author_detail": {
                    "name": "Talal Rahwan"
                },
                "author": "Talal Rahwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15927v2",
                "updated": "2025-02-13T14:55:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    55,
                    26,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-24T17:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    32,
                    20,
                    6,
                    329,
                    0
                ],
                "title": "Generative Prompt Internalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Prompt Internalization"
                },
                "summary": "Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts."
                },
                "authors": [
                    {
                        "name": "Haebin Shin"
                    },
                    {
                        "name": "Lei Ji"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Sungdong Kim"
                    },
                    {
                        "name": "Eunbi Choi"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "NAACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14682v2",
                "updated": "2025-02-13T14:54:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    54,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-02T19:56:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    56,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards\n  Spatial-Temporal Cognition with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards\n  Spatial-Temporal Cognition with Foundation Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred numerous\nattempts to apply these technologies to embodied tasks, particularly focusing\non high-level task planning and task decomposition. To further explore this\narea, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which\nspecifically targets embodied task planning using LLMs. It features a\ncontrollable and diverse set of embodied tasks varying in different levels of\ndifficulties and complexities, and is designed to evaluate two critical\ndimensions of LLMs' application in embodied task understanding: spatial\n(relation constraint, occlusion for target objects) and temporal & causal\nunderstanding of the sequence of actions in the environment. By using\nmulti-source simulators as the backend simulator, it can provide immediate\nenvironment feedback to LLMs, which enables LLMs to interact dynamically with\nthe environment and re-plan as necessary. We evaluated the state-of-the-art\nopen source and closed source foundation models, including GPT-4, LLAMA and\nMistral on our proposed benchmark. While they perform adequately well on simple\nnavigation tasks, their performance can significantly deteriorate when faced\nwith tasks that require a deeper understanding of spatial, temporal, and causal\nrelationships. Thus, our benchmark distinguishes itself as a large-scale,\nquantifiable, highly automated, and fine-grained diagnostic framework that\npresents a significant challenge to the latest foundation models. We hope it\ncan spark and drive further research in embodied task planning using foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred numerous\nattempts to apply these technologies to embodied tasks, particularly focusing\non high-level task planning and task decomposition. To further explore this\narea, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which\nspecifically targets embodied task planning using LLMs. It features a\ncontrollable and diverse set of embodied tasks varying in different levels of\ndifficulties and complexities, and is designed to evaluate two critical\ndimensions of LLMs' application in embodied task understanding: spatial\n(relation constraint, occlusion for target objects) and temporal & causal\nunderstanding of the sequence of actions in the environment. By using\nmulti-source simulators as the backend simulator, it can provide immediate\nenvironment feedback to LLMs, which enables LLMs to interact dynamically with\nthe environment and re-plan as necessary. We evaluated the state-of-the-art\nopen source and closed source foundation models, including GPT-4, LLAMA and\nMistral on our proposed benchmark. While they perform adequately well on simple\nnavigation tasks, their performance can significantly deteriorate when faced\nwith tasks that require a deeper understanding of spatial, temporal, and causal\nrelationships. Thus, our benchmark distinguishes itself as a large-scale,\nquantifiable, highly automated, and fine-grained diagnostic framework that\npresents a significant challenge to the latest foundation models. We hope it\ncan spark and drive further research in embodied task planning using foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Yuening Wang"
                    },
                    {
                        "name": "Hongjian Gu"
                    },
                    {
                        "name": "Atia Hamidizadeh"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Yuecheng Liu"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "David Gamaliel Arcos Bravo"
                    },
                    {
                        "name": "Junyi Dong"
                    },
                    {
                        "name": "Shunbo Zhou"
                    },
                    {
                        "name": "Tongtong Cao"
                    },
                    {
                        "name": "Xingyue Quan"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09378v1",
                "updated": "2025-02-13T14:46:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    46,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:46:04Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    46,
                    4,
                    3,
                    44,
                    0
                ],
                "title": "A Deep Inverse-Mapping Model for a Flapping Robotic Wing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Inverse-Mapping Model for a Flapping Robotic Wing"
                },
                "summary": "In systems control, the dynamics of a system are governed by modulating its\ninputs to achieve a desired outcome. For example, to control the thrust of a\nquad-copter propeller the controller modulates its rotation rate, relying on a\nstraightforward mapping between the input rotation rate and the resulting\nthrust. This mapping can be inverted to determine the rotation rate needed to\ngenerate a desired thrust. However, in complex systems, such as flapping-wing\nrobots where intricate fluid motions are involved, mapping inputs (wing\nkinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this\nmapping for real-time control is computationally impractical. Here, we report a\nmachine-learning solution for the inverse mapping of a flapping-wing system\nbased on data from an experimental system we have developed. Our model learns\nthe input wing motion required to generate a desired aerodynamic force outcome.\nWe used a sequence-to-sequence model tailored for time-series data and\naugmented it with a novel adaptive-spectrum layer that implements\nrepresentation learning in the frequency domain. To train our model, we\ndeveloped a flapping wing system that simultaneously measures the wing's\naerodynamic force and its 3D motion using high-speed cameras. We demonstrate\nthe performance of our system on an additional open-source dataset of a\nflapping wing in a different flow regime. Results show superior performance\ncompared with more complex state-of-the-art transformer-based models, with 11%\nimprovement on the test datasets median loss. Moreover, our model shows\nsuperior inference time, making it practical for onboard robotic control. Our\nopen-source data and framework may improve modeling and real-time control of\nsystems governed by complex dynamics, from biomimetic robots to biomedical\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In systems control, the dynamics of a system are governed by modulating its\ninputs to achieve a desired outcome. For example, to control the thrust of a\nquad-copter propeller the controller modulates its rotation rate, relying on a\nstraightforward mapping between the input rotation rate and the resulting\nthrust. This mapping can be inverted to determine the rotation rate needed to\ngenerate a desired thrust. However, in complex systems, such as flapping-wing\nrobots where intricate fluid motions are involved, mapping inputs (wing\nkinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this\nmapping for real-time control is computationally impractical. Here, we report a\nmachine-learning solution for the inverse mapping of a flapping-wing system\nbased on data from an experimental system we have developed. Our model learns\nthe input wing motion required to generate a desired aerodynamic force outcome.\nWe used a sequence-to-sequence model tailored for time-series data and\naugmented it with a novel adaptive-spectrum layer that implements\nrepresentation learning in the frequency domain. To train our model, we\ndeveloped a flapping wing system that simultaneously measures the wing's\naerodynamic force and its 3D motion using high-speed cameras. We demonstrate\nthe performance of our system on an additional open-source dataset of a\nflapping wing in a different flow regime. Results show superior performance\ncompared with more complex state-of-the-art transformer-based models, with 11%\nimprovement on the test datasets median loss. Moreover, our model shows\nsuperior inference time, making it practical for onboard robotic control. Our\nopen-source data and framework may improve modeling and real-time control of\nsystems governed by complex dynamics, from biomimetic robots to biomedical\ndevices."
                },
                "authors": [
                    {
                        "name": "Hadar Sharvit"
                    },
                    {
                        "name": "Raz Karl"
                    },
                    {
                        "name": "Tsevi Beatus"
                    }
                ],
                "author_detail": {
                    "name": "Tsevi Beatus"
                },
                "author": "Tsevi Beatus",
                "arxiv_comment": "Accepted to ICLR 2025. 10 Pages 5 figures + 2 figures in appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09374v1",
                "updated": "2025-02-13T14:43:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    43,
                    22,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:43:22Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    43,
                    22,
                    3,
                    44,
                    0
                ],
                "title": "Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training"
                },
                "summary": "Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3."
                },
                "authors": [
                    {
                        "name": "Toon Vinck"
                    },
                    {
                        "name": "Nan Jonckers"
                    },
                    {
                        "name": "Gert Dekkers"
                    },
                    {
                        "name": "Jeffrey Prinzie"
                    },
                    {
                        "name": "Peter Karsmakers"
                    }
                ],
                "author_detail": {
                    "name": "Peter Karsmakers"
                },
                "author": "Peter Karsmakers",
                "arxiv_comment": "7 pages, 4 figures, Topical Workshop on Electronics for Particle\n  Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09373v1",
                "updated": "2025-02-13T14:43:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    43,
                    3,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:43:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    43,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Low-Acceleration Gravitational Anomaly from Bayesian 3D Modeling of Wide\n  Binary Orbits: Methodology and Results with Gaia DR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Acceleration Gravitational Anomaly from Bayesian 3D Modeling of Wide\n  Binary Orbits: Methodology and Results with Gaia DR3"
                },
                "summary": "Isolated wide binary stars provide natural laboratories to directly test or\nmeasure weak gravity for Newtonian acceleration $g_{\\rm{N}}\\lesssim 10^{-9}$ m\ns$^{-2}$. Recent statistical analyses of wide binaries have been performed only\nwith sky-projected relative velocities $v_p$ in the pairs. A new method of\nBayesian orbit modeling exploiting three relative velocity components including\nthe radial (line-of-sight) component $v_r$ is developed to measure a\ngravitational anomaly parameter\n$\\Gamma\\equiv\\log_{10}\\sqrt{G_{\\rm{eff}}/G_{\\rm{N}}}$ where $G_{\\rm{eff}}$ is\nthe effective gravitational constant for pseudo-Newtonian elliptical orbits,\nwhile $G_{\\rm{N}}$ is Newton's constant. The method infers individual\nprobability distributions of $\\Gamma$ and then combines the independent\ndistributions to obtain a consolidated distribution in a specific range of\n$g_{\\rm{N}}$. Here the method is described and applied to a sample of 312 wide\nbinaries in a broad dynamic range $10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim\n10^{-6.7}$ m s$^{-2}$ with $v_r$ uncertainties in the range\n$168<\\sigma_{v_r}<380$ m s$^{-1}$ selected from the Gaia DR3 database. The\nfollowing results are obtained: $\\Gamma = 0.000\\pm 0.011$\n($N_{\\rm{binary}}=125$) for a high acceleration regime ($10^{-7.9} \\lesssim\ng_{\\rm{N}} \\lesssim 10^{-6.7}$ m s$^{-2}$) agreeing well with Newton, but\n$\\Gamma = 0.085\\pm 0.040$ (35) for a MOND regime ($10^{-11.0}\\lesssim\ng_{\\rm{N}}\\lesssim 10^{-9.5}$ m s$^{-2}$) and $\\Gamma = 0.063\\pm 0.015$ (111)\nfor a MOND+transition regime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-8.5}$\nm s$^{-2}$). These results show that gravitational anomaly is evident for\n$g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$ and $\\Gamma$ in the MOND regime\n($\\lesssim 10^{-9.5}$ m s$^{-2}$) agrees with the first-tier prediction\n($\\approx 0.07$) of MOND gravity theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolated wide binary stars provide natural laboratories to directly test or\nmeasure weak gravity for Newtonian acceleration $g_{\\rm{N}}\\lesssim 10^{-9}$ m\ns$^{-2}$. Recent statistical analyses of wide binaries have been performed only\nwith sky-projected relative velocities $v_p$ in the pairs. A new method of\nBayesian orbit modeling exploiting three relative velocity components including\nthe radial (line-of-sight) component $v_r$ is developed to measure a\ngravitational anomaly parameter\n$\\Gamma\\equiv\\log_{10}\\sqrt{G_{\\rm{eff}}/G_{\\rm{N}}}$ where $G_{\\rm{eff}}$ is\nthe effective gravitational constant for pseudo-Newtonian elliptical orbits,\nwhile $G_{\\rm{N}}$ is Newton's constant. The method infers individual\nprobability distributions of $\\Gamma$ and then combines the independent\ndistributions to obtain a consolidated distribution in a specific range of\n$g_{\\rm{N}}$. Here the method is described and applied to a sample of 312 wide\nbinaries in a broad dynamic range $10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim\n10^{-6.7}$ m s$^{-2}$ with $v_r$ uncertainties in the range\n$168<\\sigma_{v_r}<380$ m s$^{-1}$ selected from the Gaia DR3 database. The\nfollowing results are obtained: $\\Gamma = 0.000\\pm 0.011$\n($N_{\\rm{binary}}=125$) for a high acceleration regime ($10^{-7.9} \\lesssim\ng_{\\rm{N}} \\lesssim 10^{-6.7}$ m s$^{-2}$) agreeing well with Newton, but\n$\\Gamma = 0.085\\pm 0.040$ (35) for a MOND regime ($10^{-11.0}\\lesssim\ng_{\\rm{N}}\\lesssim 10^{-9.5}$ m s$^{-2}$) and $\\Gamma = 0.063\\pm 0.015$ (111)\nfor a MOND+transition regime ($10^{-11.0}\\lesssim g_{\\rm{N}}\\lesssim 10^{-8.5}$\nm s$^{-2}$). These results show that gravitational anomaly is evident for\n$g_{\\rm{N}}\\lesssim 10^{-9}$ m s$^{-2}$ and $\\Gamma$ in the MOND regime\n($\\lesssim 10^{-9.5}$ m s$^{-2}$) agrees with the first-tier prediction\n($\\approx 0.07$) of MOND gravity theories."
                },
                "authors": [
                    {
                        "name": "Kyu-Hyun Chae"
                    }
                ],
                "author_detail": {
                    "name": "Kyu-Hyun Chae"
                },
                "author": "Kyu-Hyun Chae",
                "arxiv_comment": "34 pages, 24 figures, 3 tables (submitted to the AAS journals)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09372v1",
                "updated": "2025-02-13T14:38:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    38,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    38,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "Inverse problems with experiment-guided AlphaFold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse problems with experiment-guided AlphaFold"
                },
                "summary": "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity."
                },
                "authors": [
                    {
                        "name": "Advaith Maddipatla"
                    },
                    {
                        "name": "Nadav Bojan Sellam"
                    },
                    {
                        "name": "Meital Bojan"
                    },
                    {
                        "name": "Sanketh Vedula"
                    },
                    {
                        "name": "Paul Schanda"
                    },
                    {
                        "name": "Ailie Marx"
                    },
                    {
                        "name": "Alex M. Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Alex M. Bronstein"
                },
                "author": "Alex M. Bronstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08514v2",
                "updated": "2025-02-13T14:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    34,
                    29,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-12T15:46:50Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation"
                },
                "summary": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries."
                },
                "authors": [
                    {
                        "name": "Mahnaz Koupaee"
                    },
                    {
                        "name": "Jake W. Vincent"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Igor Shalyminov"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Raphael Shu"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Amy Wing-mei Wong"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09354v1",
                "updated": "2025-02-13T14:19:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "Trajectory Inference for Single Cell Omics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Inference for Single Cell Omics"
                },
                "summary": "Trajectory inference is used to order single-cell omics data along a path\nthat reflects a continuous transition between cells. This approach is useful\nfor studying processes like cell differentiation, where a stem cell matures\ninto a specialized cell type, or investigating state changes in pathological\nconditions. In the current article, we provide a general introduction to\ntrajectory inference, explaining the concepts and assumptions underlying the\ndifferent methods. We then briefly discuss the strengths and weaknesses of\ndifferent trajectory inference methods. We also describe best practices for\nusing trajectory inference, such as how to validate the results and how to\ninterpret them in the context of biological knowledge. Finally, the article\nwill discuss some of the applications of trajectory inference in single-cell\nomics research. These applications include studying cell differentiation,\ndevelopment, and disease. We provide examples of how trajectory inference has\nbeen used to gain new insights into these processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory inference is used to order single-cell omics data along a path\nthat reflects a continuous transition between cells. This approach is useful\nfor studying processes like cell differentiation, where a stem cell matures\ninto a specialized cell type, or investigating state changes in pathological\nconditions. In the current article, we provide a general introduction to\ntrajectory inference, explaining the concepts and assumptions underlying the\ndifferent methods. We then briefly discuss the strengths and weaknesses of\ndifferent trajectory inference methods. We also describe best practices for\nusing trajectory inference, such as how to validate the results and how to\ninterpret them in the context of biological knowledge. Finally, the article\nwill discuss some of the applications of trajectory inference in single-cell\nomics research. These applications include studying cell differentiation,\ndevelopment, and disease. We provide examples of how trajectory inference has\nbeen used to gain new insights into these processes."
                },
                "authors": [
                    {
                        "name": "Alexandre Hutton"
                    },
                    {
                        "name": "Jesse G. Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Jesse G. Meyer"
                },
                "author": "Jesse G. Meyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10853v3",
                "updated": "2025-02-13T14:13:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    13,
                    41,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-15T16:04:44Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    16,
                    4,
                    44,
                    0,
                    197,
                    0
                ],
                "title": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases"
                },
                "summary": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Bouchard"
                },
                "author": "Dylan Bouchard",
                "arxiv_comment": "LangFair repository: https://github.com/cvs-health/langfair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09344v1",
                "updated": "2025-02-13T14:07:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    51,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:07:51Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    51,
                    3,
                    44,
                    0
                ],
                "title": "Revisiting Topological Interference Management: A Learning-to-Code on\n  Graphs Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Topological Interference Management: A Learning-to-Code on\n  Graphs Perspective"
                },
                "summary": "The advance of topological interference management (TIM) has been one of the\ndriving forces of recent developments in network information theory. However,\nstate-of-the-art coding schemes for TIM are usually handcrafted for specific\nfamilies of network topologies, relying critically on experts' domain knowledge\nand sophisticated treatments. The lack of systematic and automatic generation\nof solutions inevitably restricts their potential wider applications to\nwireless communication systems, due to the limited generalizability of coding\nschemes to wider network configurations. To address such an issue, this work\nmakes the first attempt to advocate revisiting topological interference\nalignment (IA) from a novel learning-to-code perspective. Specifically, we\nrecast the one-to-one and subspace IA conditions as vector assignment policies\nand propose a unifying learning-to-code on graphs (LCG) framework by leveraging\ngraph neural networks (GNNs) for capturing topological structures and\nreinforcement learning (RL) for decision-making of IA beamforming vector\nassignment. Interestingly, the proposed LCG framework is capable of recovering\nknown one-to-one scalar/vector IA solutions for a significantly wider range of\nnetwork topologies, and more remarkably of discovering new subspace IA coding\nschemes for multiple-antenna cases that are challenging to be handcrafted. The\nextensive experiments demonstrate that the LCG framework is an effective way to\nautomatically produce systematic coding solutions to the TIM instances with\narbitrary network topologies, and at the same time, the underlying learning\nalgorithm is efficient with respect to online inference time and possesses\nexcellent generalizability and transferability for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advance of topological interference management (TIM) has been one of the\ndriving forces of recent developments in network information theory. However,\nstate-of-the-art coding schemes for TIM are usually handcrafted for specific\nfamilies of network topologies, relying critically on experts' domain knowledge\nand sophisticated treatments. The lack of systematic and automatic generation\nof solutions inevitably restricts their potential wider applications to\nwireless communication systems, due to the limited generalizability of coding\nschemes to wider network configurations. To address such an issue, this work\nmakes the first attempt to advocate revisiting topological interference\nalignment (IA) from a novel learning-to-code perspective. Specifically, we\nrecast the one-to-one and subspace IA conditions as vector assignment policies\nand propose a unifying learning-to-code on graphs (LCG) framework by leveraging\ngraph neural networks (GNNs) for capturing topological structures and\nreinforcement learning (RL) for decision-making of IA beamforming vector\nassignment. Interestingly, the proposed LCG framework is capable of recovering\nknown one-to-one scalar/vector IA solutions for a significantly wider range of\nnetwork topologies, and more remarkably of discovering new subspace IA coding\nschemes for multiple-antenna cases that are challenging to be handcrafted. The\nextensive experiments demonstrate that the LCG framework is an effective way to\nautomatically produce systematic coding solutions to the TIM instances with\narbitrary network topologies, and at the same time, the underlying learning\nalgorithm is efficient with respect to online inference time and possesses\nexcellent generalizability and transferability for practical deployment."
                },
                "authors": [
                    {
                        "name": "Zhiwei Shan"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Chung-Shou Liao"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2305.07186",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15896v2",
                "updated": "2025-02-13T14:07:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    25,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-20T13:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "title": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models"
                },
                "summary": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting."
                },
                "authors": [
                    {
                        "name": "Manuel Pratelli"
                    },
                    {
                        "name": "John Bianchi"
                    },
                    {
                        "name": "Fabio Pinelli"
                    },
                    {
                        "name": "Marinella Petrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Marinella Petrocchi"
                },
                "author": "Marinella Petrocchi",
                "arxiv_doi": "10.1145/3717867.3717924",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3717867.3717924",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04708v2",
                "updated": "2025-02-13T14:06:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    6,
                    51,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-07T13:45:26Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "title": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs"
                },
                "summary": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs."
                },
                "authors": [
                    {
                        "name": "Chengxin Hu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yihe Yuan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ivor Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor Tsang"
                },
                "author": "Ivor Tsang",
                "arxiv_comment": "9 pages, 4 tables, 1 figure, paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05031v2",
                "updated": "2025-02-13T14:02:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    2,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-06T09:52:29Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    52,
                    29,
                    2,
                    311,
                    0
                ],
                "title": "On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard"
                },
                "summary": "Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement."
                },
                "authors": [
                    {
                        "name": "Hossam Amer"
                    },
                    {
                        "name": "Joe Osborne"
                    },
                    {
                        "name": "Michael Zaki"
                    },
                    {
                        "name": "Mohamed Afify"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Afify"
                },
                "author": "Mohamed Afify",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09334v1",
                "updated": "2025-02-13T13:53:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    53,
                    32,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:53:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    53,
                    32,
                    3,
                    44,
                    0
                ],
                "title": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments"
                },
                "summary": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Taiyi Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Ana Klimovic"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09331v1",
                "updated": "2025-02-13T13:49:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:49:30Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "title": "Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs"
                },
                "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Itai Mondshine"
                    },
                    {
                        "name": "Tzuf Paz-Argaman"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    }
                ],
                "author_detail": {
                    "name": "Reut Tsarfaty"
                },
                "author": "Reut Tsarfaty",
                "arxiv_comment": "Accepted for NAACL findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09328v1",
                "updated": "2025-02-13T13:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    40,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:40:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    40,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild"
                },
                "summary": "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants."
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09316v1",
                "updated": "2025-02-13T13:30:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    30,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    30,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis"
                },
                "summary": "Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities."
                },
                "authors": [
                    {
                        "name": "Kentaro Imajo"
                    },
                    {
                        "name": "Masanori Hirano"
                    },
                    {
                        "name": "Shuji Suzuki"
                    },
                    {
                        "name": "Hiroaki Mikami"
                    }
                ],
                "author_detail": {
                    "name": "Hiroaki Mikami"
                },
                "author": "Hiroaki Mikami",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05497v2",
                "updated": "2025-02-13T13:22:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    22,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-08T09:04:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    9,
                    4,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "DeepThink: Aligning Language Models with Domain-Specific User Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepThink: Aligning Language Models with Domain-Specific User Intents"
                },
                "summary": "Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Mingxuan Luo"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kaili Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaili Huang"
                },
                "author": "Kaili Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09307v1",
                "updated": "2025-02-13T13:19:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models"
                },
                "summary": "Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions."
                },
                "authors": [
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Aya Meltzer-Asscher"
                    },
                    {
                        "name": "Jonathan Berant"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Berant"
                },
                "author": "Jonathan Berant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09304v1",
                "updated": "2025-02-13T13:16:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    16,
                    16,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:16:16Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    16,
                    16,
                    3,
                    44,
                    0
                ],
                "title": "KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG"
                },
                "summary": "Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%."
                },
                "authors": [
                    {
                        "name": "Yiqian Huang"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01401v2",
                "updated": "2025-02-13T13:15:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    15,
                    27,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-03T14:32:36Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    32,
                    36,
                    0,
                    34,
                    0
                ],
                "title": "Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection"
                },
                "summary": "3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe."
                },
                "authors": [
                    {
                        "name": "Boyu Mi"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16106v2",
                "updated": "2025-02-13T13:11:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    11,
                    46,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-21T15:34:44Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    34,
                    44,
                    0,
                    295,
                    0
                ],
                "title": "Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation"
                },
                "summary": "Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Weichen Wu"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuting Wei"
                    },
                    {
                        "name": "Alessandro Rinaldo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Rinaldo"
                },
                "author": "Alessandro Rinaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15330v2",
                "updated": "2025-02-13T13:06:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    6,
                    0,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-21T17:42:52Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    42,
                    52,
                    4,
                    173,
                    0
                ],
                "title": "Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection"
                },
                "summary": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT."
                },
                "authors": [
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11905v2",
                "updated": "2025-02-13T13:04:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    4,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-16T16:38:47Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    16,
                    38,
                    47,
                    1,
                    198,
                    0
                ],
                "title": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge"
                },
                "summary": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart."
                },
                "authors": [
                    {
                        "name": "Andrej op"
                    },
                    {
                        "name": "Bla Bertalani"
                    },
                    {
                        "name": "Carolina Fortuna"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Fortuna"
                },
                "author": "Carolina Fortuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09285v1",
                "updated": "2025-02-13T13:00:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    0,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:00:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    0,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "EmoAssist: Emotional Assistant for Visual Impairment Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoAssist: Emotional Assistant for Visual Impairment Community"
                },
                "summary": "The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o."
                },
                "authors": [
                    {
                        "name": "Xingyu Qi"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Zhenyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Wu"
                },
                "author": "Zhenyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09284v1",
                "updated": "2025-02-13T12:57:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "SparQLe: Speech Queries to Text Translation Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQLe: Speech Queries to Text Translation Through LLMs"
                },
                "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications."
                },
                "authors": [
                    {
                        "name": "Amirbek Djanibekov"
                    },
                    {
                        "name": "Hanan Aldarmaki"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Aldarmaki"
                },
                "author": "Hanan Aldarmaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11283v2",
                "updated": "2025-02-13T12:48:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    48,
                    58,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-20T05:34:38Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    5,
                    34,
                    38,
                    0,
                    20,
                    0
                ],
                "title": "Large Language Model Agents for Radio Map Generation and Wireless\n  Network Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agents for Radio Map Generation and Wireless\n  Network Planning"
                },
                "summary": "Using commercial software for radio map generation and wireless network\nplanning often require complex manual operations, posing significant challenges\nin terms of scalability, adaptability, and user-friendliness, due to heavy\nmanual operations. To address these issues, we propose an automated solution\nthat employs large language model (LLM) agents. These agents are designed to\nautonomously generate radio maps and facilitate wireless network planning for\nspecified areas, thereby minimizing the necessity for extensive manual\nintervention. To validate the effectiveness of our proposed solution, we\ndevelop a software platform that integrates LLM agents. Experimental results\ndemonstrate that a large amount manual operations can be saved via the proposed\nLLM agent, and the automated solutions can achieve an enhanced coverage and\nsignal-to-interference-noise ratio (SINR), especially in urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using commercial software for radio map generation and wireless network\nplanning often require complex manual operations, posing significant challenges\nin terms of scalability, adaptability, and user-friendliness, due to heavy\nmanual operations. To address these issues, we propose an automated solution\nthat employs large language model (LLM) agents. These agents are designed to\nautonomously generate radio maps and facilitate wireless network planning for\nspecified areas, thereby minimizing the necessity for extensive manual\nintervention. To validate the effectiveness of our proposed solution, we\ndevelop a software platform that integrates LLM agents. Experimental results\ndemonstrate that a large amount manual operations can be saved via the proposed\nLLM agent, and the automated solutions can achieve an enhanced coverage and\nsignal-to-interference-noise ratio (SINR), especially in urban environments."
                },
                "authors": [
                    {
                        "name": "Hongye Quan"
                    },
                    {
                        "name": "Wanli Ni"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Xiangyu Ye"
                    },
                    {
                        "name": "Ziyi Xie"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Hui Song"
                    }
                ],
                "author_detail": {
                    "name": "Hui Song"
                },
                "author": "Hui Song",
                "arxiv_comment": "5 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v3",
                "updated": "2025-02-13T12:43:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    43,
                    59,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Usage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Usage?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09268v1",
                "updated": "2025-02-13T12:29:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    29,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:29:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    29,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation"
                },
                "summary": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks."
                },
                "authors": [
                    {
                        "name": "Hongyin Zhang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Ying Peng"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09266v1",
                "updated": "2025-02-13T12:28:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    28,
                    42,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    28,
                    42,
                    3,
                    44,
                    0
                ],
                "title": "Accelerating Bayesian Sampling for Massive Black Hole Binaries with\n  Prior Constraints from Conditional Variational Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Bayesian Sampling for Massive Black Hole Binaries with\n  Prior Constraints from Conditional Variational Autoencoder"
                },
                "summary": "We employ a Conditional Variational Autoencoder (CVAE) for parameter\ninference on massive black hole binaries (MBHBs), considering joint\nobservations from a network of three space-based gravitational wave detectors.\nOur result demonstrates that the trained CVAE model can estimate the posterior\ndistribution of source parameters in approximately 0.5 seconds, while the\nstandard Bayesian sampling method, utilizing parallel computation across 16 CPU\ncores, takes an average of 22 hours across 25 MBHB signals. While the CVAE\nmodel achieves remarkable efficiency, its estimated distributions exhibit\nslight differences in shape compared to the standard Bayesian results,\nparticularly showing lighter tails with broader widths. By using CVAE result to\nconstrain the prior range for Bayesian sampling, we reduce the sampling time to\n$14.0\\%$ of the original runtime on average, while maintaining similar Bayesian\nresult.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ a Conditional Variational Autoencoder (CVAE) for parameter\ninference on massive black hole binaries (MBHBs), considering joint\nobservations from a network of three space-based gravitational wave detectors.\nOur result demonstrates that the trained CVAE model can estimate the posterior\ndistribution of source parameters in approximately 0.5 seconds, while the\nstandard Bayesian sampling method, utilizing parallel computation across 16 CPU\ncores, takes an average of 22 hours across 25 MBHB signals. While the CVAE\nmodel achieves remarkable efficiency, its estimated distributions exhibit\nslight differences in shape compared to the standard Bayesian results,\nparticularly showing lighter tails with broader widths. By using CVAE result to\nconstrain the prior range for Bayesian sampling, we reduce the sampling time to\n$14.0\\%$ of the original runtime on average, while maintaining similar Bayesian\nresult."
                },
                "authors": [
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Jibo He"
                    }
                ],
                "author_detail": {
                    "name": "Jibo He"
                },
                "author": "Jibo He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17301v2",
                "updated": "2025-02-13T12:25:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    25,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-26T10:48:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation"
                },
                "summary": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16290v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16290v3",
                "updated": "2025-02-13T12:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    16,
                    47,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-05T20:03:57Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    20,
                    3,
                    57,
                    5,
                    279,
                    0
                ],
                "title": "A Unified Model for Compressed Sensing MRI Across Undersampling Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Model for Compressed Sensing MRI Across Undersampling Patterns"
                },
                "summary": "Compressed Sensing MRI reconstructs images of the body's internal anatomy\nfrom undersampled measurements, thereby reducing the scan time - the time\nsubjects need to remain still. Recently, deep neural networks have shown great\npotential for reconstructing high-fidelity images from highly undersampled\nmeasurements in the frequency space. However, one needs to train multiple\nmodels for different undersampling patterns and desired output image\nresolutions, since most networks operate on a fixed discretization. Such\napproaches are highly impractical in clinical settings, where undersampling\npatterns and image resolutions are frequently changed to accommodate different\nreal-time imaging and diagnostic requirements.\n  We propose a unified model robust to different measurement undersampling\npatterns and image resolutions in compressed sensing MRI. Our model is based on\nneural operators, a discretization-agnostic architecture. Neural operators are\nemployed in both image and measurement space, which capture local and global\nimage features for MRI reconstruction. Empirically, we achieve consistent\nperformance across different undersampling rates and patterns, with an average\n11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN,\nEnd-to-End VarNet. For efficiency, our inference speed is also 1,400x faster\nthan diffusion methods. The resolution-agnostic design also enhances zero-shot\nsuper-resolution and extended field of view in reconstructed images. Our\nunified model offers a versatile solution for MRI, adapting seamlessly to\nvarious measurement undersampling and imaging resolutions, making it highly\neffective for flexible and reliable clinical imaging. Our code is available at\nhttps://armeet.ca/nomri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Sensing MRI reconstructs images of the body's internal anatomy\nfrom undersampled measurements, thereby reducing the scan time - the time\nsubjects need to remain still. Recently, deep neural networks have shown great\npotential for reconstructing high-fidelity images from highly undersampled\nmeasurements in the frequency space. However, one needs to train multiple\nmodels for different undersampling patterns and desired output image\nresolutions, since most networks operate on a fixed discretization. Such\napproaches are highly impractical in clinical settings, where undersampling\npatterns and image resolutions are frequently changed to accommodate different\nreal-time imaging and diagnostic requirements.\n  We propose a unified model robust to different measurement undersampling\npatterns and image resolutions in compressed sensing MRI. Our model is based on\nneural operators, a discretization-agnostic architecture. Neural operators are\nemployed in both image and measurement space, which capture local and global\nimage features for MRI reconstruction. Empirically, we achieve consistent\nperformance across different undersampling rates and patterns, with an average\n11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN,\nEnd-to-End VarNet. For efficiency, our inference speed is also 1,400x faster\nthan diffusion methods. The resolution-agnostic design also enhances zero-shot\nsuper-resolution and extended field of view in reconstructed images. Our\nunified model offers a versatile solution for MRI, adapting seamlessly to\nvarious measurement undersampling and imaging resolutions, making it highly\neffective for flexible and reliable clinical imaging. Our code is available at\nhttps://armeet.ca/nomri."
                },
                "authors": [
                    {
                        "name": "Armeet Singh Jatyani"
                    },
                    {
                        "name": "Jiayun Wang"
                    },
                    {
                        "name": "Aditi Chandrashekar"
                    },
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Miguel Liu-Schiaffini"
                    },
                    {
                        "name": "Bahareh Tolooshams"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16290v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16290v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09258v1",
                "updated": "2025-02-13T12:15:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    15,
                    32,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:15:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    15,
                    32,
                    3,
                    44,
                    0
                ],
                "title": "Accounting for motion of supernova host galaxy in statistical inference\n  from SNIa data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accounting for motion of supernova host galaxy in statistical inference\n  from SNIa data"
                },
                "summary": "We investigate the impact of peculiar motion of Type Ia supernova host\ngalaxies on cosmological parameter estimation. This motion causes their\nredshift to deviate from that of the comoving observer at their position and is\na source of noise. To this end, we develop an estimator for parameter\nestimation in models with errors in independent variables. Using the Bayesian\nframework, errors in independent variables are treated as nuisance parameters\nby making the independent variables parameters of the model. Our method applied\nto the Pantheon sample of Type Ia supernova indicates a few percent shift in\nthe central values of inferred cosmological parameters. For the $w$CDM model,\nwe find that accounting for peculiar velocities makes the data marginally more\nconsistent with the cosmological constant model. By using simulated data, we\nshow that not accounting for peculiar velocities will significantly impact\nparameter estimation from higher precision future data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the impact of peculiar motion of Type Ia supernova host\ngalaxies on cosmological parameter estimation. This motion causes their\nredshift to deviate from that of the comoving observer at their position and is\na source of noise. To this end, we develop an estimator for parameter\nestimation in models with errors in independent variables. Using the Bayesian\nframework, errors in independent variables are treated as nuisance parameters\nby making the independent variables parameters of the model. Our method applied\nto the Pantheon sample of Type Ia supernova indicates a few percent shift in\nthe central values of inferred cosmological parameters. For the $w$CDM model,\nwe find that accounting for peculiar velocities makes the data marginally more\nconsistent with the cosmological constant model. By using simulated data, we\nshow that not accounting for peculiar velocities will significantly impact\nparameter estimation from higher precision future data sets."
                },
                "authors": [
                    {
                        "name": "Ujjwal Upadhyay"
                    },
                    {
                        "name": "Tarun Deep Saini"
                    },
                    {
                        "name": "Shiv K. Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Shiv K. Sethi"
                },
                "author": "Shiv K. Sethi",
                "arxiv_comment": "18 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07179v3",
                "updated": "2025-02-13T12:14:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    14,
                    51,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-11T11:41:41Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    11,
                    41,
                    41,
                    1,
                    163,
                    0
                ],
                "title": "Comprehensive Study of $k$-essence Model: Dynamical System Analysis and\n  Observational Constraints from Latest Type Ia Supernova and BAO Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Study of $k$-essence Model: Dynamical System Analysis and\n  Observational Constraints from Latest Type Ia Supernova and BAO Observations"
                },
                "summary": "We constrain the parameters of the $k$-essence scalar field model with\ninverse square and exponential potentials using data sets including\nPantheon+SHOES and the Dark Energy Survey (DES) of Type Ia supernovae, Baryon\nAcoustic Oscillation (BAO) data from SDSS and DESI surveys, and direct\nmeasurements of the Hubble parameter and redshift obtained from the\ndifferential age method (CC). We also provide a brief perspective on the\ndynamical evolution of both models and derive stability constraints on the\nmodel parameters, which are then used to set appropriate priors. We adopt a\nBayesian inference procedure to estimate the model parameters that best fit the\ndata. A comprehensive analysis in light of observational data shows that the\n$k$-essence model fits well across all data combinations. However, according to\nthe BIC criterion, the $\\Lambda$CDM model provides a slightly better fit\ncompared to the $k$-essence model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We constrain the parameters of the $k$-essence scalar field model with\ninverse square and exponential potentials using data sets including\nPantheon+SHOES and the Dark Energy Survey (DES) of Type Ia supernovae, Baryon\nAcoustic Oscillation (BAO) data from SDSS and DESI surveys, and direct\nmeasurements of the Hubble parameter and redshift obtained from the\ndifferential age method (CC). We also provide a brief perspective on the\ndynamical evolution of both models and derive stability constraints on the\nmodel parameters, which are then used to set appropriate priors. We adopt a\nBayesian inference procedure to estimate the model parameters that best fit the\ndata. A comprehensive analysis in light of observational data shows that the\n$k$-essence model fits well across all data combinations. However, according to\nthe BIC criterion, the $\\Lambda$CDM model provides a slightly better fit\ncompared to the $k$-essence model."
                },
                "authors": [
                    {
                        "name": "Saddam Hussain"
                    },
                    {
                        "name": "Sarath Nelleri"
                    },
                    {
                        "name": "Kaushik Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Bhattacharya"
                },
                "author": "Kaushik Bhattacharya",
                "arxiv_comment": "35 pages, 17 figures, 8 tables, Included critical points analysis at\n  infinity, Accepted for the publication in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08116v2",
                "updated": "2025-02-13T12:12:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    12,
                    22,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-10T17:01:53Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    1,
                    53,
                    3,
                    284,
                    0
                ],
                "title": "BOWIE-ALIGN: JWST reveals hints of planetesimal accretion and complex\n  sulphur chemistry in the atmosphere of the misaligned hot Jupiter WASP-15b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOWIE-ALIGN: JWST reveals hints of planetesimal accretion and complex\n  sulphur chemistry in the atmosphere of the misaligned hot Jupiter WASP-15b"
                },
                "summary": "We present a transmission spectrum of the misaligned hot Jupiter WASP-15b\nfrom 2.8-5.2 microns observed with JWST's NIRSpec/G395H grating. Our high\nsignal to noise data, which has negligible red noise, reveals significant\nabsorption by H$_2$O ($4.2\\sigma$) and CO$_2$ ($8.9\\sigma$). From independent\ndata reduction and atmospheric retrieval approaches, we infer that WASP-15b's\natmospheric metallicity is super-solar ($\\gtrsim 15\\times$ solar) and its\ncarbon-to-oxygen ratio is consistent with solar, that together imply\nplanetesimal accretion. Our general circulation model simulations for WASP-15b\nsuggest that the carbon-to-oxygen we measure at the limb is likely\nrepresentative of the entire photosphere due to the mostly uniform spatial\ndistribution of H$_2$O, CO$_2$ and CO. We additionally see evidence for\nabsorption by SO$_2$ and absorption at 4.9$\\mu$m, for which the current leading\ncandidate is OCS, albeit with several caveats. If confirmed, this would be the\nfirst detection of OCS in an exoplanet atmosphere and point towards complex\nphotochemistry of sulphur-bearing species in the upper atmosphere. These are\nthe first observations from the BOWIE-ALIGN survey which is using JWST's\nNIRSpec/G395H instrument to compare the atmospheric compositions of\naligned/low-obliquity and misaligned/high-obliquity hot Jupiters around F stars\nabove the Kraft break. The goal of our survey is to determine whether the\natmospheric composition differs across two populations of planets that have\nlikely undergone different migration histories (disc versus disc-free) as\nevidenced by their obliquities (aligned versus misaligned).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a transmission spectrum of the misaligned hot Jupiter WASP-15b\nfrom 2.8-5.2 microns observed with JWST's NIRSpec/G395H grating. Our high\nsignal to noise data, which has negligible red noise, reveals significant\nabsorption by H$_2$O ($4.2\\sigma$) and CO$_2$ ($8.9\\sigma$). From independent\ndata reduction and atmospheric retrieval approaches, we infer that WASP-15b's\natmospheric metallicity is super-solar ($\\gtrsim 15\\times$ solar) and its\ncarbon-to-oxygen ratio is consistent with solar, that together imply\nplanetesimal accretion. Our general circulation model simulations for WASP-15b\nsuggest that the carbon-to-oxygen we measure at the limb is likely\nrepresentative of the entire photosphere due to the mostly uniform spatial\ndistribution of H$_2$O, CO$_2$ and CO. We additionally see evidence for\nabsorption by SO$_2$ and absorption at 4.9$\\mu$m, for which the current leading\ncandidate is OCS, albeit with several caveats. If confirmed, this would be the\nfirst detection of OCS in an exoplanet atmosphere and point towards complex\nphotochemistry of sulphur-bearing species in the upper atmosphere. These are\nthe first observations from the BOWIE-ALIGN survey which is using JWST's\nNIRSpec/G395H instrument to compare the atmospheric compositions of\naligned/low-obliquity and misaligned/high-obliquity hot Jupiters around F stars\nabove the Kraft break. The goal of our survey is to determine whether the\natmospheric composition differs across two populations of planets that have\nlikely undergone different migration histories (disc versus disc-free) as\nevidenced by their obliquities (aligned versus misaligned)."
                },
                "authors": [
                    {
                        "name": "James Kirk"
                    },
                    {
                        "name": "Eva-Maria Ahrer"
                    },
                    {
                        "name": "Alastair B. Claringbold"
                    },
                    {
                        "name": "Maria Zamyatina"
                    },
                    {
                        "name": "Chloe Fisher"
                    },
                    {
                        "name": "Mason McCormack"
                    },
                    {
                        "name": "Vatsal Panwar"
                    },
                    {
                        "name": "Diana Powell"
                    },
                    {
                        "name": "Jake Taylor"
                    },
                    {
                        "name": "Daniel P. Thorngren"
                    },
                    {
                        "name": "Duncan A. Christie"
                    },
                    {
                        "name": "Emma Esparza-Borges"
                    },
                    {
                        "name": "Shang-Min Tsai"
                    },
                    {
                        "name": "Lili Alderson"
                    },
                    {
                        "name": "Richard A. Booth"
                    },
                    {
                        "name": "Charlotte Fairman"
                    },
                    {
                        "name": "Mercedes Lpez-Morales"
                    },
                    {
                        "name": "N. J. Mayne"
                    },
                    {
                        "name": "Annabella Meech"
                    },
                    {
                        "name": "Paul Molliere"
                    },
                    {
                        "name": "James E. Owen"
                    },
                    {
                        "name": "Anna B. T. Penzlin"
                    },
                    {
                        "name": "Denis E. Sergeev"
                    },
                    {
                        "name": "Daniel Valentine"
                    },
                    {
                        "name": "Hannah R. Wakeford"
                    },
                    {
                        "name": "Peter J. Wheatley"
                    }
                ],
                "author_detail": {
                    "name": "Peter J. Wheatley"
                },
                "author": "Peter J. Wheatley",
                "arxiv_doi": "10.1093/mnras/staf208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.08116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 25 figures, 6 tables. Accepted to MNRAS on 30th January\n  2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00008v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00008v5",
                "updated": "2025-02-13T12:10:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2023-03-27T18:00:01Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    18,
                    0,
                    1,
                    0,
                    86,
                    0
                ],
                "title": "On the Creativity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Creativity of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_doi": "10.1007/s00146-024-02127-3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00146-024-02127-3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.00008v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00008v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3",
                "arxiv_journal_ref": "AI & Soc (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09255v1",
                "updated": "2025-02-13T12:10:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    11,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:10:11Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    11,
                    3,
                    44,
                    0
                ],
                "title": "Bayesian Matrix Factor Models for Demographic Analysis Across Age and\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Matrix Factor Models for Demographic Analysis Across Age and\n  Time"
                },
                "summary": "Analyzing demographic data collected across multiple populations, time\nperiods, and age groups is challenging due to the interplay of high\ndimensionality, demographic heterogeneity among groups, and stochastic\nvariability within smaller groups. This paper proposes a Bayesian matrix factor\nmodel to address these challenges. By factorizing count data matrices as the\nproduct of low-dimensional latent age and time factors, the model achieves a\nparsimonious representation that mitigates overfitting and remains\ncomputationally feasible even when hundreds of subpopulations are involved.\nSmoothness in age factors and a dynamic evolution of time factors are achieved\nthrough informative priors, and an efficient Markov chain Monte Carlo algorithm\nis developed for posterior inference. Applying the model to Austrian\ndistrict-level emigration data from 2002 to 2023 demonstrates its ability to\nreconstruct demographic processes using only a fraction of the parameters\nrequired by conventional factor models. Extensive cross-validation and\nout-of-sample forecasting exercises show that the proposed matrix factor model\nconsistently outperforms standard benchmarks. Beyond statistical demography,\nthe framework holds promise for a wide range of applications involving noisy,\nheterogeneous, and high-dimensional non-Gaussian matrix-valued data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing demographic data collected across multiple populations, time\nperiods, and age groups is challenging due to the interplay of high\ndimensionality, demographic heterogeneity among groups, and stochastic\nvariability within smaller groups. This paper proposes a Bayesian matrix factor\nmodel to address these challenges. By factorizing count data matrices as the\nproduct of low-dimensional latent age and time factors, the model achieves a\nparsimonious representation that mitigates overfitting and remains\ncomputationally feasible even when hundreds of subpopulations are involved.\nSmoothness in age factors and a dynamic evolution of time factors are achieved\nthrough informative priors, and an efficient Markov chain Monte Carlo algorithm\nis developed for posterior inference. Applying the model to Austrian\ndistrict-level emigration data from 2002 to 2023 demonstrates its ability to\nreconstruct demographic processes using only a fraction of the parameters\nrequired by conventional factor models. Extensive cross-validation and\nout-of-sample forecasting exercises show that the proposed matrix factor model\nconsistently outperforms standard benchmarks. Beyond statistical demography,\nthe framework holds promise for a wide range of applications involving noisy,\nheterogeneous, and high-dimensional non-Gaussian matrix-valued data."
                },
                "authors": [
                    {
                        "name": "Gregor Zens"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Zens"
                },
                "author": "Gregor Zens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09254v1",
                "updated": "2025-02-13T12:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:10:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection"
                },
                "summary": "Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings."
                },
                "authors": [
                    {
                        "name": "Hezhe Qiao"
                    },
                    {
                        "name": "Chaoxi Niu"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Guansong Pang"
                    }
                ],
                "author_detail": {
                    "name": "Guansong Pang"
                },
                "author": "Guansong Pang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05033v2",
                "updated": "2025-02-13T12:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    9,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-06T09:58:58Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    9,
                    58,
                    58,
                    5,
                    188,
                    0
                ],
                "title": "PeaPOD: Personalized Prompt Distillation for Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeaPOD: Personalized Prompt Distillation for Generative Recommendation"
                },
                "summary": "Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks."
                },
                "authors": [
                    {
                        "name": "Jerome Ramos"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Aldo Lipani"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Lipani"
                },
                "author": "Aldo Lipani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09238v1",
                "updated": "2025-02-13T11:55:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    55,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:55:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    55,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart\n  Logistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart\n  Logistics"
                },
                "summary": "The increasing demand for efficient last-mile delivery in smart logistics\nunderscores the role of autonomous robots in enhancing operational efficiency\nand reducing costs. Traditional navigation methods, which depend on\nhigh-precision maps, are resource-intensive, while learning-based approaches\noften struggle with generalization in real-world scenarios. To address these\nchallenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic\nNavigation (OPEN) system that combines foundation models with classic\nalgorithms for scalable outdoor navigation. The system uses off-the-shelf\nOpenStreetMap (OSM) for flexible map representation, thereby eliminating the\nneed for extensive pre-mapping efforts. It also employs Large Language Models\n(LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs)\nfor global localization, map updates, and house number recognition. To\ncompensate the limitations of existing benchmarks that are inadequate for\nassessing last-mile delivery, this work introduces a new benchmark specifically\ndesigned for outdoor navigation in residential areas, reflecting the real-world\nchallenges faced by autonomous delivery systems. Extensive experiments in\nsimulated and real-world environments demonstrate the proposed system's\nefficacy in enhancing navigation efficiency and reliability. To facilitate\nfurther research, our code and benchmark are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for efficient last-mile delivery in smart logistics\nunderscores the role of autonomous robots in enhancing operational efficiency\nand reducing costs. Traditional navigation methods, which depend on\nhigh-precision maps, are resource-intensive, while learning-based approaches\noften struggle with generalization in real-world scenarios. To address these\nchallenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic\nNavigation (OPEN) system that combines foundation models with classic\nalgorithms for scalable outdoor navigation. The system uses off-the-shelf\nOpenStreetMap (OSM) for flexible map representation, thereby eliminating the\nneed for extensive pre-mapping efforts. It also employs Large Language Models\n(LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs)\nfor global localization, map updates, and house number recognition. To\ncompensate the limitations of existing benchmarks that are inadequate for\nassessing last-mile delivery, this work introduces a new benchmark specifically\ndesigned for outdoor navigation in residential areas, reflecting the real-world\nchallenges faced by autonomous delivery systems. Extensive experiments in\nsimulated and real-world environments demonstrate the proposed system's\nefficacy in enhancing navigation efficiency and reliability. To facilitate\nfurther research, our code and benchmark are publicly available."
                },
                "authors": [
                    {
                        "name": "Junhui Wang"
                    },
                    {
                        "name": "Dongjie Huo"
                    },
                    {
                        "name": "Zehui Xu"
                    },
                    {
                        "name": "Yongliang Shi"
                    },
                    {
                        "name": "Yimin Yan"
                    },
                    {
                        "name": "Yuanxin Wang"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Yan Qiao"
                    },
                    {
                        "name": "Guyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guyue Zhou"
                },
                "author": "Guyue Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09237v1",
                "updated": "2025-02-13T11:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    54,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:54:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    54,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Reliable Conversational Agents under ASP Control that Understand Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Conversational Agents under ASP Control that Understand Natural\n  Language"
                },
                "summary": "Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable."
                },
                "authors": [
                    {
                        "name": "Yankai Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yankai Zeng"
                },
                "arxiv_affiliation": "The University of Texas at Dallas",
                "author": "Yankai Zeng",
                "arxiv_doi": "10.4204/EPTCS.416.41",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.41",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 398-406",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09213v1",
                "updated": "2025-02-13T11:48:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    46,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:48:46Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    46,
                    3,
                    44,
                    0
                ],
                "title": "Neuro-Symbolic Contrastive Learning for Cross-domain Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Contrastive Learning for Cross-domain Inference"
                },
                "summary": "Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyue Liu"
                    },
                    {
                        "name": "Ryo Ueda"
                    },
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Katsumi Inoue"
                    },
                    {
                        "name": "Chris G. Willcocks"
                    }
                ],
                "author_detail": {
                    "name": "Chris G. Willcocks"
                },
                "arxiv_affiliation": "Durham University",
                "author": "Chris G. Willcocks",
                "arxiv_doi": "10.4204/EPTCS.416.6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 78-94",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09212v1",
                "updated": "2025-02-13T11:48:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:48:31Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    31,
                    3,
                    44,
                    0
                ],
                "title": "LP-LM: No Hallucinations in Question Answering with Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LP-LM: No Hallucinations in Question Answering with Logic Programming"
                },
                "summary": "Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM."
                },
                "authors": [
                    {
                        "name": "Katherine Wu"
                    },
                    {
                        "name": "Yanhong A. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yanhong A. Liu"
                },
                "author": "Yanhong A. Liu",
                "arxiv_doi": "10.4204/EPTCS.416.5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 69-77",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12433v3",
                "updated": "2025-02-13T11:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2024-05-21T01:16:34Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    1,
                    16,
                    34,
                    1,
                    142,
                    0
                ],
                "title": "LLM+Reasoning+Planning for Supporting Incomplete User Queries in\n  Presence of APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+Reasoning+Planning for Supporting Incomplete User Queries in\n  Presence of APIs"
                },
                "summary": "Recent availability of Large Language Models (LLMs) has led to the\ndevelopment of numerous LLM-based approaches aimed at providing natural\nlanguage interfaces for various end-user tasks. These end-user tasks in turn\ncan typically be accomplished by orchestrating a given set of APIs. In\npractice, natural language task requests (user queries) are often incomplete,\ni.e., they may not contain all the information required by the APIs. While LLMs\nexcel at natural language processing (NLP) tasks, they frequently hallucinate\non missing information or struggle with orchestrating the APIs. The key idea\nbehind our proposed approach is to leverage logical reasoning and classical AI\nplanning along with an LLM for accurately answering user queries including\nidentification and gathering of any missing information in these queries. Our\napproach uses an LLM and ASP (Answer Set Programming) solver to translate a\nuser query to a representation in Planning Domain Definition Language (PDDL)\nvia an intermediate representation in ASP. We introduce a special API\n\"get_info_api\" for gathering missing information. We model all the APIs as PDDL\nactions in a way that supports dataflow between the APIs. Our approach then\nuses a classical AI planner to generate an orchestration of API calls\n(including calls to get_info_api) to answer the user query. Our evaluation\nresults show that our approach significantly outperforms a pure LLM based\napproach by achieving over 95% success rate in most cases on a dataset\ncontaining complete and incomplete single goal and multi-goal queries where the\nmulti-goal queries may or may not require dataflow among the APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent availability of Large Language Models (LLMs) has led to the\ndevelopment of numerous LLM-based approaches aimed at providing natural\nlanguage interfaces for various end-user tasks. These end-user tasks in turn\ncan typically be accomplished by orchestrating a given set of APIs. In\npractice, natural language task requests (user queries) are often incomplete,\ni.e., they may not contain all the information required by the APIs. While LLMs\nexcel at natural language processing (NLP) tasks, they frequently hallucinate\non missing information or struggle with orchestrating the APIs. The key idea\nbehind our proposed approach is to leverage logical reasoning and classical AI\nplanning along with an LLM for accurately answering user queries including\nidentification and gathering of any missing information in these queries. Our\napproach uses an LLM and ASP (Answer Set Programming) solver to translate a\nuser query to a representation in Planning Domain Definition Language (PDDL)\nvia an intermediate representation in ASP. We introduce a special API\n\"get_info_api\" for gathering missing information. We model all the APIs as PDDL\nactions in a way that supports dataflow between the APIs. Our approach then\nuses a classical AI planner to generate an orchestration of API calls\n(including calls to get_info_api) to answer the user query. Our evaluation\nresults show that our approach significantly outperforms a pure LLM based\napproach by achieving over 95% success rate in most cases on a dataset\ncontaining complete and incomplete single goal and multi-goal queries where the\nmulti-goal queries may or may not require dataflow among the APIs."
                },
                "authors": [
                    {
                        "name": "Sudhir Agarwal"
                    },
                    {
                        "name": "Anu Sreepathy"
                    },
                    {
                        "name": "David H. Alonso"
                    },
                    {
                        "name": "Prarit Lamba"
                    }
                ],
                "author_detail": {
                    "name": "Prarit Lamba"
                },
                "arxiv_affiliation": "Intuit Inc.",
                "author": "Prarit Lamba",
                "arxiv_doi": "10.4204/EPTCS.416.3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 29-58",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09211v1",
                "updated": "2025-02-13T11:47:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    59,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:47:59Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    59,
                    3,
                    44,
                    0
                ],
                "title": "Visual Graph Question Answering with ASP and LLMs for Language Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Graph Question Answering with ASP and LLMs for Language Parsing"
                },
                "summary": "Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks."
                },
                "authors": [
                    {
                        "name": "Jakob Johannes Bauer"
                    },
                    {
                        "name": "Thomas Eiter"
                    },
                    {
                        "name": "Nelson Higuera Ruiz"
                    },
                    {
                        "name": "Johannes Oetsch"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Oetsch"
                },
                "arxiv_affiliation": "Jonkoping University, Sweden",
                "author": "Johannes Oetsch",
                "arxiv_doi": "10.4204/EPTCS.416.2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 15-28",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09209v1",
                "updated": "2025-02-13T11:47:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:47:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "On LLM-generated Logic Programs and their Inference Execution Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On LLM-generated Logic Programs and their Inference Execution Methods"
                },
                "summary": "Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs."
                },
                "authors": [
                    {
                        "name": "Paul Tarau"
                    }
                ],
                "author_detail": {
                    "name": "Paul Tarau"
                },
                "arxiv_affiliation": "University of North Texas",
                "author": "Paul Tarau",
                "arxiv_doi": "10.4204/EPTCS.416.1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 1-14",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v3",
                "updated": "2025-02-13T11:46:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    46,
                    25,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Lee"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09204v1",
                "updated": "2025-02-13T11:45:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    45,
                    38,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:45:38Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    45,
                    38,
                    3,
                    44,
                    0
                ],
                "title": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in\n  New York",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in\n  New York"
                },
                "summary": "Legal cases require careful logical reasoning following the laws, whereas\ninteractions with non- technical users must be in natural language. As an\napplication combining logical reasoning using Prolog and natural language\nprocessing using large language models (LLMs), this paper presents a novel\napproach and system, LogicLease, to automate the analysis of landlord-tenant\nlegal cases in the state of New York. LogicLease determines compliance with\nrelevant legal requirements by analyzing case descriptions and citing all\nrelevant laws. It leverages LLMs for information extraction and Prolog for\nlegal reasoning. By separating information extraction from legal reasoning,\nLogicLease achieves greater transparency and control over the legal logic\napplied to each case. We evaluate the accuracy, efficiency, and robustness of\nLogicLease through a series of tests, achieving 100% accuracy and an average\nprocessing time of 2.57 seconds. LogicLease presents advantages over\nstate-of-the-art LLM- based legal analysis systems by providing clear,\nstep-by-step reasoning, citing specific laws, and distinguishing itself by its\nability to avoid hallucinations - a common issue in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal cases require careful logical reasoning following the laws, whereas\ninteractions with non- technical users must be in natural language. As an\napplication combining logical reasoning using Prolog and natural language\nprocessing using large language models (LLMs), this paper presents a novel\napproach and system, LogicLease, to automate the analysis of landlord-tenant\nlegal cases in the state of New York. LogicLease determines compliance with\nrelevant legal requirements by analyzing case descriptions and citing all\nrelevant laws. It leverages LLMs for information extraction and Prolog for\nlegal reasoning. By separating information extraction from legal reasoning,\nLogicLease achieves greater transparency and control over the legal logic\napplied to each case. We evaluate the accuracy, efficiency, and robustness of\nLogicLease through a series of tests, achieving 100% accuracy and an average\nprocessing time of 2.57 seconds. LogicLease presents advantages over\nstate-of-the-art LLM- based legal analysis systems by providing clear,\nstep-by-step reasoning, citing specific laws, and distinguishing itself by its\nability to avoid hallucinations - a common issue in LLMs."
                },
                "authors": [
                    {
                        "name": "Sanskar Sehgal"
                    },
                    {
                        "name": "Yanhong A. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yanhong A. Liu"
                },
                "author": "Yanhong A. Liu",
                "arxiv_doi": "10.4204/EPTCS.416.4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 59-68",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10053v2",
                "updated": "2025-02-13T11:43:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    43,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2024-08-19T14:48:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory"
                },
                "summary": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Xuebing Zhou"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "To appear at NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09200v1",
                "updated": "2025-02-13T11:39:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    39,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:39:31Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    39,
                    31,
                    3,
                    44,
                    0
                ],
                "title": "PSR J0952-0607: Probing the Stiffest Equations of State and r-Mode\n  Suppression Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSR J0952-0607: Probing the Stiffest Equations of State and r-Mode\n  Suppression Mechanisms"
                },
                "summary": "We analyze PSR J0952-0607, the most massive and fastest spinning neutron star\nobserved to date, to refine constraints on the neutron star equation of state\n(EoS) and investigate its robustness against r-mode instabilities. With a mass\nof \\( 2.35 \\pm 0.17 \\, M_{\\odot} \\) and a spin frequency of 709.2 Hz, PSR\nJ0952-0607 provides a unique opportunity to examine the effects of rapid\nrotation on the structure of a neutron star. Using a Bayesian framework, we\nincorporate the rotationally corrected mass of PSR J0952-0607, alongside PSR\nJ0740+6620's static mass measurement, to constrain the EoS. Our findings\ndemonstrate that neglecting rotational effects leads to biases in the inferred\nEoS, while including the neutron star spin produces tighter constraints on\npressure-density and mass-radius relations. Additionally, we explore the r-mode\ninstability window for PSR J0952-0607 under the assumption of both rigid and\nelastic crust models and find that a rigid crust allows a higher stable\ntemperature range, whereas an elastic crust places the star within the\ninstability window under certain thermal insulation conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze PSR J0952-0607, the most massive and fastest spinning neutron star\nobserved to date, to refine constraints on the neutron star equation of state\n(EoS) and investigate its robustness against r-mode instabilities. With a mass\nof \\( 2.35 \\pm 0.17 \\, M_{\\odot} \\) and a spin frequency of 709.2 Hz, PSR\nJ0952-0607 provides a unique opportunity to examine the effects of rapid\nrotation on the structure of a neutron star. Using a Bayesian framework, we\nincorporate the rotationally corrected mass of PSR J0952-0607, alongside PSR\nJ0740+6620's static mass measurement, to constrain the EoS. Our findings\ndemonstrate that neglecting rotational effects leads to biases in the inferred\nEoS, while including the neutron star spin produces tighter constraints on\npressure-density and mass-radius relations. Additionally, we explore the r-mode\ninstability window for PSR J0952-0607 under the assumption of both rigid and\nelastic crust models and find that a rigid crust allows a higher stable\ntemperature range, whereas an elastic crust places the star within the\ninstability window under certain thermal insulation conditions."
                },
                "authors": [
                    {
                        "name": "Zeyue Wu"
                    },
                    {
                        "name": "Bhaskar Biswas"
                    },
                    {
                        "name": "Stephan Rosswog"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Rosswog"
                },
                "author": "Stephan Rosswog",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15151v3",
                "updated": "2025-02-13T11:37:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    37,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-19T18:28:41Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "title": "Language Models as Continuous Self-Evolving Data Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Continuous Self-Evolving Data Engineers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE."
                },
                "authors": [
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Kaisong Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaisong Song"
                },
                "author": "Kaisong Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09192v1",
                "updated": "2025-02-13T11:32:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:32:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking beyond the anthropomorphic paradigm benefits LLM research"
                },
                "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08441v2",
                "updated": "2025-02-13T11:30:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    30,
                    41,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-11T12:30:19Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    30,
                    19,
                    3,
                    193,
                    0
                ],
                "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Giada Cosenza"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "arxiv_doi": "10.1007/978-3-031-78977-9_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78977-9_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09188v1",
                "updated": "2025-02-13T11:22:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    22,
                    19,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:22:19Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    22,
                    19,
                    3,
                    44,
                    0
                ],
                "title": "Matina: A Large-Scale 73B Token Persian Text Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matina: A Large-Scale 73B Token Persian Text Corpus"
                },
                "summary": "Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements."
                },
                "authors": [
                    {
                        "name": "Sara Bourbour Hosseinbeigi"
                    },
                    {
                        "name": "Fatemeh Taherinezhad"
                    },
                    {
                        "name": "Heshaam Faili"
                    },
                    {
                        "name": "Hamed Baghbani"
                    },
                    {
                        "name": "Fatemeh Nadi"
                    },
                    {
                        "name": "Mostafa Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mostafa Amiri"
                },
                "author": "Mostafa Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.09621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09621v1",
                "updated": "2025-02-13T18:59:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    46,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:46Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    46,
                    3,
                    44,
                    0
                ],
                "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency"
                },
                "summary": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/"
                },
                "authors": [
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Yu Qi"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Liuhui Wang"
                    },
                    {
                        "name": "Jianhan Jin"
                    },
                    {
                        "name": "Claire Guo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mmecot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09620v1",
                "updated": "2025-02-13T18:59:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:59:45Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    59,
                    45,
                    3,
                    44,
                    0
                ],
                "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Encoder-free Architectures in 3D LMMs"
                },
                "summary": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL"
                },
                "authors": [
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Zoey Guo"
                    },
                    {
                        "name": "Zhuhao Wang"
                    },
                    {
                        "name": "Ray Zhang"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Junli Liu"
                    },
                    {
                        "name": "Delin Qu"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "arxiv_comment": "The code is released at https://github.com/Ivan-Tang-3D/ENEL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13904v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13904v3",
                "updated": "2025-02-13T18:58:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    58,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-23T18:34:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    34,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Stacy Patterson"
                    },
                    {
                        "name": "Ana Milanova"
                    }
                ],
                "author_detail": {
                    "name": "Ana Milanova"
                },
                "author": "Ana Milanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13904v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13904v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09606v1",
                "updated": "2025-02-13T18:55:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    56,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:55:56Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    56,
                    3,
                    44,
                    0
                ],
                "title": "Human-LLM Coevolution: Evidence from Academic Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM Coevolution: Evidence from Academic Writing"
                },
                "summary": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency."
                },
                "authors": [
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09604v1",
                "updated": "2025-02-13T18:55:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    13,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:55:13Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    55,
                    13,
                    3,
                    44,
                    0
                ],
                "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models"
                },
                "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Benjamin Cohen-Wang"
                    },
                    {
                        "name": "Shannon Zejiang Shen"
                    },
                    {
                        "name": "Zhaofeng Wu"
                    },
                    {
                        "name": "Hu Xu"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Shang-Wen Li"
                    },
                    {
                        "name": "Wen-tau Yih"
                    }
                ],
                "author_detail": {
                    "name": "Wen-tau Yih"
                },
                "author": "Wen-tau Yih",
                "arxiv_comment": "Implementation available at https://github.com/voidism/SelfCite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09597v1",
                "updated": "2025-02-13T18:52:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    3,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:52:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    52,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Devamanyu Hazarika"
                    },
                    {
                        "name": "Kaixiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Kaixiang Lin"
                },
                "author": "Kaixiang Lin",
                "arxiv_comment": "Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09596v1",
                "updated": "2025-02-13T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    51,
                    12,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    51,
                    12,
                    3,
                    44,
                    0
                ],
                "title": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System"
                },
                "summary": "Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive conversations supported by large language models (LLMs)\nhave become one of the most popular and helpful applications that can assist\npeople in different aspects. Many current knowledge-intensive applications are\ncentered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based\napplications, they often fall short in handling practical scenarios complicated\nby heterogeneous data in topics and formats, conversational context management,\nand the requirement of low-latency response times. This technical report\npresents a configurable knowledge integrated multi-agent system, KIMAs, to\naddress these challenges. KIMAs features a flexible and configurable system for\nintegrating diverse knowledge sources with 1) context management and query\nrewrite mechanisms to improve retrieval accuracy and multi-turn conversational\ncoherency, 2) efficient knowledge routing and retrieval, 3) simple but\neffective filter and reference generation mechanisms, and 4) optimized\nparallelizable multi-agent pipeline execution. Our work provides a scalable\nframework for advancing the deployment of LLMs in real-world settings. To show\nhow KIMAs can help developers build knowledge-intensive applications with\ndifferent scales and emphases, we demonstrate how we configure the system to\nthree applications already running in practice with reliable performance."
                },
                "authors": [
                    {
                        "name": "Zitao Li"
                    },
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Weirui Kuang"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Bingchen Qian"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09589v1",
                "updated": "2025-02-13T18:46:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    46,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:46:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    46,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "Logical forms complement probability in understanding language model\n  (and human) performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical forms complement probability in understanding language model\n  (and human) performance"
                },
                "summary": "With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Freda Shi"
                    }
                ],
                "author_detail": {
                    "name": "Freda Shi"
                },
                "author": "Freda Shi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09577v1",
                "updated": "2025-02-13T18:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks"
                },
                "summary": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting."
                },
                "authors": [
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Jiannan Li"
                    },
                    {
                        "name": "Huanchen Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "arxiv_comment": "Accepted to CSCW 2025 with minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09566v1",
                "updated": "2025-02-13T18:21:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    21,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:21:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    21,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "Zero-shot generation of synthetic neurosurgical data with large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot generation of synthetic neurosurgical data with large language\n  models"
                },
                "summary": "Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance."
                },
                "authors": [
                    {
                        "name": "Austin A. Barr"
                    },
                    {
                        "name": "Eddie Guo"
                    },
                    {
                        "name": "Emre Sezgin"
                    }
                ],
                "author_detail": {
                    "name": "Emre Sezgin"
                },
                "author": "Emre Sezgin",
                "arxiv_comment": "13 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09565v1",
                "updated": "2025-02-13T18:19:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    19,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T18:19:20Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    19,
                    20,
                    3,
                    44,
                    0
                ],
                "title": "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models"
                },
                "summary": "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models."
                },
                "authors": [
                    {
                        "name": "Quintina Campbell"
                    },
                    {
                        "name": "Sam Cox"
                    },
                    {
                        "name": "Jorge Medina"
                    },
                    {
                        "name": "Brittany Watterson"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04001v2",
                "updated": "2025-02-13T18:14:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    14,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-07T18:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos"
                },
                "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project page: https://lxtgh.github.io/project/sa2va",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05925v2",
                "updated": "2025-02-13T18:02:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    2,
                    34,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-09T21:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    21,
                    58,
                    32,
                    6,
                    161,
                    0
                ],
                "title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue"
                },
                "summary": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04103v2",
                "updated": "2025-02-13T17:57:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-06T14:27:54Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases."
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Aprille Xi"
                    },
                    {
                        "name": "Canwen Wang"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Kenneth R Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R Koedinger"
                },
                "author": "Kenneth R Koedinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06773v2",
                "updated": "2025-02-13T17:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    50,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-10T20:19:55Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    20,
                    19,
                    55,
                    0,
                    162,
                    0
                ],
                "title": "Evaluating Zero-Shot Long-Context LLM Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Zero-Shot Long-Context LLM Compression"
                },
                "summary": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Kai Li"
                    }
                ],
                "author_detail": {
                    "name": "Kai Li"
                },
                "author": "Kai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09532v1",
                "updated": "2025-02-13T17:49:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:49:30Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages"
                },
                "summary": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks."
                },
                "authors": [
                    {
                        "name": "Shreyan Biswas"
                    },
                    {
                        "name": "Alexander Erlei"
                    },
                    {
                        "name": "Ujwal Gadiraju"
                    }
                ],
                "author_detail": {
                    "name": "Ujwal Gadiraju"
                },
                "author": "Ujwal Gadiraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05331v2",
                "updated": "2025-02-13T17:27:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    27,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-07T21:13:27Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    21,
                    13,
                    27,
                    4,
                    38,
                    0
                ],
                "title": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books"
                },
                "summary": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch."
                },
                "authors": [
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Robert Morabito"
                    },
                    {
                        "name": "Skye Reid"
                    },
                    {
                        "name": "Nikta Gohari Sadr"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "9 pages (excluding references), accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09497v1",
                "updated": "2025-02-13T17:09:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    9,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:09:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    9,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Improve LLM-based Automatic Essay Scoring with Linguistic Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improve LLM-based Automatic Essay Scoring with Linguistic Features"
                },
                "summary": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Joey Hou"
                    },
                    {
                        "name": "Alejandro Ciuba"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Lorraine Li"
                },
                "author": "Xiang Lorraine Li",
                "arxiv_comment": "To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v9",
                "updated": "2025-02-13T17:06:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    6,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09487v1",
                "updated": "2025-02-13T16:52:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T16:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "title": "Objective quantification of mood states using large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective quantification of mood states using large language models"
                },
                "summary": "Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings."
                },
                "authors": [
                    {
                        "name": "Jakub Onysk"
                    },
                    {
                        "name": "Quentin Huys"
                    }
                ],
                "author_detail": {
                    "name": "Quentin Huys"
                },
                "author": "Quentin Huys",
                "arxiv_comment": "main text - 9 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09467v1",
                "updated": "2025-02-13T16:31:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    31,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T16:31:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    31,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "Just Trial Once: Ongoing Causal Validation of Machine Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Trial Once: Ongoing Causal Validation of Machine Learning Models"
                },
                "summary": "Machine learning (ML) models are increasingly used as decision-support tools\nin high-risk domains. Evaluating the causal impact of deploying such models can\nbe done with a randomized controlled trial (RCT) that randomizes users to ML\nvs. control groups and assesses the effect on relevant outcomes. However, ML\nmodels are inevitably updated over time, and we often lack evidence for the\ncausal impact of these updates. While the causal effect could be repeatedly\nvalidated with ongoing RCTs, such experiments are expensive and time-consuming\nto run. In this work, we present an alternative solution: using only data from\na prior RCT, we give conditions under which the causal impact of a new ML model\ncan be precisely bounded or estimated, even if it was not included in the RCT.\nOur assumptions incorporate two realistic constraints: ML predictions are often\ndeterministic, and their impacts depend on user trust in the model. Based on\nour analysis, we give recommendations for trial designs that maximize our\nability to assess future versions of an ML model. Our hope is that our trial\ndesign recommendations will save practitioners time and resources while\nallowing for quicker deployments of updates to ML models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are increasingly used as decision-support tools\nin high-risk domains. Evaluating the causal impact of deploying such models can\nbe done with a randomized controlled trial (RCT) that randomizes users to ML\nvs. control groups and assesses the effect on relevant outcomes. However, ML\nmodels are inevitably updated over time, and we often lack evidence for the\ncausal impact of these updates. While the causal effect could be repeatedly\nvalidated with ongoing RCTs, such experiments are expensive and time-consuming\nto run. In this work, we present an alternative solution: using only data from\na prior RCT, we give conditions under which the causal impact of a new ML model\ncan be precisely bounded or estimated, even if it was not included in the RCT.\nOur assumptions incorporate two realistic constraints: ML predictions are often\ndeterministic, and their impacts depend on user trust in the model. Based on\nour analysis, we give recommendations for trial designs that maximize our\nability to assess future versions of an ML model. Our hope is that our trial\ndesign recommendations will save practitioners time and resources while\nallowing for quicker deployments of updates to ML models."
                },
                "authors": [
                    {
                        "name": "Jacob M. Chen"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.00958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.00958v2",
                "updated": "2025-02-13T16:22:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    22,
                    0,
                    3,
                    44,
                    0
                ],
                "published": "2023-02-02T08:53:32Z",
                "published_parsed": [
                    2023,
                    2,
                    2,
                    8,
                    53,
                    32,
                    3,
                    33,
                    0
                ],
                "title": "A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs"
                },
                "summary": "The extensive deployment of probabilistic algorithms has radically changed\nour perspective on several well-established computational notions. Correctness\nis probably the most basic one. While a typical probabilistic program cannot be\nsaid to compute the correct result, we often have quite strong expectations\nabout the frequency with which it should return certain outputs. In these\ncases, trust as a generalisation of correctness fares better. One way to\nunderstand it is to say that a probabilistic computational process is\ntrustworthy if the frequency of its outputs is compliant with a probability\ndistribution which models its expected behaviour. We present a formal\ncomputational framework that formalises this idea. In order to do so, we define\na typed lambda-calculus that features operators for conducting experiments at\nruntime on probabilistic programs and for evaluating whether they compute\noutputs as determined by a target probability distribution. After proving some\nfundamental computational properties of the calculus, such as progress and\ntermination, we define a static notion of confidence that allows to prove that\nour notion of trust behaves correctly with respect to the basic tenets of\nprobability theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extensive deployment of probabilistic algorithms has radically changed\nour perspective on several well-established computational notions. Correctness\nis probably the most basic one. While a typical probabilistic program cannot be\nsaid to compute the correct result, we often have quite strong expectations\nabout the frequency with which it should return certain outputs. In these\ncases, trust as a generalisation of correctness fares better. One way to\nunderstand it is to say that a probabilistic computational process is\ntrustworthy if the frequency of its outputs is compliant with a probability\ndistribution which models its expected behaviour. We present a formal\ncomputational framework that formalises this idea. In order to do so, we define\na typed lambda-calculus that features operators for conducting experiments at\nruntime on probabilistic programs and for evaluating whether they compute\noutputs as determined by a target probability distribution. After proving some\nfundamental computational properties of the calculus, such as progress and\ntermination, we define a static notion of confidence that allows to prove that\nour notion of trust behaves correctly with respect to the basic tenets of\nprobability theory."
                },
                "authors": [
                    {
                        "name": "Francesco A. Genco"
                    },
                    {
                        "name": "Giuseppe Primiero"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Primiero"
                },
                "author": "Giuseppe Primiero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.00958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.00958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09447v1",
                "updated": "2025-02-13T16:16:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    16,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T16:16:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    16,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations"
                },
                "summary": "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST."
                },
                "authors": [
                    {
                        "name": "Dexian Cai"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09419v1",
                "updated": "2025-02-13T15:42:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    42,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:42:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    42,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "On multi-token prediction for efficient LLM inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On multi-token prediction for efficient LLM inference"
                },
                "summary": "We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction."
                },
                "authors": [
                    {
                        "name": "Somesh Mehra"
                    },
                    {
                        "name": "Javier Alonso Garcia"
                    },
                    {
                        "name": "Lukas Mauch"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Mauch"
                },
                "author": "Lukas Mauch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09417v1",
                "updated": "2025-02-13T15:40:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    40,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:40:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    40,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Optimization in Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Optimization in Automation"
                },
                "summary": "Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain."
                },
                "authors": [
                    {
                        "name": "Ahmad Farooq"
                    },
                    {
                        "name": "Kamran Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Kamran Iqbal"
                },
                "author": "Kamran Iqbal",
                "arxiv_doi": "10.1109/CASE59546.2024.10711718",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CASE59546.2024.10711718",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024",
                "arxiv_journal_ref": "A. Farooq and K. Iqbal, \"A Survey of Reinforcement Learning for\n  Optimization in Automation,\" 2024 IEEE 20th International Conference on\n  Automation Science and Engineering (CASE), Bari, Italy, 2024, pp. 2487-2494",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 90C40, 49M37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.2.9; G.1.6; C.4; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v2",
                "updated": "2025-02-13T15:36:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    36,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "17 pages, 8 figures; figures corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08244v2",
                "updated": "2025-02-13T15:33:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    33,
                    56,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-11T09:53:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    53,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Call to Protect the Dark and Quiet Sky from Harmful Interference by\n  Satellite Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Call to Protect the Dark and Quiet Sky from Harmful Interference by\n  Satellite Constellations"
                },
                "summary": "The growing number of satellite constellations in low Earth orbit (LEO)\nenhances global communications and Earth observation, and support of space\ncommerce is a high priority of many governments. At the same time, the\nproliferation of satellites in LEO has negative effects on astronomical\nobservations and research, and the preservation of the dark and quiet sky.\nThese satellite constellations reflect sunlight onto optical telescopes, and\ntheir radio emission impacts radio observatories, jeopardising our access to\nessential scientific discoveries through astronomy. The changing visual\nappearance of the sky also impacts our cultural heritage and environment. Both\nground-based observatories and space-based telescopes in LEO are affected, and\nthere are no places on Earth that can escape the effects of satellite\nconstellations given their global nature. The minimally disturbed dark and\nradio-quiet sky is crucial for conducting fundamental research in astronomy and\nimportant public services such as planetary defence, technology development,\nand high-precision geolocation. Some aspects of satellite deployment and\noperation are regulated by States and intergovernmental organisations. While\nregulatory agencies in some States have started to require operators to\ncoordinate with their national astronomy agencies over impacts, mitigation of\nthe impact of space objects on astronomical activities is not sufficiently\nregulated. To address this issue, the CPS urges States and the international\ncommunity to take steps to protect the dark and quiet sky as specified in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing number of satellite constellations in low Earth orbit (LEO)\nenhances global communications and Earth observation, and support of space\ncommerce is a high priority of many governments. At the same time, the\nproliferation of satellites in LEO has negative effects on astronomical\nobservations and research, and the preservation of the dark and quiet sky.\nThese satellite constellations reflect sunlight onto optical telescopes, and\ntheir radio emission impacts radio observatories, jeopardising our access to\nessential scientific discoveries through astronomy. The changing visual\nappearance of the sky also impacts our cultural heritage and environment. Both\nground-based observatories and space-based telescopes in LEO are affected, and\nthere are no places on Earth that can escape the effects of satellite\nconstellations given their global nature. The minimally disturbed dark and\nradio-quiet sky is crucial for conducting fundamental research in astronomy and\nimportant public services such as planetary defence, technology development,\nand high-precision geolocation. Some aspects of satellite deployment and\noperation are regulated by States and intergovernmental organisations. While\nregulatory agencies in some States have started to require operators to\ncoordinate with their national astronomy agencies over impacts, mitigation of\nthe impact of space objects on astronomical activities is not sufficiently\nregulated. To address this issue, the CPS urges States and the international\ncommunity to take steps to protect the dark and quiet sky as specified in this\npaper."
                },
                "authors": [
                    {
                        "name": "IAU Centre for the Protection of the Dark"
                    },
                    {
                        "name": "Quiet Sky from Satellite Constellation Interference"
                    },
                    {
                        "name": "Gyula I. G. Jzsa"
                    },
                    {
                        "name": "Andrew Williams"
                    },
                    {
                        "name": "Richard Green"
                    },
                    {
                        "name": "Isabel Marsh"
                    },
                    {
                        "name": "John Antoniadis"
                    },
                    {
                        "name": "Domingos Barbosa"
                    },
                    {
                        "name": "John Barentine"
                    },
                    {
                        "name": "Guillermo Blanc"
                    },
                    {
                        "name": "Aaron Boley"
                    },
                    {
                        "name": "Bruno Coelho"
                    },
                    {
                        "name": "Patricia Cooper"
                    },
                    {
                        "name": "Sara Dalledonne"
                    },
                    {
                        "name": "Federico Di Vruno"
                    },
                    {
                        "name": "Joe Diamond"
                    },
                    {
                        "name": "Adam Dong"
                    },
                    {
                        "name": "Ronald Drimmel"
                    },
                    {
                        "name": "Siegfried Eggl"
                    },
                    {
                        "name": "Nusrin Habeeb"
                    },
                    {
                        "name": "Jessica Heim"
                    },
                    {
                        "name": "Chris Hofer"
                    },
                    {
                        "name": "Narae Hwang"
                    },
                    {
                        "name": "Mathieu Isidro"
                    },
                    {
                        "name": "David Koplow"
                    },
                    {
                        "name": "James Lowenthal"
                    },
                    {
                        "name": "Sara Lucatello"
                    },
                    {
                        "name": "Mariya Lyubenova"
                    },
                    {
                        "name": "Robert Massey"
                    },
                    {
                        "name": "Mike Peel"
                    },
                    {
                        "name": "Meredith Rawls"
                    },
                    {
                        "name": "Adrien Saada"
                    },
                    {
                        "name": "Alejandro Sanchez"
                    },
                    {
                        "name": "Pedro Sanhueza"
                    },
                    {
                        "name": "Warren Skidmore"
                    },
                    {
                        "name": "Boris Sorokin"
                    },
                    {
                        "name": "P. Sreekumar"
                    },
                    {
                        "name": "Tim Stevenson"
                    },
                    {
                        "name": "Paula Tartari"
                    },
                    {
                        "name": "Vincenza Tornatore"
                    },
                    {
                        "name": "Connie Walker"
                    },
                    {
                        "name": "Benjamin Winkel"
                    },
                    {
                        "name": "Yana Yakushina"
                    }
                ],
                "author_detail": {
                    "name": "Yana Yakushina"
                },
                "arxiv_affiliation": "CPS",
                "author": "Yana Yakushina",
                "arxiv_comment": "This position paper was developed by the IAU Centre for the\n  Protection of the Dark and Quiet Sky from Satellite Constellation\n  Interference (CPS). It can also be downloaded at the CPS website at\n  https://cps.iau.org/news/cps-urges-action-in-first-recommendations-paper/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01117v2",
                "updated": "2025-02-13T15:25:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    25,
                    34,
                    3,
                    44,
                    0
                ],
                "published": "2024-09-02T09:49:16Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    49,
                    16,
                    0,
                    246,
                    0
                ],
                "title": "Scenario-based assessment of automated driving systems: How (not) to\n  parameterize scenarios?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario-based assessment of automated driving systems: How (not) to\n  parameterize scenarios?"
                },
                "summary": "The development of Automated Driving Systems (ADSs) has advanced\nsignificantly. To enable their large-scale deployment, the United Nations\nRegulation 157 (UN R157) concerning the approval of Automated Lane Keeping\nSystems (ALKSs) has been approved in 2021. UN R157 requires an activated ALKS\nto avoid any collisions that are reasonably preventable and proposes a method\nto distinguish reasonably preventable collisions from unpreventable ones using\n\"the simulated performance of a skilled and attentive human driver\". With\ndifferent driver models, benchmarks are set for ALKSs in three types of\nscenarios. The three types of scenarios considered in the proposed method in UN\nR157 assume a certain parameterization without any further consideration.\n  This work investigates the parameterization of these scenarios, showing that\nthe choice of parameterization significantly affects the simulation outcomes.\nBy comparing real-world and parameterized scenarios, we show that the influence\nof parameterization depends on the scenario type, driver model, and evaluation\ncriterion. Alternative parameterizations are proposed, leading to results that\nare closer to the non-parameterized scenarios in terms of recall, precision,\nand F1 score. The study highlights the importance of careful scenario\nparameterization and suggests improvements to the current UN R157 approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Automated Driving Systems (ADSs) has advanced\nsignificantly. To enable their large-scale deployment, the United Nations\nRegulation 157 (UN R157) concerning the approval of Automated Lane Keeping\nSystems (ALKSs) has been approved in 2021. UN R157 requires an activated ALKS\nto avoid any collisions that are reasonably preventable and proposes a method\nto distinguish reasonably preventable collisions from unpreventable ones using\n\"the simulated performance of a skilled and attentive human driver\". With\ndifferent driver models, benchmarks are set for ALKSs in three types of\nscenarios. The three types of scenarios considered in the proposed method in UN\nR157 assume a certain parameterization without any further consideration.\n  This work investigates the parameterization of these scenarios, showing that\nthe choice of parameterization significantly affects the simulation outcomes.\nBy comparing real-world and parameterized scenarios, we show that the influence\nof parameterization depends on the scenario type, driver model, and evaluation\ncriterion. Alternative parameterizations are proposed, leading to results that\nare closer to the non-parameterized scenarios in terms of recall, precision,\nand F1 score. The study highlights the importance of careful scenario\nparameterization and suggests improvements to the current UN R157 approach."
                },
                "authors": [
                    {
                        "name": "Erwin de Gelder"
                    },
                    {
                        "name": "Olaf Op den Camp"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Op den Camp"
                },
                "author": "Olaf Op den Camp",
                "arxiv_doi": "10.1109/IAVVC63304.2024.10786407",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IAVVC63304.2024.10786407",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.01117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for the 2024 IEEE International Automated Vehicle Validation\n  (IAVVC2024) Conference",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19347v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19347v4",
                "updated": "2025-02-13T15:25:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    25,
                    2,
                    3,
                    44,
                    0
                ],
                "published": "2023-10-30T08:40:16Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    8,
                    40,
                    16,
                    0,
                    303,
                    0
                ],
                "title": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization"
                },
                "summary": "Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Yan Fan"
                    },
                    {
                        "name": "Xiong Liu"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Zekun Yao"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19347v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19347v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02280v2",
                "updated": "2025-02-13T15:21:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    21,
                    43,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-04T17:09:10Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain."
                },
                "authors": [
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Schrimpf"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrimpf"
                },
                "author": "Martin Schrimpf",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17395v2",
                "updated": "2025-02-13T15:11:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    11,
                    24,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-23T08:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    8,
                    47,
                    42,
                    0,
                    358,
                    0
                ],
                "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models"
                },
                "summary": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09393v1",
                "updated": "2025-02-13T15:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    10,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:10:45Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    10,
                    45,
                    3,
                    44,
                    0
                ],
                "title": "Generalizable Reinforcement Learning with Biologically Inspired\n  Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable Reinforcement Learning with Biologically Inspired\n  Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path\n  Planning"
                },
                "summary": "Real-time autonomous systems utilize multi-layer computational frameworks to\nperform critical tasks such as perception, goal finding, and path planning.\nTraditional methods implement perception using occupancy grid mapping (OGM),\nsegmenting the environment into discretized cells with probabilistic\ninformation. This classical approach is well-established and provides a\nstructured input for downstream processes like goal finding and path planning\nalgorithms. Recent approaches leverage a biologically inspired mathematical\nframework known as vector symbolic architectures (VSA), commonly known as\nhyperdimensional computing, to perform probabilistic OGM in hyperdimensional\nspace. This approach, VSA-OGM, provides native compatibility with spiking\nneural networks, positioning VSA-OGM as a potential neuromorphic alternative to\nconventional OGM. However, for large-scale integration, it is essential to\nassess the performance implications of VSA-OGM on downstream tasks compared to\nestablished OGM methods. This study examines the efficacy of VSA-OGM against a\ntraditional OGM approach, Bayesian Hilbert Maps (BHM), within reinforcement\nlearning based goal finding and path planning frameworks, across a controlled\nexploration environment and an autonomous driving scenario inspired by the\nF1-Tenth challenge. Our results demonstrate that VSA-OGM maintains comparable\nlearning performance across single and multi-scenario training configurations\nwhile improving performance on unseen environments by approximately 47%. These\nfindings highlight the increased generalizability of policy networks trained\nwith VSA-OGM over BHM, reinforcing its potential for real-world deployment in\ndiverse environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time autonomous systems utilize multi-layer computational frameworks to\nperform critical tasks such as perception, goal finding, and path planning.\nTraditional methods implement perception using occupancy grid mapping (OGM),\nsegmenting the environment into discretized cells with probabilistic\ninformation. This classical approach is well-established and provides a\nstructured input for downstream processes like goal finding and path planning\nalgorithms. Recent approaches leverage a biologically inspired mathematical\nframework known as vector symbolic architectures (VSA), commonly known as\nhyperdimensional computing, to perform probabilistic OGM in hyperdimensional\nspace. This approach, VSA-OGM, provides native compatibility with spiking\nneural networks, positioning VSA-OGM as a potential neuromorphic alternative to\nconventional OGM. However, for large-scale integration, it is essential to\nassess the performance implications of VSA-OGM on downstream tasks compared to\nestablished OGM methods. This study examines the efficacy of VSA-OGM against a\ntraditional OGM approach, Bayesian Hilbert Maps (BHM), within reinforcement\nlearning based goal finding and path planning frameworks, across a controlled\nexploration environment and an autonomous driving scenario inspired by the\nF1-Tenth challenge. Our results demonstrate that VSA-OGM maintains comparable\nlearning performance across single and multi-scenario training configurations\nwhile improving performance on unseen environments by approximately 47%. These\nfindings highlight the increased generalizability of policy networks trained\nwith VSA-OGM over BHM, reinforcing its potential for real-world deployment in\ndiverse environments."
                },
                "authors": [
                    {
                        "name": "Shay Snyder"
                    },
                    {
                        "name": "Ryan Shea"
                    },
                    {
                        "name": "Andrew Capodieci"
                    },
                    {
                        "name": "David Gorsich"
                    },
                    {
                        "name": "Maryam Parsa"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Parsa"
                },
                "arxiv_affiliation": "George Mason University",
                "author": "Maryam Parsa",
                "arxiv_comment": "9 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09390v1",
                "updated": "2025-02-13T15:07:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    7,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:07:20Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    7,
                    20,
                    3,
                    44,
                    0
                ],
                "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models"
                },
                "summary": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square."
                },
                "authors": [
                    {
                        "name": "Daniel Fleischer"
                    },
                    {
                        "name": "Moshe Berchansky"
                    },
                    {
                        "name": "Gad Markovits"
                    },
                    {
                        "name": "Moshe Wasserblat"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Wasserblat"
                },
                "author": "Moshe Wasserblat",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09387v1",
                "updated": "2025-02-13T15:04:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    4,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:04:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    4,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "Truth Knows No Language: Evaluating Truthfulness Beyond English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth Knows No Language: Evaluating Truthfulness Beyond English"
                },
                "summary": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses."
                },
                "authors": [
                    {
                        "name": "Blanca Calvo Figueras"
                    },
                    {
                        "name": "Eneko Sagarzazu"
                    },
                    {
                        "name": "Julen Etxaniz"
                    },
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Pablo Gamallo"
                    },
                    {
                        "name": "Iria De Dios Flores"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "13 pages, 5 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09385v1",
                "updated": "2025-02-13T15:01:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    1,
                    18,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T15:01:18Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    1,
                    18,
                    3,
                    44,
                    0
                ],
                "title": "APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent\n  Threats Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent\n  Threats Using Large Language Models"
                },
                "summary": "Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due\nto their stealth and ability to mimic normal system behavior, making detection\nparticularly difficult in highly imbalanced datasets. Traditional anomaly\ndetection methods struggle to effectively differentiate APT-related activities\nfrom benign processes, limiting their applicability in real-world scenarios.\nThis paper introduces APT-LLM, a novel embedding-based anomaly detection\nframework that integrates large language models (LLMs) -- BERT, ALBERT,\nDistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.\nUnlike prior approaches, which rely on manually engineered features or\nconventional anomaly detection models, APT-LLM leverages LLMs to encode\nprocess-action provenance traces into semantically rich embeddings, capturing\nnuanced behavioral patterns. These embeddings are analyzed using three\nautoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder\n(VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and\nidentify anomalies. The best-performing model is selected for comparison\nagainst traditional methods. The framework is evaluated on real-world, highly\nimbalanced provenance trace datasets from the DARPA Transparent Computing\nprogram, where APT-like attacks constitute as little as 0.004\\% of the data\nacross multiple operating systems (Android, Linux, BSD, and Windows) and attack\nscenarios. Results demonstrate that APT-LLM significantly improves detection\nperformance under extreme imbalance conditions, outperforming existing anomaly\ndetection methods and highlighting the effectiveness of LLM-based feature\nextraction in cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) pose a major cybersecurity challenge due\nto their stealth and ability to mimic normal system behavior, making detection\nparticularly difficult in highly imbalanced datasets. Traditional anomaly\ndetection methods struggle to effectively differentiate APT-related activities\nfrom benign processes, limiting their applicability in real-world scenarios.\nThis paper introduces APT-LLM, a novel embedding-based anomaly detection\nframework that integrates large language models (LLMs) -- BERT, ALBERT,\nDistilBERT, and RoBERTa -- with autoencoder architectures to detect APTs.\nUnlike prior approaches, which rely on manually engineered features or\nconventional anomaly detection models, APT-LLM leverages LLMs to encode\nprocess-action provenance traces into semantically rich embeddings, capturing\nnuanced behavioral patterns. These embeddings are analyzed using three\nautoencoder architectures -- Baseline Autoencoder (AE), Variational Autoencoder\n(VAE), and Denoising Autoencoder (DAE) -- to model normal process behavior and\nidentify anomalies. The best-performing model is selected for comparison\nagainst traditional methods. The framework is evaluated on real-world, highly\nimbalanced provenance trace datasets from the DARPA Transparent Computing\nprogram, where APT-like attacks constitute as little as 0.004\\% of the data\nacross multiple operating systems (Android, Linux, BSD, and Windows) and attack\nscenarios. Results demonstrate that APT-LLM significantly improves detection\nperformance under extreme imbalance conditions, outperforming existing anomaly\ndetection methods and highlighting the effectiveness of LLM-based feature\nextraction in cybersecurity."
                },
                "authors": [
                    {
                        "name": "Sidahmed Benabderrahmane"
                    },
                    {
                        "name": "Petko Valtchev"
                    },
                    {
                        "name": "James Cheney"
                    },
                    {
                        "name": "Talal Rahwan"
                    }
                ],
                "author_detail": {
                    "name": "Talal Rahwan"
                },
                "author": "Talal Rahwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14682v2",
                "updated": "2025-02-13T14:54:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    54,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-02T19:56:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    56,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards\n  Spatial-Temporal Cognition with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards\n  Spatial-Temporal Cognition with Foundation Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred numerous\nattempts to apply these technologies to embodied tasks, particularly focusing\non high-level task planning and task decomposition. To further explore this\narea, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which\nspecifically targets embodied task planning using LLMs. It features a\ncontrollable and diverse set of embodied tasks varying in different levels of\ndifficulties and complexities, and is designed to evaluate two critical\ndimensions of LLMs' application in embodied task understanding: spatial\n(relation constraint, occlusion for target objects) and temporal & causal\nunderstanding of the sequence of actions in the environment. By using\nmulti-source simulators as the backend simulator, it can provide immediate\nenvironment feedback to LLMs, which enables LLMs to interact dynamically with\nthe environment and re-plan as necessary. We evaluated the state-of-the-art\nopen source and closed source foundation models, including GPT-4, LLAMA and\nMistral on our proposed benchmark. While they perform adequately well on simple\nnavigation tasks, their performance can significantly deteriorate when faced\nwith tasks that require a deeper understanding of spatial, temporal, and causal\nrelationships. Thus, our benchmark distinguishes itself as a large-scale,\nquantifiable, highly automated, and fine-grained diagnostic framework that\npresents a significant challenge to the latest foundation models. We hope it\ncan spark and drive further research in embodied task planning using foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred numerous\nattempts to apply these technologies to embodied tasks, particularly focusing\non high-level task planning and task decomposition. To further explore this\narea, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which\nspecifically targets embodied task planning using LLMs. It features a\ncontrollable and diverse set of embodied tasks varying in different levels of\ndifficulties and complexities, and is designed to evaluate two critical\ndimensions of LLMs' application in embodied task understanding: spatial\n(relation constraint, occlusion for target objects) and temporal & causal\nunderstanding of the sequence of actions in the environment. By using\nmulti-source simulators as the backend simulator, it can provide immediate\nenvironment feedback to LLMs, which enables LLMs to interact dynamically with\nthe environment and re-plan as necessary. We evaluated the state-of-the-art\nopen source and closed source foundation models, including GPT-4, LLAMA and\nMistral on our proposed benchmark. While they perform adequately well on simple\nnavigation tasks, their performance can significantly deteriorate when faced\nwith tasks that require a deeper understanding of spatial, temporal, and causal\nrelationships. Thus, our benchmark distinguishes itself as a large-scale,\nquantifiable, highly automated, and fine-grained diagnostic framework that\npresents a significant challenge to the latest foundation models. We hope it\ncan spark and drive further research in embodied task planning using foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Yuening Wang"
                    },
                    {
                        "name": "Hongjian Gu"
                    },
                    {
                        "name": "Atia Hamidizadeh"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Yuecheng Liu"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "David Gamaliel Arcos Bravo"
                    },
                    {
                        "name": "Junyi Dong"
                    },
                    {
                        "name": "Shunbo Zhou"
                    },
                    {
                        "name": "Tongtong Cao"
                    },
                    {
                        "name": "Xingyue Quan"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08514v2",
                "updated": "2025-02-13T14:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    34,
                    29,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-12T15:46:50Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation"
                },
                "summary": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries."
                },
                "authors": [
                    {
                        "name": "Mahnaz Koupaee"
                    },
                    {
                        "name": "Jake W. Vincent"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Igor Shalyminov"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Raphael Shu"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Amy Wing-mei Wong"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10853v3",
                "updated": "2025-02-13T14:13:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    13,
                    41,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-15T16:04:44Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    16,
                    4,
                    44,
                    0,
                    197,
                    0
                ],
                "title": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases"
                },
                "summary": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments."
                },
                "authors": [
                    {
                        "name": "Dylan Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Bouchard"
                },
                "author": "Dylan Bouchard",
                "arxiv_comment": "LangFair repository: https://github.com/cvs-health/langfair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09344v1",
                "updated": "2025-02-13T14:07:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    51,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:07:51Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    51,
                    3,
                    44,
                    0
                ],
                "title": "Revisiting Topological Interference Management: A Learning-to-Code on\n  Graphs Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Topological Interference Management: A Learning-to-Code on\n  Graphs Perspective"
                },
                "summary": "The advance of topological interference management (TIM) has been one of the\ndriving forces of recent developments in network information theory. However,\nstate-of-the-art coding schemes for TIM are usually handcrafted for specific\nfamilies of network topologies, relying critically on experts' domain knowledge\nand sophisticated treatments. The lack of systematic and automatic generation\nof solutions inevitably restricts their potential wider applications to\nwireless communication systems, due to the limited generalizability of coding\nschemes to wider network configurations. To address such an issue, this work\nmakes the first attempt to advocate revisiting topological interference\nalignment (IA) from a novel learning-to-code perspective. Specifically, we\nrecast the one-to-one and subspace IA conditions as vector assignment policies\nand propose a unifying learning-to-code on graphs (LCG) framework by leveraging\ngraph neural networks (GNNs) for capturing topological structures and\nreinforcement learning (RL) for decision-making of IA beamforming vector\nassignment. Interestingly, the proposed LCG framework is capable of recovering\nknown one-to-one scalar/vector IA solutions for a significantly wider range of\nnetwork topologies, and more remarkably of discovering new subspace IA coding\nschemes for multiple-antenna cases that are challenging to be handcrafted. The\nextensive experiments demonstrate that the LCG framework is an effective way to\nautomatically produce systematic coding solutions to the TIM instances with\narbitrary network topologies, and at the same time, the underlying learning\nalgorithm is efficient with respect to online inference time and possesses\nexcellent generalizability and transferability for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advance of topological interference management (TIM) has been one of the\ndriving forces of recent developments in network information theory. However,\nstate-of-the-art coding schemes for TIM are usually handcrafted for specific\nfamilies of network topologies, relying critically on experts' domain knowledge\nand sophisticated treatments. The lack of systematic and automatic generation\nof solutions inevitably restricts their potential wider applications to\nwireless communication systems, due to the limited generalizability of coding\nschemes to wider network configurations. To address such an issue, this work\nmakes the first attempt to advocate revisiting topological interference\nalignment (IA) from a novel learning-to-code perspective. Specifically, we\nrecast the one-to-one and subspace IA conditions as vector assignment policies\nand propose a unifying learning-to-code on graphs (LCG) framework by leveraging\ngraph neural networks (GNNs) for capturing topological structures and\nreinforcement learning (RL) for decision-making of IA beamforming vector\nassignment. Interestingly, the proposed LCG framework is capable of recovering\nknown one-to-one scalar/vector IA solutions for a significantly wider range of\nnetwork topologies, and more remarkably of discovering new subspace IA coding\nschemes for multiple-antenna cases that are challenging to be handcrafted. The\nextensive experiments demonstrate that the LCG framework is an effective way to\nautomatically produce systematic coding solutions to the TIM instances with\narbitrary network topologies, and at the same time, the underlying learning\nalgorithm is efficient with respect to online inference time and possesses\nexcellent generalizability and transferability for practical deployment."
                },
                "authors": [
                    {
                        "name": "Zhiwei Shan"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Chung-Shou Liao"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2305.07186",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15896v2",
                "updated": "2025-02-13T14:07:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    7,
                    25,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-20T13:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    13,
                    50,
                    18,
                    4,
                    355,
                    0
                ],
                "title": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Reliability Criteria for News Publishers with Large\n  Language Models"
                },
                "summary": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the use of a large language model to assist in\nthe evaluation of the reliability of the vast number of existing online news\npublishers, addressing the impracticality of relying solely on human expert\nannotators for this task. In the context of the Italian news media market, we\nfirst task the model with evaluating expert-designed reliability criteria using\na representative sample of news articles. We then compare the model's answers\nwith those of human experts. The dataset consists of 340 news articles, each\nannotated by two human experts and the LLM. Six criteria are taken into\naccount, for a total of 6,120 annotations. We observe good agreement between\nLLM and human annotators in three of the six evaluated criteria, including the\ncritical ability to detect instances where a text negatively targets an entity\nor individual. For two additional criteria, such as the detection of\nsensational language and the recognition of bias in news content, LLMs generate\nfair annotations, albeit with certain trade-offs. Furthermore, we show that the\nLLM is able to help resolve disagreements among human experts, especially in\ntasks such as identifying cases of negative targeting."
                },
                "authors": [
                    {
                        "name": "Manuel Pratelli"
                    },
                    {
                        "name": "John Bianchi"
                    },
                    {
                        "name": "Fabio Pinelli"
                    },
                    {
                        "name": "Marinella Petrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Marinella Petrocchi"
                },
                "author": "Marinella Petrocchi",
                "arxiv_doi": "10.1145/3717867.3717924",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3717867.3717924",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04708v2",
                "updated": "2025-02-13T14:06:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    6,
                    51,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-07T13:45:26Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    45,
                    26,
                    3,
                    312,
                    0
                ],
                "title": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs"
                },
                "summary": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs."
                },
                "authors": [
                    {
                        "name": "Chengxin Hu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yihe Yuan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ivor Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor Tsang"
                },
                "author": "Ivor Tsang",
                "arxiv_comment": "9 pages, 4 tables, 1 figure, paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09334v1",
                "updated": "2025-02-13T13:53:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    53,
                    32,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:53:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    53,
                    32,
                    3,
                    44,
                    0
                ],
                "title": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud\n  Environments"
                },
                "summary": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have demonstrated their\nremarkable proficiency in a range of tasks. Compared to in-house homogeneous\nGPU clusters, deploying LLMs in cloud environments with diverse types of GPUs\nis crucial for addressing the GPU shortage problem and being more\ncost-effective. However, the diversity of network environments and various GPU\ntypes on the cloud bring difficulties to achieving high-performance serving. In\nthis work, we propose ThunderServe, a high-performance and cost-efficient LLM\nserving system for heterogeneous cloud environments. We introduce a novel\nscheduling algorithm, which optimizes the deployment plan of LLM serving to\naccommodate the heterogeneous resource and network bandwidth conditions in\ncloud environments. Furthermore, we propose a lightweight re-scheduling\nmechanism, designed to adapt to fluctuating online conditions (e.g., node\nfailures, workload shifts) without the need for costly restarts of ongoing\nservices. Empirical results in both heterogeneous cloud and homogeneous\nin-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and\non average a $1.7\\times$ increase in throughput and achieves up to a\n2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines\ncompared with state-of-the-art systems given the same price budget, suggesting\nopting for cloud services provides a more cost-efficient solution."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaozhe Yao"
                    },
                    {
                        "name": "Taiyi Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Ana Klimovic"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09331v1",
                "updated": "2025-02-13T13:49:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:49:30Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    49,
                    30,
                    3,
                    44,
                    0
                ],
                "title": "Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs"
                },
                "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings."
                },
                "authors": [
                    {
                        "name": "Itai Mondshine"
                    },
                    {
                        "name": "Tzuf Paz-Argaman"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    }
                ],
                "author_detail": {
                    "name": "Reut Tsarfaty"
                },
                "author": "Reut Tsarfaty",
                "arxiv_comment": "Accepted for NAACL findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09328v1",
                "updated": "2025-02-13T13:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    40,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:40:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    40,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild"
                },
                "summary": "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants."
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Chris Donahue"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09316v1",
                "updated": "2025-02-13T13:30:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    30,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    30,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis"
                },
                "summary": "Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities."
                },
                "authors": [
                    {
                        "name": "Kentaro Imajo"
                    },
                    {
                        "name": "Masanori Hirano"
                    },
                    {
                        "name": "Shuji Suzuki"
                    },
                    {
                        "name": "Hiroaki Mikami"
                    }
                ],
                "author_detail": {
                    "name": "Hiroaki Mikami"
                },
                "author": "Hiroaki Mikami",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05497v2",
                "updated": "2025-02-13T13:22:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    22,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-08T09:04:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    9,
                    4,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "DeepThink: Aligning Language Models with Domain-Specific User Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepThink: Aligning Language Models with Domain-Specific User Intents"
                },
                "summary": "Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Mingxuan Luo"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kaili Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaili Huang"
                },
                "author": "Kaili Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09307v1",
                "updated": "2025-02-13T13:19:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    19,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models"
                },
                "summary": "Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions."
                },
                "authors": [
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Aya Meltzer-Asscher"
                    },
                    {
                        "name": "Jonathan Berant"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Berant"
                },
                "author": "Jonathan Berant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09304v1",
                "updated": "2025-02-13T13:16:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    16,
                    16,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:16:16Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    16,
                    16,
                    3,
                    44,
                    0
                ],
                "title": "KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG"
                },
                "summary": "Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%."
                },
                "authors": [
                    {
                        "name": "Yiqian Huang"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01401v2",
                "updated": "2025-02-13T13:15:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    15,
                    27,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-03T14:32:36Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    32,
                    36,
                    0,
                    34,
                    0
                ],
                "title": "Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection"
                },
                "summary": "3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe."
                },
                "authors": [
                    {
                        "name": "Boyu Mi"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15330v2",
                "updated": "2025-02-13T13:06:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    6,
                    0,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-21T17:42:52Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    42,
                    52,
                    4,
                    173,
                    0
                ],
                "title": "Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection"
                },
                "summary": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT."
                },
                "authors": [
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11905v2",
                "updated": "2025-02-13T13:04:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    4,
                    20,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-16T16:38:47Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    16,
                    38,
                    47,
                    1,
                    198,
                    0
                ],
                "title": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge"
                },
                "summary": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart."
                },
                "authors": [
                    {
                        "name": "Andrej op"
                    },
                    {
                        "name": "Bla Bertalani"
                    },
                    {
                        "name": "Carolina Fortuna"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Fortuna"
                },
                "author": "Carolina Fortuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09285v1",
                "updated": "2025-02-13T13:00:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    0,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T13:00:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    13,
                    0,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "EmoAssist: Emotional Assistant for Visual Impairment Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoAssist: Emotional Assistant for Visual Impairment Community"
                },
                "summary": "The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o."
                },
                "authors": [
                    {
                        "name": "Xingyu Qi"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Zhenyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Wu"
                },
                "author": "Zhenyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09284v1",
                "updated": "2025-02-13T12:57:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:57:15Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    57,
                    15,
                    3,
                    44,
                    0
                ],
                "title": "SparQLe: Speech Queries to Text Translation Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQLe: Speech Queries to Text Translation Through LLMs"
                },
                "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications."
                },
                "authors": [
                    {
                        "name": "Amirbek Djanibekov"
                    },
                    {
                        "name": "Hanan Aldarmaki"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Aldarmaki"
                },
                "author": "Hanan Aldarmaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11283v2",
                "updated": "2025-02-13T12:48:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    48,
                    58,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-20T05:34:38Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    5,
                    34,
                    38,
                    0,
                    20,
                    0
                ],
                "title": "Large Language Model Agents for Radio Map Generation and Wireless\n  Network Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agents for Radio Map Generation and Wireless\n  Network Planning"
                },
                "summary": "Using commercial software for radio map generation and wireless network\nplanning often require complex manual operations, posing significant challenges\nin terms of scalability, adaptability, and user-friendliness, due to heavy\nmanual operations. To address these issues, we propose an automated solution\nthat employs large language model (LLM) agents. These agents are designed to\nautonomously generate radio maps and facilitate wireless network planning for\nspecified areas, thereby minimizing the necessity for extensive manual\nintervention. To validate the effectiveness of our proposed solution, we\ndevelop a software platform that integrates LLM agents. Experimental results\ndemonstrate that a large amount manual operations can be saved via the proposed\nLLM agent, and the automated solutions can achieve an enhanced coverage and\nsignal-to-interference-noise ratio (SINR), especially in urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using commercial software for radio map generation and wireless network\nplanning often require complex manual operations, posing significant challenges\nin terms of scalability, adaptability, and user-friendliness, due to heavy\nmanual operations. To address these issues, we propose an automated solution\nthat employs large language model (LLM) agents. These agents are designed to\nautonomously generate radio maps and facilitate wireless network planning for\nspecified areas, thereby minimizing the necessity for extensive manual\nintervention. To validate the effectiveness of our proposed solution, we\ndevelop a software platform that integrates LLM agents. Experimental results\ndemonstrate that a large amount manual operations can be saved via the proposed\nLLM agent, and the automated solutions can achieve an enhanced coverage and\nsignal-to-interference-noise ratio (SINR), especially in urban environments."
                },
                "authors": [
                    {
                        "name": "Hongye Quan"
                    },
                    {
                        "name": "Wanli Ni"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Xiangyu Ye"
                    },
                    {
                        "name": "Ziyi Xie"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Hui Song"
                    }
                ],
                "author_detail": {
                    "name": "Hui Song"
                },
                "author": "Hui Song",
                "arxiv_comment": "5 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v3",
                "updated": "2025-02-13T12:43:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    43,
                    59,
                    3,
                    44,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Usage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Usage?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09268v1",
                "updated": "2025-02-13T12:29:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    29,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T12:29:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    29,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation"
                },
                "summary": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks."
                },
                "authors": [
                    {
                        "name": "Hongyin Zhang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Ying Peng"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17301v2",
                "updated": "2025-02-13T12:25:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    25,
                    54,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-26T10:48:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation"
                },
                "summary": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00008v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00008v5",
                "updated": "2025-02-13T12:10:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2023-03-27T18:00:01Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    18,
                    0,
                    1,
                    0,
                    86,
                    0
                ],
                "title": "On the Creativity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Creativity of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview."
                },
                "authors": [
                    {
                        "name": "Giorgio Franceschelli"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_doi": "10.1007/s00146-024-02127-3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00146-024-02127-3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.00008v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00008v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3",
                "arxiv_journal_ref": "AI & Soc (2024)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05033v2",
                "updated": "2025-02-13T12:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    9,
                    50,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-06T09:58:58Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    9,
                    58,
                    58,
                    5,
                    188,
                    0
                ],
                "title": "PeaPOD: Personalized Prompt Distillation for Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeaPOD: Personalized Prompt Distillation for Generative Recommendation"
                },
                "summary": "Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks."
                },
                "authors": [
                    {
                        "name": "Jerome Ramos"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Aldo Lipani"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Lipani"
                },
                "author": "Aldo Lipani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09238v1",
                "updated": "2025-02-13T11:55:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    55,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:55:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    55,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart\n  Logistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart\n  Logistics"
                },
                "summary": "The increasing demand for efficient last-mile delivery in smart logistics\nunderscores the role of autonomous robots in enhancing operational efficiency\nand reducing costs. Traditional navigation methods, which depend on\nhigh-precision maps, are resource-intensive, while learning-based approaches\noften struggle with generalization in real-world scenarios. To address these\nchallenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic\nNavigation (OPEN) system that combines foundation models with classic\nalgorithms for scalable outdoor navigation. The system uses off-the-shelf\nOpenStreetMap (OSM) for flexible map representation, thereby eliminating the\nneed for extensive pre-mapping efforts. It also employs Large Language Models\n(LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs)\nfor global localization, map updates, and house number recognition. To\ncompensate the limitations of existing benchmarks that are inadequate for\nassessing last-mile delivery, this work introduces a new benchmark specifically\ndesigned for outdoor navigation in residential areas, reflecting the real-world\nchallenges faced by autonomous delivery systems. Extensive experiments in\nsimulated and real-world environments demonstrate the proposed system's\nefficacy in enhancing navigation efficiency and reliability. To facilitate\nfurther research, our code and benchmark are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for efficient last-mile delivery in smart logistics\nunderscores the role of autonomous robots in enhancing operational efficiency\nand reducing costs. Traditional navigation methods, which depend on\nhigh-precision maps, are resource-intensive, while learning-based approaches\noften struggle with generalization in real-world scenarios. To address these\nchallenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic\nNavigation (OPEN) system that combines foundation models with classic\nalgorithms for scalable outdoor navigation. The system uses off-the-shelf\nOpenStreetMap (OSM) for flexible map representation, thereby eliminating the\nneed for extensive pre-mapping efforts. It also employs Large Language Models\n(LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs)\nfor global localization, map updates, and house number recognition. To\ncompensate the limitations of existing benchmarks that are inadequate for\nassessing last-mile delivery, this work introduces a new benchmark specifically\ndesigned for outdoor navigation in residential areas, reflecting the real-world\nchallenges faced by autonomous delivery systems. Extensive experiments in\nsimulated and real-world environments demonstrate the proposed system's\nefficacy in enhancing navigation efficiency and reliability. To facilitate\nfurther research, our code and benchmark are publicly available."
                },
                "authors": [
                    {
                        "name": "Junhui Wang"
                    },
                    {
                        "name": "Dongjie Huo"
                    },
                    {
                        "name": "Zehui Xu"
                    },
                    {
                        "name": "Yongliang Shi"
                    },
                    {
                        "name": "Yimin Yan"
                    },
                    {
                        "name": "Yuanxin Wang"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Yan Qiao"
                    },
                    {
                        "name": "Guyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guyue Zhou"
                },
                "author": "Guyue Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09237v1",
                "updated": "2025-02-13T11:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    54,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:54:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    54,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Reliable Conversational Agents under ASP Control that Understand Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Conversational Agents under ASP Control that Understand Natural\n  Language"
                },
                "summary": "Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable."
                },
                "authors": [
                    {
                        "name": "Yankai Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yankai Zeng"
                },
                "arxiv_affiliation": "The University of Texas at Dallas",
                "author": "Yankai Zeng",
                "arxiv_doi": "10.4204/EPTCS.416.41",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.41",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 398-406",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09212v1",
                "updated": "2025-02-13T11:48:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    31,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:48:31Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    31,
                    3,
                    44,
                    0
                ],
                "title": "LP-LM: No Hallucinations in Question Answering with Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LP-LM: No Hallucinations in Question Answering with Logic Programming"
                },
                "summary": "Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM."
                },
                "authors": [
                    {
                        "name": "Katherine Wu"
                    },
                    {
                        "name": "Yanhong A. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yanhong A. Liu"
                },
                "author": "Yanhong A. Liu",
                "arxiv_doi": "10.4204/EPTCS.416.5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 69-77",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12433v3",
                "updated": "2025-02-13T11:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    48,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2024-05-21T01:16:34Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    1,
                    16,
                    34,
                    1,
                    142,
                    0
                ],
                "title": "LLM+Reasoning+Planning for Supporting Incomplete User Queries in\n  Presence of APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+Reasoning+Planning for Supporting Incomplete User Queries in\n  Presence of APIs"
                },
                "summary": "Recent availability of Large Language Models (LLMs) has led to the\ndevelopment of numerous LLM-based approaches aimed at providing natural\nlanguage interfaces for various end-user tasks. These end-user tasks in turn\ncan typically be accomplished by orchestrating a given set of APIs. In\npractice, natural language task requests (user queries) are often incomplete,\ni.e., they may not contain all the information required by the APIs. While LLMs\nexcel at natural language processing (NLP) tasks, they frequently hallucinate\non missing information or struggle with orchestrating the APIs. The key idea\nbehind our proposed approach is to leverage logical reasoning and classical AI\nplanning along with an LLM for accurately answering user queries including\nidentification and gathering of any missing information in these queries. Our\napproach uses an LLM and ASP (Answer Set Programming) solver to translate a\nuser query to a representation in Planning Domain Definition Language (PDDL)\nvia an intermediate representation in ASP. We introduce a special API\n\"get_info_api\" for gathering missing information. We model all the APIs as PDDL\nactions in a way that supports dataflow between the APIs. Our approach then\nuses a classical AI planner to generate an orchestration of API calls\n(including calls to get_info_api) to answer the user query. Our evaluation\nresults show that our approach significantly outperforms a pure LLM based\napproach by achieving over 95% success rate in most cases on a dataset\ncontaining complete and incomplete single goal and multi-goal queries where the\nmulti-goal queries may or may not require dataflow among the APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent availability of Large Language Models (LLMs) has led to the\ndevelopment of numerous LLM-based approaches aimed at providing natural\nlanguage interfaces for various end-user tasks. These end-user tasks in turn\ncan typically be accomplished by orchestrating a given set of APIs. In\npractice, natural language task requests (user queries) are often incomplete,\ni.e., they may not contain all the information required by the APIs. While LLMs\nexcel at natural language processing (NLP) tasks, they frequently hallucinate\non missing information or struggle with orchestrating the APIs. The key idea\nbehind our proposed approach is to leverage logical reasoning and classical AI\nplanning along with an LLM for accurately answering user queries including\nidentification and gathering of any missing information in these queries. Our\napproach uses an LLM and ASP (Answer Set Programming) solver to translate a\nuser query to a representation in Planning Domain Definition Language (PDDL)\nvia an intermediate representation in ASP. We introduce a special API\n\"get_info_api\" for gathering missing information. We model all the APIs as PDDL\nactions in a way that supports dataflow between the APIs. Our approach then\nuses a classical AI planner to generate an orchestration of API calls\n(including calls to get_info_api) to answer the user query. Our evaluation\nresults show that our approach significantly outperforms a pure LLM based\napproach by achieving over 95% success rate in most cases on a dataset\ncontaining complete and incomplete single goal and multi-goal queries where the\nmulti-goal queries may or may not require dataflow among the APIs."
                },
                "authors": [
                    {
                        "name": "Sudhir Agarwal"
                    },
                    {
                        "name": "Anu Sreepathy"
                    },
                    {
                        "name": "David H. Alonso"
                    },
                    {
                        "name": "Prarit Lamba"
                    }
                ],
                "author_detail": {
                    "name": "Prarit Lamba"
                },
                "arxiv_affiliation": "Intuit Inc.",
                "author": "Prarit Lamba",
                "arxiv_doi": "10.4204/EPTCS.416.3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 29-58",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09211v1",
                "updated": "2025-02-13T11:47:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    59,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:47:59Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    59,
                    3,
                    44,
                    0
                ],
                "title": "Visual Graph Question Answering with ASP and LLMs for Language Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Graph Question Answering with ASP and LLMs for Language Parsing"
                },
                "summary": "Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks."
                },
                "authors": [
                    {
                        "name": "Jakob Johannes Bauer"
                    },
                    {
                        "name": "Thomas Eiter"
                    },
                    {
                        "name": "Nelson Higuera Ruiz"
                    },
                    {
                        "name": "Johannes Oetsch"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Oetsch"
                },
                "arxiv_affiliation": "Jonkoping University, Sweden",
                "author": "Johannes Oetsch",
                "arxiv_doi": "10.4204/EPTCS.416.2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 15-28",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09209v1",
                "updated": "2025-02-13T11:47:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:47:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    47,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "On LLM-generated Logic Programs and their Inference Execution Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On LLM-generated Logic Programs and their Inference Execution Methods"
                },
                "summary": "Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on petabytes of data are highly\ncompressed repositories of a significant proportion of the knowledge\naccumulated and distilled so far. In this paper we study techniques to elicit\nthis knowledge in the form of several classes of logic programs, including\npropositional Horn clauses, Dual Horn clauses, relational triplets and Definite\nClause Grammars. Exposing this knowledge as logic programs enables sound\nreasoning methods that can verify alignment of LLM outputs to their intended\nuses and extend their inference capabilities. We study new execution methods\nfor the generated programs, including soft-unification of abducible facts\nagainst LLM-generated content stored in a vector database as well as GPU-based\nacceleration of minimal model computation that supports inference with large\nLLM-generated programs."
                },
                "authors": [
                    {
                        "name": "Paul Tarau"
                    }
                ],
                "author_detail": {
                    "name": "Paul Tarau"
                },
                "arxiv_affiliation": "University of North Texas",
                "author": "Paul Tarau",
                "arxiv_doi": "10.4204/EPTCS.416.1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 1-14",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06447v2",
                "updated": "2025-02-13T11:46:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    46,
                    41,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-08T23:11:47Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    23,
                    11,
                    47,
                    0,
                    190,
                    0
                ],
                "title": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for\n  Independent Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for\n  Independent Testing"
                },
                "summary": "The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors."
                },
                "authors": [
                    {
                        "name": "Divyagna Bavikadi"
                    },
                    {
                        "name": "Dyuman Aditya"
                    },
                    {
                        "name": "Devendra Parkar"
                    },
                    {
                        "name": "Paulo Shakarian"
                    },
                    {
                        "name": "Graham Mueller"
                    },
                    {
                        "name": "Chad Parvis"
                    },
                    {
                        "name": "Gerardo I. Simari"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo I. Simari"
                },
                "author": "Gerardo I. Simari",
                "arxiv_doi": "10.4204/EPTCS.416.24",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.24",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.06447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 274-287",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16495v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16495v3",
                "updated": "2025-02-13T11:46:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    46,
                    25,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-25T15:35:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    35,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning"
                },
                "summary": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets."
                },
                "authors": [
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Zhicheng Lee"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16495v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16495v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09204v1",
                "updated": "2025-02-13T11:45:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    45,
                    38,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:45:38Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    45,
                    38,
                    3,
                    44,
                    0
                ],
                "title": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in\n  New York",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in\n  New York"
                },
                "summary": "Legal cases require careful logical reasoning following the laws, whereas\ninteractions with non- technical users must be in natural language. As an\napplication combining logical reasoning using Prolog and natural language\nprocessing using large language models (LLMs), this paper presents a novel\napproach and system, LogicLease, to automate the analysis of landlord-tenant\nlegal cases in the state of New York. LogicLease determines compliance with\nrelevant legal requirements by analyzing case descriptions and citing all\nrelevant laws. It leverages LLMs for information extraction and Prolog for\nlegal reasoning. By separating information extraction from legal reasoning,\nLogicLease achieves greater transparency and control over the legal logic\napplied to each case. We evaluate the accuracy, efficiency, and robustness of\nLogicLease through a series of tests, achieving 100% accuracy and an average\nprocessing time of 2.57 seconds. LogicLease presents advantages over\nstate-of-the-art LLM- based legal analysis systems by providing clear,\nstep-by-step reasoning, citing specific laws, and distinguishing itself by its\nability to avoid hallucinations - a common issue in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal cases require careful logical reasoning following the laws, whereas\ninteractions with non- technical users must be in natural language. As an\napplication combining logical reasoning using Prolog and natural language\nprocessing using large language models (LLMs), this paper presents a novel\napproach and system, LogicLease, to automate the analysis of landlord-tenant\nlegal cases in the state of New York. LogicLease determines compliance with\nrelevant legal requirements by analyzing case descriptions and citing all\nrelevant laws. It leverages LLMs for information extraction and Prolog for\nlegal reasoning. By separating information extraction from legal reasoning,\nLogicLease achieves greater transparency and control over the legal logic\napplied to each case. We evaluate the accuracy, efficiency, and robustness of\nLogicLease through a series of tests, achieving 100% accuracy and an average\nprocessing time of 2.57 seconds. LogicLease presents advantages over\nstate-of-the-art LLM- based legal analysis systems by providing clear,\nstep-by-step reasoning, citing specific laws, and distinguishing itself by its\nability to avoid hallucinations - a common issue in LLMs."
                },
                "authors": [
                    {
                        "name": "Sanskar Sehgal"
                    },
                    {
                        "name": "Yanhong A. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yanhong A. Liu"
                },
                "author": "Yanhong A. Liu",
                "arxiv_doi": "10.4204/EPTCS.416.4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.416.4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings ICLP 2024, arXiv:2502.08453",
                "arxiv_journal_ref": "EPTCS 416, 2025, pp. 59-68",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10053v2",
                "updated": "2025-02-13T11:43:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    43,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2024-08-19T14:48:04Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    14,
                    48,
                    4,
                    0,
                    232,
                    0
                ],
                "title": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory"
                },
                "summary": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Xuebing Zhou"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "To appear at NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15151v3",
                "updated": "2025-02-13T11:37:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    37,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-19T18:28:41Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "title": "Language Models as Continuous Self-Evolving Data Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Continuous Self-Evolving Data Engineers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE."
                },
                "authors": [
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Kaisong Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaisong Song"
                },
                "author": "Kaisong Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09194v1",
                "updated": "2025-02-13T11:33:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    33,
                    29,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:33:29Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    33,
                    29,
                    3,
                    44,
                    0
                ],
                "title": "XAInomaly: Explainable and Interpretable Deep Contractive Autoencoder\n  for O-RAN Traffic Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAInomaly: Explainable and Interpretable Deep Contractive Autoencoder\n  for O-RAN Traffic Anomaly Detection"
                },
                "summary": "Generative Artificial Intelligence (AI) techniques have become integral part\nin advancing next generation wireless communication systems by enabling\nsophisticated data modeling and feature extraction for enhanced network\nperformance. In the realm of open radio access networks (O-RAN), characterized\nby their disaggregated architecture and heterogeneous components from multiple\nvendors, the deployment of generative models offers significant advantages for\nnetwork management such as traffic analysis, traffic forecasting and anomaly\ndetection. However, the complex and dynamic nature of O-RAN introduces\nchallenges that necessitate not only accurate detection mechanisms but also\nreduced complexity, scalability, and most importantly interpretability to\nfacilitate effective network management. In this study, we introduce the\nXAInomaly framework, an explainable and interpretable Semi-supervised (SS) Deep\nContractive Autoencoder (DeepCAE) design for anomaly detection in O-RAN. Our\napproach leverages the generative modeling capabilities of our SS-DeepCAE model\nto learn compressed, robust representations of normal network behavior, which\ncaptures essential features, enabling the identification of deviations\nindicative of anomalies. To address the black-box nature of deep learning\nmodels, we propose reactive Explainable AI (XAI) technique called fastshap-C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (AI) techniques have become integral part\nin advancing next generation wireless communication systems by enabling\nsophisticated data modeling and feature extraction for enhanced network\nperformance. In the realm of open radio access networks (O-RAN), characterized\nby their disaggregated architecture and heterogeneous components from multiple\nvendors, the deployment of generative models offers significant advantages for\nnetwork management such as traffic analysis, traffic forecasting and anomaly\ndetection. However, the complex and dynamic nature of O-RAN introduces\nchallenges that necessitate not only accurate detection mechanisms but also\nreduced complexity, scalability, and most importantly interpretability to\nfacilitate effective network management. In this study, we introduce the\nXAInomaly framework, an explainable and interpretable Semi-supervised (SS) Deep\nContractive Autoencoder (DeepCAE) design for anomaly detection in O-RAN. Our\napproach leverages the generative modeling capabilities of our SS-DeepCAE model\nto learn compressed, robust representations of normal network behavior, which\ncaptures essential features, enabling the identification of deviations\nindicative of anomalies. To address the black-box nature of deep learning\nmodels, we propose reactive Explainable AI (XAI) technique called fastshap-C."
                },
                "authors": [
                    {
                        "name": "Osman Tugay Basaran"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "22 pages, 9 Figures, Submitted to Journal (First revision completed)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09192v1",
                "updated": "2025-02-13T11:32:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:32:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    32,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking beyond the anthropomorphic paradigm benefits LLM research"
                },
                "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08441v2",
                "updated": "2025-02-13T11:30:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    30,
                    41,
                    3,
                    44,
                    0
                ],
                "published": "2024-07-11T12:30:19Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    30,
                    19,
                    3,
                    193,
                    0
                ],
                "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Giada Cosenza"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "arxiv_doi": "10.1007/978-3-031-78977-9_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78977-9_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09188v1",
                "updated": "2025-02-13T11:22:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    22,
                    19,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:22:19Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    22,
                    19,
                    3,
                    44,
                    0
                ],
                "title": "Matina: A Large-Scale 73B Token Persian Text Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matina: A Large-Scale 73B Token Persian Text Corpus"
                },
                "summary": "Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements."
                },
                "authors": [
                    {
                        "name": "Sara Bourbour Hosseinbeigi"
                    },
                    {
                        "name": "Fatemeh Taherinezhad"
                    },
                    {
                        "name": "Heshaam Faili"
                    },
                    {
                        "name": "Hamed Baghbani"
                    },
                    {
                        "name": "Fatemeh Nadi"
                    },
                    {
                        "name": "Mostafa Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mostafa Amiri"
                },
                "author": "Mostafa Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09834v3",
                "updated": "2025-02-13T11:18:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    18,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2024-06-14T08:44:10Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    44,
                    10,
                    4,
                    166,
                    0
                ],
                "title": "LLMs Meet Library Evolution: Evaluating Deprecated API Usage in\n  LLM-based Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Meet Library Evolution: Evaluating Deprecated API Usage in\n  LLM-based Code Completion"
                },
                "summary": "Large language models (LLMs), pre-trained or fine-tuned on large code\ncorpora, have shown effectiveness in generating code completions. However, in\nLLM-based code completion, LLMs may struggle to use correct and up-to-date\nApplication Programming Interfaces (APIs) due to the rapid and continuous\nevolution of libraries. While existing studies have highlighted issues with\npredicting incorrect APIs, the specific problem of deprecated API usage in\nLLM-based code completion has not been thoroughly investigated. To address this\ngap, we conducted the first evaluation study on deprecated API usage in\nLLM-based code completion. This study involved seven advanced LLMs, 145 API\nmappings from eight popular Python libraries, and 28,125 completion prompts.\nThe study results reveal the status quo (i.e., API usage plausibility and\ndeprecated usage rate) of deprecated API and replacing API usage in LLM-based\ncode completion from the perspectives of model, prompt, and library, and\nindicate the root causes behind. Based on these findings, we propose two\nlightweight fixing approaches, REPLACEAPI and INSERTPROMPT, which can serve as\nbaseline approaches for future research on mitigating deprecated API usage in\nLLM-based completion. Additionally, we provide implications for future research\non integrating library evolution with LLM-driven software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), pre-trained or fine-tuned on large code\ncorpora, have shown effectiveness in generating code completions. However, in\nLLM-based code completion, LLMs may struggle to use correct and up-to-date\nApplication Programming Interfaces (APIs) due to the rapid and continuous\nevolution of libraries. While existing studies have highlighted issues with\npredicting incorrect APIs, the specific problem of deprecated API usage in\nLLM-based code completion has not been thoroughly investigated. To address this\ngap, we conducted the first evaluation study on deprecated API usage in\nLLM-based code completion. This study involved seven advanced LLMs, 145 API\nmappings from eight popular Python libraries, and 28,125 completion prompts.\nThe study results reveal the status quo (i.e., API usage plausibility and\ndeprecated usage rate) of deprecated API and replacing API usage in LLM-based\ncode completion from the perspectives of model, prompt, and library, and\nindicate the root causes behind. Based on these findings, we propose two\nlightweight fixing approaches, REPLACEAPI and INSERTPROMPT, which can serve as\nbaseline approaches for future research on mitigating deprecated API usage in\nLLM-based completion. Additionally, we provide implications for future research\non integrating library evolution with LLM-driven software development."
                },
                "authors": [
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Kaifeng Huang"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Yebo Feng"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09183v1",
                "updated": "2025-02-13T11:17:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    17,
                    53,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:17:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    17,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation"
                },
                "summary": "Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data."
                },
                "authors": [
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Dandan Song"
                    },
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Wanli Gu"
                    },
                    {
                        "name": "Huipeng Ma"
                    },
                    {
                        "name": "Yuhang Tian"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Linmei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Linmei Hu"
                },
                "author": "Linmei Hu",
                "arxiv_comment": "work in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07838v2",
                "updated": "2025-02-13T11:13:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    13,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T02:31:45Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    2,
                    31,
                    45,
                    1,
                    42,
                    0
                ],
                "title": "NanoVLMs: How small can we go and still make coherent Vision Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLMs: How small can we go and still make coherent Vision Language\n  Models?"
                },
                "summary": "Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have\ngarnered significant research attention for their ability to leverage Large\nLanguage Models (LLMs) in multimodal tasks. However, their potential is\nconstrained by inherent challenges, including proprietary restrictions,\nsubstantial computational demands, and limited accessibility. Smaller models,\nsuch as GIT and BLIP, exhibit marked limitations, often failing to generate\ncoherent and consistent text beyond a few tokens, even with extensive training.\nThis underscores a pivotal inquiry: how small can a VLM be and still produce\nfluent and consistent text? Drawing inspiration from the exceptional learning\nprocess of 3-4 year old children, who rely heavily on visual cues for\nunderstanding and communication, we introduce two novel datasets: ShortDesc\n(featuring concise image descriptions) and LongDesc (containing more detailed\nimage descriptions). These datasets consist of image-text pairs where the text\nis restricted to the simple vocabulary and syntax typically used by young\nchildren, generated with a scaled- down model, GPT-4o. Using these datasets, we\ndemonstrate that it is possible to train VLMs that are significantly smaller,\nup to 10 times smaller than state of the art(SOTA) small VLMs while maintaining\narchitectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade\nthe text, as if stories written by students, on creativity, meaningfulness, and\nconsistency, assigning scores out of 10. This method addresses limitations of\nstandard benchmarks by accommodating unstructured outputs and providing a\nmultidimensional evaluation of the model capabilities. Our findings contribute\nto the development of lightweight, accessible multimodal models for resource\nconstrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have\ngarnered significant research attention for their ability to leverage Large\nLanguage Models (LLMs) in multimodal tasks. However, their potential is\nconstrained by inherent challenges, including proprietary restrictions,\nsubstantial computational demands, and limited accessibility. Smaller models,\nsuch as GIT and BLIP, exhibit marked limitations, often failing to generate\ncoherent and consistent text beyond a few tokens, even with extensive training.\nThis underscores a pivotal inquiry: how small can a VLM be and still produce\nfluent and consistent text? Drawing inspiration from the exceptional learning\nprocess of 3-4 year old children, who rely heavily on visual cues for\nunderstanding and communication, we introduce two novel datasets: ShortDesc\n(featuring concise image descriptions) and LongDesc (containing more detailed\nimage descriptions). These datasets consist of image-text pairs where the text\nis restricted to the simple vocabulary and syntax typically used by young\nchildren, generated with a scaled- down model, GPT-4o. Using these datasets, we\ndemonstrate that it is possible to train VLMs that are significantly smaller,\nup to 10 times smaller than state of the art(SOTA) small VLMs while maintaining\narchitectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade\nthe text, as if stories written by students, on creativity, meaningfulness, and\nconsistency, assigning scores out of 10. This method addresses limitations of\nstandard benchmarks by accommodating unstructured outputs and providing a\nmultidimensional evaluation of the model capabilities. Our findings contribute\nto the development of lightweight, accessible multimodal models for resource\nconstrained environments."
                },
                "authors": [
                    {
                        "name": "Mukund Agarwalla"
                    },
                    {
                        "name": "Himanshu Kumar"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "11 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09175v1",
                "updated": "2025-02-13T11:05:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    5,
                    55,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T11:05:55Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    11,
                    5,
                    55,
                    3,
                    44,
                    0
                ],
                "title": "FLAME: Flexible LLM-Assisted Moderation Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Flexible LLM-Assisted Moderation Engine"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs."
                },
                "authors": [
                    {
                        "name": "Ivan Bakulin"
                    },
                    {
                        "name": "Ilia Kopanichuk"
                    },
                    {
                        "name": "Iaroslav Bespalov"
                    },
                    {
                        "name": "Nikita Radchenko"
                    },
                    {
                        "name": "Vladimir Shaposhnikov"
                    },
                    {
                        "name": "Dmitry Dylov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "arxiv_affiliation": "Skolkovo Institute of Science and Technology",
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09170v1",
                "updated": "2025-02-13T10:53:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    53,
                    38,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T10:53:38Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    53,
                    38,
                    3,
                    44,
                    0
                ],
                "title": "LimSim Series: An Autonomous Driving Simulation Platform for Validation\n  and Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LimSim Series: An Autonomous Driving Simulation Platform for Validation\n  and Enhancement"
                },
                "summary": "Closed-loop simulation environments play a crucial role in the validation and\nenhancement of autonomous driving systems (ADS). However, certain challenges\nwarrant significant attention, including balancing simulation accuracy with\nduration, reconciling functionality with practicality, and establishing\ncomprehensive evaluation mechanisms. This paper addresses these challenges by\nintroducing the LimSim Series, a comprehensive simulation platform designed to\nsupport the rapid deployment and efficient iteration of ADS. The LimSim Series\nintegrates multi-type information from road networks, employs human-like\ndecision-making and planning algorithms for background vehicles, and introduces\nthe concept of the Area of Interest (AoI) to optimize computational resources.\nThe platform offers a variety of baseline algorithms and user-friendly\ninterfaces, facilitating flexible validation of multiple technical pipelines.\nAdditionally, the LimSim Series incorporates multi-dimensional evaluation\nmetrics, delivering thorough insights into system performance, thus enabling\nresearchers to promptly identify issues for further improvements. Experiments\ndemonstrate that the LimSim Series is compatible with modular, end-to-end, and\nVLM-based knowledge-driven systems. It can assist in the iteration and updating\nof ADS by evaluating performance across various scenarios. The code of the\nLimSim Series is released at: https://github.com/PJLab-ADG/LimSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closed-loop simulation environments play a crucial role in the validation and\nenhancement of autonomous driving systems (ADS). However, certain challenges\nwarrant significant attention, including balancing simulation accuracy with\nduration, reconciling functionality with practicality, and establishing\ncomprehensive evaluation mechanisms. This paper addresses these challenges by\nintroducing the LimSim Series, a comprehensive simulation platform designed to\nsupport the rapid deployment and efficient iteration of ADS. The LimSim Series\nintegrates multi-type information from road networks, employs human-like\ndecision-making and planning algorithms for background vehicles, and introduces\nthe concept of the Area of Interest (AoI) to optimize computational resources.\nThe platform offers a variety of baseline algorithms and user-friendly\ninterfaces, facilitating flexible validation of multiple technical pipelines.\nAdditionally, the LimSim Series incorporates multi-dimensional evaluation\nmetrics, delivering thorough insights into system performance, thus enabling\nresearchers to promptly identify issues for further improvements. Experiments\ndemonstrate that the LimSim Series is compatible with modular, end-to-end, and\nVLM-based knowledge-driven systems. It can assist in the iteration and updating\nof ADS by evaluating performance across various scenarios. The code of the\nLimSim Series is released at: https://github.com/PJLab-ADG/LimSim."
                },
                "authors": [
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Naiting Zhong"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13916v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13916v5",
                "updated": "2025-02-13T10:45:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    45,
                    15,
                    3,
                    44,
                    0
                ],
                "published": "2023-08-26T16:51:17Z",
                "published_parsed": [
                    2023,
                    8,
                    26,
                    16,
                    51,
                    17,
                    5,
                    238,
                    0
                ],
                "title": "Exploring Large Language Models for Knowledge Graph Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Knowledge Graph Completion"
                },
                "summary": "Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4."
                },
                "authors": [
                    {
                        "name": "Liang Yao"
                    },
                    {
                        "name": "Jiazhen Peng"
                    },
                    {
                        "name": "Chengsheng Mao"
                    },
                    {
                        "name": "Yuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Luo"
                },
                "author": "Yuan Luo",
                "arxiv_comment": "Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13916v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13916v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09156v1",
                "updated": "2025-02-13T10:36:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    36,
                    18,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T10:36:18Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    36,
                    18,
                    3,
                    44,
                    0
                ],
                "title": "Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs"
                },
                "summary": "Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM."
                },
                "authors": [
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying Chang"
                    },
                    {
                        "name": "Jianmin Li"
                    },
                    {
                        "name": "Yiqian Qu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Lingyong Cao"
                    },
                    {
                        "name": "Shuyuan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shuyuan Lin"
                },
                "author": "Shuyuan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09142v1",
                "updated": "2025-02-13T10:17:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    17,
                    12,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T10:17:12Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    17,
                    12,
                    3,
                    44,
                    0
                ],
                "title": "LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded\n  Robot Teleoperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded\n  Robot Teleoperation"
                },
                "summary": "The integration of robotics and augmented reality (AR) presents\ntransformative opportunities for advancing human-robot interaction (HRI) by\nimproving usability, intuitiveness, and accessibility. This work introduces a\ncontroller-free, LLM-driven voice-commanded AR puppeteering system, enabling\nusers to teleoperate a robot by manipulating its virtual counterpart in real\ntime. By leveraging natural language processing (NLP) and AR technologies, our\nsystem -- prototyped using Meta Quest 3 -- eliminates the need for physical\ncontrollers, enhancing ease of use while minimizing potential safety risks\nassociated with direct robot operation. A preliminary user demonstration\nsuccessfully validated the system's functionality, demonstrating its potential\nfor safer, more intuitive, and immersive robotic control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of robotics and augmented reality (AR) presents\ntransformative opportunities for advancing human-robot interaction (HRI) by\nimproving usability, intuitiveness, and accessibility. This work introduces a\ncontroller-free, LLM-driven voice-commanded AR puppeteering system, enabling\nusers to teleoperate a robot by manipulating its virtual counterpart in real\ntime. By leveraging natural language processing (NLP) and AR technologies, our\nsystem -- prototyped using Meta Quest 3 -- eliminates the need for physical\ncontrollers, enhancing ease of use while minimizing potential safety risks\nassociated with direct robot operation. A preliminary user demonstration\nsuccessfully validated the system's functionality, demonstrating its potential\nfor safer, more intuitive, and immersive robotic control."
                },
                "authors": [
                    {
                        "name": "Yuchong Zhang"
                    },
                    {
                        "name": "Bastian Orthmann"
                    },
                    {
                        "name": "Michael C. Welle"
                    },
                    {
                        "name": "Jonne Van Haastregt"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_comment": "Accepted as conference proceeding in International Conference on\n  Human-Computer Interaction 2025 (HCI International 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04890v2",
                "updated": "2025-02-13T10:09:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    9,
                    37,
                    3,
                    44,
                    0
                ],
                "published": "2024-11-07T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    28,
                    10,
                    3,
                    312,
                    0
                ],
                "title": "GUI Agents with Foundation Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI Agents with Foundation Models: A Comprehensive Survey"
                },
                "summary": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), have facilitated the\ndevelopment of intelligent agents capable of performing complex tasks. By\nleveraging the ability of (M)LLMs to process and interpret Graphical User\nInterfaces (GUIs), these agents can autonomously execute user instructions,\nsimulating human-like interactions such as clicking and typing. This survey\nconsolidates recent research on (M)LLM-based GUI agents, highlighting key\ninnovations in data resources, frameworks, and applications. We begin by\nreviewing representative datasets and benchmarks, followed by an overview of a\ngeneralized, unified framework that encapsulates the essential components of\nprior studies, supported by a detailed taxonomy. Additionally, we explore\nrelevant commercial applications. Drawing insights from existing work, we\nidentify key challenges and propose future research directions. We hope this\nsurvey will inspire further advancements in the field of (M)LLM-based GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), have facilitated the\ndevelopment of intelligent agents capable of performing complex tasks. By\nleveraging the ability of (M)LLMs to process and interpret Graphical User\nInterfaces (GUIs), these agents can autonomously execute user instructions,\nsimulating human-like interactions such as clicking and typing. This survey\nconsolidates recent research on (M)LLM-based GUI agents, highlighting key\ninnovations in data resources, frameworks, and applications. We begin by\nreviewing representative datasets and benchmarks, followed by an overview of a\ngeneralized, unified framework that encapsulates the essential components of\nprior studies, supported by a detailed taxonomy. Additionally, we explore\nrelevant commercial applications. Drawing insights from existing work, we\nidentify key challenges and propose future research directions. We hope this\nsurvey will inspire further advancements in the field of (M)LLM-based GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Yuqi Zhou"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Yuhan Che"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Kun Shao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09111v1",
                "updated": "2025-02-13T09:41:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    41,
                    8,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T09:41:08Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    41,
                    8,
                    3,
                    44,
                    0
                ],
                "title": "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior"
                },
                "summary": "Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Mingrui Li"
                    },
                    {
                        "name": "Shuhong Liu"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Hongyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Wang"
                },
                "author": "Hongyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09104v1",
                "updated": "2025-02-13T09:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    26,
                    44,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T09:26:44Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    26,
                    44,
                    3,
                    44,
                    0
                ],
                "title": "One-shot Federated Learning Methods: A Practical Guide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-shot Federated Learning Methods: A Practical Guide"
                },
                "summary": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm\nthat constrains client-server communication to a single round, addressing\nprivacy and communication overhead issues associated with multiple rounds of\ndata exchange in traditional Federated Learning (FL). OFL demonstrates the\npractical potential for integration with future approaches that require\ncollaborative training models, such as large language models (LLMs). However,\ncurrent OFL methods face two major challenges: data heterogeneity and model\nheterogeneity, which result in subpar performance compared to conventional FL\nmethods. Worse still, despite numerous studies addressing these limitations, a\ncomprehensive summary is still lacking. To address these gaps, this paper\npresents a systematic analysis of the challenges faced by OFL and thoroughly\nreviews the current methods. We also offer an innovative categorization method\nand analyze the trade-offs of various techniques. Additionally, we discuss the\nmost promising future directions and the technologies that should be integrated\ninto the OFL field. This work aims to provide guidance and insights for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm\nthat constrains client-server communication to a single round, addressing\nprivacy and communication overhead issues associated with multiple rounds of\ndata exchange in traditional Federated Learning (FL). OFL demonstrates the\npractical potential for integration with future approaches that require\ncollaborative training models, such as large language models (LLMs). However,\ncurrent OFL methods face two major challenges: data heterogeneity and model\nheterogeneity, which result in subpar performance compared to conventional FL\nmethods. Worse still, despite numerous studies addressing these limitations, a\ncomprehensive summary is still lacking. To address these gaps, this paper\npresents a systematic analysis of the challenges faced by OFL and thoroughly\nreviews the current methods. We also offer an innovative categorization method\nand analyze the trade-offs of various techniques. Additionally, we discuss the\nmost promising future directions and the technologies that should be integrated\ninto the OFL field. This work aims to provide guidance and insights for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xia Li"
                    },
                    {
                        "name": "Yijun Song"
                    },
                    {
                        "name": "Sijie Ji"
                    },
                    {
                        "name": "Zemin Liu"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Linshan Jiang"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09101v1",
                "updated": "2025-02-13T09:19:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    19,
                    42,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T09:19:42Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    19,
                    42,
                    3,
                    44,
                    0
                ],
                "title": "Bridging the Gap Between LLMs and Human Intentions: Progresses and\n  Challenges in Instruction Understanding, Intention Reasoning, and Reliable\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between LLMs and Human Intentions: Progresses and\n  Challenges in Instruction Understanding, Intention Reasoning, and Reliable\n  Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generation. However, when interacting with human instructions\nin real-world scenarios, LLMs still face significant challenges, particularly\nin accurately capturing and comprehending human instructions and intentions.\nThis paper focuses on three challenges in LLM-based text generation tasks:\ninstruction understanding, intention reasoning, and reliable generation.\nRegarding human complex instruction, LLMs have deficiencies in understanding\nlong contexts and instructions in multi-round conversations. For intention\nreasoning, LLMs may have inconsistent command reasoning, difficulty reasoning\nabout commands containing incorrect information, difficulty understanding user\nambiguous language commands, and a weak understanding of user intention in\ncommands. Besides, In terms of reliable generation, LLMs may have unstable\ngenerated content and unethical generation. To this end, we classify and\nanalyze the performance of LLMs in challenging scenarios and conduct a\ncomprehensive evaluation of existing solutions. Furthermore, we introduce\nbenchmarks and categorize them based on the aforementioned three core\nchallenges. Finally, we explore potential directions for future research to\nenhance the reliability and adaptability of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generation. However, when interacting with human instructions\nin real-world scenarios, LLMs still face significant challenges, particularly\nin accurately capturing and comprehending human instructions and intentions.\nThis paper focuses on three challenges in LLM-based text generation tasks:\ninstruction understanding, intention reasoning, and reliable generation.\nRegarding human complex instruction, LLMs have deficiencies in understanding\nlong contexts and instructions in multi-round conversations. For intention\nreasoning, LLMs may have inconsistent command reasoning, difficulty reasoning\nabout commands containing incorrect information, difficulty understanding user\nambiguous language commands, and a weak understanding of user intention in\ncommands. Besides, In terms of reliable generation, LLMs may have unstable\ngenerated content and unethical generation. To this end, we classify and\nanalyze the performance of LLMs in challenging scenarios and conduct a\ncomprehensive evaluation of existing solutions. Furthermore, we introduce\nbenchmarks and categorize them based on the aforementioned three core\nchallenges. Finally, we explore potential directions for future research to\nenhance the reliability and adaptability of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Zongyu Chang"
                    },
                    {
                        "name": "Feihong Lu"
                    },
                    {
                        "name": "Ziqin Zhu"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Cheng Ji"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Shangguang Wang"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09100v1",
                "updated": "2025-02-13T09:19:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    19,
                    14,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T09:19:14Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    19,
                    14,
                    3,
                    44,
                    0
                ],
                "title": "Logical Reasoning in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Reasoning in Large Language Models: A Survey"
                },
                "summary": "With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems."
                },
                "authors": [
                    {
                        "name": "Hanmeng Liu"
                    },
                    {
                        "name": "Zhizhang Fu"
                    },
                    {
                        "name": "Mengru Ding"
                    },
                    {
                        "name": "Ruoxi Ning"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Xiaozhang Liu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14445v2",
                "updated": "2025-02-13T09:07:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    7,
                    10,
                    3,
                    44,
                    0
                ],
                "published": "2024-05-23T11:24:23Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    24,
                    23,
                    3,
                    144,
                    0
                ],
                "title": "Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study"
                },
                "summary": "This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM."
                },
                "authors": [
                    {
                        "name": "Lena Schmidt"
                    },
                    {
                        "name": "Kaitlyn Hair"
                    },
                    {
                        "name": "Sergio Graziosi"
                    },
                    {
                        "name": "Fiona Campbell"
                    },
                    {
                        "name": "Claudia Kapp"
                    },
                    {
                        "name": "Alireza Khanteymoori"
                    },
                    {
                        "name": "Dawn Craig"
                    },
                    {
                        "name": "Mark Engelbert"
                    },
                    {
                        "name": "James Thomas"
                    }
                ],
                "author_detail": {
                    "name": "James Thomas"
                },
                "author": "James Thomas",
                "arxiv_comment": "Conference proceedings, peer-reviewed and presented at the 3rd\n  Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems,\n  Glasgow, 2024",
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on Augmented Intelligence for\n  Technology-Assisted Reviews Systems (ALTARS 2024), Glasgow.\n  https://ceur-ws.org/Vol-3832/paper2.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06931v2",
                "updated": "2025-02-13T09:03:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    9,
                    3,
                    57,
                    3,
                    44,
                    0
                ],
                "published": "2024-12-09T19:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    19,
                    21,
                    5,
                    0,
                    344,
                    0
                ],
                "title": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based\n  Planning and Manoeuvrability-Driven Controls"
                },
                "summary": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to wield tools was once considered exclusive to human\nintelligence, but it's now known that many other animals, like crows, possess\nthis capability. Yet, robotic systems still fall short of matching biological\ndexterity. In this paper, we investigate the use of Large Language Models\n(LLMs), tool affordances, and object manoeuvrability for non-prehensile\ntool-based manipulation tasks. Our novel method leverages LLMs based on scene\ninformation and natural language instructions to enable symbolic task planning\nfor tool-object manipulation. This approach allows the system to convert the\nhuman language sentence into a sequence of feasible motion functions. We have\ndeveloped a novel manoeuvrability-driven controller using a new tool affordance\nmodel derived from visual feedback. This controller helps guide the robot's\ntool utilization and manipulation actions, even within confined areas, using a\nstepping incremental approach. The proposed methodology is evaluated with\nexperiments to prove its effectiveness under various manipulation scenarios."
                },
                "authors": [
                    {
                        "name": "Hoi-Yin Lee"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Anqing Duan"
                    },
                    {
                        "name": "Wanyu Ma"
                    },
                    {
                        "name": "Chenguang Yang"
                    },
                    {
                        "name": "David Navarro-Alarcon"
                    }
                ],
                "author_detail": {
                    "name": "David Navarro-Alarcon"
                },
                "author": "David Navarro-Alarcon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17770v2",
                "updated": "2025-02-13T08:58:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    58,
                    17,
                    3,
                    44,
                    0
                ],
                "published": "2024-10-23T11:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    19,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "Small Singular Values Matter: A Random Matrix Analysis of Transformer\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Singular Values Matter: A Random Matrix Analysis of Transformer\n  Models"
                },
                "summary": "As large language models (LLMs) become increasingly central to AI\napplications, understanding their inner workings is essential. In this work, we\nanalyze the spectra of weight matrices in pretrained transformer models through\nthe lens of random matrix theory (RMT) to uncover learned structures. We find\nthat certain regions of the weight matrix spectra deviate markedly from RMT\npredictions, indicating richer feature encoding. By comparing the corresponding\nsingular vectors to eigenvectors of activation covariance matrices, we observe\nsubstantial overlap precisely where the spectra deviate from RMT expectations.\nOur analysis further reveals the important role of small singular values in\nLLMs, showing that these values contain significant information, a claim\nsupported by increased perplexity when they are removed from the model.\nAlthough these small values may appear unimportant prior to task-specific\nfine-tuning, removing them afterward significantly degrades performance,\nrevealing that fine-tuning refines the model primarily in these spectral\nregions. These results emphasize the critical role of small singular values,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly central to AI\napplications, understanding their inner workings is essential. In this work, we\nanalyze the spectra of weight matrices in pretrained transformer models through\nthe lens of random matrix theory (RMT) to uncover learned structures. We find\nthat certain regions of the weight matrix spectra deviate markedly from RMT\npredictions, indicating richer feature encoding. By comparing the corresponding\nsingular vectors to eigenvectors of activation covariance matrices, we observe\nsubstantial overlap precisely where the spectra deviate from RMT expectations.\nOur analysis further reveals the important role of small singular values in\nLLMs, showing that these values contain significant information, a claim\nsupported by increased perplexity when they are removed from the model.\nAlthough these small values may appear unimportant prior to task-specific\nfine-tuning, removing them afterward significantly degrades performance,\nrevealing that fine-tuning refines the model primarily in these spectral\nregions. These results emphasize the critical role of small singular values,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment."
                },
                "authors": [
                    {
                        "name": "Max Staats"
                    },
                    {
                        "name": "Matthias Thamm"
                    },
                    {
                        "name": "Bernd Rosenow"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Rosenow"
                },
                "author": "Bernd Rosenow",
                "arxiv_comment": "12 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09082v1",
                "updated": "2025-02-13T08:55:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    55,
                    24,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T08:55:24Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    55,
                    24,
                    3,
                    44,
                    0
                ],
                "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles"
                },
                "summary": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively."
                },
                "authors": [
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Haoran Guo"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Shuchang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Zhou"
                },
                "author": "Shuchang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09073v1",
                "updated": "2025-02-13T08:42:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    42,
                    29,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T08:42:29Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    42,
                    29,
                    3,
                    44,
                    0
                ],
                "title": "Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables"
                },
                "summary": "Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics."
                },
                "authors": [
                    {
                        "name": "Xuzhao Geng"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05068v4",
                "updated": "2025-02-13T08:32:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    32,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2024-09-08T12:10:13Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    10,
                    13,
                    6,
                    252,
                    0
                ],
                "title": "Improving early detection of gravitational waves from binary neutron\n  stars using CNNs and FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving early detection of gravitational waves from binary neutron\n  stars using CNNs and FPGAs"
                },
                "summary": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel WaveNet-based method, leveraging the state-of-the-art ML model,\nto produce early-warning alerts for BNS systems. Using simulated GW signals\nembedded in Gaussian noise from the Advanced LIGO and Advanced Virgo detectors'\nthird observing run (O3) as a proof-of-concept dataset, we demonstrate\nsignificant performance improvements. Compared to the current leading ML-based\nearly-warning system, our approach enhances detection accuracy from 66.81% to\n76.22% at a 1% false alarm probability. Furthermore, we evaluate the time,\nenergy, and economical cost of our model across CPU, GPU, and FPGA platforms,\nshowcasing its potential for deployment in real-time gravitational wave\ndetection pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel WaveNet-based method, leveraging the state-of-the-art ML model,\nto produce early-warning alerts for BNS systems. Using simulated GW signals\nembedded in Gaussian noise from the Advanced LIGO and Advanced Virgo detectors'\nthird observing run (O3) as a proof-of-concept dataset, we demonstrate\nsignificant performance improvements. Compared to the current leading ML-based\nearly-warning system, our approach enhances detection accuracy from 66.81% to\n76.22% at a 1% false alarm probability. Furthermore, we evaluate the time,\nenergy, and economical cost of our model across CPU, GPU, and FPGA platforms,\nshowcasing its potential for deployment in real-time gravitational wave\ndetection pipelines."
                },
                "authors": [
                    {
                        "name": "Ana Martins"
                    },
                    {
                        "name": "Melissa Lopez"
                    },
                    {
                        "name": "Quirijn Meijer"
                    },
                    {
                        "name": "Gregory Baltus"
                    },
                    {
                        "name": "Marc van der Sluys"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Caudill"
                },
                "author": "Sarah Caudill",
                "arxiv_comment": "21 pages, 7 figures, 3 tables, submitted to Machine Learning Science\n  and Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10913v2",
                "updated": "2025-02-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    29,
                    58,
                    3,
                    44,
                    0
                ],
                "published": "2024-09-17T06:11:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers"
                },
                "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Mehak Chhokar"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Mahendra Meena"
                    },
                    {
                        "name": "Hamid Abdullah"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Ruchit Nagar"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09061v1",
                "updated": "2025-02-13T08:23:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    23,
                    42,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T08:23:42Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    23,
                    42,
                    3,
                    44,
                    0
                ],
                "title": "CRANE: Reasoning with constrained LLM generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRANE: Reasoning with constrained LLM generation"
                },
                "summary": "Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO."
                },
                "authors": [
                    {
                        "name": "Debangshu Banerjee"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Sasa Misailovic"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09058v1",
                "updated": "2025-02-13T08:19:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    19,
                    45,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T08:19:45Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    19,
                    45,
                    3,
                    44,
                    0
                ],
                "title": "Unleashing the Power of Large Language Model for Denoising\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Large Language Model for Denoising\n  Recommendation"
                },
                "summary": "Recommender systems are crucial for personalizing user experiences but often\ndepend on implicit feedback data, which can be noisy and misleading. Existing\ndenoising studies involve incorporating auxiliary information or learning\nstrategies from interaction data. However, they struggle with the inherent\nlimitations of external knowledge and interaction data, as well as the\nnon-universality of certain predefined assumptions, hindering accurate noise\nidentification. Recently, large language models (LLMs) have gained attention\nfor their extensive world knowledge and reasoning abilities, yet their\npotential in enhancing denoising in recommendations remains underexplored. In\nthis paper, we introduce LLaRD, a framework leveraging LLMs to improve\ndenoising in recommender systems, thereby boosting overall recommendation\nperformance. Specifically, LLaRD generates denoising-related knowledge by first\nenriching semantic insights from observational data via LLMs and inferring\nuser-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)\ntechnique over user-item interaction graphs to reveal relation knowledge for\ndenoising. Finally, it applies the Information Bottleneck (IB) principle to\nalign LLM-generated denoising knowledge with recommendation targets, filtering\nout noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's\neffectiveness in enhancing denoising and recommendation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are crucial for personalizing user experiences but often\ndepend on implicit feedback data, which can be noisy and misleading. Existing\ndenoising studies involve incorporating auxiliary information or learning\nstrategies from interaction data. However, they struggle with the inherent\nlimitations of external knowledge and interaction data, as well as the\nnon-universality of certain predefined assumptions, hindering accurate noise\nidentification. Recently, large language models (LLMs) have gained attention\nfor their extensive world knowledge and reasoning abilities, yet their\npotential in enhancing denoising in recommendations remains underexplored. In\nthis paper, we introduce LLaRD, a framework leveraging LLMs to improve\ndenoising in recommender systems, thereby boosting overall recommendation\nperformance. Specifically, LLaRD generates denoising-related knowledge by first\nenriching semantic insights from observational data via LLMs and inferring\nuser-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)\ntechnique over user-item interaction graphs to reveal relation knowledge for\ndenoising. Finally, it applies the Information Bottleneck (IB) principle to\nalign LLM-generated denoising knowledge with recommendation targets, filtering\nout noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's\neffectiveness in enhancing denoising and recommendation accuracy."
                },
                "authors": [
                    {
                        "name": "Shuyao Wang"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yongduo Sui"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_doi": "10.1145/3696410.3714758",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714758",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 5 figures, 4 tables. Accecpted by WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13835v2",
                "updated": "2025-02-13T08:13:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    13,
                    52,
                    3,
                    44,
                    0
                ],
                "published": "2024-01-24T22:21:04Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    22,
                    21,
                    4,
                    2,
                    24,
                    0
                ],
                "title": "What Large Language Models Know and What People Think They Know",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Large Language Models Know and What People Think They Know"
                },
                "summary": "As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 ."
                },
                "authors": [
                    {
                        "name": "Mark Steyvers"
                    },
                    {
                        "name": "Heliodoro Tejeda"
                    },
                    {
                        "name": "Aakriti Kumar"
                    },
                    {
                        "name": "Catarina Belem"
                    },
                    {
                        "name": "Sheer Karny"
                    },
                    {
                        "name": "Xinyue Hu"
                    },
                    {
                        "name": "Lukas Mayer"
                    },
                    {
                        "name": "Padhraic Smyth"
                    }
                ],
                "author_detail": {
                    "name": "Padhraic Smyth"
                },
                "author": "Padhraic Smyth",
                "arxiv_doi": "10.1038/s42256-024-00976-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-024-00976-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.13835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/",
                "arxiv_journal_ref": "Nat Mach Intell (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11817v2",
                "updated": "2025-02-13T08:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    11,
                    25,
                    3,
                    44,
                    0
                ],
                "published": "2024-01-22T10:26:14Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    10,
                    26,
                    14,
                    0,
                    22,
                    0
                ],
                "title": "Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models"
                },
                "summary": "Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "Sanjay Jain"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]