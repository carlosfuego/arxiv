[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v2",
                "updated": "2024-09-25T06:46:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    46,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.18964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18964v1",
                "updated": "2024-09-27T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    59,
                    57,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    59,
                    57,
                    4,
                    271,
                    0
                ],
                "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation"
                },
                "summary": "We present PhysGen, a novel image-to-video generation method that converts a\nsingle image and an input condition (e.g., force and torque applied to an\nobject in the image) to produce a realistic, physically plausible, and\ntemporally consistent video. Our key insight is to integrate model-based\nphysical simulation with a data-driven video generation process, enabling\nplausible image-space dynamics. At the heart of our system are three core\ncomponents: (i) an image understanding module that effectively captures the\ngeometry, materials, and physical parameters of the image; (ii) an image-space\ndynamics simulation model that utilizes rigid-body physics and inferred\nparameters to simulate realistic behaviors; and (iii) an image-based rendering\nand refinement module that leverages generative video diffusion to produce\nrealistic video footage featuring the simulated motion. The resulting videos\nare realistic in both physics and appearance and are even precisely\ncontrollable, showcasing superior results over existing data-driven\nimage-to-video generation works through quantitative comparison and\ncomprehensive user study. PhysGen's resulting videos can be used for various\ndownstream applications, such as turning an image into a realistic animation or\nallowing users to interact with the image and create various dynamics. Project\npage: https://stevenlsw.github.io/physgen/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PhysGen, a novel image-to-video generation method that converts a\nsingle image and an input condition (e.g., force and torque applied to an\nobject in the image) to produce a realistic, physically plausible, and\ntemporally consistent video. Our key insight is to integrate model-based\nphysical simulation with a data-driven video generation process, enabling\nplausible image-space dynamics. At the heart of our system are three core\ncomponents: (i) an image understanding module that effectively captures the\ngeometry, materials, and physical parameters of the image; (ii) an image-space\ndynamics simulation model that utilizes rigid-body physics and inferred\nparameters to simulate realistic behaviors; and (iii) an image-based rendering\nand refinement module that leverages generative video diffusion to produce\nrealistic video footage featuring the simulated motion. The resulting videos\nare realistic in both physics and appearance and are even precisely\ncontrollable, showcasing superior results over existing data-driven\nimage-to-video generation works through quantitative comparison and\ncomprehensive user study. PhysGen's resulting videos can be used for various\ndownstream applications, such as turning an image into a realistic animation or\nallowing users to interact with the image and create various dynamics. Project\npage: https://stevenlsw.github.io/physgen/"
                },
                "authors": [
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Zhongzheng Ren"
                    },
                    {
                        "name": "Saurabh Gupta"
                    },
                    {
                        "name": "Shenlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shenlong Wang"
                },
                "author": "Shenlong Wang",
                "arxiv_comment": "Accepted to ECCV 2024. Project page:\n  https://stevenlsw.github.io/physgen/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18961v1",
                "updated": "2024-09-27T17:59:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    59,
                    42,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:59:42Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    59,
                    42,
                    4,
                    271,
                    0
                ],
                "title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation"
                },
                "summary": "Unsupervised instance segmentation aims to segment distinct object instances\nin an image without relying on human-labeled data. This field has recently seen\nsignificant advancements, partly due to the strong local correspondences\nafforded by rich visual feature representations from self-supervised models\n(e.g., DINO). Recent state-of-the-art approaches use self-supervised features\nto represent images as graphs and solve a generalized eigenvalue system (i.e.,\nnormalized-cut) to generate foreground masks. While effective, this strategy is\nlimited by its attendant computational demands, leading to slow inference\nspeeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages\nself-supervised visual features to obtain initial groupings of patches and\napplies a strategic merging to these segments, aided by a sophisticated\nbackground-based mask pruning technique. ProMerge not only yields competitive\nresults but also offers a significant reduction in inference time compared to\nstate-of-the-art normalized-cut-based approaches. Furthermore, when training an\nobject detector using our mask predictions as pseudo-labels, the resulting\ndetector surpasses the current leading unsupervised model on various\nchallenging instance segmentation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised instance segmentation aims to segment distinct object instances\nin an image without relying on human-labeled data. This field has recently seen\nsignificant advancements, partly due to the strong local correspondences\nafforded by rich visual feature representations from self-supervised models\n(e.g., DINO). Recent state-of-the-art approaches use self-supervised features\nto represent images as graphs and solve a generalized eigenvalue system (i.e.,\nnormalized-cut) to generate foreground masks. While effective, this strategy is\nlimited by its attendant computational demands, leading to slow inference\nspeeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages\nself-supervised visual features to obtain initial groupings of patches and\napplies a strategic merging to these segments, aided by a sophisticated\nbackground-based mask pruning technique. ProMerge not only yields competitive\nresults but also offers a significant reduction in inference time compared to\nstate-of-the-art normalized-cut-based approaches. Furthermore, when training an\nobject detector using our mask predictions as pseudo-labels, the resulting\ndetector surpasses the current leading unsupervised model on various\nchallenging instance segmentation benchmarks."
                },
                "authors": [
                    {
                        "name": "Dylan Li"
                    },
                    {
                        "name": "Gyungin Shin"
                    }
                ],
                "author_detail": {
                    "name": "Gyungin Shin"
                },
                "author": "Gyungin Shin",
                "arxiv_comment": "ECCV2024 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18957v1",
                "updated": "2024-09-27T17:58:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:58:50Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LML: Language Model Learning a Dataset for Data-Augmented Prediction"
                },
                "summary": "This paper introduces a new approach to using Large Language Models (LLMs)\nfor classification tasks, which are typically handled using Machine Learning\n(ML) models. Unlike ML models that rely heavily on data cleaning and feature\nengineering, this method streamlines the process using LLMs. This paper\nproposes a new concept called \"Language Model Learning (LML)\" powered by a new\nmethod called \"Data-Augmented Prediction (DAP)\". The classification is\nperformed by LLMs using a method similar to humans manually exploring and\nunderstanding the data and deciding classifications using data as a reference.\nTraining data is summarized and evaluated to determine the features that lead\nto the classification of each label the most. In the process of DAP, the system\nuses the data summary to automatically create a query, which is used to\nretrieve relevant rows from the dataset. A classification is generated by the\nLLM using data summary and relevant rows, ensuring satisfactory accuracy even\nwith complex data. Usage of data summary and similar data in DAP ensures\ncontext-aware decision-making. The proposed method uses the words \"Act as an\nExplainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new approach to using Large Language Models (LLMs)\nfor classification tasks, which are typically handled using Machine Learning\n(ML) models. Unlike ML models that rely heavily on data cleaning and feature\nengineering, this method streamlines the process using LLMs. This paper\nproposes a new concept called \"Language Model Learning (LML)\" powered by a new\nmethod called \"Data-Augmented Prediction (DAP)\". The classification is\nperformed by LLMs using a method similar to humans manually exploring and\nunderstanding the data and deciding classifications using data as a reference.\nTraining data is summarized and evaluated to determine the features that lead\nto the classification of each label the most. In the process of DAP, the system\nuses the data summary to automatically create a query, which is used to\nretrieve relevant rows from the dataset. A classification is generated by the\nLLM using data summary and relevant rows, ensuring satisfactory accuracy even\nwith complex data. Usage of data summary and similar data in DAP ensures\ncontext-aware decision-making. The proposed method uses the words \"Act as an\nExplainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP"
                },
                "authors": [
                    {
                        "name": "Praneeth Vadlapati"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vadlapati"
                },
                "author": "Praneeth Vadlapati",
                "arxiv_comment": "First version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18943v1",
                "updated": "2024-09-27T17:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:44:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models"
                },
                "summary": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler."
                },
                "authors": [
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "yuelin bai"
                    },
                    {
                        "name": "Run Luo"
                    },
                    {
                        "name": "Longze Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18938v1",
                "updated": "2024-09-27T17:38:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding"
                },
                "summary": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding."
                },
                "authors": [
                    {
                        "name": "Heqing Zou"
                    },
                    {
                        "name": "Tianze Luo"
                    },
                    {
                        "name": "Guiyang Xie"
                    },
                    {
                        "name": "Victor"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Guangcong Wang"
                    },
                    {
                        "name": "Juanyang Chen"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Hansheng Zhang"
                    },
                    {
                        "name": "Huaijian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huaijian Zhang"
                },
                "arxiv_affiliation": "Xiao Jie",
                "author": "Huaijian Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03332v2",
                "updated": "2024-09-27T17:34:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    34,
                    20,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-05T14:45:07Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    14,
                    45,
                    7,
                    2,
                    157,
                    0
                ],
                "title": "Interpreting Mass and Radius Measurements of Neutron Stars with Dark\n  Matter Halos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Mass and Radius Measurements of Neutron Stars with Dark\n  Matter Halos"
                },
                "summary": "The high densities of neutron stars (NSs) could provide astrophysical\nlocations for dark matter (DM) to accumulate. Depending on the DM model, these\nDM admixed NSs (DANSs) could have significantly different properties than pure\nbaryonic NSs, accessible through X-ray observations of rotation-powered\npulsars. We adopt the two-fluid formalism in general relativity to numerically\nsimulate stable configurations of DANSs, assuming a fermionic equation of state\n(EOS) for the DM with repulsive self-interaction. The distribution of DM in the\nDANS as a halo affects the path of X-rays emitted from hot spots on the visible\nbaryonic surface causing notable changes in the pulse profile observed by\ntelescopes such as NICER, compared to pure baryonic NSs. We explore how various\nDM models affect the DM mass distribution, leading to different types of dark\nhalos. We quantify the deviation in observed X-ray flux from stars with each of\nthese halos. We identify the pitfalls in interpreting mass and radius\nmeasurements of NSs inferred from electromagnetic radiation and constraining\nthe baryonic matter EOS, if these dark halos exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high densities of neutron stars (NSs) could provide astrophysical\nlocations for dark matter (DM) to accumulate. Depending on the DM model, these\nDM admixed NSs (DANSs) could have significantly different properties than pure\nbaryonic NSs, accessible through X-ray observations of rotation-powered\npulsars. We adopt the two-fluid formalism in general relativity to numerically\nsimulate stable configurations of DANSs, assuming a fermionic equation of state\n(EOS) for the DM with repulsive self-interaction. The distribution of DM in the\nDANS as a halo affects the path of X-rays emitted from hot spots on the visible\nbaryonic surface causing notable changes in the pulse profile observed by\ntelescopes such as NICER, compared to pure baryonic NSs. We explore how various\nDM models affect the DM mass distribution, leading to different types of dark\nhalos. We quantify the deviation in observed X-ray flux from stars with each of\nthese halos. We identify the pitfalls in interpreting mass and radius\nmeasurements of NSs inferred from electromagnetic radiation and constraining\nthe baryonic matter EOS, if these dark halos exist."
                },
                "authors": [
                    {
                        "name": "Shafayat Shawqi"
                    },
                    {
                        "name": "Sharon M. Morsink"
                    }
                ],
                "author_detail": {
                    "name": "Sharon M. Morsink"
                },
                "author": "Sharon M. Morsink",
                "arxiv_comment": "Accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13027v2",
                "updated": "2024-09-27T17:29:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    29,
                    43,
                    4,
                    271,
                    0
                ],
                "published": "2024-07-17T21:28:20Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    21,
                    28,
                    20,
                    2,
                    199,
                    0
                ],
                "title": "SpaRED benchmark: Enhancing Gene Expression Prediction from Histology\n  Images with Spatial Transcriptomics Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpaRED benchmark: Enhancing Gene Expression Prediction from Histology\n  Images with Spatial Transcriptomics Completion"
                },
                "summary": "Spatial Transcriptomics is a novel technology that aligns histology images\nwith spatially resolved gene expression profiles. Although groundbreaking, it\nstruggles with gene capture yielding high corruption in acquired data. Given\npotential applications, recent efforts have focused on predicting\ntranscriptomic profiles solely from histology images. However, differences in\ndatabases, preprocessing techniques, and training hyperparameters hinder a fair\ncomparison between methods. To address these challenges, we present a\nsystematically curated and processed database collected from 26 public sources,\nrepresenting an 8.6-fold increase compared to previous works. Additionally, we\npropose a state-of-the-art transformer based completion technique for inferring\nmissing gene expression, which significantly boosts the performance of\ntranscriptomic profile predictions across all datasets. Altogether, our\ncontributions constitute the most comprehensive benchmark of gene expression\nprediction from histology images to date and a stepping stone for future\nresearch on spatial transcriptomics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Transcriptomics is a novel technology that aligns histology images\nwith spatially resolved gene expression profiles. Although groundbreaking, it\nstruggles with gene capture yielding high corruption in acquired data. Given\npotential applications, recent efforts have focused on predicting\ntranscriptomic profiles solely from histology images. However, differences in\ndatabases, preprocessing techniques, and training hyperparameters hinder a fair\ncomparison between methods. To address these challenges, we present a\nsystematically curated and processed database collected from 26 public sources,\nrepresenting an 8.6-fold increase compared to previous works. Additionally, we\npropose a state-of-the-art transformer based completion technique for inferring\nmissing gene expression, which significantly boosts the performance of\ntranscriptomic profile predictions across all datasets. Altogether, our\ncontributions constitute the most comprehensive benchmark of gene expression\nprediction from histology images to date and a stepping stone for future\nresearch on spatial transcriptomics."
                },
                "authors": [
                    {
                        "name": "Gabriel Mejia"
                    },
                    {
                        "name": "Daniela Ruiz"
                    },
                    {
                        "name": "Paula Cárdenas"
                    },
                    {
                        "name": "Leonardo Manrique"
                    },
                    {
                        "name": "Daniela Vega"
                    },
                    {
                        "name": "Pablo Arbeláez"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Arbeláez"
                },
                "author": "Pablo Arbeláez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18924v1",
                "updated": "2024-09-27T17:17:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:17:15Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow"
                },
                "summary": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p<0.1), and stability (ANOVA F-value\n0.782, p<0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p<0.1), and stability (ANOVA F-value\n0.782, p<0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration."
                },
                "authors": [
                    {
                        "name": "Huizi Yu"
                    },
                    {
                        "name": "Jiayan Zhou"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Anye Shi"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Guang Chen"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Trisha Gupte"
                    },
                    {
                        "name": "Ming-Li Chen"
                    },
                    {
                        "name": "Zahra Azizi"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Themistocles L. Assimes"
                    },
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Lizhou Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lizhou Fan"
                },
                "author": "Lizhou Fan",
                "arxiv_comment": "42 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18911v1",
                "updated": "2024-09-27T16:54:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    36,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:54:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "Soft Measures for Extracting Causal Collective Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Measures for Extracting Causal Collective Intelligence"
                },
                "summary": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP."
                },
                "authors": [
                    {
                        "name": "Maryam Berijanian"
                    },
                    {
                        "name": "Spencer Dork"
                    },
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Michael Riley Millikan"
                    },
                    {
                        "name": "Ashlin Riggs"
                    },
                    {
                        "name": "Aadarsh Swaminathan"
                    },
                    {
                        "name": "Sarah L. Gibbs"
                    },
                    {
                        "name": "Scott E. Friedman"
                    },
                    {
                        "name": "Nathan Brugnone"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Brugnone"
                },
                "author": "Nathan Brugnone",
                "arxiv_comment": "Camera-ready version accepted for publication in the EMNLP 2024\n  Workshop NLP4Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09299v2",
                "updated": "2024-09-27T16:54:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-14T16:41:35Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    16,
                    41,
                    35,
                    2,
                    45,
                    0
                ],
                "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code"
                },
                "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets."
                },
                "authors": [
                    {
                        "name": "Vahid Majdinasab"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "Accepted for publication in TOSEM (ACM Transactions on Software\n  Engineering and Methodology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18908v1",
                "updated": "2024-09-27T16:46:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    46,
                    2,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:46:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    46,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "Inference with Sequential Monte-Carlo Computation of $p$-values: Fast\n  and Valid Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Sequential Monte-Carlo Computation of $p$-values: Fast\n  and Valid Approaches"
                },
                "summary": "Hypothesis tests calibrated by (re)sampling methods (such as permutation,\nrank and bootstrap tests) are useful tools for statistical analysis, at the\ncomputational cost of requiring Monte-Carlo sampling for calibration. It is\ncommon and almost universal practice to execute such tests with predetermined\nand large number of Monte-Carlo samples, and disregard any randomness from this\nsampling at the time of drawing and reporting inference. At best, this approach\nleads to computational inefficiency, and at worst to invalid inference. That\nbeing said, a number of approaches in the literature have been proposed to\nadaptively guide analysts in choosing the number of Monte-Carlo samples, by\nsequentially deciding when to stop collecting samples and draw inference. These\nworks introduce varying competing notions of what constitutes \"valid\"\ninference, complicating the landscape for analysts seeking suitable\nmethodology. Furthermore, the majority of these approaches solely guarantee a\nmeaningful estimate of the testing outcome, not the $p$-value itself\n$\\unicode{x2014}$ which is insufficient for many practical applications. In\nthis paper, we survey the relevant literature, and build bridges between the\nscattered validity notions, highlighting some of their complementary roles. We\nalso introduce a new practical methodology that provides an estimate of the\n$p$-value of the Monte-Carlo test, endowed with practically relevant validity\nguarantees. Moreover, our methodology is sequential, updating the $p$-value\nestimate after each new Monte-Carlo sample has been drawn, while retaining\nimportant validity guarantees regardless of the selected stopping time. We\nconclude this paper with a set of recommendations for the practitioner, both in\nterms of selection of methodology and manner of reporting results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis tests calibrated by (re)sampling methods (such as permutation,\nrank and bootstrap tests) are useful tools for statistical analysis, at the\ncomputational cost of requiring Monte-Carlo sampling for calibration. It is\ncommon and almost universal practice to execute such tests with predetermined\nand large number of Monte-Carlo samples, and disregard any randomness from this\nsampling at the time of drawing and reporting inference. At best, this approach\nleads to computational inefficiency, and at worst to invalid inference. That\nbeing said, a number of approaches in the literature have been proposed to\nadaptively guide analysts in choosing the number of Monte-Carlo samples, by\nsequentially deciding when to stop collecting samples and draw inference. These\nworks introduce varying competing notions of what constitutes \"valid\"\ninference, complicating the landscape for analysts seeking suitable\nmethodology. Furthermore, the majority of these approaches solely guarantee a\nmeaningful estimate of the testing outcome, not the $p$-value itself\n$\\unicode{x2014}$ which is insufficient for many practical applications. In\nthis paper, we survey the relevant literature, and build bridges between the\nscattered validity notions, highlighting some of their complementary roles. We\nalso introduce a new practical methodology that provides an estimate of the\n$p$-value of the Monte-Carlo test, endowed with practically relevant validity\nguarantees. Moreover, our methodology is sequential, updating the $p$-value\nestimate after each new Monte-Carlo sample has been drawn, while retaining\nimportant validity guarantees regardless of the selected stopping time. We\nconclude this paper with a set of recommendations for the practitioner, both in\nterms of selection of methodology and manner of reporting results."
                },
                "authors": [
                    {
                        "name": "Ivo V. Stoepker"
                    },
                    {
                        "name": "Rui M. Castro"
                    }
                ],
                "author_detail": {
                    "name": "Rui M. Castro"
                },
                "author": "Rui M. Castro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62L10, 62L12, 62G10, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18892v1",
                "updated": "2024-09-27T16:29:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    29,
                    12,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:29:12Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    29,
                    12,
                    4,
                    271,
                    0
                ],
                "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation"
                },
                "summary": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Lin"
                    },
                    {
                        "name": "Shuyi Xie"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04928v2",
                "updated": "2024-09-27T16:16:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    16,
                    17,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-07T23:06:23Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    23,
                    6,
                    23,
                    5,
                    251,
                    0
                ],
                "title": "Evolutionary emergence of biological intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary emergence of biological intelligence"
                },
                "summary": "Characterising the intelligence of biological organisms is challenging. This\nwork considers intelligent algorithms developed evolutionarily within neural\nsystems. Mathematical analyses unveil a natural equivalence between canonical\nneural networks, variational Bayesian inference under a class of partially\nobservable Markov decision processes, and differentiable Turing machines, by\nshowing that they minimise the shared Helmholtz energy. Consequently, canonical\nneural networks can biologically plausibly equip Turing machines and conduct\nvariational Bayesian inferences of external Turing machines in the environment.\nApplying Helmholtz energy minimisation at the species level facilitates\nderiving active Bayesian model selection inherent in natural selection,\nresulting in the emergence of adaptive algorithms. In particular, canonical\nneural networks with two mental actions can separately memorise transition\nmappings of multiple external Turing machines to form a universal machine.\nThese propositions were corroborated by numerical simulations of algorithm\nimplementation and neural network evolution. These notions offer a universal\ncharacterisation of biological intelligence emerging from evolution in terms of\nBayesian model selection and belief updating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising the intelligence of biological organisms is challenging. This\nwork considers intelligent algorithms developed evolutionarily within neural\nsystems. Mathematical analyses unveil a natural equivalence between canonical\nneural networks, variational Bayesian inference under a class of partially\nobservable Markov decision processes, and differentiable Turing machines, by\nshowing that they minimise the shared Helmholtz energy. Consequently, canonical\nneural networks can biologically plausibly equip Turing machines and conduct\nvariational Bayesian inferences of external Turing machines in the environment.\nApplying Helmholtz energy minimisation at the species level facilitates\nderiving active Bayesian model selection inherent in natural selection,\nresulting in the emergence of adaptive algorithms. In particular, canonical\nneural networks with two mental actions can separately memorise transition\nmappings of multiple external Turing machines to form a universal machine.\nThese propositions were corroborated by numerical simulations of algorithm\nimplementation and neural network evolution. These notions offer a universal\ncharacterisation of biological intelligence emerging from evolution in terms of\nBayesian model selection and belief updating."
                },
                "authors": [
                    {
                        "name": "Takuya Isomura"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Isomura"
                },
                "author": "Takuya Isomura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18879v1",
                "updated": "2024-09-27T16:13:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    13,
                    43,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:13:43Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    13,
                    43,
                    4,
                    271,
                    0
                ],
                "title": "Inferring Drug-induced Plasticity via Drug Screen Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Drug-induced Plasticity via Drug Screen Data"
                },
                "summary": "Resistance to therapy remains a significant challenge in cancer treatment,\noften due to the presence of a stem-like cell population that drives tumor\nrecurrence post-treatment. Moreover, many anticancer drugs inadvertently induce\nplasticity, reverting differentiated cancer cells to drug-resistant stem-like\nstates. Addressing this phenomenon is crucial for advancing cancer\ntherapeutics. In this study, we introduce a robust statistical framework based\non multi-type branching processes that can dissect tumor dynamics and drug\neffects from high throughput drug screening data. Through comprehensive in\nsilico experiments, we show the efficacy of our framework in estimating\nparameters governing population dynamics and drug responses in a heterogeneous\ntumor population where cell state transitions are influenced by the drug.\nFinally, using recent in vitro data involving AGS-SORE6+/- cells treated with\nciclopirox olamine, we show how our framework can be used to confirm the\npresence of drug-induced cell plasticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resistance to therapy remains a significant challenge in cancer treatment,\noften due to the presence of a stem-like cell population that drives tumor\nrecurrence post-treatment. Moreover, many anticancer drugs inadvertently induce\nplasticity, reverting differentiated cancer cells to drug-resistant stem-like\nstates. Addressing this phenomenon is crucial for advancing cancer\ntherapeutics. In this study, we introduce a robust statistical framework based\non multi-type branching processes that can dissect tumor dynamics and drug\neffects from high throughput drug screening data. Through comprehensive in\nsilico experiments, we show the efficacy of our framework in estimating\nparameters governing population dynamics and drug responses in a heterogeneous\ntumor population where cell state transitions are influenced by the drug.\nFinally, using recent in vitro data involving AGS-SORE6+/- cells treated with\nciclopirox olamine, we show how our framework can be used to confirm the\npresence of drug-induced cell plasticity."
                },
                "authors": [
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Einar Bjarki Gunnarsson"
                    },
                    {
                        "name": "Jasmine Foo"
                    },
                    {
                        "name": "Kevin Leder"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leder"
                },
                "author": "Kevin Leder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17827v2",
                "updated": "2024-09-27T16:07:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    7,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T13:26:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text"
                },
                "summary": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Wang"
                    },
                    {
                        "name": "Bradford Levy"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Levy"
                },
                "author": "Bradford Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18869v1",
                "updated": "2024-09-27T16:06:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    6,
                    11,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:06:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    6,
                    11,
                    4,
                    271,
                    0
                ],
                "title": "Emu3: Next-Token Prediction is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emu3: Next-Token Prediction is All You Need"
                },
                "summary": "While next-token prediction is considered a promising path towards artificial\ngeneral intelligence, it has struggled to excel in multimodal tasks, which are\nstill dominated by diffusion models (e.g., Stable Diffusion) and compositional\napproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a\nnew suite of state-of-the-art multimodal models trained solely with next-token\nprediction. By tokenizing images, text, and videos into a discrete space, we\ntrain a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 outperforms several well-established task-specific models in both\ngeneration and perception tasks, surpassing flagship models such as SDXL and\nLLaVA-1.6, while eliminating the need for diffusion or compositional\narchitectures. Emu3 is also capable of generating high-fidelity video via\npredicting the next token in a video sequence. We simplify complex multimodal\nmodel designs by converging on a singular focus: tokens, unlocking great\npotential for scaling both during training and inference. Our results\ndemonstrate that next-token prediction is a promising path towards building\ngeneral multimodal intelligence beyond language. We open-source key techniques\nand models to support further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While next-token prediction is considered a promising path towards artificial\ngeneral intelligence, it has struggled to excel in multimodal tasks, which are\nstill dominated by diffusion models (e.g., Stable Diffusion) and compositional\napproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a\nnew suite of state-of-the-art multimodal models trained solely with next-token\nprediction. By tokenizing images, text, and videos into a discrete space, we\ntrain a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 outperforms several well-established task-specific models in both\ngeneration and perception tasks, surpassing flagship models such as SDXL and\nLLaVA-1.6, while eliminating the need for diffusion or compositional\narchitectures. Emu3 is also capable of generating high-fidelity video via\npredicting the next token in a video sequence. We simplify complex multimodal\nmodel designs by converging on a singular focus: tokens, unlocking great\npotential for scaling both during training and inference. Our results\ndemonstrate that next-token prediction is a promising path towards building\ngeneral multimodal intelligence beyond language. We open-source key techniques\nand models to support further research in this direction."
                },
                "authors": [
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Zhengxiong Luo"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Jinsheng Wang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Qiying Yu"
                    },
                    {
                        "name": "Yingli Zhao"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Xuebin Min"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Boya Wu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Liangdong Wang"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Yonghua Lin"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyuan Wang"
                },
                "author": "Zhongyuan Wang",
                "arxiv_comment": "Project Page: https://emu.baai.ac.cn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03291v2",
                "updated": "2024-09-27T16:04:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    4,
                    40,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-05T06:55:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts"
                },
                "summary": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection)."
                },
                "authors": [
                    {
                        "name": "Henrique Da Silva Gameiro"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "arxiv_comment": "20 pages, 7 tables, 13 figures, under consideration for EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09371v2",
                "updated": "2024-09-27T15:59:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    59,
                    13,
                    4,
                    271,
                    0
                ],
                "published": "2024-07-12T15:52:12Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    15,
                    52,
                    12,
                    4,
                    194,
                    0
                ],
                "title": "Computationally Efficient Estimation of Large Probit Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally Efficient Estimation of Large Probit Models"
                },
                "summary": "Probit models are useful for modeling correlated discrete responses in many\ndisciplines, including consumer choice data in economics and marketing.\nHowever, the Gaussian latent variable feature of probit models coupled with\nidentification constraints pose significant computational challenges for its\nestimation and inference, especially when the dimension of the discrete\nresponse variable is large. In this paper, we propose a computationally\nefficient Expectation-Maximization (EM) algorithm for estimating large probit\nmodels. Our work is distinct from existing methods in two important aspects.\nFirst, instead of simulation or sampling methods, we apply and customize\nexpectation propagation (EP), a deterministic method originally proposed for\napproximate Bayesian inference, to estimate moments of the truncated\nmultivariate normal (TMVN) in the E (expectation) step. Second, we take\nadvantage of a symmetric identification condition to transform the constrained\noptimization problem in the M (maximization) step into a one-dimensional\nproblem, which is solved efficiently using Newton's method instead of\noff-the-shelf solvers. Our method enables the analysis of correlated choice\ndata in the presence of more than 100 alternatives, which is a reasonable size\nin modern applications, such as online shopping and booking platforms, but has\nbeen difficult in practice with probit models. We apply our probit estimation\nmethod to study ordering effects in hotel search results on Expedia's online\nbooking platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probit models are useful for modeling correlated discrete responses in many\ndisciplines, including consumer choice data in economics and marketing.\nHowever, the Gaussian latent variable feature of probit models coupled with\nidentification constraints pose significant computational challenges for its\nestimation and inference, especially when the dimension of the discrete\nresponse variable is large. In this paper, we propose a computationally\nefficient Expectation-Maximization (EM) algorithm for estimating large probit\nmodels. Our work is distinct from existing methods in two important aspects.\nFirst, instead of simulation or sampling methods, we apply and customize\nexpectation propagation (EP), a deterministic method originally proposed for\napproximate Bayesian inference, to estimate moments of the truncated\nmultivariate normal (TMVN) in the E (expectation) step. Second, we take\nadvantage of a symmetric identification condition to transform the constrained\noptimization problem in the M (maximization) step into a one-dimensional\nproblem, which is solved efficiently using Newton's method instead of\noff-the-shelf solvers. Our method enables the analysis of correlated choice\ndata in the presence of more than 100 alternatives, which is a reasonable size\nin modern applications, such as online shopping and booking platforms, but has\nbeen difficult in practice with probit models. We apply our probit estimation\nmethod to study ordering effects in hotel search results on Expedia's online\nbooking platform."
                },
                "authors": [
                    {
                        "name": "Patrick Ding"
                    },
                    {
                        "name": "Guido Imbens"
                    },
                    {
                        "name": "Zhaonan Qu"
                    },
                    {
                        "name": "Yinyu Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yinyu Ye"
                },
                "author": "Yinyu Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18858v1",
                "updated": "2024-09-27T15:53:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    55,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:53:55Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    55,
                    4,
                    271,
                    0
                ],
                "title": "Predicting and analyzing memorization within fine-tuned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting and analyzing memorization within fine-tuned Large Language\n  Models"
                },
                "summary": "Large Language Models have received significant attention due to their\nabilities to solve a wide range of complex tasks. However these models memorize\na significant proportion of their training data, posing a serious threat when\ndisclosed at inference time. To mitigate this unintended memorization, it is\ncrucial to understand what elements are memorized and why. Most existing works\nprovide a posteriori explanations, which has a limited interest in practice. To\naddress this gap, we propose a new approach based on sliced mutual information\nto detect memorized samples a priori, in a classification setting. It is\nefficient from the early stages of training, and is readily adaptable to\npractical scenarios. Our method is supported by new theoretical results that we\ndemonstrate, and requires a low computational budget. We obtain strong\nempirical results, paving the way for systematic inspection and protection of\nthese vulnerable samples before memorization happens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have received significant attention due to their\nabilities to solve a wide range of complex tasks. However these models memorize\na significant proportion of their training data, posing a serious threat when\ndisclosed at inference time. To mitigate this unintended memorization, it is\ncrucial to understand what elements are memorized and why. Most existing works\nprovide a posteriori explanations, which has a limited interest in practice. To\naddress this gap, we propose a new approach based on sliced mutual information\nto detect memorized samples a priori, in a classification setting. It is\nefficient from the early stages of training, and is readily adaptable to\npractical scenarios. Our method is supported by new theoretical results that we\ndemonstrate, and requires a low computational budget. We obtain strong\nempirical results, paving the way for systematic inspection and protection of\nthese vulnerable samples before memorization happens."
                },
                "authors": [
                    {
                        "name": "Jérémie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Aymen Shabou"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18857v1",
                "updated": "2024-09-27T15:53:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:53:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "Mitigating Selection Bias with Node Pruning and Auxiliary Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Selection Bias with Node Pruning and Auxiliary Options"
                },
                "summary": "Large language models (LLMs) often show unwarranted preference for certain\nchoice options when responding to multiple-choice questions, posing significant\nreliability concerns in LLM-automated systems. To mitigate this selection bias\nproblem, previous solutions utilized debiasing methods to adjust the model's\ninput and/or output. Our work, in contrast, investigates the model's internal\nrepresentation of the selection bias. Specifically, we introduce a novel\ndebiasing approach, Bias Node Pruning (BNP), which eliminates the linear layer\nparameters that contribute to the bias. Furthermore, we present Auxiliary\nOption Injection (AOI), a simple yet effective input modification technique for\ndebiasing, which is compatible even with black-box LLMs. To provide a more\nsystematic evaluation of selection bias, we review existing metrics and\nintroduce Choice Kullback-Leibler Divergence (CKLD), which addresses the\ninsensitivity of the commonly used metrics to label imbalance. Experiments show\nthat our methods are robust and adaptable across various datasets when applied\nto three LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often show unwarranted preference for certain\nchoice options when responding to multiple-choice questions, posing significant\nreliability concerns in LLM-automated systems. To mitigate this selection bias\nproblem, previous solutions utilized debiasing methods to adjust the model's\ninput and/or output. Our work, in contrast, investigates the model's internal\nrepresentation of the selection bias. Specifically, we introduce a novel\ndebiasing approach, Bias Node Pruning (BNP), which eliminates the linear layer\nparameters that contribute to the bias. Furthermore, we present Auxiliary\nOption Injection (AOI), a simple yet effective input modification technique for\ndebiasing, which is compatible even with black-box LLMs. To provide a more\nsystematic evaluation of selection bias, we review existing metrics and\nintroduce Choice Kullback-Leibler Divergence (CKLD), which addresses the\ninsensitivity of the commonly used metrics to label imbalance. Experiments show\nthat our methods are robust and adaptable across various datasets when applied\nto three LLMs."
                },
                "authors": [
                    {
                        "name": "Hyeong Kyu Choi"
                    },
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Stephanie Eckman"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18849v1",
                "updated": "2024-09-27T15:45:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    45,
                    17,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:45:17Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    45,
                    17,
                    4,
                    271,
                    0
                ],
                "title": "Temperature anisotropy instabilities driven by intermittent velocity\n  shears in the solar wind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temperature anisotropy instabilities driven by intermittent velocity\n  shears in the solar wind"
                },
                "summary": "Where and under what conditions the transfer of energy between\nelectromagnetic fields and particles takes place in the solar wind remains an\nopen question. We investigate the conditions that promote the growth of kinetic\ninstabilities predicted by linear theory, to infer how turbulence and\ntemperature-anisotropy-driven instabilities are interrelated. Using a large\ndataset from Solar Orbiter, we introduce the radial rate of strain, a novel\nmeasure computed from single-spacecraft data, that we interpret as a proxy for\nthe double-adiabatic strain rate. The solar wind exhibits high absolute values\nof the radial rate of strain at locations with large temperature anisotropy. We\nmeasure the kurtosis and skewness of the radial rate of strain from the\nstatistical moments to show that it is non-Gaussian for unstable intervals and\nincreasingly intermittent at smaller scales with a power-law scaling. We\nconclude that the velocity field fluctuations in the solar wind contribute to\nthe presence of temperature anisotropy sufficient to create potentially\nunstable conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where and under what conditions the transfer of energy between\nelectromagnetic fields and particles takes place in the solar wind remains an\nopen question. We investigate the conditions that promote the growth of kinetic\ninstabilities predicted by linear theory, to infer how turbulence and\ntemperature-anisotropy-driven instabilities are interrelated. Using a large\ndataset from Solar Orbiter, we introduce the radial rate of strain, a novel\nmeasure computed from single-spacecraft data, that we interpret as a proxy for\nthe double-adiabatic strain rate. The solar wind exhibits high absolute values\nof the radial rate of strain at locations with large temperature anisotropy. We\nmeasure the kurtosis and skewness of the radial rate of strain from the\nstatistical moments to show that it is non-Gaussian for unstable intervals and\nincreasingly intermittent at smaller scales with a power-law scaling. We\nconclude that the velocity field fluctuations in the solar wind contribute to\nthe presence of temperature anisotropy sufficient to create potentially\nunstable conditions."
                },
                "authors": [
                    {
                        "name": "Simon Opie"
                    },
                    {
                        "name": "Daniel Verscharen"
                    },
                    {
                        "name": "Christopher H. K. Chen"
                    },
                    {
                        "name": "Christopher J. Owen"
                    },
                    {
                        "name": "Philip A. Isenberg"
                    },
                    {
                        "name": "Luca Sorriso-Valvo"
                    },
                    {
                        "name": "Luca Franci"
                    },
                    {
                        "name": "Lorenzo Matteini"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Matteini"
                },
                "author": "Lorenzo Matteini",
                "arxiv_comment": "Accepted for publication in the Journal of Plasma Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18838v1",
                "updated": "2024-09-27T15:34:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    34,
                    51,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:34:51Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    34,
                    51,
                    4,
                    271,
                    0
                ],
                "title": "Lipschitz inextendibility of weak null singularities from curvature\n  blow-up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lipschitz inextendibility of weak null singularities from curvature\n  blow-up"
                },
                "summary": "We prove the $C^{0,1}_{\\mathrm{loc}}$-inextendibility of weak null\nsingularities without any symmetry assumptions. The proof introduces a new\nstrategy to infer $C^{0,1}_{\\mathrm{loc}}$-inextendibility from the blow-up of\ncurvature. The assumed blow-up is expected to be satisfied for weak null\nsingularities in the interior of generic rotating black holes. Thus, we expect\nthe result presented here to directly contribute to the resolution of the\n$C^{0,1}_{\\mathrm{loc}}$-formulation of the strong cosmic censorship conjecture\nin a neighbourhood of subextremal Kerr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove the $C^{0,1}_{\\mathrm{loc}}$-inextendibility of weak null\nsingularities without any symmetry assumptions. The proof introduces a new\nstrategy to infer $C^{0,1}_{\\mathrm{loc}}$-inextendibility from the blow-up of\ncurvature. The assumed blow-up is expected to be satisfied for weak null\nsingularities in the interior of generic rotating black holes. Thus, we expect\nthe result presented here to directly contribute to the resolution of the\n$C^{0,1}_{\\mathrm{loc}}$-formulation of the strong cosmic censorship conjecture\nin a neighbourhood of subextremal Kerr."
                },
                "authors": [
                    {
                        "name": "Jan Sbierski"
                    }
                ],
                "author_detail": {
                    "name": "Jan Sbierski"
                },
                "author": "Jan Sbierski",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18828v1",
                "updated": "2024-09-27T15:22:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    22,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:22:44Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    22,
                    44,
                    4,
                    271,
                    0
                ],
                "title": "MECG-E: Mamba-based ECG Enhancer for Baseline Wander Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECG-E: Mamba-based ECG Enhancer for Baseline Wander Removal"
                },
                "summary": "Electrocardiogram (ECG) is an important non-invasive method for diagnosing\ncardiovascular disease. However, ECG signals are susceptible to noise\ncontamination, such as electrical interference or signal wandering, which\nreduces diagnostic accuracy. Various ECG denoising methods have been proposed,\nbut most existing methods yield suboptimal performance under very noisy\nconditions or require several steps during inference, leading to latency during\nonline processing. In this paper, we propose a novel ECG denoising model,\nnamely Mamba-based ECG Enhancer (MECG-E), which leverages the Mamba\narchitecture known for its fast inference and outstanding nonlinear mapping\ncapabilities. Experimental results indicate that MECG-E surpasses several\nwell-known existing models across multiple metrics under different noise\nconditions. Additionally, MECG-E requires less inference time than\nstate-of-the-art diffusion-based ECG denoisers, demonstrating the model's\nfunctionality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) is an important non-invasive method for diagnosing\ncardiovascular disease. However, ECG signals are susceptible to noise\ncontamination, such as electrical interference or signal wandering, which\nreduces diagnostic accuracy. Various ECG denoising methods have been proposed,\nbut most existing methods yield suboptimal performance under very noisy\nconditions or require several steps during inference, leading to latency during\nonline processing. In this paper, we propose a novel ECG denoising model,\nnamely Mamba-based ECG Enhancer (MECG-E), which leverages the Mamba\narchitecture known for its fast inference and outstanding nonlinear mapping\ncapabilities. Experimental results indicate that MECG-E surpasses several\nwell-known existing models across multiple metrics under different noise\nconditions. Additionally, MECG-E requires less inference time than\nstate-of-the-art diffusion-based ECG denoisers, demonstrating the model's\nfunctionality and efficiency."
                },
                "authors": [
                    {
                        "name": "Kuo-Hsuan Hung"
                    },
                    {
                        "name": "Kuan-Chen Wang"
                    },
                    {
                        "name": "Kai-Chun Liu"
                    },
                    {
                        "name": "Wei-Lun Chen"
                    },
                    {
                        "name": "Xugang Lu"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Chii-Wann Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chii-Wann Lin"
                },
                "author": "Chii-Wann Lin",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15204v2",
                "updated": "2024-09-27T15:19:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    19,
                    23,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T16:51:43Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    16,
                    51,
                    43,
                    0,
                    267,
                    0
                ],
                "title": "RAMBO: Enhancing RAG-based Repository-Level Method Body Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAMBO: Enhancing RAG-based Repository-Level Method Body Completion"
                },
                "summary": "Code completion is essential in software development, helping developers by\npredicting code snippets based on context. Among completion tasks, Method Body\nCompletion (MBC) is particularly challenging as it involves generating complete\nmethod bodies based on their signatures and context. This task becomes\nsignificantly harder in large repositories, where method bodies must integrate\nrepositoryspecific elements such as custom APIs, inter-module dependencies, and\nproject-specific conventions. In this paper, we introduce RAMBO, a novel\nRAG-based approach for repository-level MBC. Instead of retrieving similar\nmethod bodies, RAMBO identifies essential repository-specific elements, such as\nclasses, methods, and variables/fields, and their relevant usages. By\nincorporating these elements and their relevant usages into the code generation\nprocess, RAMBO ensures more accurate and contextually relevant method bodies.\nOur experimental results with leading code LLMs across 40 Java projects show\nthat RAMBO significantly outperformed the state-of-the-art repository-level MBC\napproaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in\nCompilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed\nRepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark\nfor repository-level MBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion is essential in software development, helping developers by\npredicting code snippets based on context. Among completion tasks, Method Body\nCompletion (MBC) is particularly challenging as it involves generating complete\nmethod bodies based on their signatures and context. This task becomes\nsignificantly harder in large repositories, where method bodies must integrate\nrepositoryspecific elements such as custom APIs, inter-module dependencies, and\nproject-specific conventions. In this paper, we introduce RAMBO, a novel\nRAG-based approach for repository-level MBC. Instead of retrieving similar\nmethod bodies, RAMBO identifies essential repository-specific elements, such as\nclasses, methods, and variables/fields, and their relevant usages. By\nincorporating these elements and their relevant usages into the code generation\nprocess, RAMBO ensures more accurate and contextually relevant method bodies.\nOur experimental results with leading code LLMs across 40 Java projects show\nthat RAMBO significantly outperformed the state-of-the-art repository-level MBC\napproaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in\nCompilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed\nRepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark\nfor repository-level MBC."
                },
                "authors": [
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Duc-Thieu Luu-Van"
                    },
                    {
                        "name": "Thanh-Phat Nguyen"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14744v2",
                "updated": "2024-09-27T15:10:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    10,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-23T16:13:33Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    13,
                    33,
                    3,
                    144,
                    0
                ],
                "title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View"
                },
                "summary": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents."
                },
                "authors": [
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haoyang Shang"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18812v1",
                "updated": "2024-09-27T15:04:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    4,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    4,
                    39,
                    4,
                    271,
                    0
                ],
                "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis"
                },
                "summary": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_comment": "12 pages, 3 figures, Accepted to JCDL 2024 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18798v1",
                "updated": "2024-09-27T14:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    53,
                    4,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    53,
                    4,
                    4,
                    271,
                    0
                ],
                "title": "Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public\n  Perceptions with BERTopic and GPT-4 Topic Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public\n  Perceptions with BERTopic and GPT-4 Topic Fine-Tuning"
                },
                "summary": "This study examined the public opinions of esports at the 2023 Asian Games\nand value co-creation during the event using an LLM-enhanced BERTopic modeling\nanalysis. We identified five major themes representing public perceptions, as\nwell as how major stakeholders co-created value within and beyond the esports\necosystem. Key findings highlighted the strategic use of social media marketing\nto influence public opinion and promote esports events and brands, emphasizing\nthe importance of event logistics and infrastructure. Additionally, the study\nrevealed the co-creation value contributed by stakeholders outside the\ntraditional esports ecosystem, particularly in promoting national\nrepresentation and performance. Our findings supported the ongoing efforts to\nlegitimize esports as a sport, noting that mainstream recognition remains a\nchallenge. The inclusion of esports as a medal event showcased broader\nacceptance and helped mitigate negative public perceptions. Moreover,\ncontributions from non-traditional stakeholders underscored the value of\ncross-subcultural collaborations in esports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examined the public opinions of esports at the 2023 Asian Games\nand value co-creation during the event using an LLM-enhanced BERTopic modeling\nanalysis. We identified five major themes representing public perceptions, as\nwell as how major stakeholders co-created value within and beyond the esports\necosystem. Key findings highlighted the strategic use of social media marketing\nto influence public opinion and promote esports events and brands, emphasizing\nthe importance of event logistics and infrastructure. Additionally, the study\nrevealed the co-creation value contributed by stakeholders outside the\ntraditional esports ecosystem, particularly in promoting national\nrepresentation and performance. Our findings supported the ongoing efforts to\nlegitimize esports as a sport, noting that mainstream recognition remains a\nchallenge. The inclusion of esports as a medal event showcased broader\nacceptance and helped mitigate negative public perceptions. Moreover,\ncontributions from non-traditional stakeholders underscored the value of\ncross-subcultural collaborations in esports."
                },
                "authors": [
                    {
                        "name": "Tyreal Yizhou Qian"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Weizhe Li"
                    },
                    {
                        "name": "Chenglong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglong Xu"
                },
                "author": "Chenglong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16807v2",
                "updated": "2024-09-27T14:50:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    50,
                    59,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-25T17:52:39Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    52,
                    39,
                    3,
                    116,
                    0
                ],
                "title": "Improving Diversity of Commonsense Generation by Large Language Models\n  via In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Diversity of Commonsense Generation by Large Language Models\n  via In-Context Learning"
                },
                "summary": "Generative Commonsense Reasoning (GCR) requires a model to reason about a\nsituation using commonsense knowledge, while generating coherent sentences.\nAlthough the quality of the generated sentences is crucial, the diversity of\nthe generation is equally important because it reflects the model's ability to\nuse a range of commonsense knowledge facts. Large Language Models (LLMs) have\nshown proficiency in enhancing the generation quality across various tasks\nthrough in-context learning (ICL) using given examples without the need for any\nfine-tuning. However, the diversity aspect in LLM outputs has not been\nsystematically studied before. To address this, we propose a simple method that\ndiversifies the LLM generations, while preserving their quality. Experimental\nresults on three benchmark GCR datasets show that our method achieves an ideal\nbalance between the quality and diversity. Moreover, the sentences generated by\nour proposed method can be used as training data to improve diversity in\nexisting commonsense generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Commonsense Reasoning (GCR) requires a model to reason about a\nsituation using commonsense knowledge, while generating coherent sentences.\nAlthough the quality of the generated sentences is crucial, the diversity of\nthe generation is equally important because it reflects the model's ability to\nuse a range of commonsense knowledge facts. Large Language Models (LLMs) have\nshown proficiency in enhancing the generation quality across various tasks\nthrough in-context learning (ICL) using given examples without the need for any\nfine-tuning. However, the diversity aspect in LLM outputs has not been\nsystematically studied before. To address this, we propose a simple method that\ndiversifies the LLM generations, while preserving their quality. Experimental\nresults on three benchmark GCR datasets show that our method achieves an ideal\nbalance between the quality and diversity. Moreover, the sentences generated by\nour proposed method can be used as training data to improve diversity in\nexisting commonsense generators."
                },
                "authors": [
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Bei Peng"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "arxiv_comment": "EMNLP 2024 Findings, Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18794v1",
                "updated": "2024-09-27T14:47:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    47,
                    18,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:47:18Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    47,
                    18,
                    4,
                    271,
                    0
                ],
                "title": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs"
                },
                "summary": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Wenqi Lyu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Zerui Li"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18786v1",
                "updated": "2024-09-27T14:34:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    34,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:34:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    34,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "A Survey on the Honesty of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on the Honesty of Large Language Models"
                },
                "summary": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area."
                },
                "authors": [
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Project Page: https://github.com/SihengLi99/LLM-Honesty-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18783v1",
                "updated": "2024-09-27T14:30:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    30,
                    24,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:30:24Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    30,
                    24,
                    4,
                    271,
                    0
                ],
                "title": "DualDn: Dual-domain Denoising via Differentiable ISP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualDn: Dual-domain Denoising via Differentiable ISP"
                },
                "summary": "Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/"
                },
                "authors": [
                    {
                        "name": "Ruikang Li"
                    },
                    {
                        "name": "Yujin Wang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jinwei Gu"
                    },
                    {
                        "name": "Tianfan Xue"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Xue"
                },
                "author": "Tianfan Xue",
                "arxiv_comment": "Accepted at ECCV 2024, Project page:\n  https://openimaginglab.github.io/DualDn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05579v2",
                "updated": "2024-09-27T14:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    29,
                    13,
                    4,
                    271,
                    0
                ],
                "published": "2024-07-08T03:26:45Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    3,
                    26,
                    45,
                    0,
                    190,
                    0
                ],
                "title": "Probing Dark Energy Evolution Post-DESI 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Dark Energy Evolution Post-DESI 2024"
                },
                "summary": "We study the evidence for dark energy (DE) evolution at low redshift, using\nbaryonic acoustic oscillations (BAOs) from the DESI Early Data Release,\nPantheon+ Type Ia supernovae (SNe-Ia), and redshift space distortions (RSDs) to\nconstrain cosmological parameters. Furthermore, we make use of the angular\nacoustic scale to analyse the effect of introducing condensed CMB information\non the cosmological parameters informing DE evolution. The analysis is divided\ninto cases based on the variability of priors inferred from early-time physics.\nUsing a quadratic parametrisation, $X(z)$, for DE density, we find evidence for\nDE evolution in all cases, both with and without the angular acoustic scale. We\nreconstruct $X(z)$ using best fit parameters and find that DE density starts to\nexhibit dynamical behaviour at $z \\sim 0.5 $, assuming negative values beyond\n$z\\sim 1.5$. The data show no significant preference for $X(z)$CDM over a\n$\\Lambda$CDM, with both models performing equally well according to our chosen\nmetrics of reduced $\\chi^2$ and the Durbin-Watson statistic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the evidence for dark energy (DE) evolution at low redshift, using\nbaryonic acoustic oscillations (BAOs) from the DESI Early Data Release,\nPantheon+ Type Ia supernovae (SNe-Ia), and redshift space distortions (RSDs) to\nconstrain cosmological parameters. Furthermore, we make use of the angular\nacoustic scale to analyse the effect of introducing condensed CMB information\non the cosmological parameters informing DE evolution. The analysis is divided\ninto cases based on the variability of priors inferred from early-time physics.\nUsing a quadratic parametrisation, $X(z)$, for DE density, we find evidence for\nDE evolution in all cases, both with and without the angular acoustic scale. We\nreconstruct $X(z)$ using best fit parameters and find that DE density starts to\nexhibit dynamical behaviour at $z \\sim 0.5 $, assuming negative values beyond\n$z\\sim 1.5$. The data show no significant preference for $X(z)$CDM over a\n$\\Lambda$CDM, with both models performing equally well according to our chosen\nmetrics of reduced $\\chi^2$ and the Durbin-Watson statistic."
                },
                "authors": [
                    {
                        "name": "Lili Orchard"
                    },
                    {
                        "name": "Víctor H. Cárdenas"
                    }
                ],
                "author_detail": {
                    "name": "Víctor H. Cárdenas"
                },
                "author": "Víctor H. Cárdenas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13833v2",
                "updated": "2024-09-27T14:04:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    4,
                    35,
                    4,
                    271,
                    0
                ],
                "published": "2023-11-23T07:33:38Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    7,
                    33,
                    38,
                    3,
                    327,
                    0
                ],
                "title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models"
                },
                "summary": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept."
                },
                "authors": [
                    {
                        "name": "Saman Motamed"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18764v1",
                "updated": "2024-09-27T14:02:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    2,
                    48,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    2,
                    48,
                    4,
                    271,
                    0
                ],
                "title": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations"
                },
                "summary": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field."
                },
                "authors": [
                    {
                        "name": "James Ford"
                    },
                    {
                        "name": "Xingmeng Zhao"
                    },
                    {
                        "name": "Dan Schumacher"
                    },
                    {
                        "name": "Anthony Rios"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Rios"
                },
                "author": "Anthony Rios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18760v1",
                "updated": "2024-09-27T13:53:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    53,
                    23,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T13:53:23Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    53,
                    23,
                    4,
                    271,
                    0
                ],
                "title": "Forecasting Macroeconomic Dynamics using a Calibrated Data-Driven\n  Agent-based Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Macroeconomic Dynamics using a Calibrated Data-Driven\n  Agent-based Model"
                },
                "summary": "In the last few years, economic agent-based models have made the transition\nfrom qualitative models calibrated to match stylised facts to quantitative\nmodels for time series forecasting, and in some cases, their predictions have\nperformed as well or better than those of standard models (see, e.g. Poledna et\nal. (2023a); Hommes et al. (2022); Pichler et al. (2022)). Here, we build on\nthe model of Poledna et al., adding several new features such as housing\nmarkets, realistic synthetic populations of individuals with income, wealth and\nconsumption heterogeneity, enhanced behavioural rules and market mechanisms,\nand an enhanced credit market. We calibrate our model for all 38 OECD member\ncountries using state-of-the-art approximate Bayesian inference methods and\ntest it by making out-of-sample forecasts. It outperforms both the Poledna and\nAR(1) time series models by a highly statistically significant margin. Our\nmodel is built within a platform we have developed, making it easy to build,\nrun, and evaluate alternative models, which we hope will encourage future work\nin this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few years, economic agent-based models have made the transition\nfrom qualitative models calibrated to match stylised facts to quantitative\nmodels for time series forecasting, and in some cases, their predictions have\nperformed as well or better than those of standard models (see, e.g. Poledna et\nal. (2023a); Hommes et al. (2022); Pichler et al. (2022)). Here, we build on\nthe model of Poledna et al., adding several new features such as housing\nmarkets, realistic synthetic populations of individuals with income, wealth and\nconsumption heterogeneity, enhanced behavioural rules and market mechanisms,\nand an enhanced credit market. We calibrate our model for all 38 OECD member\ncountries using state-of-the-art approximate Bayesian inference methods and\ntest it by making out-of-sample forecasts. It outperforms both the Poledna and\nAR(1) time series models by a highly statistically significant margin. Our\nmodel is built within a platform we have developed, making it easy to build,\nrun, and evaluate alternative models, which we hope will encourage future work\nin this area."
                },
                "authors": [
                    {
                        "name": "Samuel Wiese"
                    },
                    {
                        "name": "Jagoda Kaszowska-Mojsa"
                    },
                    {
                        "name": "Joel Dyer"
                    },
                    {
                        "name": "Jose Moran"
                    },
                    {
                        "name": "Marco Pangallo"
                    },
                    {
                        "name": "Francois Lafond"
                    },
                    {
                        "name": "John Muellbauer"
                    },
                    {
                        "name": "Anisoara Calinescu"
                    },
                    {
                        "name": "J. Doyne Farmer"
                    }
                ],
                "author_detail": {
                    "name": "J. Doyne Farmer"
                },
                "author": "J. Doyne Farmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18753v1",
                "updated": "2024-09-27T13:44:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    44,
                    55,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T13:44:55Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    44,
                    55,
                    4,
                    271,
                    0
                ],
                "title": "Enhancing Explainability in Multimodal Large Language Models Using\n  Ontological Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Explainability in Multimodal Large Language Models Using\n  Ontological Context"
                },
                "summary": "Recently, there has been a growing interest in Multimodal Large Language\nModels (MLLMs) due to their remarkable potential in various tasks integrating\ndifferent modalities, such as image and text, as well as applications such as\nimage captioning and visual question answering. However, such models still face\nchallenges in accurately captioning and interpreting specific visual concepts\nand classes, particularly in domain-specific applications. We argue that\nintegrating domain knowledge in the form of an ontology can significantly\naddress these issues. In this work, as a proof of concept, we propose a new\nframework that combines ontology with MLLMs to classify images of plant\ndiseases. Our method uses concepts about plant diseases from an existing\ndisease ontology to query MLLMs and extract relevant visual concepts from\nimages. Then, we use the reasoning capabilities of the ontology to classify the\ndisease according to the identified concepts. Ensuring that the model\naccurately uses the concepts describing the disease is crucial in\ndomain-specific applications. By employing an ontology, we can assist in\nverifying this alignment. Additionally, using the ontology's inference\ncapabilities increases transparency, explainability, and trust in the\ndecision-making process while serving as a judge by checking if the annotations\nof the concepts by MLLMs are aligned with those in the ontology and displaying\nthe rationales behind their errors. Our framework offers a new direction for\nsynergizing ontologies and MLLMs, supported by an empirical study using\ndifferent well-known MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing interest in Multimodal Large Language\nModels (MLLMs) due to their remarkable potential in various tasks integrating\ndifferent modalities, such as image and text, as well as applications such as\nimage captioning and visual question answering. However, such models still face\nchallenges in accurately captioning and interpreting specific visual concepts\nand classes, particularly in domain-specific applications. We argue that\nintegrating domain knowledge in the form of an ontology can significantly\naddress these issues. In this work, as a proof of concept, we propose a new\nframework that combines ontology with MLLMs to classify images of plant\ndiseases. Our method uses concepts about plant diseases from an existing\ndisease ontology to query MLLMs and extract relevant visual concepts from\nimages. Then, we use the reasoning capabilities of the ontology to classify the\ndisease according to the identified concepts. Ensuring that the model\naccurately uses the concepts describing the disease is crucial in\ndomain-specific applications. By employing an ontology, we can assist in\nverifying this alignment. Additionally, using the ontology's inference\ncapabilities increases transparency, explainability, and trust in the\ndecision-making process while serving as a judge by checking if the annotations\nof the concepts by MLLMs are aligned with those in the ontology and displaying\nthe rationales behind their errors. Our framework offers a new direction for\nsynergizing ontologies and MLLMs, supported by an empirical study using\ndifferent well-known MLLMs."
                },
                "authors": [
                    {
                        "name": "Jihen Amara"
                    },
                    {
                        "name": "Birgitta König-Ries"
                    },
                    {
                        "name": "Sheeba Samuel"
                    }
                ],
                "author_detail": {
                    "name": "Sheeba Samuel"
                },
                "author": "Sheeba Samuel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18747v1",
                "updated": "2024-09-27T13:38:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T13:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "Cottention: Linear Transformers With Cosine Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cottention: Linear Transformers With Cosine Attention"
                },
                "summary": "Attention mechanisms, particularly softmax attention, have been instrumental\nin the success of transformer-based models such as GPT. However, the quadratic\nmemory complexity of softmax attention with respect to sequence length poses\nsignificant challenges for processing longer sequences. We introduce\nCottention, a novel attention mechanism that replaces the softmax operation\nwith cosine similarity. By leveraging the properties of cosine similarity and\nrearranging the attention equation, Cottention achieves native linear memory\ncomplexity with respect to sequence length, making it inherently more\nmemory-efficient than softmax attention. We demonstrate that Cottention can be\nreformulated as a recurrent neural network (RNN) with a finite hidden state,\nallowing for constant memory usage during inference. We evaluate Cottention on\nboth the bidirectional BERT and causal GPT tasks, demonstrating comparable\nperformance to softmax attention while significantly reducing memory\nrequirements. To ensure efficient computation, we develop a custom CUDA kernel\nfor Cottention. Our results show that Cottention is a promising alternative to\nsoftmax attention, enabling the processing of longer sequences without\nsacrificing performance, due to its native linear memory complexity and ability\nto maintain a constant memory footprint during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms, particularly softmax attention, have been instrumental\nin the success of transformer-based models such as GPT. However, the quadratic\nmemory complexity of softmax attention with respect to sequence length poses\nsignificant challenges for processing longer sequences. We introduce\nCottention, a novel attention mechanism that replaces the softmax operation\nwith cosine similarity. By leveraging the properties of cosine similarity and\nrearranging the attention equation, Cottention achieves native linear memory\ncomplexity with respect to sequence length, making it inherently more\nmemory-efficient than softmax attention. We demonstrate that Cottention can be\nreformulated as a recurrent neural network (RNN) with a finite hidden state,\nallowing for constant memory usage during inference. We evaluate Cottention on\nboth the bidirectional BERT and causal GPT tasks, demonstrating comparable\nperformance to softmax attention while significantly reducing memory\nrequirements. To ensure efficient computation, we develop a custom CUDA kernel\nfor Cottention. Our results show that Cottention is a promising alternative to\nsoftmax attention, enabling the processing of longer sequences without\nsacrificing performance, due to its native linear memory complexity and ability\nto maintain a constant memory footprint during inference."
                },
                "authors": [
                    {
                        "name": "Gabriel Mongaras"
                    },
                    {
                        "name": "Trevor Dohm"
                    },
                    {
                        "name": "Eric C. Larson"
                    }
                ],
                "author_detail": {
                    "name": "Eric C. Larson"
                },
                "author": "Eric C. Larson",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07736v2",
                "updated": "2024-09-27T13:20:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    20,
                    18,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-11T21:46:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    46,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems."
                },
                "authors": [
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Seohyun Park"
                    },
                    {
                        "name": "Hyeyun Jeong"
                    },
                    {
                        "name": "Youngeun Koo"
                    },
                    {
                        "name": "Soonha Hwang"
                    },
                    {
                        "name": "Seonwoo Park"
                    },
                    {
                        "name": "Sungeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Lee"
                },
                "author": "Sungeun Lee",
                "arxiv_comment": "The 2nd GenBench workshop on generalisation (benchmarking) in NLP -\n  EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03471v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03471v3",
                "updated": "2024-09-27T13:12:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    12,
                    23,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-04T14:24:06Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    14,
                    24,
                    6,
                    3,
                    95,
                    0
                ],
                "title": "The Impact of Unstated Norms in Bias Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Unstated Norms in Bias Analysis of Language Models"
                },
                "summary": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task, performed by an LLM, is invariant to a change of group membership.\nIn this work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task, performed by an LLM, is invariant to a change of group membership.\nIn this work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification."
                },
                "authors": [
                    {
                        "name": "Farnaz Kohankhaki"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Jacob-Junqi Tian"
                    },
                    {
                        "name": "Laleh Seyyed-Kalantari"
                    },
                    {
                        "name": "Faiza Khan Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Faiza Khan Khattak"
                },
                "author": "Faiza Khan Khattak",
                "arxiv_comment": "23 Pages, 5 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03471v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03471v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07678v2",
                "updated": "2024-09-27T12:58:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    58,
                    43,
                    4,
                    271,
                    0
                ],
                "published": "2024-07-10T14:06:15Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    6,
                    15,
                    2,
                    192,
                    0
                ],
                "title": "The ESO SupJup Survey II: The 12C/13C ratios of three young brown dwarfs\n  with CRIRES$^+$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ESO SupJup Survey II: The 12C/13C ratios of three young brown dwarfs\n  with CRIRES$^+$"
                },
                "summary": "Young brown dwarfs exhibit atmospheric characteristics similar to those of\nsuper-Jupiters, providing a unique opportunity to study planetary atmospheres.\nThe ESO SupJup Survey, utilizing CRIRES$^+$ on the Very Large Telescope, aims\nto assess the role of $^{12}$C/$^{13}$C as a formation tracer. We present\nobservations of three young brown dwarfs: 2MASS J12003792-7845082, TWA 28, and\n2MASS J08561384-1342242, with the goal of constraining their chemical\ncompositions, thermal profiles, surface gravities, spin rotations, and\n$^{12}$C/$^{13}$C. Atmospheric retrievals of CRIRES$^+$ K-band spectra were\nconducted using the radiative transfer code petitRADTRANS coupled with the\nBayesian inference algorithm MultiNest, resulting in a detailed\ncharacterization of the atmospheres of these objects. We report the volume\nmixing ratios of main molecular and atomic species, including the novel\ndetection of hydrogen fluoride (HF) in a brown dwarf's atmosphere, and\ndetermine $^{12}$C/$^{13}$C values of $81^{+28}_{-19}$ and $79^{+20}_{-14}$ in\nthe atmospheres of TWA 28 and J0856, respectively, with strong significance\n($>3\\sigma$). Tentative evidence ($\\sim 2\\sigma$) of $^{13}$C in J1200 was\nfound, with $^{12}$C/$^{13}$C = $114^{+69}_{-33}$, along with $^{18}$O detected\nat moderate significance in J0856 (3.3$\\sigma$) and TWA 28 (2.1$\\sigma$). The\nretrieved thermal profiles indicate hot atmospheres (2300-2600 K) with low\nsurface gravities and slow spins, consistent with young objects. The consistent\ncarbon isotope ratios among the three objects, showing no significant deviation\nfrom the local ISM, suggest a fragmentation-based formation mechanism similar\nto star formation. The tentative detection of $^{18}$O in two objects\nhighlights the potential of high-resolution spectroscopy to probe additional\nisotope ratios, such as $^{16}$O/$^{18}$O, in the atmospheres of brown dwarfs\nand super-Jupiters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young brown dwarfs exhibit atmospheric characteristics similar to those of\nsuper-Jupiters, providing a unique opportunity to study planetary atmospheres.\nThe ESO SupJup Survey, utilizing CRIRES$^+$ on the Very Large Telescope, aims\nto assess the role of $^{12}$C/$^{13}$C as a formation tracer. We present\nobservations of three young brown dwarfs: 2MASS J12003792-7845082, TWA 28, and\n2MASS J08561384-1342242, with the goal of constraining their chemical\ncompositions, thermal profiles, surface gravities, spin rotations, and\n$^{12}$C/$^{13}$C. Atmospheric retrievals of CRIRES$^+$ K-band spectra were\nconducted using the radiative transfer code petitRADTRANS coupled with the\nBayesian inference algorithm MultiNest, resulting in a detailed\ncharacterization of the atmospheres of these objects. We report the volume\nmixing ratios of main molecular and atomic species, including the novel\ndetection of hydrogen fluoride (HF) in a brown dwarf's atmosphere, and\ndetermine $^{12}$C/$^{13}$C values of $81^{+28}_{-19}$ and $79^{+20}_{-14}$ in\nthe atmospheres of TWA 28 and J0856, respectively, with strong significance\n($>3\\sigma$). Tentative evidence ($\\sim 2\\sigma$) of $^{13}$C in J1200 was\nfound, with $^{12}$C/$^{13}$C = $114^{+69}_{-33}$, along with $^{18}$O detected\nat moderate significance in J0856 (3.3$\\sigma$) and TWA 28 (2.1$\\sigma$). The\nretrieved thermal profiles indicate hot atmospheres (2300-2600 K) with low\nsurface gravities and slow spins, consistent with young objects. The consistent\ncarbon isotope ratios among the three objects, showing no significant deviation\nfrom the local ISM, suggest a fragmentation-based formation mechanism similar\nto star formation. The tentative detection of $^{18}$O in two objects\nhighlights the potential of high-resolution spectroscopy to probe additional\nisotope ratios, such as $^{16}$O/$^{18}$O, in the atmospheres of brown dwarfs\nand super-Jupiters."
                },
                "authors": [
                    {
                        "name": "D. González Picos"
                    },
                    {
                        "name": "I. A. G. Snellen"
                    },
                    {
                        "name": "S. de Regt"
                    },
                    {
                        "name": "R. Landman"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "S. Gandhi"
                    },
                    {
                        "name": "C. Ginski"
                    },
                    {
                        "name": "A. Y. Kesseli"
                    },
                    {
                        "name": "P. Mollière"
                    },
                    {
                        "name": "T. Stolker"
                    }
                ],
                "author_detail": {
                    "name": "T. Stolker"
                },
                "author": "T. Stolker",
                "arxiv_doi": "10.1051/0004-6361/202450028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in A&A Volume 689, September 2024",
                "arxiv_journal_ref": "A&A 689, A212 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v1",
                "updated": "2024-09-27T12:54:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18701v1",
                "updated": "2024-09-27T12:44:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    44,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:44:06Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    44,
                    6,
                    4,
                    271,
                    0
                ],
                "title": "3DPX: Single Panoramic X-ray Analysis Guided by 3D Oral Structure\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DPX: Single Panoramic X-ray Analysis Guided by 3D Oral Structure\n  Reconstruction"
                },
                "summary": "Panoramic X-ray (PX) is a prevalent modality in dentistry practice owing to\nits wide availability and low cost. However, as a 2D projection of a 3D\nstructure, PX suffers from anatomical information loss and PX diagnosis is\nlimited compared to that with 3D imaging modalities. 2D-to-3D reconstruction\nmethods have been explored for the ability to synthesize the absent 3D\nanatomical information from 2D PX for use in PX image analysis. However, there\nare challenges in leveraging such 3D synthesized reconstructions. First,\ninferring 3D depth from 2D images remains a challenging task with limited\naccuracy. The second challenge is the joint analysis of 2D PX with its 3D\nsynthesized counterpart, with the aim to maximize the 2D-3D synergy while\nminimizing the errors arising from the synthesized image. In this study, we\npropose a new method termed 3DPX - PX image analysis guided by 2D-to-3D\nreconstruction, to overcome these challenges. 3DPX consists of (i) a novel\nprogressive reconstruction network to improve 2D-to-3D reconstruction and, (ii)\na contrastive-guided bidirectional multimodality alignment module for 3D-guided\n2D PX classification and segmentation tasks. The reconstruction network\nprogressively reconstructs 3D images with knowledge imposed on the intermediate\nreconstructions at multiple pyramid levels and incorporates Multilayer\nPerceptrons to improve semantic understanding. The downstream networks leverage\nthe reconstructed images as 3D anatomical guidance to the PX analysis through\nfeature alignment, which increases the 2D-3D synergy with bidirectional feature\nprojection and decease the impact of potential errors with contrastive\nguidance. Extensive experiments on two oral datasets involving 464 studies\ndemonstrate that 3DPX outperforms the state-of-the-art methods in various tasks\nincluding 2D-to-3D reconstruction, PX classification and lesion segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic X-ray (PX) is a prevalent modality in dentistry practice owing to\nits wide availability and low cost. However, as a 2D projection of a 3D\nstructure, PX suffers from anatomical information loss and PX diagnosis is\nlimited compared to that with 3D imaging modalities. 2D-to-3D reconstruction\nmethods have been explored for the ability to synthesize the absent 3D\nanatomical information from 2D PX for use in PX image analysis. However, there\nare challenges in leveraging such 3D synthesized reconstructions. First,\ninferring 3D depth from 2D images remains a challenging task with limited\naccuracy. The second challenge is the joint analysis of 2D PX with its 3D\nsynthesized counterpart, with the aim to maximize the 2D-3D synergy while\nminimizing the errors arising from the synthesized image. In this study, we\npropose a new method termed 3DPX - PX image analysis guided by 2D-to-3D\nreconstruction, to overcome these challenges. 3DPX consists of (i) a novel\nprogressive reconstruction network to improve 2D-to-3D reconstruction and, (ii)\na contrastive-guided bidirectional multimodality alignment module for 3D-guided\n2D PX classification and segmentation tasks. The reconstruction network\nprogressively reconstructs 3D images with knowledge imposed on the intermediate\nreconstructions at multiple pyramid levels and incorporates Multilayer\nPerceptrons to improve semantic understanding. The downstream networks leverage\nthe reconstructed images as 3D anatomical guidance to the PX analysis through\nfeature alignment, which increases the 2D-3D synergy with bidirectional feature\nprojection and decease the impact of potential errors with contrastive\nguidance. Extensive experiments on two oral datasets involving 464 studies\ndemonstrate that 3DPX outperforms the state-of-the-art methods in various tasks\nincluding 2D-to-3D reconstruction, PX classification and lesion segmentation."
                },
                "authors": [
                    {
                        "name": "Xiaoshuang Li"
                    },
                    {
                        "name": "Zimo Huang"
                    },
                    {
                        "name": "Mingyuan Meng"
                    },
                    {
                        "name": "Eduardo Delamare"
                    },
                    {
                        "name": "Dagan Feng"
                    },
                    {
                        "name": "Lei Bi"
                    },
                    {
                        "name": "Bin Sheng"
                    },
                    {
                        "name": "Lingyong Jiang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Jinman Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Kim"
                },
                "author": "Jinman Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15360v2",
                "updated": "2024-09-27T12:36:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    36,
                    58,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-18T02:35:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "Reward-Robust RLHF in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Robust RLHF in LLMs"
                },
                "summary": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yiping Zhang"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yuan Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Shen"
                },
                "author": "Yuan Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18693v1",
                "updated": "2024-09-27T12:28:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    28,
                    20,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:28:20Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    28,
                    20,
                    4,
                    271,
                    0
                ],
                "title": "Heavy Elements Abundances Inferred from the First Adiabatic Exponent in\n  the Solar Envelope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heavy Elements Abundances Inferred from the First Adiabatic Exponent in\n  the Solar Envelope"
                },
                "summary": "The first adiabatic exponent profile, noted $\\Gamma_1$, computed along\nadiabatic coordinats $(T, \\rho)$ is in the focus of our study. Under conditions\nof almost fully ionized hydrogen and helium, the $\\Gamma_1$ profile is quite\nsensitive to heavy elements ionization. $\\Gamma_1$ decreases in regions where\nan element is partially ionized. The recent helioseismic structural inversion\nis obtained with an accuracy better than $10^{-4}$ in the most of the adiabatic\nconvective zone that allows to study ionization variations. The aim is to\ndetermine the major heavy elements content in the solar convective zone. The\nmethod of our research is synthesis of the $\\Gamma_1$ profile which is based on\na linear combination of the contributions of individual heavy elements. The\nidea of the approach was proposed and justified by Baturin et al. (Astron.\nAstrophys., 660, A125, 2022). We find the best approximation of the inverted\nprofile $\\Gamma_1$ adjusting the abundances of major elements (C, N, O, Ne),\nmeanwhile the abundances of elements heavier than neon are fixed. We synthesize\nthe theoretical $\\Gamma_1$ profile using the SAHA-S equation of state, and are\nable to reproduce the inverted profiles with an accuracy of $(1-2)\\cdot\n10^{-5}$. Total mass fraction of heavy elements found by this method is\n$Z=0.0148\\pm 0.0004$. The oxygen logarithmic abundance is $8.70\\pm 0.03$,\ncarbon $8.44\\pm 0.04$, nitrogen $8.12\\pm 0.08$, and neon $8.17\\pm 0.09$. The\nobtained estimations of oxygen and carbon agree with spectroscopic abundances\nby Asplund et al. (Astron. Astrophys., 653, A141, 2021).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first adiabatic exponent profile, noted $\\Gamma_1$, computed along\nadiabatic coordinats $(T, \\rho)$ is in the focus of our study. Under conditions\nof almost fully ionized hydrogen and helium, the $\\Gamma_1$ profile is quite\nsensitive to heavy elements ionization. $\\Gamma_1$ decreases in regions where\nan element is partially ionized. The recent helioseismic structural inversion\nis obtained with an accuracy better than $10^{-4}$ in the most of the adiabatic\nconvective zone that allows to study ionization variations. The aim is to\ndetermine the major heavy elements content in the solar convective zone. The\nmethod of our research is synthesis of the $\\Gamma_1$ profile which is based on\na linear combination of the contributions of individual heavy elements. The\nidea of the approach was proposed and justified by Baturin et al. (Astron.\nAstrophys., 660, A125, 2022). We find the best approximation of the inverted\nprofile $\\Gamma_1$ adjusting the abundances of major elements (C, N, O, Ne),\nmeanwhile the abundances of elements heavier than neon are fixed. We synthesize\nthe theoretical $\\Gamma_1$ profile using the SAHA-S equation of state, and are\nable to reproduce the inverted profiles with an accuracy of $(1-2)\\cdot\n10^{-5}$. Total mass fraction of heavy elements found by this method is\n$Z=0.0148\\pm 0.0004$. The oxygen logarithmic abundance is $8.70\\pm 0.03$,\ncarbon $8.44\\pm 0.04$, nitrogen $8.12\\pm 0.08$, and neon $8.17\\pm 0.09$. The\nobtained estimations of oxygen and carbon agree with spectroscopic abundances\nby Asplund et al. (Astron. Astrophys., 653, A141, 2021)."
                },
                "authors": [
                    {
                        "name": "V. A. Baturin"
                    },
                    {
                        "name": "A. V. Oreshina"
                    },
                    {
                        "name": "G. Buldgen"
                    },
                    {
                        "name": "S. V. Ayukov"
                    },
                    {
                        "name": "V. K. Gryaznov"
                    },
                    {
                        "name": "I. L. Iosilevskiy"
                    },
                    {
                        "name": "A. Noels"
                    },
                    {
                        "name": "R. Scuflaire"
                    }
                ],
                "author_detail": {
                    "name": "R. Scuflaire"
                },
                "author": "R. Scuflaire",
                "arxiv_comment": "Accepted for publication in Solar Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v3",
                "updated": "2024-09-27T12:23:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    23,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06069v2",
                "updated": "2024-09-27T12:23:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    23,
                    4,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-10T03:22:57Z",
                "published_parsed": [
                    2024,
                    3,
                    10,
                    3,
                    22,
                    57,
                    6,
                    70,
                    0
                ],
                "title": "Implicit Image-to-Image Schrodinger Bridge for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Image-to-Image Schrodinger Bridge for Image Restoration"
                },
                "summary": "Diffusion-based models are widely recognized for their effectiveness in image\nrestoration tasks; however, their iterative denoising process, which begins\nfrom Gaussian noise, often results in slow inference speeds. The Image-to-Image\nSchr\\\"odinger Bridge (I$^2$SB) presents a promising alternative by starting the\ngenerative process from corrupted images and leveraging training techniques\nfrom score-based diffusion models. In this paper, we introduce the Implicit\nImage-to-Image Schr\\\"odinger Bridge (I$^3$SB) to further accelerate the\ngenerative process of I$^2$SB. I$^3$SB reconfigures the generative process into\na non-Markovian framework by incorporating the initial corrupted image into\neach step, while ensuring that the marginal distribution aligns with that of\nI$^2$SB. This allows for the direct use of the pretrained network from I$^2$SB.\nExtensive experiments on natural images, human face images, and medical images\nvalidate the acceleration benefits of I$^3$SB. Compared to I$^2$SB, I$^3$SB\nachieves the same perceptual quality with fewer generative steps, while\nmaintaining equal or improved fidelity to the ground truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models are widely recognized for their effectiveness in image\nrestoration tasks; however, their iterative denoising process, which begins\nfrom Gaussian noise, often results in slow inference speeds. The Image-to-Image\nSchr\\\"odinger Bridge (I$^2$SB) presents a promising alternative by starting the\ngenerative process from corrupted images and leveraging training techniques\nfrom score-based diffusion models. In this paper, we introduce the Implicit\nImage-to-Image Schr\\\"odinger Bridge (I$^3$SB) to further accelerate the\ngenerative process of I$^2$SB. I$^3$SB reconfigures the generative process into\na non-Markovian framework by incorporating the initial corrupted image into\neach step, while ensuring that the marginal distribution aligns with that of\nI$^2$SB. This allows for the direct use of the pretrained network from I$^2$SB.\nExtensive experiments on natural images, human face images, and medical images\nvalidate the acceleration benefits of I$^3$SB. Compared to I$^2$SB, I$^3$SB\nachieves the same perceptual quality with fewer generative steps, while\nmaintaining equal or improved fidelity to the ground truth."
                },
                "authors": [
                    {
                        "name": "Yuang Wang"
                    },
                    {
                        "name": "Siyeop Yoon"
                    },
                    {
                        "name": "Pengfei Jin"
                    },
                    {
                        "name": "Matthew Tivnan"
                    },
                    {
                        "name": "Sifan Song"
                    },
                    {
                        "name": "Zhennong Chen"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Dufan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Dufan Wu"
                },
                "author": "Dufan Wu",
                "arxiv_comment": "23 pages, 8 figures, submitted to Pattern Recognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14277v2",
                "updated": "2024-09-27T12:18:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    18,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-20T12:59:27Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    12,
                    59,
                    27,
                    3,
                    172,
                    0
                ],
                "title": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs"
                },
                "summary": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}."
                },
                "authors": [
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Cheoneum Park"
                    },
                    {
                        "name": "Seungjun Baek"
                    }
                ],
                "author_detail": {
                    "name": "Seungjun Baek"
                },
                "author": "Seungjun Baek",
                "arxiv_comment": "The 2024 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v2",
                "updated": "2024-09-27T12:12:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    12,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v3",
                "updated": "2024-09-27T12:12:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    12,
                    19,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06164v2",
                "updated": "2024-09-27T12:08:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    8,
                    7,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-10T10:30:34Z",
                "published_parsed": [
                    2024,
                    3,
                    10,
                    10,
                    30,
                    34,
                    6,
                    70,
                    0
                ],
                "title": "Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion\n  Estimation"
                },
                "summary": "Single camera 3D pose estimation is an ill-defined problem due to inherent\nambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose\nestimation accounts for this uncertainty by providing multiple 3D poses\nconsistent with the 2D measurements. Current research has predominantly\nconcentrated on generating multiple hypotheses for single frame static pose\nestimation or single hypothesis motion estimation. In this study we focus on\nthe new task of multi-hypothesis motion estimation. Multi-hypothesis motion\nestimation is not simply multi-hypothesis pose estimation applied to multiple\nframes, which would ignore temporal correlation across frames. Instead, it\nrequires distributions which are capable of generating temporally consistent\nsamples, which is significantly more challenging than multi-hypothesis pose\nestimation or single-hypothesis motion estimation. To this end, we introduce\nPlatypose, a framework that uses a diffusion model pretrained on 3D human\nmotion sequences for zero-shot 3D pose sequence estimation. Platypose\noutperforms baseline methods on multiple hypotheses for motion estimation.\nAdditionally, Platypose also achieves state-of-the-art calibration and\ncompetitive joint error when tested on static poses from Human3.6M,\nMPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes\nflexibly to different settings such as multi-camera inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single camera 3D pose estimation is an ill-defined problem due to inherent\nambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose\nestimation accounts for this uncertainty by providing multiple 3D poses\nconsistent with the 2D measurements. Current research has predominantly\nconcentrated on generating multiple hypotheses for single frame static pose\nestimation or single hypothesis motion estimation. In this study we focus on\nthe new task of multi-hypothesis motion estimation. Multi-hypothesis motion\nestimation is not simply multi-hypothesis pose estimation applied to multiple\nframes, which would ignore temporal correlation across frames. Instead, it\nrequires distributions which are capable of generating temporally consistent\nsamples, which is significantly more challenging than multi-hypothesis pose\nestimation or single-hypothesis motion estimation. To this end, we introduce\nPlatypose, a framework that uses a diffusion model pretrained on 3D human\nmotion sequences for zero-shot 3D pose sequence estimation. Platypose\noutperforms baseline methods on multiple hypotheses for motion estimation.\nAdditionally, Platypose also achieves state-of-the-art calibration and\ncompetitive joint error when tested on static poses from Human3.6M,\nMPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes\nflexibly to different settings such as multi-camera inference."
                },
                "authors": [
                    {
                        "name": "Paweł A. Pierzchlewicz"
                    },
                    {
                        "name": "Caio O. da Silva"
                    },
                    {
                        "name": "R. James Cotton"
                    },
                    {
                        "name": "Fabian H. Sinz"
                    }
                ],
                "author_detail": {
                    "name": "Fabian H. Sinz"
                },
                "author": "Fabian H. Sinz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18680v1",
                "updated": "2024-09-27T12:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models"
                },
                "summary": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "EMNLP24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.09510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.09510v2",
                "updated": "2024-09-27T12:06:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    15,
                    4,
                    271,
                    0
                ],
                "published": "2023-04-19T08:59:34Z",
                "published_parsed": [
                    2023,
                    4,
                    19,
                    8,
                    59,
                    34,
                    2,
                    109,
                    0
                ],
                "title": "Maybenot: A Framework for Traffic Analysis Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maybenot: A Framework for Traffic Analysis Defenses"
                },
                "summary": "End-to-end encryption is a powerful tool for protecting the privacy of\nInternet users. Together with the increasing use of technologies such as Tor,\nVPNs, and encrypted messaging, it is becoming increasingly difficult for\nnetwork adversaries to monitor and censor Internet traffic. One remaining\navenue for adversaries is traffic analysis: the analysis of patterns in\nencrypted traffic to infer information about the users and their activities.\nRecent improvements using deep learning have made traffic analysis attacks more\neffective than ever before.\n  We present Maybenot, a framework for traffic analysis defenses. Maybenot is\ndesigned to be easy to use and integrate into existing end-to-end encrypted\nprotocols. It is implemented in the Rust programming language as a crate\n(library), together with a simulator to further the development of defenses.\nDefenses in Maybenot are expressed as probabilistic state machines that\nschedule actions to inject padding or block outgoing traffic. Maybenot is an\nevolution from the Tor Circuit Padding Framework by Perry and Kadianakis,\ndesigned to support a wide range of protocols and use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end encryption is a powerful tool for protecting the privacy of\nInternet users. Together with the increasing use of technologies such as Tor,\nVPNs, and encrypted messaging, it is becoming increasingly difficult for\nnetwork adversaries to monitor and censor Internet traffic. One remaining\navenue for adversaries is traffic analysis: the analysis of patterns in\nencrypted traffic to infer information about the users and their activities.\nRecent improvements using deep learning have made traffic analysis attacks more\neffective than ever before.\n  We present Maybenot, a framework for traffic analysis defenses. Maybenot is\ndesigned to be easy to use and integrate into existing end-to-end encrypted\nprotocols. It is implemented in the Rust programming language as a crate\n(library), together with a simulator to further the development of defenses.\nDefenses in Maybenot are expressed as probabilistic state machines that\nschedule actions to inject padding or block outgoing traffic. Maybenot is an\nevolution from the Tor Circuit Padding Framework by Perry and Kadianakis,\ndesigned to support a wide range of protocols and use cases."
                },
                "authors": [
                    {
                        "name": "Tobias Pulls"
                    },
                    {
                        "name": "Ethan Witwer"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Witwer"
                },
                "author": "Ethan Witwer",
                "arxiv_comment": "Version 2 of the Maybenot framework",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.09510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.09510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18679v1",
                "updated": "2024-09-27T12:05:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    12,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:05:12Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    12,
                    4,
                    271,
                    0
                ],
                "title": "\"Why\" Has the Least Side Effect on Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Why\" Has the Least Side Effect on Model Editing"
                },
                "summary": "Training large language models (LLMs) from scratch is an expensive endeavor,\nparticularly as world knowledge continually evolves. To maintain relevance and\naccuracy of LLMs, model editing has emerged as a pivotal research area. While\nthese methods hold promise, they can also produce unintended side effects.\nTheir underlying factors and causes remain largely unexplored. This paper\ndelves into a critical factor-question type-by categorizing model editing\nquestions. Our findings reveal that the extent of performance degradation\nvaries significantly across different question types, providing new insights\nfor experimental design in knowledge editing. Furthermore, we investigate\nwhether insights from smaller models can be extrapolated to larger models. Our\nresults indicate discrepancies in findings between models of different sizes,\nsuggesting that insights from smaller models may not necessarily apply to\nlarger models. Additionally, we examine the impact of batch size on side\neffects, discovering that increasing the batch size can mitigate performance\ndrops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) from scratch is an expensive endeavor,\nparticularly as world knowledge continually evolves. To maintain relevance and\naccuracy of LLMs, model editing has emerged as a pivotal research area. While\nthese methods hold promise, they can also produce unintended side effects.\nTheir underlying factors and causes remain largely unexplored. This paper\ndelves into a critical factor-question type-by categorizing model editing\nquestions. Our findings reveal that the extent of performance degradation\nvaries significantly across different question types, providing new insights\nfor experimental design in knowledge editing. Furthermore, we investigate\nwhether insights from smaller models can be extrapolated to larger models. Our\nresults indicate discrepancies in findings between models of different sizes,\nsuggesting that insights from smaller models may not necessarily apply to\nlarger models. Additionally, we examine the impact of batch size on side\neffects, discovering that increasing the batch size can mitigate performance\ndrops."
                },
                "authors": [
                    {
                        "name": "Tsung-Hsuan Pan"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18678v1",
                "updated": "2024-09-27T12:05:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    5,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:05:05Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    5,
                    4,
                    271,
                    0
                ],
                "title": "Rehearsing Answers to Probable Questions with Perspective-Taking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsing Answers to Probable Questions with Perspective-Taking"
                },
                "summary": "Question answering (QA) has been a long-standing focus in the NLP field,\npredominantly addressing reading comprehension and common sense QA. However,\nscenarios involving the preparation of answers to probable questions during\nprofessional oral presentations remain underexplored. In this paper, we pioneer\nthe examination of this crucial yet overlooked topic by utilizing real-world QA\nconversation transcripts between company managers and professional analysts. We\nexplore the proposed task using three causal knowledge graphs (KGs) and three\nlarge language models (LLMs). This work provides foundational insights into the\napplication of LLMs in professional QA scenarios, highlighting the importance\nof causal KGs and perspective-taking in generating effective responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) has been a long-standing focus in the NLP field,\npredominantly addressing reading comprehension and common sense QA. However,\nscenarios involving the preparation of answers to probable questions during\nprofessional oral presentations remain underexplored. In this paper, we pioneer\nthe examination of this crucial yet overlooked topic by utilizing real-world QA\nconversation transcripts between company managers and professional analysts. We\nexplore the proposed task using three causal knowledge graphs (KGs) and three\nlarge language models (LLMs). This work provides foundational insights into the\napplication of LLMs in professional QA scenarios, highlighting the importance\nof causal KGs and perspective-taking in generating effective responses."
                },
                "authors": [
                    {
                        "name": "Yung-Yu Shih"
                    },
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18668v1",
                "updated": "2024-09-27T11:53:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    53,
                    38,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:53:38Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    53,
                    38,
                    4,
                    271,
                    0
                ],
                "title": "The axis of systematic bias in SN~Ia cosmology and implications for DESI\n  2024 results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The axis of systematic bias in SN~Ia cosmology and implications for DESI\n  2024 results"
                },
                "summary": "Relative distances between a high-redshift sample of Type Ia supernovae\n(SNe~Ia), anchored to a low-redshift sample, have been instrumental in drawing\ninsights on the nature of the dark energy driving the accelerated expansion of\nthe universe. A combination (hereafter called SBC) of the SNe~Ia with baryon\nacoustic oscillations (BAO) from the Dark Energy Spectroscopic Instrument\n(DESI) and the cosmic microwave background (CMB) recently indicated deviations\nfrom the standard interpretation of dark energy as a cosmological constant. In\nthis paper, we analyse various systematic uncertainties in the distance\nmeasurement of SNe~Ia and their impact on the inferred dark energy properties\nin the canonical Chevallier-Polarski-Linder (CPL) model. We model systematic\neffects like photometric calibration, progenitor and dust evolution, and\nuncertainty in the galactic extinction law. We find that all the dominant\nsystematic errors shift the dark energy inference towards the DESI 2024 results\nfrom an underlying $\\Lambda$CDM cosmology. A small change in the calibration,\nand change in the Milky Way dust, can give rise to systematic-driven shifts on\n$w_0$-$w_a$ constraints, comparable to the deviation reported from the DESI\n2024 results. We forecast that the systematic uncertainties can shift the\ninference of $w_0-w_a$ by a few times the error ellipse for future low- and\nhigh-$z$ SN~Ia compilations and hence, it is critical to circumvent them to\nrobustly test for deviations from $\\Lambda$. A slider and visualisation tool\nfor quantifying the impact of systematic effects on the fitted cosmological\nparameters is publicly available at: https://github.com/sdhawan21/DEslider.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative distances between a high-redshift sample of Type Ia supernovae\n(SNe~Ia), anchored to a low-redshift sample, have been instrumental in drawing\ninsights on the nature of the dark energy driving the accelerated expansion of\nthe universe. A combination (hereafter called SBC) of the SNe~Ia with baryon\nacoustic oscillations (BAO) from the Dark Energy Spectroscopic Instrument\n(DESI) and the cosmic microwave background (CMB) recently indicated deviations\nfrom the standard interpretation of dark energy as a cosmological constant. In\nthis paper, we analyse various systematic uncertainties in the distance\nmeasurement of SNe~Ia and their impact on the inferred dark energy properties\nin the canonical Chevallier-Polarski-Linder (CPL) model. We model systematic\neffects like photometric calibration, progenitor and dust evolution, and\nuncertainty in the galactic extinction law. We find that all the dominant\nsystematic errors shift the dark energy inference towards the DESI 2024 results\nfrom an underlying $\\Lambda$CDM cosmology. A small change in the calibration,\nand change in the Milky Way dust, can give rise to systematic-driven shifts on\n$w_0$-$w_a$ constraints, comparable to the deviation reported from the DESI\n2024 results. We forecast that the systematic uncertainties can shift the\ninference of $w_0-w_a$ by a few times the error ellipse for future low- and\nhigh-$z$ SN~Ia compilations and hence, it is critical to circumvent them to\nrobustly test for deviations from $\\Lambda$. A slider and visualisation tool\nfor quantifying the impact of systematic effects on the fitted cosmological\nparameters is publicly available at: https://github.com/sdhawan21/DEslider.git"
                },
                "authors": [
                    {
                        "name": "Suhail Dhawan"
                    },
                    {
                        "name": "Brodie Popovic"
                    },
                    {
                        "name": "Ariel Goobar"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goobar"
                },
                "author": "Ariel Goobar",
                "arxiv_comment": "9 pages, 5 figures: see figure 3 for the axis of systematic bias.\n  User interface tool to test the impact of systematic variation is available\n  https://github.com/sdhawan21/DEslider.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18664v1",
                "updated": "2024-09-27T11:50:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    50,
                    10,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:50:10Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    50,
                    10,
                    4,
                    271,
                    0
                ],
                "title": "How green is continual learning, really? Analyzing the energy\n  consumption in continual training of vision foundation models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How green is continual learning, really? Analyzing the energy\n  consumption in continual training of vision foundation models"
                },
                "summary": "With the ever-growing adoption of AI, its impact on the environment is no\nlonger negligible. Despite the potential that continual learning could have\ntowards Green AI, its environmental sustainability remains relatively\nuncharted. In this work we aim to gain a systematic understanding of the energy\nefficiency of continual learning algorithms. To that end, we conducted an\nextensive set of empirical experiments comparing the energy consumption of\nrecent representation-, prompt-, and exemplar-based continual learning\nalgorithms and two standard baseline (fine tuning and joint training) when used\nto continually adapt a pre-trained ViT-B/16 foundation model. We performed our\nexperiments on three standard datasets: CIFAR-100, ImageNet-R, and DomainNet.\nAdditionally, we propose a novel metric, the Energy NetScore, which we use\nmeasure the algorithm efficiency in terms of energy-accuracy trade-off. Through\nnumerous evaluations varying the number and size of the incremental learning\nsteps, our experiments demonstrate that different types of continual learning\nalgorithms have very different impacts on energy consumption during both\ntraining and inference. Although often overlooked in the continual learning\nliterature, we found that the energy consumed during the inference phase is\ncrucial for evaluating the environmental sustainability of continual learning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ever-growing adoption of AI, its impact on the environment is no\nlonger negligible. Despite the potential that continual learning could have\ntowards Green AI, its environmental sustainability remains relatively\nuncharted. In this work we aim to gain a systematic understanding of the energy\nefficiency of continual learning algorithms. To that end, we conducted an\nextensive set of empirical experiments comparing the energy consumption of\nrecent representation-, prompt-, and exemplar-based continual learning\nalgorithms and two standard baseline (fine tuning and joint training) when used\nto continually adapt a pre-trained ViT-B/16 foundation model. We performed our\nexperiments on three standard datasets: CIFAR-100, ImageNet-R, and DomainNet.\nAdditionally, we propose a novel metric, the Energy NetScore, which we use\nmeasure the algorithm efficiency in terms of energy-accuracy trade-off. Through\nnumerous evaluations varying the number and size of the incremental learning\nsteps, our experiments demonstrate that different types of continual learning\nalgorithms have very different impacts on energy consumption during both\ntraining and inference. Although often overlooked in the continual learning\nliterature, we found that the energy consumed during the inference phase is\ncrucial for evaluating the environmental sustainability of continual learning\nmodels."
                },
                "authors": [
                    {
                        "name": "Tomaso Trinci"
                    },
                    {
                        "name": "Simone Magistri"
                    },
                    {
                        "name": "Roberto Verdecchia"
                    },
                    {
                        "name": "Andrew D. Bagdanov"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. Bagdanov"
                },
                "author": "Andrew D. Bagdanov",
                "arxiv_comment": "This manuscript has been accepted at the Green FOundation MOdels\n  (GreenFOMO) ECCV 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07393v3",
                "updated": "2024-09-27T11:46:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    46,
                    37,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-11T15:58:59Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    15,
                    58,
                    59,
                    1,
                    163,
                    0
                ],
                "title": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning"
                },
                "summary": "Large Language Models (LLMs) possess extensive knowledge and strong\ncapabilities in performing in-context reasoning. However, previous work\nchallenges their out-of-context reasoning ability, i.e., the ability to infer\ninformation from their training data, instead of from the context or prompt.\nThis paper focuses on a significant aspect of out-of-context reasoning:\nOut-of-Context Knowledge Reasoning (OCKR), which is to combine multiple\nknowledge to infer new knowledge. We designed a synthetic dataset with seven\nrepresentative OCKR tasks to systematically assess the OCKR capabilities of\nLLMs. Using this dataset, we evaluated several LLMs and discovered that their\nproficiency in this aspect is limited, regardless of whether the knowledge is\ntrained in a separate or adjacent training settings. Moreover, training the\nmodel to reason with reasoning examples does not result in significant\nimprovement, while training the model to perform explicit knowledge retrieval\nhelps for retrieving attribute knowledge but not the relation knowledge,\nindicating that the model's limited OCKR capabilities are due to difficulties\nin knowledge retrieval. Furthermore, we treat cross-lingual knowledge transfer\nas a distinct form of OCKR, and evaluate this ability. Our results show that\nthe evaluated model also exhibits limited ability in transferring knowledge\nacross languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess extensive knowledge and strong\ncapabilities in performing in-context reasoning. However, previous work\nchallenges their out-of-context reasoning ability, i.e., the ability to infer\ninformation from their training data, instead of from the context or prompt.\nThis paper focuses on a significant aspect of out-of-context reasoning:\nOut-of-Context Knowledge Reasoning (OCKR), which is to combine multiple\nknowledge to infer new knowledge. We designed a synthetic dataset with seven\nrepresentative OCKR tasks to systematically assess the OCKR capabilities of\nLLMs. Using this dataset, we evaluated several LLMs and discovered that their\nproficiency in this aspect is limited, regardless of whether the knowledge is\ntrained in a separate or adjacent training settings. Moreover, training the\nmodel to reason with reasoning examples does not result in significant\nimprovement, while training the model to perform explicit knowledge retrieval\nhelps for retrieving attribute knowledge but not the relation knowledge,\nindicating that the model's limited OCKR capabilities are due to difficulties\nin knowledge retrieval. Furthermore, we treat cross-lingual knowledge transfer\nas a distinct form of OCKR, and evaluate this ability. Our results show that\nthe evaluated model also exhibits limited ability in transferring knowledge\nacross languages."
                },
                "authors": [
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Ruiqi Gao"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08717v2",
                "updated": "2024-09-27T11:46:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    46,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-13T11:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents"
                },
                "summary": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Zhicheng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dong"
                },
                "author": "Zhicheng Dong",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18661v1",
                "updated": "2024-09-27T11:45:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    56,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:45:56Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    56,
                    4,
                    271,
                    0
                ],
                "title": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are\n  Ineffective in Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are\n  Ineffective in Practice"
                },
                "summary": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\na disruptive impact throughout the computing education community. LLMs have\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\neven act as friendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in being able to\nexplain and resolve compiler error messages -- for decades, one of the most\nfrustrating parts of learning how to code. However, LLM-generated error message\nexplanations have only been assessed by expert programmers in artificial\nconditions. This work sought to understand how novice programmers resolve\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\nwithin-subjects study with $n$ = 106 participants in which students were tasked\nto fix six buggy C programs. For each program, participants were randomly\nassigned to fix the problem using either a stock compiler error message, an\nexpert-handwritten error message, or an error message explanation generated by\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\ngenerated error messages outperformed conventional compiler error messages in\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\nHandwritten explanations still outperform LLM and conventional error messages,\nboth on objective and subjective measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\na disruptive impact throughout the computing education community. LLMs have\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\neven act as friendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in being able to\nexplain and resolve compiler error messages -- for decades, one of the most\nfrustrating parts of learning how to code. However, LLM-generated error message\nexplanations have only been assessed by expert programmers in artificial\nconditions. This work sought to understand how novice programmers resolve\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\nwithin-subjects study with $n$ = 106 participants in which students were tasked\nto fix six buggy C programs. For each program, participants were randomly\nassigned to fix the problem using either a stock compiler error message, an\nexpert-handwritten error message, or an error message explanation generated by\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\ngenerated error messages outperformed conventional compiler error messages in\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\nHandwritten explanations still outperform LLM and conventional error messages,\nboth on objective and subjective measures."
                },
                "authors": [
                    {
                        "name": "Eddie Antonio Santos"
                    },
                    {
                        "name": "Brett A. Becker"
                    }
                ],
                "author_detail": {
                    "name": "Brett A. Becker"
                },
                "author": "Brett A. Becker",
                "arxiv_doi": "10.1145/3689535.3689554",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689535.3689554",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of the 2024 UK and Ireland Computing\n  Education Research conference (UKICER '24)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06349v2",
                "updated": "2024-09-27T11:45:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    9,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-09T14:40:08Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    14,
                    40,
                    8,
                    1,
                    100,
                    0
                ],
                "title": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of\n  LLMs"
                },
                "summary": "The ability to understand causality significantly impacts the competence of\nlarge language models (LLMs) in output explanation and counterfactual\nreasoning, as causality reveals the underlying data distribution. However, the\nlack of a comprehensive benchmark currently limits the evaluation of LLMs'\ncausal learning capabilities. To fill this gap, this paper develops CausalBench\nbased on data from the causal research community, enabling comparative\nevaluations of LLMs against traditional causal learning algorithms. To provide\na comprehensive investigation, we offer three tasks of varying difficulties,\nincluding correlation, causal skeleton, and causality identification.\nEvaluations of 19 leading LLMs reveal that, while closed-source LLMs show\npotential for simple causal relationships, they significantly lag behind\ntraditional algorithms on larger-scale networks ($>50$ nodes). Specifically,\nLLMs struggle with collider structures but excel at chain structures,\nespecially at long-chain causality analogous to Chains-of-Thought techniques.\nThis supports the current prompt approaches while suggesting directions to\nenhance LLMs' causal reasoning capability. Furthermore, CausalBench\nincorporates background knowledge and training data into prompts to thoroughly\nunlock LLMs' text-comprehension ability during evaluation, whose findings\nindicate that, LLM understand causality through semantic associations with\ndistinct entities, rather than directly from contextual information or\nnumerical distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to understand causality significantly impacts the competence of\nlarge language models (LLMs) in output explanation and counterfactual\nreasoning, as causality reveals the underlying data distribution. However, the\nlack of a comprehensive benchmark currently limits the evaluation of LLMs'\ncausal learning capabilities. To fill this gap, this paper develops CausalBench\nbased on data from the causal research community, enabling comparative\nevaluations of LLMs against traditional causal learning algorithms. To provide\na comprehensive investigation, we offer three tasks of varying difficulties,\nincluding correlation, causal skeleton, and causality identification.\nEvaluations of 19 leading LLMs reveal that, while closed-source LLMs show\npotential for simple causal relationships, they significantly lag behind\ntraditional algorithms on larger-scale networks ($>50$ nodes). Specifically,\nLLMs struggle with collider structures but excel at chain structures,\nespecially at long-chain causality analogous to Chains-of-Thought techniques.\nThis supports the current prompt approaches while suggesting directions to\nenhance LLMs' causal reasoning capability. Furthermore, CausalBench\nincorporates background knowledge and training data into prompts to thoroughly\nunlock LLMs' text-comprehension ability during evaluation, whose findings\nindicate that, LLM understand causality through semantic associations with\ndistinct entities, rather than directly from contextual information or\nnumerical distributions."
                },
                "authors": [
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Beicheng Huang"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18659v1",
                "updated": "2024-09-27T11:43:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    43,
                    19,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:43:19Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    43,
                    19,
                    4,
                    271,
                    0
                ],
                "title": "Explainable Enrichment-Driven GrAph Reasoner (EDGAR) for Large Knowledge\n  Graphs with Applications in Drug Repurposing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Enrichment-Driven GrAph Reasoner (EDGAR) for Large Knowledge\n  Graphs with Applications in Drug Repurposing"
                },
                "summary": "Knowledge graphs (KGs) represent connections and relationships between\nreal-world entities. We propose a link prediction framework for KGs named\nEnrichment-Driven GrAph Reasoner (EDGAR), which infers new edges by mining\nentity-local rules. This approach leverages enrichment analysis, a\nwell-established statistical method used to identify mechanisms common to sets\nof differentially expressed genes. EDGAR's inference results are inherently\nexplainable and rankable, with p-values indicating the statistical significance\nof each enrichment-based rule.\n  We demonstrate the framework's effectiveness on a large-scale biomedical KG,\nROBOKOP, focusing on drug repurposing for Alzheimer disease (AD) as a case\nstudy. Initially, we extracted 14 known drugs from the KG and identified 20\ncontextual biomarkers through enrichment analysis, revealing functional\npathways relevant to shared drug efficacy for AD. Subsequently, using the top\n1000 enrichment results, our system identified 1246 additional drug candidates\nfor AD treatment. The top 10 candidates were validated using evidence from\nmedical literature.\n  EDGAR is deployed within ROBOKOP, complete with a web user interface. This is\nthe first study to apply enrichment analysis to large graph completion and drug\nrepurposing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) represent connections and relationships between\nreal-world entities. We propose a link prediction framework for KGs named\nEnrichment-Driven GrAph Reasoner (EDGAR), which infers new edges by mining\nentity-local rules. This approach leverages enrichment analysis, a\nwell-established statistical method used to identify mechanisms common to sets\nof differentially expressed genes. EDGAR's inference results are inherently\nexplainable and rankable, with p-values indicating the statistical significance\nof each enrichment-based rule.\n  We demonstrate the framework's effectiveness on a large-scale biomedical KG,\nROBOKOP, focusing on drug repurposing for Alzheimer disease (AD) as a case\nstudy. Initially, we extracted 14 known drugs from the KG and identified 20\ncontextual biomarkers through enrichment analysis, revealing functional\npathways relevant to shared drug efficacy for AD. Subsequently, using the top\n1000 enrichment results, our system identified 1246 additional drug candidates\nfor AD treatment. The top 10 candidates were validated using evidence from\nmedical literature.\n  EDGAR is deployed within ROBOKOP, complete with a web user interface. This is\nthe first study to apply enrichment analysis to large graph completion and drug\nrepurposing."
                },
                "authors": [
                    {
                        "name": "Olawumi Olasunkanmi"
                    },
                    {
                        "name": "Evan Morris"
                    },
                    {
                        "name": "Yaphet Kebede"
                    },
                    {
                        "name": "Harlin Lee"
                    },
                    {
                        "name": "Stanley Ahalt"
                    },
                    {
                        "name": "Alexander Tropsha"
                    },
                    {
                        "name": "Chris Bizon"
                    }
                ],
                "author_detail": {
                    "name": "Chris Bizon"
                },
                "author": "Chris Bizon",
                "arxiv_comment": "10 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15861v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15861v5",
                "updated": "2024-09-27T11:28:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    28,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-24T17:08:45Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    17,
                    8,
                    45,
                    5,
                    55,
                    0
                ],
                "title": "MATHWELL: Generating Educational Math Word Problems Using Teacher\n  Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATHWELL: Generating Educational Math Word Problems Using Teacher\n  Annotations"
                },
                "summary": "Math word problems are critical K-8 educational tools, but writing them is\ntime consuming and requires extensive expertise. To be educational, problems\nmust be solvable, have accurate answers, and, most importantly, be\neducationally appropriate. We propose that language models have potential to\nsupport K-8 math education by automatically generating word problems. However,\nevaluating educational appropriateness is hard to quantify. We fill this gap by\nhaving teachers evaluate problems generated by LLMs, who find existing models\nand data often fail to be educationally appropriate. We then explore\nautomatically generating educational word problems, ultimately using our expert\nannotations to finetune a 70B language model. Our model, MATHWELL, is the first\nK-8 word problem generator targeted at educational appropriateness. Further\nexpert studies find MATHWELL generates problems far more solvable, accurate,\nand appropriate than public models. MATHWELL also matches GPT-4's problem\nquality while attaining more appropriate reading levels for K-8 students and\navoiding generating harmful questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math word problems are critical K-8 educational tools, but writing them is\ntime consuming and requires extensive expertise. To be educational, problems\nmust be solvable, have accurate answers, and, most importantly, be\neducationally appropriate. We propose that language models have potential to\nsupport K-8 math education by automatically generating word problems. However,\nevaluating educational appropriateness is hard to quantify. We fill this gap by\nhaving teachers evaluate problems generated by LLMs, who find existing models\nand data often fail to be educationally appropriate. We then explore\nautomatically generating educational word problems, ultimately using our expert\nannotations to finetune a 70B language model. Our model, MATHWELL, is the first\nK-8 word problem generator targeted at educational appropriateness. Further\nexpert studies find MATHWELL generates problems far more solvable, accurate,\nand appropriate than public models. MATHWELL also matches GPT-4's problem\nquality while attaining more appropriate reading levels for K-8 students and\navoiding generating harmful questions."
                },
                "authors": [
                    {
                        "name": "Bryan R Christ"
                    },
                    {
                        "name": "Jonathan Kropko"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "arxiv_comment": "24 pages, 10 figures Accepted to EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15861v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15861v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18644v1",
                "updated": "2024-09-27T11:24:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    24,
                    35,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:24:35Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    24,
                    35,
                    4,
                    271,
                    0
                ],
                "title": "Incorporating Precedents for Legal Judgement Prediction on European\n  Court of Human Rights Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating Precedents for Legal Judgement Prediction on European\n  Court of Human Rights Cases"
                },
                "summary": "Inspired by the legal doctrine of stare decisis, which leverages precedents\n(prior cases) for informed decision-making, we explore methods to integrate\nthem into LJP models. To facilitate precedent retrieval, we train a retriever\nwith a fine-grained relevance signal based on the overlap ratio of alleged\narticles between cases. We investigate two strategies to integrate precedents:\ndirect incorporation at inference via label interpolation based on case\nproximity and during training via a precedent fusion module using a\nstacked-cross attention model. We employ joint training of the retriever and\nLJP models to address latent space divergence between them. Our experiments on\nLJP tasks from the ECHR jurisdiction reveal that integrating precedents during\ntraining coupled with joint training of the retriever and LJP model,\noutperforms models without precedents or with precedents incorporated only at\ninference, particularly benefiting sparser articles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the legal doctrine of stare decisis, which leverages precedents\n(prior cases) for informed decision-making, we explore methods to integrate\nthem into LJP models. To facilitate precedent retrieval, we train a retriever\nwith a fine-grained relevance signal based on the overlap ratio of alleged\narticles between cases. We investigate two strategies to integrate precedents:\ndirect incorporation at inference via label interpolation based on case\nproximity and during training via a precedent fusion module using a\nstacked-cross attention model. We employ joint training of the retriever and\nLJP models to address latent space divergence between them. Our experiments on\nLJP tasks from the ECHR jurisdiction reveal that integrating precedents during\ntraining coupled with joint training of the retriever and LJP model,\noutperforms models without precedents or with precedents incorporated only at\ninference, particularly benefiting sparser articles."
                },
                "authors": [
                    {
                        "name": "T. Y. S. S. Santosh"
                    },
                    {
                        "name": "Mohamed Hesham Elganayni"
                    },
                    {
                        "name": "Stanisław Sójka"
                    },
                    {
                        "name": "Matthias Grabmair"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Grabmair"
                },
                "author": "Matthias Grabmair",
                "arxiv_comment": "Accepted to EMNLP Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18640v1",
                "updated": "2024-09-27T11:15:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    15,
                    17,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:15:17Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    15,
                    17,
                    4,
                    271,
                    0
                ],
                "title": "Time-Varying Multi-Seasonal AR Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Varying Multi-Seasonal AR Models"
                },
                "summary": "We propose a seasonal AR model with time-varying parameter processes in both\nthe regular and seasonal parameters. The model is parameterized to guarantee\nstability at every time point and can accommodate multiple seasonal periods.\nThe time evolution is modeled by dynamic shrinkage processes to allow for long\nperiods of essentially constant parameters, periods of rapid change as well as\nabrupt jumps. A Gibbs sampler is developed with a particle Gibbs update step\nfor the AR parameter trajectories. The near-degeneracy of the model, caused by\nthe dynamic shrinkage processes, is shown to pose a challenge for particle\nmethods. To address this, a more robust, faster and accurate approximate\nsampler based on the extended Kalman filter is proposed. The model and the\nnumerical effectiveness of the Gibbs sampler are investigated on simulated and\nreal data. An application to more than a century of monthly US industrial\nproduction data shows interesting clear changes in seasonality over time,\nparticularly during the Great Depression and the recent Covid-19 pandemic.\nKeywords: Bayesian inference; Extended Kalman filter; Locally stationary\nprocesses; Particle MCMC; Seasonality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a seasonal AR model with time-varying parameter processes in both\nthe regular and seasonal parameters. The model is parameterized to guarantee\nstability at every time point and can accommodate multiple seasonal periods.\nThe time evolution is modeled by dynamic shrinkage processes to allow for long\nperiods of essentially constant parameters, periods of rapid change as well as\nabrupt jumps. A Gibbs sampler is developed with a particle Gibbs update step\nfor the AR parameter trajectories. The near-degeneracy of the model, caused by\nthe dynamic shrinkage processes, is shown to pose a challenge for particle\nmethods. To address this, a more robust, faster and accurate approximate\nsampler based on the extended Kalman filter is proposed. The model and the\nnumerical effectiveness of the Gibbs sampler are investigated on simulated and\nreal data. An application to more than a century of monthly US industrial\nproduction data shows interesting clear changes in seasonality over time,\nparticularly during the Great Depression and the recent Covid-19 pandemic.\nKeywords: Bayesian inference; Extended Kalman filter; Locally stationary\nprocesses; Particle MCMC; Seasonality."
                },
                "authors": [
                    {
                        "name": "Ganna Fagerberg"
                    },
                    {
                        "name": "Mattias Villani"
                    },
                    {
                        "name": "Robert Kohn"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kohn"
                },
                "author": "Robert Kohn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18637v1",
                "updated": "2024-09-27T11:08:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    8,
                    2,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:08:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    8,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "Defect density quantification in monolayer MoS2 using helium atom\n  micro-diffraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defect density quantification in monolayer MoS2 using helium atom\n  micro-diffraction"
                },
                "summary": "Sulfur vacancy defects mediate a wide range of optoelectronic properties in\nMoS2, with precise control of defect density allowing for tuneable\noptoelectronic devices. However, accurate measurement of defect density in\nmonolayer and few-layer samples poses a challenge due to their small scattering\ncross-sections to photon or electron probes. Conventional lab-based techniques\nsuch as Raman and photoluminescence can infer approximate defect density in\nmicro-scale samples via optoelectronic properties, but they require validation\nusing stoichiometric beam-line XPS. We introduce an ultra-low energy (~64 meV)\nand non-intrusive lab-based technique to quantify the surface defect density in\nmicron-scale monolayer MoS2. Here we show that a recently developed technique,\nhelium atom micro-diffraction (referred to as scanning helium microscopy (SHeM)\nin literature), can be used to directly measure vacancy-type defect density in\n2D materials by performing atom diffraction from a microscopic spot. SHeM uses\na neutral, inert, and thermal energy probe of helium-4 atoms to measure ordered\nand disordered atom-surface scattering allowing the level of surface order to\nbe inferred. The presented method enables rapid, non-damaging, and\nmaterial-agnostic lab-based quantification of defect density in 2D materials, a\ncrucial step towards the wider adoption of 2D semiconductors in devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sulfur vacancy defects mediate a wide range of optoelectronic properties in\nMoS2, with precise control of defect density allowing for tuneable\noptoelectronic devices. However, accurate measurement of defect density in\nmonolayer and few-layer samples poses a challenge due to their small scattering\ncross-sections to photon or electron probes. Conventional lab-based techniques\nsuch as Raman and photoluminescence can infer approximate defect density in\nmicro-scale samples via optoelectronic properties, but they require validation\nusing stoichiometric beam-line XPS. We introduce an ultra-low energy (~64 meV)\nand non-intrusive lab-based technique to quantify the surface defect density in\nmicron-scale monolayer MoS2. Here we show that a recently developed technique,\nhelium atom micro-diffraction (referred to as scanning helium microscopy (SHeM)\nin literature), can be used to directly measure vacancy-type defect density in\n2D materials by performing atom diffraction from a microscopic spot. SHeM uses\na neutral, inert, and thermal energy probe of helium-4 atoms to measure ordered\nand disordered atom-surface scattering allowing the level of surface order to\nbe inferred. The presented method enables rapid, non-damaging, and\nmaterial-agnostic lab-based quantification of defect density in 2D materials, a\ncrucial step towards the wider adoption of 2D semiconductors in devices."
                },
                "authors": [
                    {
                        "name": "Aleksandar Radic"
                    },
                    {
                        "name": "Nick von Jeinsen"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yiru Zhu"
                    },
                    {
                        "name": "Ismail Sami"
                    },
                    {
                        "name": "Vivian Perez"
                    },
                    {
                        "name": "David Ward"
                    },
                    {
                        "name": "Andrew Jardine"
                    },
                    {
                        "name": "Manish Chhowalla"
                    },
                    {
                        "name": "Sam Lambrick"
                    }
                ],
                "author_detail": {
                    "name": "Sam Lambrick"
                },
                "author": "Sam Lambrick",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.04863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.04863v2",
                "updated": "2024-09-27T11:07:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    7,
                    28,
                    4,
                    271,
                    0
                ],
                "published": "2023-07-10T19:19:33Z",
                "published_parsed": [
                    2023,
                    7,
                    10,
                    19,
                    19,
                    33,
                    0,
                    191,
                    0
                ],
                "title": "Interpretable ML for High-Frequency Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable ML for High-Frequency Execution"
                },
                "summary": "Order placement tactics play a crucial role in high-frequency trading\nalgorithms and their design is based on understanding the dynamics of the order\nbook. Using high quality high-frequency data and a set of microstructural\nfeatures, we exhibit strong state dependence properties of the fill probability\nfunction. We train a neural network to infer the fill probability function for\na fixed horizon. Since we aim at providing a high-frequency execution\nframework, we use a simple architecture. A weighting method is applied to the\nloss function such that the model learns from censored data. By comparing\nnumerical results obtained on both digital asset centralized exchanges (CEXs)\nand stock markets, we are able to analyze dissimilarities between feature\nimportances of the fill probability of small tick crypto pairs and Euronext\nequities. The practical use of this model is illustrated with a fixed time\nhorizon execution problem in which both the decision to post a limit order or\nto immediately execute and the optimal distance of placement are characterized.\nWe discuss the importance of accurately estimating the clean-up cost that\noccurs in the case of a non-execution and we show it can be well approximated\nby a smooth function of market features. We finally assess the performance of\nour model with a backtesting approach that avoids the insertion of hypothetical\norders and makes possible to test the order placement algorithm with orders\nthat realistically impact the price formation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order placement tactics play a crucial role in high-frequency trading\nalgorithms and their design is based on understanding the dynamics of the order\nbook. Using high quality high-frequency data and a set of microstructural\nfeatures, we exhibit strong state dependence properties of the fill probability\nfunction. We train a neural network to infer the fill probability function for\na fixed horizon. Since we aim at providing a high-frequency execution\nframework, we use a simple architecture. A weighting method is applied to the\nloss function such that the model learns from censored data. By comparing\nnumerical results obtained on both digital asset centralized exchanges (CEXs)\nand stock markets, we are able to analyze dissimilarities between feature\nimportances of the fill probability of small tick crypto pairs and Euronext\nequities. The practical use of this model is illustrated with a fixed time\nhorizon execution problem in which both the decision to post a limit order or\nto immediately execute and the optimal distance of placement are characterized.\nWe discuss the importance of accurately estimating the clean-up cost that\noccurs in the case of a non-execution and we show it can be well approximated\nby a smooth function of market features. We finally assess the performance of\nour model with a backtesting approach that avoids the insertion of hypothetical\norders and makes possible to test the order placement algorithm with orders\nthat realistically impact the price formation process."
                },
                "authors": [
                    {
                        "name": "Timothée Fabre"
                    },
                    {
                        "name": "Vincent Ragel"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Ragel"
                },
                "author": "Vincent Ragel",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.04863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.04863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18618v1",
                "updated": "2024-09-27T10:35:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T10:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback"
                },
                "summary": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback."
                },
                "authors": [
                    {
                        "name": "Jaepill Choi"
                    },
                    {
                        "name": "Kyubyung Chae"
                    },
                    {
                        "name": "Jiwoo Song"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17699v2",
                "updated": "2024-09-27T10:16:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    16,
                    37,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T10:12:19Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hamed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Mark Purcell"
                    }
                ],
                "author_detail": {
                    "name": "Mark Purcell"
                },
                "author": "Mark Purcell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18602v1",
                "updated": "2024-09-27T10:07:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    7,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T10:07:33Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    7,
                    33,
                    4,
                    271,
                    0
                ],
                "title": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to\n  Addressee Recognition and Response Selection in Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to\n  Addressee Recognition and Response Selection in Conversations"
                },
                "summary": "Assessing the performance of systems to classify Multi-Party Conversations\n(MPC) is challenging due to the interconnection between linguistic and\nstructural characteristics of conversations. Conventional evaluation methods\noften overlook variances in model behavior across different levels of\nstructural complexity on interaction graphs. In this work, we propose a\nmethodological pipeline to investigate model performance across specific\nstructural attributes of conversations. As a proof of concept we focus on\nResponse Selection and Addressee Recognition tasks, to diagnose model\nweaknesses. To this end, we extract representative diagnostic subdatasets with\na fixed number of users and a good structural variety from a large and open\ncorpus of online MPCs. We further frame our work in terms of data minimization,\navoiding the use of original usernames to preserve privacy, and propose\nalternatives to using original text messages. Results show that response\nselection relies more on the textual content of conversations, while addressee\nrecognition requires capturing their structural dimension. Using an LLM in a\nzero-shot setting, we further highlight how sensitivity to prompt variations is\ntask-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the performance of systems to classify Multi-Party Conversations\n(MPC) is challenging due to the interconnection between linguistic and\nstructural characteristics of conversations. Conventional evaluation methods\noften overlook variances in model behavior across different levels of\nstructural complexity on interaction graphs. In this work, we propose a\nmethodological pipeline to investigate model performance across specific\nstructural attributes of conversations. As a proof of concept we focus on\nResponse Selection and Addressee Recognition tasks, to diagnose model\nweaknesses. To this end, we extract representative diagnostic subdatasets with\na fixed number of users and a good structural variety from a large and open\ncorpus of online MPCs. We further frame our work in terms of data minimization,\navoiding the use of original usernames to preserve privacy, and propose\nalternatives to using original text messages. Results show that response\nselection relies more on the textual content of conversations, while addressee\nrecognition requires capturing their structural dimension. Using an LLM in a\nzero-shot setting, we further highlight how sensitivity to prompt variations is\ntask-dependent."
                },
                "authors": [
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Maryam Sajedinia"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Sara Tonelli"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Accepted to EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08424v2",
                "updated": "2024-09-27T10:05:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    5,
                    56,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-12T12:15:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    12,
                    15,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task"
                },
                "summary": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot."
                },
                "authors": [
                    {
                        "name": "Hassan Ali"
                    },
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Accepted at ICSR 2024,14 pages,5 figures,2 tables; work was co-funded\n  by Horizon Europe project TERAIS under Grant agreement number 101079338",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18596v1",
                "updated": "2024-09-27T09:56:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    56,
                    2,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    56,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "ASAG2024: A Combined Benchmark for Short Answer Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAG2024: A Combined Benchmark for Short Answer Grading"
                },
                "summary": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems."
                },
                "authors": [
                    {
                        "name": "Gérôme Meyer"
                    },
                    {
                        "name": "Philip Breuer"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "arxiv_doi": "10.1145/3649409.3691083",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649409.3691083",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGCSE-Virtual 2024",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18594v1",
                "updated": "2024-09-27T09:53:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    53,
                    48,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:53:48Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    53,
                    48,
                    4,
                    271,
                    0
                ],
                "title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models"
                },
                "summary": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime."
                },
                "authors": [
                    {
                        "name": "Ricardo Knauer"
                    },
                    {
                        "name": "Mario Koddenbrock"
                    },
                    {
                        "name": "Raphael Wallsberger"
                    },
                    {
                        "name": "Nicholas M. Brisson"
                    },
                    {
                        "name": "Georg N. Duda"
                    },
                    {
                        "name": "Deborah Falla"
                    },
                    {
                        "name": "David W. Evans"
                    },
                    {
                        "name": "Erik Rodner"
                    }
                ],
                "author_detail": {
                    "name": "Erik Rodner"
                },
                "author": "Erik Rodner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14842v2",
                "updated": "2024-09-27T09:52:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    52,
                    57,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T09:20:19Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    20,
                    19,
                    0,
                    267,
                    0
                ],
                "title": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks"
                },
                "summary": "This paper presents the submission of Huawei Translation Services Center\n(HW-TSC) to machine translation tasks of the 20th China Conference on Machine\nTranslation (CCMT 2024). We participate in the bilingual machine translation\ntask and multi-domain machine translation task. For these two translation\ntasks, we use training strategies such as regularized dropout, bidirectional\ntraining, data diversification, forward translation, back translation,\nalternated training, curriculum learning, and transductive ensemble learning to\ntrain neural machine translation (NMT) models based on the deep Transformer-big\narchitecture. Furthermore, to explore whether large language model (LLM) can\nhelp improve the translation quality of NMT systems, we use supervised\nfine-tuning to train llama2-13b as an Automatic post-editing (APE) model to\nimprove the translation results of the NMT model on the multi-domain machine\ntranslation task. By using these plyometric strategies, our submission achieves\na competitive result in the final evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the submission of Huawei Translation Services Center\n(HW-TSC) to machine translation tasks of the 20th China Conference on Machine\nTranslation (CCMT 2024). We participate in the bilingual machine translation\ntask and multi-domain machine translation task. For these two translation\ntasks, we use training strategies such as regularized dropout, bidirectional\ntraining, data diversification, forward translation, back translation,\nalternated training, curriculum learning, and transductive ensemble learning to\ntrain neural machine translation (NMT) models based on the deep Transformer-big\narchitecture. Furthermore, to explore whether large language model (LLM) can\nhelp improve the translation quality of NMT systems, we use supervised\nfine-tuning to train llama2-13b as an Automatic post-editing (APE) model to\nimprove the translation results of the NMT model on the multi-domain machine\ntranslation task. By using these plyometric strategies, our submission achieves\na competitive result in the final evaluation."
                },
                "authors": [
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Jiawei Zheng"
                    },
                    {
                        "name": "Bin Wei"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Weidong Zhang"
                    },
                    {
                        "name": "Ning Xie"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "14 pages, 2 figures, 6 Tables, CCMT2024. arXiv admin note:\n  substantial text overlap with arXiv:2409.14800",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18583v1",
                "updated": "2024-09-27T09:41:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    41,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:41:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    41,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models"
                },
                "summary": "Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods."
                },
                "authors": [
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18581v1",
                "updated": "2024-09-27T09:37:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    37,
                    9,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:37:09Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    37,
                    9,
                    4,
                    271,
                    0
                ],
                "title": "Using Deep Autoregressive Models as Causal Inference Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Deep Autoregressive Models as Causal Inference Engines"
                },
                "summary": "Existing causal inference (CI) models are limited to primarily handling\nlow-dimensional confounders and singleton actions. We propose an autoregressive\n(AR) CI framework capable of handling complex confounders and sequential\nactions common in modern applications. We accomplish this by {\\em\nsequencification}, transforming data from an underlying causal diagram into a\nsequence of tokens. This approach not only enables training with data generated\nfrom any DAG but also extends existing CI capabilities to accommodate\nestimating several statistical quantities using a {\\em single} model. We can\ndirectly predict interventional probabilities, simplifying inference and\nenhancing outcome prediction accuracy. We demonstrate that an AR model adapted\nfor CI is efficient and effective in various complex applications such as\nnavigating mazes, playing chess endgames, and evaluating the impact of certain\nkeywords on paper acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal inference (CI) models are limited to primarily handling\nlow-dimensional confounders and singleton actions. We propose an autoregressive\n(AR) CI framework capable of handling complex confounders and sequential\nactions common in modern applications. We accomplish this by {\\em\nsequencification}, transforming data from an underlying causal diagram into a\nsequence of tokens. This approach not only enables training with data generated\nfrom any DAG but also extends existing CI capabilities to accommodate\nestimating several statistical quantities using a {\\em single} model. We can\ndirectly predict interventional probabilities, simplifying inference and\nenhancing outcome prediction accuracy. We demonstrate that an AR model adapted\nfor CI is efficient and effective in various complex applications such as\nnavigating mazes, playing chess endgames, and evaluating the impact of certain\nkeywords on paper acceptance rates."
                },
                "authors": [
                    {
                        "name": "Daniel Jiwoong Im"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Nakul Verma"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18575v1",
                "updated": "2024-09-27T09:20:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    20,
                    42,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    20,
                    42,
                    4,
                    271,
                    0
                ],
                "title": "Corpus-informed Retrieval Augmented Generation of Clarifying Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corpus-informed Retrieval Augmented Generation of Clarifying Questions"
                },
                "summary": "This study aims to develop models that generate corpus informed clarifying\nquestions for web search, in a way that ensures the questions align with the\navailable information in the retrieval corpus. We demonstrate the effectiveness\nof Retrieval Augmented Language Models (RAG) in this process, emphasising their\nability to (i) jointly model the user query and retrieval corpus to pinpoint\nthe uncertainty and ask for clarifications end-to-end and (ii) model more\nevidence documents, which can be used towards increasing the breadth of the\nquestions asked. However, we observe that in current datasets search intents\nare largely unsupported by the corpus, which is problematic both for training\nand evaluation. This causes question generation models to ``hallucinate'', ie.\nsuggest intents that are not in the corpus, which can have detrimental effects\nin performance. To address this, we propose dataset augmentation methods that\nalign the ground truth clarifications with the retrieval corpus. Additionally,\nwe explore techniques to enhance the relevance of the evidence pool during\ninference, but find that identifying ground truth intents within the corpus\nremains challenging. Our analysis suggests that this challenge is partly due to\nthe bias of current datasets towards clarification taxonomies and calls for\ndata that can support generating corpus-informed clarifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to develop models that generate corpus informed clarifying\nquestions for web search, in a way that ensures the questions align with the\navailable information in the retrieval corpus. We demonstrate the effectiveness\nof Retrieval Augmented Language Models (RAG) in this process, emphasising their\nability to (i) jointly model the user query and retrieval corpus to pinpoint\nthe uncertainty and ask for clarifications end-to-end and (ii) model more\nevidence documents, which can be used towards increasing the breadth of the\nquestions asked. However, we observe that in current datasets search intents\nare largely unsupported by the corpus, which is problematic both for training\nand evaluation. This causes question generation models to ``hallucinate'', ie.\nsuggest intents that are not in the corpus, which can have detrimental effects\nin performance. To address this, we propose dataset augmentation methods that\nalign the ground truth clarifications with the retrieval corpus. Additionally,\nwe explore techniques to enhance the relevance of the evidence pool during\ninference, but find that identifying ground truth intents within the corpus\nremains challenging. Our analysis suggests that this challenge is partly due to\nthe bias of current datasets towards clarification taxonomies and calls for\ndata that can support generating corpus-informed clarifications."
                },
                "authors": [
                    {
                        "name": "Antonios Minas Krasakis"
                    },
                    {
                        "name": "Andrew Yates"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15866v2",
                "updated": "2024-09-27T09:17:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    17,
                    52,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-24T18:23:51Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    18,
                    23,
                    51,
                    4,
                    145,
                    0
                ],
                "title": "Governing the Commons: Code Ownership and Code-Clones in Large-Scale\n  Software Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governing the Commons: Code Ownership and Code-Clones in Large-Scale\n  Software Development"
                },
                "summary": "Context: In software development organizations employing weak or collective\nownership, different teams are allowed and expected to autonomously perform\nchanges in various components. This creates diversity both in the knowledge of,\nand in the responsibility for, individual components.\n  Objective: Our objective is to understand how and why different teams\nintroduce technical debt in the form of code clones as they change different\ncomponents.\n  Method: We collected data about change size and clone introductions made by\nten teams in eight components which was part of a large industrial software\nsystem. We then designed a Multi-Level Generalized Linear Model (MLGLM), to\nillustrate the teams' differing behavior. Finally, we discussed the results\nwith three development teams, plus line manager and the architect team,\nevaluating whether the model inferences aligned with what they expected.\nResponses were recorded and thematically coded.\n  Results: The results show that teams do behave differently in different\ncomponents, and the feedback from the teams indicates that this method of\nillustrating team behavior can be useful as a complement to traditional summary\nstatistics of ownership.\n  Conclusions: We find that our model-based approach produces useful\nvisualizations of team introductions of code clones as they change different\ncomponents. Practitioners stated that the visualizations gave them insights\nthat were useful, and by comparing with an average team, inter-team comparisons\ncan be avoided. Thus, this has the potential to be a useful feedback tool for\nteams in software development organizations that employ weak or collective\nownership.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: In software development organizations employing weak or collective\nownership, different teams are allowed and expected to autonomously perform\nchanges in various components. This creates diversity both in the knowledge of,\nand in the responsibility for, individual components.\n  Objective: Our objective is to understand how and why different teams\nintroduce technical debt in the form of code clones as they change different\ncomponents.\n  Method: We collected data about change size and clone introductions made by\nten teams in eight components which was part of a large industrial software\nsystem. We then designed a Multi-Level Generalized Linear Model (MLGLM), to\nillustrate the teams' differing behavior. Finally, we discussed the results\nwith three development teams, plus line manager and the architect team,\nevaluating whether the model inferences aligned with what they expected.\nResponses were recorded and thematically coded.\n  Results: The results show that teams do behave differently in different\ncomponents, and the feedback from the teams indicates that this method of\nillustrating team behavior can be useful as a complement to traditional summary\nstatistics of ownership.\n  Conclusions: We find that our model-based approach produces useful\nvisualizations of team introductions of code clones as they change different\ncomponents. Practitioners stated that the visualizations gave them insights\nthat were useful, and by comparing with an average team, inter-team comparisons\ncan be avoided. Thus, this has the potential to be a useful feedback tool for\nteams in software development organizations that employ weak or collective\nownership."
                },
                "authors": [
                    {
                        "name": "Anders Sundelin"
                    },
                    {
                        "name": "Javier Gonzalez-Huerta"
                    },
                    {
                        "name": "Richard Torkar"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Wnuk"
                },
                "author": "Krzysztof Wnuk",
                "arxiv_comment": "31 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18572v1",
                "updated": "2024-09-27T09:15:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    15,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:15:44Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    15,
                    44,
                    4,
                    271,
                    0
                ],
                "title": "Towards an active-learning approach to resource allocation for\n  population-based damage prognosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an active-learning approach to resource allocation for\n  population-based damage prognosis"
                },
                "summary": "Damage prognosis is, arguably, one of the most difficult tasks of structural\nhealth monitoring (SHM). To address common problems of damage prognosis, a\npopulation-based SHM (PBSHM) approach is adopted in the current work. In this\napproach the prognosis problem is considered as an information-sharing problem\nwhere data from past structures are exploited to make more accurate inferences\nregarding currently-degrading structures. For a given population, there may\nexist restrictions on the resources available to conduct monitoring; thus, the\ncurrent work studies the problem of allocating such resources within a\npopulation of degrading structures with a view to maximising the\ndamage-prognosis accuracy. The challenges of the current framework are mainly\nassociated with the inference of outliers on the level of damage evolution,\ngiven partial data from the damage-evolution phenomenon. The current approach\nconsiders an initial population of structures for which damage evolution is\nextensively observed. Subsequently, a second population of structures with\nevolving damage is considered for which two monitoring systems are available, a\nlow-availability and high-fidelity (low-uncertainty) one, and a\nwidely-available and low-fidelity (high-uncertainty) one. The task of the\ncurrent work is to follow an active-learning approach to identify the\nstructures to which the high-fidelity system should be assigned in order to\nenhance the predictive capabilities of the machine-learning model throughout\nthe population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Damage prognosis is, arguably, one of the most difficult tasks of structural\nhealth monitoring (SHM). To address common problems of damage prognosis, a\npopulation-based SHM (PBSHM) approach is adopted in the current work. In this\napproach the prognosis problem is considered as an information-sharing problem\nwhere data from past structures are exploited to make more accurate inferences\nregarding currently-degrading structures. For a given population, there may\nexist restrictions on the resources available to conduct monitoring; thus, the\ncurrent work studies the problem of allocating such resources within a\npopulation of degrading structures with a view to maximising the\ndamage-prognosis accuracy. The challenges of the current framework are mainly\nassociated with the inference of outliers on the level of damage evolution,\ngiven partial data from the damage-evolution phenomenon. The current approach\nconsiders an initial population of structures for which damage evolution is\nextensively observed. Subsequently, a second population of structures with\nevolving damage is considered for which two monitoring systems are available, a\nlow-availability and high-fidelity (low-uncertainty) one, and a\nwidely-available and low-fidelity (high-uncertainty) one. The task of the\ncurrent work is to follow an active-learning approach to identify the\nstructures to which the high-fidelity system should be assigned in order to\nenhance the predictive capabilities of the machine-learning model throughout\nthe population."
                },
                "authors": [
                    {
                        "name": "George Tsialiamanis"
                    },
                    {
                        "name": "Keith Worden"
                    },
                    {
                        "name": "Nikolaos Dervilis"
                    },
                    {
                        "name": "Aidan J Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Aidan J Hughes"
                },
                "author": "Aidan J Hughes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14364v2",
                "updated": "2024-09-27T09:13:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    13,
                    19,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    51,
                    18,
                    6,
                    266,
                    0
                ],
                "title": "More Effective LLM Compressed Tokens with Uniformly Spread Position\n  Identifiers and Compression Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Effective LLM Compressed Tokens with Uniformly Spread Position\n  Identifiers and Compression Loss"
                },
                "summary": "Compressing Transformer inputs into compressd tokens allows running LLMs with\nimproved speed and cost efficiency. Based on the compression method ICAE, we\ncarefully examine the position identifier choices for compressed tokens and\nalso propose a new compression loss. We demonstrate empirically that our\nproposed methods achieve significantly higher compression ratios (15x compared\nto 4x for ICAE), while being able to attain comparable reconstruction\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Transformer inputs into compressd tokens allows running LLMs with\nimproved speed and cost efficiency. Based on the compression method ICAE, we\ncarefully examine the position identifier choices for compressed tokens and\nalso propose a new compression loss. We demonstrate empirically that our\nproposed methods achieve significantly higher compression ratios (15x compared\nto 4x for ICAE), while being able to attain comparable reconstruction\nperformance."
                },
                "authors": [
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18566v1",
                "updated": "2024-09-27T09:10:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    10,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    10,
                    44,
                    4,
                    271,
                    0
                ],
                "title": "Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time"
                },
                "summary": "The demand for executing Deep Neural Networks (DNNs) with low latency and\nminimal power consumption at the edge has led to the development of advanced\nheterogeneous Systems-on-Chips (SoCs) that incorporate multiple specialized\ncomputing units (CUs), such as accelerators. Offloading DNN computations to a\nspecific CU from the available set often exposes accuracy vs efficiency\ntrade-offs, due to differences in their supported operations (e.g., standard\nvs. depthwise convolution) or data representations (e.g., more/less\naggressively quantized). A challenging yet unresolved issue is how to map a DNN\nonto these multi-CU systems to maximally exploit the parallelization\npossibilities while taking accuracy into account. To address this problem, we\npresent ODiMO, a hardware-aware tool that efficiently explores fine-grain\nmapping of DNNs among various on-chip CUs, during the training phase. ODiMO\nstrategically splits individual layers of the neural network and executes them\nin parallel on the multiple available CUs, aiming to balance the total\ninference energy consumption or latency with the resulting accuracy, impacted\nby the unique features of the different hardware units. We test our approach on\nCIFAR-10, CIFAR-100, and ImageNet, targeting two open-source heterogeneous\nSoCs, i.e., DIANA and Darkside. We obtain a rich collection of Pareto-optimal\nnetworks in the accuracy vs. energy or latency space. We show that ODiMO\nreduces the latency of a DNN executed on the Darkside SoC by up to 8x at\niso-accuracy, compared to manual heuristic mappings. When targeting energy, on\nthe same SoC, ODiMO produced up to 50.8x more efficient mappings, with minimal\naccuracy drop (< 0.3%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for executing Deep Neural Networks (DNNs) with low latency and\nminimal power consumption at the edge has led to the development of advanced\nheterogeneous Systems-on-Chips (SoCs) that incorporate multiple specialized\ncomputing units (CUs), such as accelerators. Offloading DNN computations to a\nspecific CU from the available set often exposes accuracy vs efficiency\ntrade-offs, due to differences in their supported operations (e.g., standard\nvs. depthwise convolution) or data representations (e.g., more/less\naggressively quantized). A challenging yet unresolved issue is how to map a DNN\nonto these multi-CU systems to maximally exploit the parallelization\npossibilities while taking accuracy into account. To address this problem, we\npresent ODiMO, a hardware-aware tool that efficiently explores fine-grain\nmapping of DNNs among various on-chip CUs, during the training phase. ODiMO\nstrategically splits individual layers of the neural network and executes them\nin parallel on the multiple available CUs, aiming to balance the total\ninference energy consumption or latency with the resulting accuracy, impacted\nby the unique features of the different hardware units. We test our approach on\nCIFAR-10, CIFAR-100, and ImageNet, targeting two open-source heterogeneous\nSoCs, i.e., DIANA and Darkside. We obtain a rich collection of Pareto-optimal\nnetworks in the accuracy vs. energy or latency space. We show that ODiMO\nreduces the latency of a DNN executed on the Darkside SoC by up to 8x at\niso-accuracy, compared to manual heuristic mappings. When targeting energy, on\nthe same SoC, ODiMO produced up to 50.8x more efficient mappings, with minimal\naccuracy drop (< 0.3%)."
                },
                "authors": [
                    {
                        "name": "Matteo Risso"
                    },
                    {
                        "name": "Alessio Burrello"
                    },
                    {
                        "name": "Daniele Jahier Pagliari"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Jahier Pagliari"
                },
                "author": "Daniele Jahier Pagliari",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19181v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19181v3",
                "updated": "2024-09-27T08:57:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    57,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-28T07:22:16Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    7,
                    22,
                    16,
                    3,
                    88,
                    0
                ],
                "title": "Make Large Language Model a Better Ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Large Language Model a Better Ranker"
                },
                "summary": "Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines."
                },
                "authors": [
                    {
                        "name": "Wen-Shuo Chao"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Hengshu Zhu"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19181v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19181v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18553v1",
                "updated": "2024-09-27T08:45:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    45,
                    55,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:45:55Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    45,
                    55,
                    4,
                    271,
                    0
                ],
                "title": "Efficient Noise Mitigation for Enhancing Inference Accuracy in DNNs on\n  Mixed-Signal Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Noise Mitigation for Enhancing Inference Accuracy in DNNs on\n  Mixed-Signal Accelerators"
                },
                "summary": "In this paper, we propose a framework to enhance the robustness of the neural\nmodels by mitigating the effects of process-induced and aging-related\nvariations of analog computing components on the accuracy of the analog neural\nnetworks. We model these variations as the noise affecting the precision of the\nactivations and introduce a denoising block inserted between selected layers of\na pre-trained model. We demonstrate that training the denoising block\nsignificantly increases the model's robustness against various noise levels. To\nminimize the overhead associated with adding these blocks, we present an\nexploration algorithm to identify optimal insertion points for the denoising\nblocks. Additionally, we propose a specialized architecture to efficiently\nexecute the denoising blocks, which can be integrated into mixed-signal\naccelerators. We evaluate the effectiveness of our approach using Deep Neural\nNetwork (DNN) models trained on the ImageNet and CIFAR-10 datasets. The results\nshow that on average, by accepting 2.03% parameter count overhead, the accuracy\ndrop due to the variations reduces from 31.7% to 1.15%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a framework to enhance the robustness of the neural\nmodels by mitigating the effects of process-induced and aging-related\nvariations of analog computing components on the accuracy of the analog neural\nnetworks. We model these variations as the noise affecting the precision of the\nactivations and introduce a denoising block inserted between selected layers of\na pre-trained model. We demonstrate that training the denoising block\nsignificantly increases the model's robustness against various noise levels. To\nminimize the overhead associated with adding these blocks, we present an\nexploration algorithm to identify optimal insertion points for the denoising\nblocks. Additionally, we propose a specialized architecture to efficiently\nexecute the denoising blocks, which can be integrated into mixed-signal\naccelerators. We evaluate the effectiveness of our approach using Deep Neural\nNetwork (DNN) models trained on the ImageNet and CIFAR-10 datasets. The results\nshow that on average, by accepting 2.03% parameter count overhead, the accuracy\ndrop due to the variations reduces from 31.7% to 1.15%."
                },
                "authors": [
                    {
                        "name": "Seyedarmin Azizi"
                    },
                    {
                        "name": "Mohammad Erfan Sadeghi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15564v2",
                "updated": "2024-09-27T08:40:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    40,
                    26,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T21:38:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    21,
                    38,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "CauSkelNet: Causal Representation Learning for Human Behaviour Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CauSkelNet: Causal Representation Learning for Human Behaviour Analysis"
                },
                "summary": "Constrained by the lack of model interpretability and a deep understanding of\nhuman movement in traditional movement recognition machine learning methods,\nthis study introduces a novel representation learning method based on causal\ninference to better understand human joint dynamics and complex behaviors. We\npropose a two-stage framework that combines the Peter-Clark (PC) algorithm and\nKullback-Leibler (KL) divergence to identify and quantify causal relationships\nbetween joints. Our method effectively captures interactions and produces\ninterpretable, robust representations. Experiments on the EmoPain dataset show\nthat our causal GCN outperforms traditional GCNs in accuracy, F1 score, and\nrecall, especially in detecting protective behaviors. The model is also highly\ninvariant to data scale changes, enhancing its reliability in practical\napplications. Our approach advances human motion analysis and paves the way for\nmore adaptive intelligent healthcare solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained by the lack of model interpretability and a deep understanding of\nhuman movement in traditional movement recognition machine learning methods,\nthis study introduces a novel representation learning method based on causal\ninference to better understand human joint dynamics and complex behaviors. We\npropose a two-stage framework that combines the Peter-Clark (PC) algorithm and\nKullback-Leibler (KL) divergence to identify and quantify causal relationships\nbetween joints. Our method effectively captures interactions and produces\ninterpretable, robust representations. Experiments on the EmoPain dataset show\nthat our causal GCN outperforms traditional GCNs in accuracy, F1 score, and\nrecall, especially in detecting protective behaviors. The model is also highly\ninvariant to data scale changes, enhancing its reliability in practical\napplications. Our approach advances human motion analysis and paves the way for\nmore adaptive intelligent healthcare solutions."
                },
                "authors": [
                    {
                        "name": "Xingrui Gu"
                    },
                    {
                        "name": "Chuyi Jiang"
                    },
                    {
                        "name": "Erte Wang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Qiang Cui"
                    },
                    {
                        "name": "Leimin Tian"
                    },
                    {
                        "name": "Lianlong Wu"
                    },
                    {
                        "name": "Siyang Song"
                    },
                    {
                        "name": "Chuang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Yu"
                },
                "author": "Chuang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00343v2",
                "updated": "2024-09-27T08:29:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    29,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-30T12:46:15Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    12,
                    46,
                    15,
                    5,
                    90,
                    0
                ],
                "title": "Commonsense Scene Graph-based Target Localization for Object Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Scene Graph-based Target Localization for Object Search"
                },
                "summary": "Object search is a fundamental skill for household robots, yet the core\nproblem lies in the robot's ability to locate the target object accurately. The\ndynamic nature of household environments, characterized by the arbitrary\nplacement of daily objects by users, makes it challenging to perform target\nlocalization. To efficiently locate the target object, the robot needs to be\nequipped with knowledge at both the object and room level. However, existing\napproaches rely solely on one type of knowledge, leading to unsatisfactory\nobject localization performance and, consequently, inefficient object search\nprocesses. To address this problem, we propose a commonsense scene graph-based\ntarget localization, CSG-TL, to enhance target object search in the household\nenvironment. Given the pre-built map with stationary items, the robot models\nthe room-level knowledge with object-level commonsense knowledge generated by a\nlarge language model (LLM) to a commonsense scene graph (CSG), supporting both\ntypes of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on\ntarget localization, extensive experiments are performed on the real-world\nScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to\nan object search framework, CSG-OS, validated in both simulated and real-world\nenvironments. Code and videos are available at\nhttps://sites.google.com/view/csg-os.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object search is a fundamental skill for household robots, yet the core\nproblem lies in the robot's ability to locate the target object accurately. The\ndynamic nature of household environments, characterized by the arbitrary\nplacement of daily objects by users, makes it challenging to perform target\nlocalization. To efficiently locate the target object, the robot needs to be\nequipped with knowledge at both the object and room level. However, existing\napproaches rely solely on one type of knowledge, leading to unsatisfactory\nobject localization performance and, consequently, inefficient object search\nprocesses. To address this problem, we propose a commonsense scene graph-based\ntarget localization, CSG-TL, to enhance target object search in the household\nenvironment. Given the pre-built map with stationary items, the robot models\nthe room-level knowledge with object-level commonsense knowledge generated by a\nlarge language model (LLM) to a commonsense scene graph (CSG), supporting both\ntypes of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on\ntarget localization, extensive experiments are performed on the real-world\nScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to\nan object search framework, CSG-OS, validated in both simulated and real-world\nenvironments. Code and videos are available at\nhttps://sites.google.com/view/csg-os."
                },
                "authors": [
                    {
                        "name": "Wenqi Ge"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09972v3",
                "updated": "2024-09-27T08:22:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    22,
                    21,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-15T02:38:26Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    2,
                    38,
                    26,
                    4,
                    75,
                    0
                ],
                "title": "Think Twice Before Trusting: Self-Detection for Large Language Models\n  through Comprehensive Answer Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice Before Trusting: Self-Detection for Large Language Models\n  through Comprehensive Answer Reflection"
                },
                "summary": "Self-detection for Large Language Models (LLMs) seeks to evaluate the\ntrustworthiness of the LLM's output by leveraging its own capabilities, thereby\nalleviating the issue of output hallucination. However, existing self-detection\napproaches only retrospectively evaluate answers generated by LLM, typically\nleading to the over-trust in incorrectly generated answers. To tackle this\nlimitation, we propose a novel self-detection paradigm that considers the\ncomprehensive answer space beyond LLM-generated answers. It thoroughly compares\nthe trustworthiness of multiple candidate answers to mitigate the over-trust in\nLLM-generated incorrect answers. Building upon this paradigm, we introduce a\ntwo-step framework, which firstly instructs LLM to reflect and provide\njustifications for each candidate answer, and then aggregates the\njustifications for comprehensive target answer evaluation. This framework can\nbe seamlessly integrated with existing approaches for superior self-detection.\nExtensive experiments on six datasets spanning three tasks demonstrate the\neffectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-detection for Large Language Models (LLMs) seeks to evaluate the\ntrustworthiness of the LLM's output by leveraging its own capabilities, thereby\nalleviating the issue of output hallucination. However, existing self-detection\napproaches only retrospectively evaluate answers generated by LLM, typically\nleading to the over-trust in incorrectly generated answers. To tackle this\nlimitation, we propose a novel self-detection paradigm that considers the\ncomprehensive answer space beyond LLM-generated answers. It thoroughly compares\nthe trustworthiness of multiple candidate answers to mitigate the over-trust in\nLLM-generated incorrect answers. Building upon this paradigm, we introduce a\ntwo-step framework, which firstly instructs LLM to reflect and provide\njustifications for each candidate answer, and then aggregates the\njustifications for comprehensive target answer evaluation. This framework can\nbe seamlessly integrated with existing approaches for superior self-detection.\nExtensive experiments on six datasets spanning three tasks demonstrate the\neffectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "EMNLP findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18541v1",
                "updated": "2024-09-27T08:20:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    20,
                    59,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:20:59Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    20,
                    59,
                    4,
                    271,
                    0
                ],
                "title": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation"
                },
                "summary": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the total training sample size from 158k\nto 14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the total training sample size from 158k\nto 14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA."
                },
                "authors": [
                    {
                        "name": "Hongzhe Huang"
                    },
                    {
                        "name": "Zhewen Yu"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Li Cai"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17745v2",
                "updated": "2024-09-27T08:19:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    19,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T11:19:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model"
                },
                "summary": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline."
                },
                "authors": [
                    {
                        "name": "Nilanjan Sinhababu"
                    },
                    {
                        "name": "Andrew Parry"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Debasis Samanta"
                    },
                    {
                        "name": "Pabitra Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Pabitra Mitra"
                },
                "author": "Pabitra Mitra",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18538v1",
                "updated": "2024-09-27T08:17:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    17,
                    53,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:17:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    17,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "A Survey on Complex Tasks for Goal-Directed Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Complex Tasks for Goal-Directed Interactive Agents"
                },
                "summary": "Goal-directed interactive agents, which autonomously complete tasks through\ninteractions with their environment, can assist humans in various domains of\ntheir daily lives. Recent advances in large language models (LLMs) led to a\nsurge of new, more and more challenging tasks to evaluate such agents. To\nproperly contextualize performance across these tasks, it is imperative to\nunderstand the different challenges they pose to agents. To this end, this\nsurvey compiles relevant tasks and environments for evaluating goal-directed\ninteractive agents, structuring them along dimensions relevant for\nunderstanding current obstacles. An up-to-date compilation of relevant\nresources can be found on our project website:\nhttps://coli-saar.github.io/interactive-agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-directed interactive agents, which autonomously complete tasks through\ninteractions with their environment, can assist humans in various domains of\ntheir daily lives. Recent advances in large language models (LLMs) led to a\nsurge of new, more and more challenging tasks to evaluate such agents. To\nproperly contextualize performance across these tasks, it is imperative to\nunderstand the different challenges they pose to agents. To this end, this\nsurvey compiles relevant tasks and environments for evaluating goal-directed\ninteractive agents, structuring them along dimensions relevant for\nunderstanding current obstacles. An up-to-date compilation of relevant\nresources can be found on our project website:\nhttps://coli-saar.github.io/interactive-agents."
                },
                "authors": [
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03553v3",
                "updated": "2024-09-27T08:16:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    16,
                    28,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-06T15:20:30Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    20,
                    30,
                    0,
                    127,
                    0
                ],
                "title": "AlphaMath Almost Zero: Process Supervision without Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMath Almost Zero: Process Supervision without Process"
                },
                "summary": "Although recent advancements in large language models (LLMs) have\nsignificantly improved their performance on various tasks, they still face\nchallenges with complex and symbolic multi-step reasoning, particularly in\nmathematical reasoning. To bolster the mathematical reasoning capabilities of\nLLMs, most existing efforts concentrate on seeking assistance from either\ndomain experts or GPT-4 for high-quality process-supervised data, which is not\nonly expensive but also labor-intensive. In our study, we propose an innovative\nframework, AlphaMath, that bypasses the need for process annotations (from\nhumans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework\nfocuses on unleashing the potential of a well-pretrained LLM to autonomously\nenhance its mathematical reasoning. Specifically, we integrate a value model\nwith the LLM, automatically generating both process supervision and step-level\nevaluation signals in MCTS. Furthermore, we propose an efficient inference\nstrategy, step-level beam search, where the value model is crafted to assist\nthe policy model (i.e., LLM) in navigating more effective reasoning paths,\nrather than solely relying on prior probabilities. The experimental results on\nboth in-domain and out-of-domain datasets demonstrate that even without GPT-4\nor human-annotated process supervision, our AlphaMath framework achieves\ncomparable or superior results to previous state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent advancements in large language models (LLMs) have\nsignificantly improved their performance on various tasks, they still face\nchallenges with complex and symbolic multi-step reasoning, particularly in\nmathematical reasoning. To bolster the mathematical reasoning capabilities of\nLLMs, most existing efforts concentrate on seeking assistance from either\ndomain experts or GPT-4 for high-quality process-supervised data, which is not\nonly expensive but also labor-intensive. In our study, we propose an innovative\nframework, AlphaMath, that bypasses the need for process annotations (from\nhumans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework\nfocuses on unleashing the potential of a well-pretrained LLM to autonomously\nenhance its mathematical reasoning. Specifically, we integrate a value model\nwith the LLM, automatically generating both process supervision and step-level\nevaluation signals in MCTS. Furthermore, we propose an efficient inference\nstrategy, step-level beam search, where the value model is crafted to assist\nthe policy model (i.e., LLM) in navigating more effective reasoning paths,\nrather than solely relying on prior probabilities. The experimental results on\nboth in-domain and out-of-domain datasets demonstrate that even without GPT-4\nor human-annotated process supervision, our AlphaMath framework achieves\ncomparable or superior results to previous state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15049v2",
                "updated": "2024-09-27T08:03:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    3,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T14:22:53Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    14,
                    22,
                    53,
                    0,
                    267,
                    0
                ],
                "title": "PackageIntel: Leveraging Large Language Models for Automated\n  Intelligence Extraction in Package Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackageIntel: Leveraging Large Language Models for Automated\n  Intelligence Extraction in Package Ecosystems"
                },
                "summary": "The rise of malicious packages in public registries poses a significant\nthreat to software supply chain (SSC) security. Although academia and industry\nemploy methods like software composition analysis (SCA) to address this issue,\nexisting approaches often lack timely and comprehensive intelligence updates.\nThis paper introduces PackageIntel, a novel platform that revolutionizes the\ncollection, processing, and retrieval of malicious package intelligence. By\nutilizing exhaustive search techniques, snowball sampling from diverse sources,\nand large language models (LLMs) with specialized prompts, PackageIntel ensures\nenhanced coverage, timeliness, and accuracy. We have developed a comprehensive\ndatabase containing 20,692 malicious NPM and PyPI packages sourced from 21\ndistinct intelligence repositories. Empirical evaluations demonstrate that\nPackageIntel achieves a precision of 98.6% and an F1 score of 92.0 in\nintelligence extraction. Additionally, it detects threats on average 70%\nearlier than leading databases like Snyk and OSV, and operates cost-effectively\nat $0.094 per intelligence piece. The platform has successfully identified and\nreported over 1,000 malicious packages in downstream package manager mirror\nregistries. This research provides a robust, efficient, and timely solution for\nidentifying and mitigating threats within the software supply chain ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of malicious packages in public registries poses a significant\nthreat to software supply chain (SSC) security. Although academia and industry\nemploy methods like software composition analysis (SCA) to address this issue,\nexisting approaches often lack timely and comprehensive intelligence updates.\nThis paper introduces PackageIntel, a novel platform that revolutionizes the\ncollection, processing, and retrieval of malicious package intelligence. By\nutilizing exhaustive search techniques, snowball sampling from diverse sources,\nand large language models (LLMs) with specialized prompts, PackageIntel ensures\nenhanced coverage, timeliness, and accuracy. We have developed a comprehensive\ndatabase containing 20,692 malicious NPM and PyPI packages sourced from 21\ndistinct intelligence repositories. Empirical evaluations demonstrate that\nPackageIntel achieves a precision of 98.6% and an F1 score of 92.0 in\nintelligence extraction. Additionally, it detects threats on average 70%\nearlier than leading databases like Snyk and OSV, and operates cost-effectively\nat $0.094 per intelligence piece. The platform has successfully identified and\nreported over 1,000 malicious packages in downstream package manager mirror\nregistries. This research provides a robust, efficient, and timely solution for\nidentifying and mitigating threats within the software supply chain ecosystem."
                },
                "authors": [
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Zhengzi Xu"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Yong Fang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10858v2",
                "updated": "2024-09-27T08:03:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    3,
                    7,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-16T09:06:17Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    6,
                    17,
                    6,
                    168,
                    0
                ],
                "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-level Value Preference Optimization for Mathematical Reasoning"
                },
                "summary": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for EMNLP2024-Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18511v1",
                "updated": "2024-09-27T07:46:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    46,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T07:46:06Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    46,
                    6,
                    4,
                    271,
                    0
                ],
                "title": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation"
                },
                "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns, even when trained on large\ngeneral-purpose corpora. This study sheds light on the necessity of developing\ndomain-specific embedding models in the LLM era, offering valuable insights for\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns, even when trained on large\ngeneral-purpose corpora. This study sheds light on the necessity of developing\ndomain-specific embedding models in the LLM era, offering valuable insights for\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/FinMTEB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18508v1",
                "updated": "2024-09-27T07:45:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    45,
                    13,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T07:45:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    45,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Adaptive inference with random ellipsoids through Conformal Conditional\n  Linear Expectation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive inference with random ellipsoids through Conformal Conditional\n  Linear Expectation"
                },
                "summary": "We propose a new conformity score for conformal prediction, in a general\nmultivariate regression framework. The underlying score function is based on a\ncovariance analysis of the residuals and the input points. We give theoretical\nguarantees on the prediction set. This set consists in an explicit ellipsoid\nthat has a reduced volume compared to a classic ball. Further, we also study\nthe asymptotic properties of the ellipsoid. Finally, we illustrate the\neffectiveness of all our results on an in-depth numerical study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new conformity score for conformal prediction, in a general\nmultivariate regression framework. The underlying score function is based on a\ncovariance analysis of the residuals and the input points. We give theoretical\nguarantees on the prediction set. This set consists in an explicit ellipsoid\nthat has a reduced volume compared to a classic ball. Further, we also study\nthe asymptotic properties of the ellipsoid. Finally, we illustrate the\neffectiveness of all our results on an in-depth numerical study."
                },
                "authors": [
                    {
                        "name": "Iain Henderson"
                    },
                    {
                        "name": "Adrien Mazoyer"
                    },
                    {
                        "name": "Fabrice Gamboa"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Gamboa"
                },
                "arxiv_affiliation": "IMT",
                "author": "Fabrice Gamboa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04222v2",
                "updated": "2024-09-27T07:08:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    8,
                    10,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-07T04:50:38Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    4,
                    50,
                    38,
                    3,
                    67,
                    0
                ],
                "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evaluation of Large Language Model based on Glass-box Features"
                },
                "summary": "The proliferation of open-source Large Language Models (LLMs) underscores the\npressing need for evaluation methods. Existing works primarily rely on external\nevaluators, focusing on training and prompting strategies. However, a crucial\naspect, model-aware glass-box features, is overlooked. In this study, we\nexplore the utility of glass-box features under the scenario of\nself-evaluation, namely applying an LLM to evaluate its own output. We\ninvestigate various glass-box feature groups and discovered that the softmax\ndistribution serves as a reliable quality indicator for self-evaluation.\nExperimental results on public benchmarks validate the feasibility of\nself-evaluation of LLMs using glass-box features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-source Large Language Models (LLMs) underscores the\npressing need for evaluation methods. Existing works primarily rely on external\nevaluators, focusing on training and prompting strategies. However, a crucial\naspect, model-aware glass-box features, is overlooked. In this study, we\nexplore the utility of glass-box features under the scenario of\nself-evaluation, namely applying an LLM to evaluate its own output. We\ninvestigate various glass-box feature groups and discovered that the softmax\ndistribution serves as a reliable quality indicator for self-evaluation.\nExperimental results on public benchmarks validate the feasibility of\nself-evaluation of LLMs using glass-box features."
                },
                "authors": [
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Wenpeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Lu"
                },
                "author": "Wenpeng Lu",
                "arxiv_comment": "accepted as Findings of EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18486v1",
                "updated": "2024-09-27T06:57:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    57,
                    0,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T06:57:00Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    57,
                    0,
                    4,
                    271,
                    0
                ],
                "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI"
                },
                "summary": "This comprehensive study evaluates the performance of OpenAI's o1-preview\nlarge language model across a diverse array of complex reasoning tasks,\nspanning multiple domains, including computer science, mathematics, natural\nsciences, medicine, linguistics, and social sciences. Through rigorous testing,\no1-preview demonstrated remarkable capabilities, often achieving human-level or\nsuperior performance in areas ranging from coding challenges to scientific\nreasoning and from language processing to creative problem-solving. Key\nfindings include:\n  -83.3% success rate in solving complex competitive programming problems,\nsurpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports,\noutperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing\ndetailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and\nspecialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized\nmodels in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep\nunderstanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive\nfinancial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis\nand emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and\nknowledge integration across various fields. While some limitations were\nobserved, including occasional errors on simpler problems and challenges with\ncertain highly specialized concepts, the overall results indicate significant\nprogress towards artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive study evaluates the performance of OpenAI's o1-preview\nlarge language model across a diverse array of complex reasoning tasks,\nspanning multiple domains, including computer science, mathematics, natural\nsciences, medicine, linguistics, and social sciences. Through rigorous testing,\no1-preview demonstrated remarkable capabilities, often achieving human-level or\nsuperior performance in areas ranging from coding challenges to scientific\nreasoning and from language processing to creative problem-solving. Key\nfindings include:\n  -83.3% success rate in solving complex competitive programming problems,\nsurpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports,\noutperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing\ndetailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and\nspecialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized\nmodels in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep\nunderstanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive\nfinancial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis\nand emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and\nknowledge integration across various fields. While some limitations were\nobserved, including occasional errors on simpler problems and challenges with\ncertain highly specialized concepts, the overall results indicate significant\nprogress towards artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shizhe Liang"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yanjun Lyu"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Xiaowei Yu"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Hanxu Chen"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Huawen Hu"
                    },
                    {
                        "name": "Yihen Liu"
                    },
                    {
                        "name": "Huaqin Zhao"
                    },
                    {
                        "name": "Shaochen Xu"
                    },
                    {
                        "name": "Haixing Dai"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Ruidong Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Wei Ruan"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiming Ren"
                    },
                    {
                        "name": "Shihuan Qin"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Jiaxi Li"
                    },
                    {
                        "name": "Arif Hassan Zidan"
                    },
                    {
                        "name": "Afrar Jahin"
                    },
                    {
                        "name": "Minheng Chen"
                    },
                    {
                        "name": "Sichen Xia"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Weiran Xia"
                    },
                    {
                        "name": "Jichao Yu"
                    },
                    {
                        "name": "Kaibo Tang"
                    },
                    {
                        "name": "Yaxuan Yang"
                    },
                    {
                        "name": "Bolun Sun"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Guoyu Lu"
                    },
                    {
                        "name": "Xianqiao Wang"
                    },
                    {
                        "name": "Lilong Chai"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Bao Ge"
                    },
                    {
                        "name": "Xintao Hu"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Hua Zhou"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Bei Jiang"
                    },
                    {
                        "name": "Linglong Kong"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Yudan Ren"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Yu Bao"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Dinggang Shen"
                    },
                    {
                        "name": "Andrea Sikora"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Dajiang Zhu"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18482v1",
                "updated": "2024-09-27T06:51:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    51,
                    11,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T06:51:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    51,
                    11,
                    4,
                    271,
                    0
                ],
                "title": "HSTFL: A Heterogeneous Federated Learning Framework for Misaligned\n  Spatiotemporal Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSTFL: A Heterogeneous Federated Learning Framework for Misaligned\n  Spatiotemporal Forecasting"
                },
                "summary": "Spatiotemporal forecasting has emerged as an indispensable building block of\ndiverse smart city applications, such as intelligent transportation and smart\nenergy management. Recent advancements have uncovered that the performance of\nspatiotemporal forecasting can be significantly improved by integrating\nknowledge in geo-distributed time series data from different domains, \\eg\nenhancing real-estate appraisal with human mobility data; joint taxi and bike\ndemand predictions. While effective, existing approaches assume a centralized\ndata collection and exploitation environment, overlooking the privacy and\ncommercial interest concerns associated with data owned by different parties.\nIn this paper, we investigate multi-party collaborative spatiotemporal\nforecasting without direct access to multi-source private data. However, this\ntask is challenging due to 1) cross-domain feature heterogeneity and 2)\ncross-client geographical heterogeneity, where standard horizontal or vertical\nfederated learning is inapplicable. To this end, we propose a Heterogeneous\nSpatioTemporal Federated Learning (HSTFL) framework to enable multiple clients\nto collaboratively harness geo-distributed time series data from different\ndomains while preserving privacy. Specifically, we first devise vertical\nfederated spatiotemporal representation learning to locally preserve\nspatiotemporal dependencies among individual participants and generate\neffective representations for heterogeneous data. Then we propose a\ncross-client virtual node alignment block to incorporate cross-client\nspatiotemporal dependencies via a multi-level knowledge fusion scheme.\nExtensive privacy analysis and experimental evaluations demonstrate that HSTFL\nnot only effectively resists inference attacks but also provides a significant\nimprovement against various baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal forecasting has emerged as an indispensable building block of\ndiverse smart city applications, such as intelligent transportation and smart\nenergy management. Recent advancements have uncovered that the performance of\nspatiotemporal forecasting can be significantly improved by integrating\nknowledge in geo-distributed time series data from different domains, \\eg\nenhancing real-estate appraisal with human mobility data; joint taxi and bike\ndemand predictions. While effective, existing approaches assume a centralized\ndata collection and exploitation environment, overlooking the privacy and\ncommercial interest concerns associated with data owned by different parties.\nIn this paper, we investigate multi-party collaborative spatiotemporal\nforecasting without direct access to multi-source private data. However, this\ntask is challenging due to 1) cross-domain feature heterogeneity and 2)\ncross-client geographical heterogeneity, where standard horizontal or vertical\nfederated learning is inapplicable. To this end, we propose a Heterogeneous\nSpatioTemporal Federated Learning (HSTFL) framework to enable multiple clients\nto collaboratively harness geo-distributed time series data from different\ndomains while preserving privacy. Specifically, we first devise vertical\nfederated spatiotemporal representation learning to locally preserve\nspatiotemporal dependencies among individual participants and generate\neffective representations for heterogeneous data. Then we propose a\ncross-client virtual node alignment block to incorporate cross-client\nspatiotemporal dependencies via a multi-level knowledge fusion scheme.\nExtensive privacy analysis and experimental evaluations demonstrate that HSTFL\nnot only effectively resists inference attacks but also provides a significant\nimprovement against various baselines."
                },
                "authors": [
                    {
                        "name": "Shuowei Cai"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18476v1",
                "updated": "2024-09-27T06:33:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    33,
                    35,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T06:33:35Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    33,
                    35,
                    4,
                    271,
                    0
                ],
                "title": "Underwater Image Enhancement with Physical-based Denoising Diffusion\n  Implicit Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater Image Enhancement with Physical-based Denoising Diffusion\n  Implicit Models"
                },
                "summary": "Underwater vision is crucial for autonomous underwater vehicles (AUVs), and\nenhancing degraded underwater images in real-time on a resource-constrained AUV\nis a key challenge due to factors like light absorption and scattering, or the\nsufficient model computational complexity to resolve such factors. Traditional\nimage enhancement techniques lack adaptability to varying underwater\nconditions, while learning-based methods, particularly those using\nconvolutional neural networks (CNNs) and generative adversarial networks\n(GANs), offer more robust solutions but face limitations such as inadequate\nenhancement, unstable training, or mode collapse. Denoising diffusion\nprobabilistic models (DDPMs) have emerged as a state-of-the-art approach in\nimage-to-image tasks but require intensive computational complexity to achieve\nthe desired underwater image enhancement (UIE) using the recent UW-DDPM\nsolution. To address these challenges, this paper introduces UW-DiffPhys, a\nnovel physical-based and diffusion-based UIE approach. UW-DiffPhys combines\nlight-computation physical-based UIE network components with a denoising U-Net\nto replace the computationally intensive distribution transformation U-Net in\nthe existing UW-DDPM framework, reducing complexity while maintaining\nperformance. Additionally, the Denoising Diffusion Implicit Model (DDIM) is\nemployed to accelerate the inference process through non-Markovian sampling.\nExperimental results demonstrate that UW-DiffPhys achieved a substantial\nreduction in computational complexity and inference time compared to UW-DDPM,\nwith competitive performance in key metrics such as PSNR, SSIM, UCIQE, and an\nimprovement in the overall underwater image quality UIQM metric. The\nimplementation code can be found at the following repository:\nhttps://github.com/bachzz/UW-DiffPhys",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater vision is crucial for autonomous underwater vehicles (AUVs), and\nenhancing degraded underwater images in real-time on a resource-constrained AUV\nis a key challenge due to factors like light absorption and scattering, or the\nsufficient model computational complexity to resolve such factors. Traditional\nimage enhancement techniques lack adaptability to varying underwater\nconditions, while learning-based methods, particularly those using\nconvolutional neural networks (CNNs) and generative adversarial networks\n(GANs), offer more robust solutions but face limitations such as inadequate\nenhancement, unstable training, or mode collapse. Denoising diffusion\nprobabilistic models (DDPMs) have emerged as a state-of-the-art approach in\nimage-to-image tasks but require intensive computational complexity to achieve\nthe desired underwater image enhancement (UIE) using the recent UW-DDPM\nsolution. To address these challenges, this paper introduces UW-DiffPhys, a\nnovel physical-based and diffusion-based UIE approach. UW-DiffPhys combines\nlight-computation physical-based UIE network components with a denoising U-Net\nto replace the computationally intensive distribution transformation U-Net in\nthe existing UW-DDPM framework, reducing complexity while maintaining\nperformance. Additionally, the Denoising Diffusion Implicit Model (DDIM) is\nemployed to accelerate the inference process through non-Markovian sampling.\nExperimental results demonstrate that UW-DiffPhys achieved a substantial\nreduction in computational complexity and inference time compared to UW-DDPM,\nwith competitive performance in key metrics such as PSNR, SSIM, UCIQE, and an\nimprovement in the overall underwater image quality UIQM metric. The\nimplementation code can be found at the following repository:\nhttps://github.com/bachzz/UW-DiffPhys"
                },
                "authors": [
                    {
                        "name": "Nguyen Gia Bach"
                    },
                    {
                        "name": "Chanh Minh Tran"
                    },
                    {
                        "name": "Eiji Kamioka"
                    },
                    {
                        "name": "Phan Xuan Tan"
                    }
                ],
                "author_detail": {
                    "name": "Phan Xuan Tan"
                },
                "author": "Phan Xuan Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10566v4",
                "updated": "2024-09-27T06:32:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    32,
                    1,
                    4,
                    271,
                    0
                ],
                "published": "2024-08-20T06:05:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    5,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning"
                },
                "summary": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases, and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases, and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks."
                },
                "authors": [
                    {
                        "name": "Yuqing Zhao"
                    },
                    {
                        "name": "Divya Saxena"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Xiaoyun Liu"
                    },
                    {
                        "name": "Changlin Song"
                    }
                ],
                "author_detail": {
                    "name": "Changlin Song"
                },
                "author": "Changlin Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12842v3",
                "updated": "2024-09-27T06:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    25,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-20T09:10:08Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    9,
                    10,
                    8,
                    1,
                    51,
                    0
                ],
                "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning"
                },
                "summary": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements."
                },
                "authors": [
                    {
                        "name": "Gyeongman Kim"
                    },
                    {
                        "name": "Doohyuk Jang"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "EMNLP 2024 Findings. Our project page: https://promptkd.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.18957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18957v1",
                "updated": "2024-09-27T17:58:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:58:50Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "title": "LML: Language Model Learning a Dataset for Data-Augmented Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LML: Language Model Learning a Dataset for Data-Augmented Prediction"
                },
                "summary": "This paper introduces a new approach to using Large Language Models (LLMs)\nfor classification tasks, which are typically handled using Machine Learning\n(ML) models. Unlike ML models that rely heavily on data cleaning and feature\nengineering, this method streamlines the process using LLMs. This paper\nproposes a new concept called \"Language Model Learning (LML)\" powered by a new\nmethod called \"Data-Augmented Prediction (DAP)\". The classification is\nperformed by LLMs using a method similar to humans manually exploring and\nunderstanding the data and deciding classifications using data as a reference.\nTraining data is summarized and evaluated to determine the features that lead\nto the classification of each label the most. In the process of DAP, the system\nuses the data summary to automatically create a query, which is used to\nretrieve relevant rows from the dataset. A classification is generated by the\nLLM using data summary and relevant rows, ensuring satisfactory accuracy even\nwith complex data. Usage of data summary and similar data in DAP ensures\ncontext-aware decision-making. The proposed method uses the words \"Act as an\nExplainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new approach to using Large Language Models (LLMs)\nfor classification tasks, which are typically handled using Machine Learning\n(ML) models. Unlike ML models that rely heavily on data cleaning and feature\nengineering, this method streamlines the process using LLMs. This paper\nproposes a new concept called \"Language Model Learning (LML)\" powered by a new\nmethod called \"Data-Augmented Prediction (DAP)\". The classification is\nperformed by LLMs using a method similar to humans manually exploring and\nunderstanding the data and deciding classifications using data as a reference.\nTraining data is summarized and evaluated to determine the features that lead\nto the classification of each label the most. In the process of DAP, the system\nuses the data summary to automatically create a query, which is used to\nretrieve relevant rows from the dataset. A classification is generated by the\nLLM using data summary and relevant rows, ensuring satisfactory accuracy even\nwith complex data. Usage of data summary and similar data in DAP ensures\ncontext-aware decision-making. The proposed method uses the words \"Act as an\nExplainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP"
                },
                "authors": [
                    {
                        "name": "Praneeth Vadlapati"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vadlapati"
                },
                "author": "Praneeth Vadlapati",
                "arxiv_comment": "First version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18943v1",
                "updated": "2024-09-27T17:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:44:58Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    44,
                    58,
                    4,
                    271,
                    0
                ],
                "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruler: A Model-Agnostic Method to Control Generated Length for Large\n  Language Models"
                },
                "summary": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instruction-following ability of large language models enables humans to\ninteract with AI agents in a natural way. However, when required to generate\nresponses of a specific length, large language models often struggle to meet\nusers' needs due to their inherent difficulty in accurately perceiving\nnumerical constraints. To explore the ability of large language models to\ncontrol the length of generated responses, we propose the Target Length\nGeneration Task (TLG) and design two metrics, Precise Match (PM) and Flexible\nMatch (FM) to evaluate the model's performance in adhering to specified\nresponse lengths. Furthermore, we introduce a novel, model-agnostic approach\ncalled Ruler, which employs Meta Length Tokens (MLTs) to enhance the\ninstruction-following ability of large language models under length-constrained\ninstructions. Specifically, Ruler equips LLMs with the ability to generate\nresponses of a specified length based on length constraints within the\ninstructions. Moreover, Ruler can automatically generate appropriate MLT when\nlength constraints are not explicitly provided, demonstrating excellent\nversatility and generalization. Comprehensive experiments show the\neffectiveness of Ruler across different LLMs on Target Length Generation Task,\ne.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In\naddition, we conduct extensive ablation experiments to further substantiate the\nefficacy and generalization of Ruler. Our code and data is available at\nhttps://github.com/Geaming2002/Ruler."
                },
                "authors": [
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "yuelin bai"
                    },
                    {
                        "name": "Run Luo"
                    },
                    {
                        "name": "Longze Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18938v1",
                "updated": "2024-09-27T17:38:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    38,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Seconds to Hours: Reviewing MultiModal Large Language Models on\n  Comprehensive Long Video Understanding"
                },
                "summary": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding."
                },
                "authors": [
                    {
                        "name": "Heqing Zou"
                    },
                    {
                        "name": "Tianze Luo"
                    },
                    {
                        "name": "Guiyang Xie"
                    },
                    {
                        "name": "Victor"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Fengmao Lv"
                    },
                    {
                        "name": "Guangcong Wang"
                    },
                    {
                        "name": "Juanyang Chen"
                    },
                    {
                        "name": "Zhuochen Wang"
                    },
                    {
                        "name": "Hansheng Zhang"
                    },
                    {
                        "name": "Huaijian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huaijian Zhang"
                },
                "arxiv_affiliation": "Xiao Jie",
                "author": "Huaijian Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18931v1",
                "updated": "2024-09-27T17:28:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    28,
                    25,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:28:25Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    28,
                    25,
                    4,
                    271,
                    0
                ],
                "title": "Social Media Bot Policies: Evaluating Passive and Active Enforcement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Media Bot Policies: Evaluating Passive and Active Enforcement"
                },
                "summary": "The emergence of Multimodal Foundation Models (MFMs) holds significant\npromise for transforming social media platforms. However, this advancement also\nintroduces substantial security and ethical concerns, as it may facilitate\nmalicious actors in the exploitation of online users. We aim to evaluate the\nstrength of security protocols on prominent social media platforms in\nmitigating the deployment of MFM bots. We examined the bot and content policies\nof eight popular social media platforms: X (formerly Twitter), Instagram,\nFacebook, Threads, TikTok, Mastodon, Reddit, and LinkedIn. Using Selenium, we\ndeveloped a web bot to test bot deployment and AI-generated content policies\nand their enforcement mechanisms. Our findings indicate significant\nvulnerabilities within the current enforcement mechanisms of these platforms.\nDespite having explicit policies against bot activity, all platforms failed to\ndetect and prevent the operation of our MFM bots. This finding reveals a\ncritical gap in the security measures employed by these social media platforms,\nunderscoring the potential for malicious actors to exploit these weaknesses to\ndisseminate misinformation, commit fraud, or manipulate users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Multimodal Foundation Models (MFMs) holds significant\npromise for transforming social media platforms. However, this advancement also\nintroduces substantial security and ethical concerns, as it may facilitate\nmalicious actors in the exploitation of online users. We aim to evaluate the\nstrength of security protocols on prominent social media platforms in\nmitigating the deployment of MFM bots. We examined the bot and content policies\nof eight popular social media platforms: X (formerly Twitter), Instagram,\nFacebook, Threads, TikTok, Mastodon, Reddit, and LinkedIn. Using Selenium, we\ndeveloped a web bot to test bot deployment and AI-generated content policies\nand their enforcement mechanisms. Our findings indicate significant\nvulnerabilities within the current enforcement mechanisms of these platforms.\nDespite having explicit policies against bot activity, all platforms failed to\ndetect and prevent the operation of our MFM bots. This finding reveals a\ncritical gap in the security measures employed by these social media platforms,\nunderscoring the potential for malicious actors to exploit these weaknesses to\ndisseminate misinformation, commit fraud, or manipulate users."
                },
                "authors": [
                    {
                        "name": "Kristina Radivojevic"
                    },
                    {
                        "name": "Christopher McAleer"
                    },
                    {
                        "name": "Catrell Conley"
                    },
                    {
                        "name": "Cormac Kennedy"
                    },
                    {
                        "name": "Paul Brenner"
                    }
                ],
                "author_detail": {
                    "name": "Paul Brenner"
                },
                "author": "Paul Brenner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18924v1",
                "updated": "2024-09-27T17:17:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T17:17:15Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    17,
                    15,
                    4,
                    271,
                    0
                ],
                "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic\n  Workflow"
                },
                "summary": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p<0.1), and stability (ANOVA F-value\n0.782, p<0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p<0.1), and stability (ANOVA F-value\n0.782, p<0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration."
                },
                "authors": [
                    {
                        "name": "Huizi Yu"
                    },
                    {
                        "name": "Jiayan Zhou"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Anye Shi"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Guang Chen"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Trisha Gupte"
                    },
                    {
                        "name": "Ming-Li Chen"
                    },
                    {
                        "name": "Zahra Azizi"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Themistocles L. Assimes"
                    },
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Lizhou Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lizhou Fan"
                },
                "author": "Lizhou Fan",
                "arxiv_comment": "42 pages, 6 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18911v1",
                "updated": "2024-09-27T16:54:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    36,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:54:36Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    36,
                    4,
                    271,
                    0
                ],
                "title": "Soft Measures for Extracting Causal Collective Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Measures for Extracting Causal Collective Intelligence"
                },
                "summary": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP."
                },
                "authors": [
                    {
                        "name": "Maryam Berijanian"
                    },
                    {
                        "name": "Spencer Dork"
                    },
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Michael Riley Millikan"
                    },
                    {
                        "name": "Ashlin Riggs"
                    },
                    {
                        "name": "Aadarsh Swaminathan"
                    },
                    {
                        "name": "Sarah L. Gibbs"
                    },
                    {
                        "name": "Scott E. Friedman"
                    },
                    {
                        "name": "Nathan Brugnone"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Brugnone"
                },
                "author": "Nathan Brugnone",
                "arxiv_comment": "Camera-ready version accepted for publication in the EMNLP 2024\n  Workshop NLP4Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09299v2",
                "updated": "2024-09-27T16:54:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    54,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-14T16:41:35Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    16,
                    41,
                    35,
                    2,
                    45,
                    0
                ],
                "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained Without My Consent: Detecting Code Inclusion In Language Models\n  Trained on Code"
                },
                "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets."
                },
                "authors": [
                    {
                        "name": "Vahid Majdinasab"
                    },
                    {
                        "name": "Amin Nikanjam"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "Accepted for publication in TOSEM (ACM Transactions on Software\n  Engineering and Methodology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18892v1",
                "updated": "2024-09-27T16:29:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    29,
                    12,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:29:12Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    29,
                    12,
                    4,
                    271,
                    0
                ],
                "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation"
                },
                "summary": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Lin"
                    },
                    {
                        "name": "Shuyi Xie"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18874v1",
                "updated": "2024-09-27T16:10:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    10,
                    11,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:10:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    10,
                    11,
                    4,
                    271,
                    0
                ],
                "title": "CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly\n  Detection and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly\n  Detection and Forecasting"
                },
                "summary": "Anomaly detection in network traffic is crucial for maintaining the security\nof computer networks and identifying malicious activities. One of the primary\napproaches to anomaly detection are methods based on forecasting. Nevertheless,\nextensive real-world network datasets for forecasting and anomaly detection\ntechniques are missing, potentially causing performance overestimation of\nanomaly detection algorithms. This manuscript addresses this gap by introducing\na dataset comprising time series data of network entities' behavior, collected\nfrom the CESNET3 network. The dataset was created from 40 weeks of network\ntraffic of 275 thousand active IP addresses. The ISP origin of the presented\ndata ensures a high level of variability among network entities, which forms a\nunique and authentic challenge for forecasting and anomaly detection models. It\nprovides valuable insights into the practical deployment of forecast-based\nanomaly detection approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in network traffic is crucial for maintaining the security\nof computer networks and identifying malicious activities. One of the primary\napproaches to anomaly detection are methods based on forecasting. Nevertheless,\nextensive real-world network datasets for forecasting and anomaly detection\ntechniques are missing, potentially causing performance overestimation of\nanomaly detection algorithms. This manuscript addresses this gap by introducing\na dataset comprising time series data of network entities' behavior, collected\nfrom the CESNET3 network. The dataset was created from 40 weeks of network\ntraffic of 275 thousand active IP addresses. The ISP origin of the presented\ndata ensures a high level of variability among network entities, which forms a\nunique and authentic challenge for forecasting and anomaly detection models. It\nprovides valuable insights into the practical deployment of forecast-based\nanomaly detection approaches."
                },
                "authors": [
                    {
                        "name": "Josef Koumar"
                    },
                    {
                        "name": "Karel Hynek"
                    },
                    {
                        "name": "Tomáš Čejka"
                    },
                    {
                        "name": "Pavel Šiška"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Šiška"
                },
                "author": "Pavel Šiška",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17827v2",
                "updated": "2024-09-27T16:07:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    7,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T13:26:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text"
                },
                "summary": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Wang"
                    },
                    {
                        "name": "Bradford Levy"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Levy"
                },
                "author": "Bradford Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18869v1",
                "updated": "2024-09-27T16:06:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    6,
                    11,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T16:06:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    6,
                    11,
                    4,
                    271,
                    0
                ],
                "title": "Emu3: Next-Token Prediction is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emu3: Next-Token Prediction is All You Need"
                },
                "summary": "While next-token prediction is considered a promising path towards artificial\ngeneral intelligence, it has struggled to excel in multimodal tasks, which are\nstill dominated by diffusion models (e.g., Stable Diffusion) and compositional\napproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a\nnew suite of state-of-the-art multimodal models trained solely with next-token\nprediction. By tokenizing images, text, and videos into a discrete space, we\ntrain a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 outperforms several well-established task-specific models in both\ngeneration and perception tasks, surpassing flagship models such as SDXL and\nLLaVA-1.6, while eliminating the need for diffusion or compositional\narchitectures. Emu3 is also capable of generating high-fidelity video via\npredicting the next token in a video sequence. We simplify complex multimodal\nmodel designs by converging on a singular focus: tokens, unlocking great\npotential for scaling both during training and inference. Our results\ndemonstrate that next-token prediction is a promising path towards building\ngeneral multimodal intelligence beyond language. We open-source key techniques\nand models to support further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While next-token prediction is considered a promising path towards artificial\ngeneral intelligence, it has struggled to excel in multimodal tasks, which are\nstill dominated by diffusion models (e.g., Stable Diffusion) and compositional\napproaches (e.g., CLIP combined with LLMs). In this paper, we introduce Emu3, a\nnew suite of state-of-the-art multimodal models trained solely with next-token\nprediction. By tokenizing images, text, and videos into a discrete space, we\ntrain a single transformer from scratch on a mixture of multimodal sequences.\nEmu3 outperforms several well-established task-specific models in both\ngeneration and perception tasks, surpassing flagship models such as SDXL and\nLLaVA-1.6, while eliminating the need for diffusion or compositional\narchitectures. Emu3 is also capable of generating high-fidelity video via\npredicting the next token in a video sequence. We simplify complex multimodal\nmodel designs by converging on a singular focus: tokens, unlocking great\npotential for scaling both during training and inference. Our results\ndemonstrate that next-token prediction is a promising path towards building\ngeneral multimodal intelligence beyond language. We open-source key techniques\nand models to support further research in this direction."
                },
                "authors": [
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Zhengxiong Luo"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Jinsheng Wang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Qiying Yu"
                    },
                    {
                        "name": "Yingli Zhao"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Xuebin Min"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Boya Wu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Liangdong Wang"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Yonghua Lin"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyuan Wang"
                },
                "author": "Zhongyuan Wang",
                "arxiv_comment": "Project Page: https://emu.baai.ac.cn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03291v2",
                "updated": "2024-09-27T16:04:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    16,
                    4,
                    40,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-05T06:55:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts"
                },
                "summary": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection)."
                },
                "authors": [
                    {
                        "name": "Henrique Da Silva Gameiro"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "arxiv_comment": "20 pages, 7 tables, 13 figures, under consideration for EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18857v1",
                "updated": "2024-09-27T15:53:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:53:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    53,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "Mitigating Selection Bias with Node Pruning and Auxiliary Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Selection Bias with Node Pruning and Auxiliary Options"
                },
                "summary": "Large language models (LLMs) often show unwarranted preference for certain\nchoice options when responding to multiple-choice questions, posing significant\nreliability concerns in LLM-automated systems. To mitigate this selection bias\nproblem, previous solutions utilized debiasing methods to adjust the model's\ninput and/or output. Our work, in contrast, investigates the model's internal\nrepresentation of the selection bias. Specifically, we introduce a novel\ndebiasing approach, Bias Node Pruning (BNP), which eliminates the linear layer\nparameters that contribute to the bias. Furthermore, we present Auxiliary\nOption Injection (AOI), a simple yet effective input modification technique for\ndebiasing, which is compatible even with black-box LLMs. To provide a more\nsystematic evaluation of selection bias, we review existing metrics and\nintroduce Choice Kullback-Leibler Divergence (CKLD), which addresses the\ninsensitivity of the commonly used metrics to label imbalance. Experiments show\nthat our methods are robust and adaptable across various datasets when applied\nto three LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often show unwarranted preference for certain\nchoice options when responding to multiple-choice questions, posing significant\nreliability concerns in LLM-automated systems. To mitigate this selection bias\nproblem, previous solutions utilized debiasing methods to adjust the model's\ninput and/or output. Our work, in contrast, investigates the model's internal\nrepresentation of the selection bias. Specifically, we introduce a novel\ndebiasing approach, Bias Node Pruning (BNP), which eliminates the linear layer\nparameters that contribute to the bias. Furthermore, we present Auxiliary\nOption Injection (AOI), a simple yet effective input modification technique for\ndebiasing, which is compatible even with black-box LLMs. To provide a more\nsystematic evaluation of selection bias, we review existing metrics and\nintroduce Choice Kullback-Leibler Divergence (CKLD), which addresses the\ninsensitivity of the commonly used metrics to label imbalance. Experiments show\nthat our methods are robust and adaptable across various datasets when applied\nto three LLMs."
                },
                "authors": [
                    {
                        "name": "Hyeong Kyu Choi"
                    },
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Stephanie Eckman"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15204v2",
                "updated": "2024-09-27T15:19:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    19,
                    23,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T16:51:43Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    16,
                    51,
                    43,
                    0,
                    267,
                    0
                ],
                "title": "RAMBO: Enhancing RAG-based Repository-Level Method Body Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAMBO: Enhancing RAG-based Repository-Level Method Body Completion"
                },
                "summary": "Code completion is essential in software development, helping developers by\npredicting code snippets based on context. Among completion tasks, Method Body\nCompletion (MBC) is particularly challenging as it involves generating complete\nmethod bodies based on their signatures and context. This task becomes\nsignificantly harder in large repositories, where method bodies must integrate\nrepositoryspecific elements such as custom APIs, inter-module dependencies, and\nproject-specific conventions. In this paper, we introduce RAMBO, a novel\nRAG-based approach for repository-level MBC. Instead of retrieving similar\nmethod bodies, RAMBO identifies essential repository-specific elements, such as\nclasses, methods, and variables/fields, and their relevant usages. By\nincorporating these elements and their relevant usages into the code generation\nprocess, RAMBO ensures more accurate and contextually relevant method bodies.\nOur experimental results with leading code LLMs across 40 Java projects show\nthat RAMBO significantly outperformed the state-of-the-art repository-level MBC\napproaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in\nCompilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed\nRepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark\nfor repository-level MBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion is essential in software development, helping developers by\npredicting code snippets based on context. Among completion tasks, Method Body\nCompletion (MBC) is particularly challenging as it involves generating complete\nmethod bodies based on their signatures and context. This task becomes\nsignificantly harder in large repositories, where method bodies must integrate\nrepositoryspecific elements such as custom APIs, inter-module dependencies, and\nproject-specific conventions. In this paper, we introduce RAMBO, a novel\nRAG-based approach for repository-level MBC. Instead of retrieving similar\nmethod bodies, RAMBO identifies essential repository-specific elements, such as\nclasses, methods, and variables/fields, and their relevant usages. By\nincorporating these elements and their relevant usages into the code generation\nprocess, RAMBO ensures more accurate and contextually relevant method bodies.\nOur experimental results with leading code LLMs across 40 Java projects show\nthat RAMBO significantly outperformed the state-of-the-art repository-level MBC\napproaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in\nCompilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed\nRepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark\nfor repository-level MBC."
                },
                "authors": [
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Duc-Thieu Luu-Van"
                    },
                    {
                        "name": "Thanh-Phat Nguyen"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14744v2",
                "updated": "2024-09-27T15:10:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    10,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-23T16:13:33Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    13,
                    33,
                    3,
                    144,
                    0
                ],
                "title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View"
                },
                "summary": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents."
                },
                "authors": [
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haoyang Shang"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18812v1",
                "updated": "2024-09-27T15:04:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    4,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T15:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    15,
                    4,
                    39,
                    4,
                    271,
                    0
                ],
                "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4Synthesis: Leveraging Large Language Models for Scientific\n  Synthesis"
                },
                "summary": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_comment": "12 pages, 3 figures, Accepted to JCDL 2024 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18800v1",
                "updated": "2024-09-27T14:54:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    54,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:54:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    54,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "MiniVLN: Efficient Vision-and-Language Navigation by Progressive\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniVLN: Efficient Vision-and-Language Navigation by Progressive\n  Knowledge Distillation"
                },
                "summary": "In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced\nrapidly, yet the increasing size of models conflicts with the limited\ncomputational capabilities of Embodied AI platforms. To address this challenge,\nwe aim to achieve both high model performance and practical deployability.\nSpecifically, we focus on Vision-and-Language Navigation (VLN), a core task in\nEmbodied AI. This paper introduces a two-stage knowledge distillation\nframework, producing a student model, MiniVLN, and showcasing the significant\npotential of distillation techniques in developing lightweight models. The\nproposed method aims to capture fine-grained knowledge during the pretraining\nphase and navigation-specific knowledge during the fine-tuning phase. Our\nfindings indicate that the two-stage distillation approach is more effective in\nnarrowing the performance gap between the teacher model and the student model\ncompared to single-stage distillation. On the public R2R and REVERIE\nbenchmarks, MiniVLN achieves performance on par with the teacher model while\nhaving only about 12% of the teacher model's parameter count.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced\nrapidly, yet the increasing size of models conflicts with the limited\ncomputational capabilities of Embodied AI platforms. To address this challenge,\nwe aim to achieve both high model performance and practical deployability.\nSpecifically, we focus on Vision-and-Language Navigation (VLN), a core task in\nEmbodied AI. This paper introduces a two-stage knowledge distillation\nframework, producing a student model, MiniVLN, and showcasing the significant\npotential of distillation techniques in developing lightweight models. The\nproposed method aims to capture fine-grained knowledge during the pretraining\nphase and navigation-specific knowledge during the fine-tuning phase. Our\nfindings indicate that the two-stage distillation approach is more effective in\nnarrowing the performance gap between the teacher model and the student model\ncompared to single-stage distillation. On the public R2R and REVERIE\nbenchmarks, MiniVLN achieves performance on par with the teacher model while\nhaving only about 12% of the teacher model's parameter count."
                },
                "authors": [
                    {
                        "name": "Junyou Zhu"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Siqi Zhang"
                    },
                    {
                        "name": "Xingjian He"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18798v1",
                "updated": "2024-09-27T14:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    53,
                    4,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    53,
                    4,
                    4,
                    271,
                    0
                ],
                "title": "Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public\n  Perceptions with BERTopic and GPT-4 Topic Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public\n  Perceptions with BERTopic and GPT-4 Topic Fine-Tuning"
                },
                "summary": "This study examined the public opinions of esports at the 2023 Asian Games\nand value co-creation during the event using an LLM-enhanced BERTopic modeling\nanalysis. We identified five major themes representing public perceptions, as\nwell as how major stakeholders co-created value within and beyond the esports\necosystem. Key findings highlighted the strategic use of social media marketing\nto influence public opinion and promote esports events and brands, emphasizing\nthe importance of event logistics and infrastructure. Additionally, the study\nrevealed the co-creation value contributed by stakeholders outside the\ntraditional esports ecosystem, particularly in promoting national\nrepresentation and performance. Our findings supported the ongoing efforts to\nlegitimize esports as a sport, noting that mainstream recognition remains a\nchallenge. The inclusion of esports as a medal event showcased broader\nacceptance and helped mitigate negative public perceptions. Moreover,\ncontributions from non-traditional stakeholders underscored the value of\ncross-subcultural collaborations in esports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examined the public opinions of esports at the 2023 Asian Games\nand value co-creation during the event using an LLM-enhanced BERTopic modeling\nanalysis. We identified five major themes representing public perceptions, as\nwell as how major stakeholders co-created value within and beyond the esports\necosystem. Key findings highlighted the strategic use of social media marketing\nto influence public opinion and promote esports events and brands, emphasizing\nthe importance of event logistics and infrastructure. Additionally, the study\nrevealed the co-creation value contributed by stakeholders outside the\ntraditional esports ecosystem, particularly in promoting national\nrepresentation and performance. Our findings supported the ongoing efforts to\nlegitimize esports as a sport, noting that mainstream recognition remains a\nchallenge. The inclusion of esports as a medal event showcased broader\nacceptance and helped mitigate negative public perceptions. Moreover,\ncontributions from non-traditional stakeholders underscored the value of\ncross-subcultural collaborations in esports."
                },
                "authors": [
                    {
                        "name": "Tyreal Yizhou Qian"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Weizhe Li"
                    },
                    {
                        "name": "Chenglong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglong Xu"
                },
                "author": "Chenglong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16807v2",
                "updated": "2024-09-27T14:50:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    50,
                    59,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-25T17:52:39Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    52,
                    39,
                    3,
                    116,
                    0
                ],
                "title": "Improving Diversity of Commonsense Generation by Large Language Models\n  via In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Diversity of Commonsense Generation by Large Language Models\n  via In-Context Learning"
                },
                "summary": "Generative Commonsense Reasoning (GCR) requires a model to reason about a\nsituation using commonsense knowledge, while generating coherent sentences.\nAlthough the quality of the generated sentences is crucial, the diversity of\nthe generation is equally important because it reflects the model's ability to\nuse a range of commonsense knowledge facts. Large Language Models (LLMs) have\nshown proficiency in enhancing the generation quality across various tasks\nthrough in-context learning (ICL) using given examples without the need for any\nfine-tuning. However, the diversity aspect in LLM outputs has not been\nsystematically studied before. To address this, we propose a simple method that\ndiversifies the LLM generations, while preserving their quality. Experimental\nresults on three benchmark GCR datasets show that our method achieves an ideal\nbalance between the quality and diversity. Moreover, the sentences generated by\nour proposed method can be used as training data to improve diversity in\nexisting commonsense generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Commonsense Reasoning (GCR) requires a model to reason about a\nsituation using commonsense knowledge, while generating coherent sentences.\nAlthough the quality of the generated sentences is crucial, the diversity of\nthe generation is equally important because it reflects the model's ability to\nuse a range of commonsense knowledge facts. Large Language Models (LLMs) have\nshown proficiency in enhancing the generation quality across various tasks\nthrough in-context learning (ICL) using given examples without the need for any\nfine-tuning. However, the diversity aspect in LLM outputs has not been\nsystematically studied before. To address this, we propose a simple method that\ndiversifies the LLM generations, while preserving their quality. Experimental\nresults on three benchmark GCR datasets show that our method achieves an ideal\nbalance between the quality and diversity. Moreover, the sentences generated by\nour proposed method can be used as training data to improve diversity in\nexisting commonsense generators."
                },
                "authors": [
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Bei Peng"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "arxiv_comment": "EMNLP 2024 Findings, Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18794v1",
                "updated": "2024-09-27T14:47:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    47,
                    18,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:47:18Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    47,
                    18,
                    4,
                    271,
                    0
                ],
                "title": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs"
                },
                "summary": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Wenqi Lyu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Zerui Li"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18788v1",
                "updated": "2024-09-27T14:36:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    36,
                    20,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:36:20Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    36,
                    20,
                    4,
                    271,
                    0
                ],
                "title": "Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation"
                },
                "summary": "The successful deployment of deep learning-based techniques for autonomous\nsystems is highly dependent on the data availability for the respective system\nin its deployment environment. Especially for unstructured outdoor\nenvironments, very few datasets exist for even fewer robotic platforms and\nscenarios. In an earlier work, we presented the German Outdoor and Offroad\nDataset (GOOSE) framework along with 10000 multimodal frames from an offroad\nvehicle to enhance the perception capabilities in unstructured environments. In\nthis work, we address the generalizability of the GOOSE framework. To\naccomplish this, we open-source the GOOSE-Ex dataset, which contains additional\n5000 labeled multimodal frames from various completely different environments,\nrecorded on a robotic excavator and a quadruped platform. We perform a\ncomprehensive analysis of the semantic segmentation performance on different\nplatforms and sensor modalities in unseen environments. In addition, we\ndemonstrate how the combined datasets can be utilized for different downstream\napplications or competitions such as offroad navigation, object manipulation or\nscene completion. The dataset, its platform documentation and pre-trained\nstate-of-the-art models for offroad perception will be made available on\nhttps://goose-dataset.de/.\n  \\",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The successful deployment of deep learning-based techniques for autonomous\nsystems is highly dependent on the data availability for the respective system\nin its deployment environment. Especially for unstructured outdoor\nenvironments, very few datasets exist for even fewer robotic platforms and\nscenarios. In an earlier work, we presented the German Outdoor and Offroad\nDataset (GOOSE) framework along with 10000 multimodal frames from an offroad\nvehicle to enhance the perception capabilities in unstructured environments. In\nthis work, we address the generalizability of the GOOSE framework. To\naccomplish this, we open-source the GOOSE-Ex dataset, which contains additional\n5000 labeled multimodal frames from various completely different environments,\nrecorded on a robotic excavator and a quadruped platform. We perform a\ncomprehensive analysis of the semantic segmentation performance on different\nplatforms and sensor modalities in unseen environments. In addition, we\ndemonstrate how the combined datasets can be utilized for different downstream\napplications or competitions such as offroad navigation, object manipulation or\nscene completion. The dataset, its platform documentation and pre-trained\nstate-of-the-art models for offroad perception will be made available on\nhttps://goose-dataset.de/.\n  \\"
                },
                "authors": [
                    {
                        "name": "Raphael Hagmanns"
                    },
                    {
                        "name": "Peter Mortimer"
                    },
                    {
                        "name": "Miguel Granero"
                    },
                    {
                        "name": "Thorsten Luettel"
                    },
                    {
                        "name": "Janko Petereit"
                    }
                ],
                "author_detail": {
                    "name": "Janko Petereit"
                },
                "author": "Janko Petereit",
                "arxiv_comment": "Submitted to IEEE for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18786v1",
                "updated": "2024-09-27T14:34:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    34,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:34:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    34,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "A Survey on the Honesty of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on the Honesty of Large Language Models"
                },
                "summary": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area."
                },
                "authors": [
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Project Page: https://github.com/SihengLi99/LLM-Honesty-Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13833v2",
                "updated": "2024-09-27T14:04:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    4,
                    35,
                    4,
                    271,
                    0
                ],
                "published": "2023-11-23T07:33:38Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    7,
                    33,
                    38,
                    3,
                    327,
                    0
                ],
                "title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models"
                },
                "summary": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept."
                },
                "authors": [
                    {
                        "name": "Saman Motamed"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18764v1",
                "updated": "2024-09-27T14:02:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    2,
                    48,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T14:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    2,
                    48,
                    4,
                    271,
                    0
                ],
                "title": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Future: Using Chart Question-Answering for Scalable\n  Evaluation of LLM-Driven Data Visualizations"
                },
                "summary": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework that leverages Visual Question Answering (VQA)\nmodels to automate the evaluation of LLM-generated data visualizations.\nTraditional evaluation methods often rely on human judgment, which is costly\nand unscalable, or focus solely on data accuracy, neglecting the effectiveness\nof visual communication. By employing VQA models, we assess data representation\nquality and the general communicative clarity of charts. Experiments were\nconducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with\nvisualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1\n70B-Instruct models. Our results indicate that LLM-generated charts do not\nmatch the accuracy of the original non-LLM-generated charts based on VQA\nperformance measures. Moreover, while our results demonstrate that few-shot\nprompting significantly boosts the accuracy of chart generation, considerable\nprogress remains to be made before LLMs can fully match the precision of\nhuman-generated graphs. This underscores the importance of our work, which\nexpedites the research process by enabling rapid iteration without the need for\nhuman annotation, thus accelerating advancements in this field."
                },
                "authors": [
                    {
                        "name": "James Ford"
                    },
                    {
                        "name": "Xingmeng Zhao"
                    },
                    {
                        "name": "Dan Schumacher"
                    },
                    {
                        "name": "Anthony Rios"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Rios"
                },
                "author": "Anthony Rios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07736v2",
                "updated": "2024-09-27T13:20:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    20,
                    18,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-11T21:46:03Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    46,
                    3,
                    1,
                    163,
                    0
                ],
                "title": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems."
                },
                "authors": [
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Seohyun Park"
                    },
                    {
                        "name": "Hyeyun Jeong"
                    },
                    {
                        "name": "Youngeun Koo"
                    },
                    {
                        "name": "Soonha Hwang"
                    },
                    {
                        "name": "Seonwoo Park"
                    },
                    {
                        "name": "Sungeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Lee"
                },
                "author": "Sungeun Lee",
                "arxiv_comment": "The 2nd GenBench workshop on generalisation (benchmarking) in NLP -\n  EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03471v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03471v3",
                "updated": "2024-09-27T13:12:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    13,
                    12,
                    23,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-04T14:24:06Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    14,
                    24,
                    6,
                    3,
                    95,
                    0
                ],
                "title": "The Impact of Unstated Norms in Bias Analysis of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Unstated Norms in Bias Analysis of Language Models"
                },
                "summary": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task, performed by an LLM, is invariant to a change of group membership.\nIn this work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task, performed by an LLM, is invariant to a change of group membership.\nIn this work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification."
                },
                "authors": [
                    {
                        "name": "Farnaz Kohankhaki"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Jacob-Junqi Tian"
                    },
                    {
                        "name": "Laleh Seyyed-Kalantari"
                    },
                    {
                        "name": "Faiza Khan Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Faiza Khan Khattak"
                },
                "author": "Faiza Khan Khattak",
                "arxiv_comment": "23 Pages, 5 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03471v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03471v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18708v1",
                "updated": "2024-09-27T12:54:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:54:13Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    54,
                    13,
                    4,
                    271,
                    0
                ],
                "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity"
                },
                "summary": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15360v2",
                "updated": "2024-09-27T12:36:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    36,
                    58,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-18T02:35:41Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    35,
                    41,
                    2,
                    262,
                    0
                ],
                "title": "Reward-Robust RLHF in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Robust RLHF in LLMs"
                },
                "summary": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment."
                },
                "authors": [
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yiping Zhang"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yuan Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Shen"
                },
                "author": "Yuan Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v3",
                "updated": "2024-09-27T12:23:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    23,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14277v2",
                "updated": "2024-09-27T12:18:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    18,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-20T12:59:27Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    12,
                    59,
                    27,
                    3,
                    172,
                    0
                ],
                "title": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPaug: Question and Passage Augmentation for Open-Domain Question\n  Answering of LLMs"
                },
                "summary": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation (QPaug) via LLMs for\nopen-domain QA. QPaug first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that QPaug outperforms the previous state-of-the-art and achieves\nsignificant performance gain over existing RAG methods. The source code is\navailable at \\url{https://github.com/kmswin1/QPaug}."
                },
                "authors": [
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Cheoneum Park"
                    },
                    {
                        "name": "Seungjun Baek"
                    }
                ],
                "author_detail": {
                    "name": "Seungjun Baek"
                },
                "author": "Seungjun Baek",
                "arxiv_comment": "The 2024 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v2",
                "updated": "2024-09-27T12:12:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    12,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v3",
                "updated": "2024-09-27T12:12:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    12,
                    19,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18680v1",
                "updated": "2024-09-27T12:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models"
                },
                "summary": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "EMNLP24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18679v1",
                "updated": "2024-09-27T12:05:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    12,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:05:12Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    12,
                    4,
                    271,
                    0
                ],
                "title": "\"Why\" Has the Least Side Effect on Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Why\" Has the Least Side Effect on Model Editing"
                },
                "summary": "Training large language models (LLMs) from scratch is an expensive endeavor,\nparticularly as world knowledge continually evolves. To maintain relevance and\naccuracy of LLMs, model editing has emerged as a pivotal research area. While\nthese methods hold promise, they can also produce unintended side effects.\nTheir underlying factors and causes remain largely unexplored. This paper\ndelves into a critical factor-question type-by categorizing model editing\nquestions. Our findings reveal that the extent of performance degradation\nvaries significantly across different question types, providing new insights\nfor experimental design in knowledge editing. Furthermore, we investigate\nwhether insights from smaller models can be extrapolated to larger models. Our\nresults indicate discrepancies in findings between models of different sizes,\nsuggesting that insights from smaller models may not necessarily apply to\nlarger models. Additionally, we examine the impact of batch size on side\neffects, discovering that increasing the batch size can mitigate performance\ndrops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) from scratch is an expensive endeavor,\nparticularly as world knowledge continually evolves. To maintain relevance and\naccuracy of LLMs, model editing has emerged as a pivotal research area. While\nthese methods hold promise, they can also produce unintended side effects.\nTheir underlying factors and causes remain largely unexplored. This paper\ndelves into a critical factor-question type-by categorizing model editing\nquestions. Our findings reveal that the extent of performance degradation\nvaries significantly across different question types, providing new insights\nfor experimental design in knowledge editing. Furthermore, we investigate\nwhether insights from smaller models can be extrapolated to larger models. Our\nresults indicate discrepancies in findings between models of different sizes,\nsuggesting that insights from smaller models may not necessarily apply to\nlarger models. Additionally, we examine the impact of batch size on side\neffects, discovering that increasing the batch size can mitigate performance\ndrops."
                },
                "authors": [
                    {
                        "name": "Tsung-Hsuan Pan"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18678v1",
                "updated": "2024-09-27T12:05:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    5,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T12:05:05Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    5,
                    5,
                    4,
                    271,
                    0
                ],
                "title": "Rehearsing Answers to Probable Questions with Perspective-Taking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsing Answers to Probable Questions with Perspective-Taking"
                },
                "summary": "Question answering (QA) has been a long-standing focus in the NLP field,\npredominantly addressing reading comprehension and common sense QA. However,\nscenarios involving the preparation of answers to probable questions during\nprofessional oral presentations remain underexplored. In this paper, we pioneer\nthe examination of this crucial yet overlooked topic by utilizing real-world QA\nconversation transcripts between company managers and professional analysts. We\nexplore the proposed task using three causal knowledge graphs (KGs) and three\nlarge language models (LLMs). This work provides foundational insights into the\napplication of LLMs in professional QA scenarios, highlighting the importance\nof causal KGs and perspective-taking in generating effective responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering (QA) has been a long-standing focus in the NLP field,\npredominantly addressing reading comprehension and common sense QA. However,\nscenarios involving the preparation of answers to probable questions during\nprofessional oral presentations remain underexplored. In this paper, we pioneer\nthe examination of this crucial yet overlooked topic by utilizing real-world QA\nconversation transcripts between company managers and professional analysts. We\nexplore the proposed task using three causal knowledge graphs (KGs) and three\nlarge language models (LLMs). This work provides foundational insights into the\napplication of LLMs in professional QA scenarios, highlighting the importance\nof causal KGs and perspective-taking in generating effective responses."
                },
                "authors": [
                    {
                        "name": "Yung-Yu Shih"
                    },
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07393v3",
                "updated": "2024-09-27T11:46:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    46,
                    37,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-11T15:58:59Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    15,
                    58,
                    59,
                    1,
                    163,
                    0
                ],
                "title": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning"
                },
                "summary": "Large Language Models (LLMs) possess extensive knowledge and strong\ncapabilities in performing in-context reasoning. However, previous work\nchallenges their out-of-context reasoning ability, i.e., the ability to infer\ninformation from their training data, instead of from the context or prompt.\nThis paper focuses on a significant aspect of out-of-context reasoning:\nOut-of-Context Knowledge Reasoning (OCKR), which is to combine multiple\nknowledge to infer new knowledge. We designed a synthetic dataset with seven\nrepresentative OCKR tasks to systematically assess the OCKR capabilities of\nLLMs. Using this dataset, we evaluated several LLMs and discovered that their\nproficiency in this aspect is limited, regardless of whether the knowledge is\ntrained in a separate or adjacent training settings. Moreover, training the\nmodel to reason with reasoning examples does not result in significant\nimprovement, while training the model to perform explicit knowledge retrieval\nhelps for retrieving attribute knowledge but not the relation knowledge,\nindicating that the model's limited OCKR capabilities are due to difficulties\nin knowledge retrieval. Furthermore, we treat cross-lingual knowledge transfer\nas a distinct form of OCKR, and evaluate this ability. Our results show that\nthe evaluated model also exhibits limited ability in transferring knowledge\nacross languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) possess extensive knowledge and strong\ncapabilities in performing in-context reasoning. However, previous work\nchallenges their out-of-context reasoning ability, i.e., the ability to infer\ninformation from their training data, instead of from the context or prompt.\nThis paper focuses on a significant aspect of out-of-context reasoning:\nOut-of-Context Knowledge Reasoning (OCKR), which is to combine multiple\nknowledge to infer new knowledge. We designed a synthetic dataset with seven\nrepresentative OCKR tasks to systematically assess the OCKR capabilities of\nLLMs. Using this dataset, we evaluated several LLMs and discovered that their\nproficiency in this aspect is limited, regardless of whether the knowledge is\ntrained in a separate or adjacent training settings. Moreover, training the\nmodel to reason with reasoning examples does not result in significant\nimprovement, while training the model to perform explicit knowledge retrieval\nhelps for retrieving attribute knowledge but not the relation knowledge,\nindicating that the model's limited OCKR capabilities are due to difficulties\nin knowledge retrieval. Furthermore, we treat cross-lingual knowledge transfer\nas a distinct form of OCKR, and evaluate this ability. Our results show that\nthe evaluated model also exhibits limited ability in transferring knowledge\nacross languages."
                },
                "authors": [
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Ruiqi Gao"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08717v2",
                "updated": "2024-09-27T11:46:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    46,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-13T11:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    11,
                    2,
                    28,
                    4,
                    257,
                    0
                ],
                "title": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with\n  LLM-based Agents"
                },
                "summary": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context where social media is increasingly becoming a significant\nplatform for social movements and the formation of public opinion, accurately\nsimulating and predicting the dynamics of user opinions is of great importance\nfor understanding social phenomena, policy making, and guiding public opinion.\nHowever, existing simulation methods face challenges in capturing the\ncomplexity and dynamics of user behavior. Addressing this issue, this paper\nproposes an innovative simulation method for the dynamics of social media user\nopinions, the FDE-LLM algorithm, which incorporates opinion dynamics and\nepidemic model. This effectively constrains the actions and opinion evolution\nprocess of large language models (LLM), making them more aligned with the real\ncyber world. In particular, the FDE-LLM categorizes users into opinion leaders\nand followers. Opinion leaders are based on LLM role-playing and are\nconstrained by the CA model, while opinion followers are integrated into a\ndynamic system that combines the CA model with the SIR model. This innovative\ndesign significantly improves the accuracy and efficiency of the simulation.\nExperiments were conducted on four real Weibo datasets and validated using the\nopen-source model ChatGLM. The results show that, compared to traditional\nagent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion\ndiffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Zhicheng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dong"
                },
                "author": "Zhicheng Dong",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18661v1",
                "updated": "2024-09-27T11:45:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    56,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T11:45:56Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    56,
                    4,
                    271,
                    0
                ],
                "title": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are\n  Ineffective in Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are\n  Ineffective in Practice"
                },
                "summary": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\na disruptive impact throughout the computing education community. LLMs have\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\neven act as friendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in being able to\nexplain and resolve compiler error messages -- for decades, one of the most\nfrustrating parts of learning how to code. However, LLM-generated error message\nexplanations have only been assessed by expert programmers in artificial\nconditions. This work sought to understand how novice programmers resolve\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\nwithin-subjects study with $n$ = 106 participants in which students were tasked\nto fix six buggy C programs. For each program, participants were randomly\nassigned to fix the problem using either a stock compiler error message, an\nexpert-handwritten error message, or an error message explanation generated by\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\ngenerated error messages outperformed conventional compiler error messages in\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\nHandwritten explanations still outperform LLM and conventional error messages,\nboth on objective and subjective measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\na disruptive impact throughout the computing education community. LLMs have\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\neven act as friendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in being able to\nexplain and resolve compiler error messages -- for decades, one of the most\nfrustrating parts of learning how to code. However, LLM-generated error message\nexplanations have only been assessed by expert programmers in artificial\nconditions. This work sought to understand how novice programmers resolve\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\nwithin-subjects study with $n$ = 106 participants in which students were tasked\nto fix six buggy C programs. For each program, participants were randomly\nassigned to fix the problem using either a stock compiler error message, an\nexpert-handwritten error message, or an error message explanation generated by\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\ngenerated error messages outperformed conventional compiler error messages in\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\nHandwritten explanations still outperform LLM and conventional error messages,\nboth on objective and subjective measures."
                },
                "authors": [
                    {
                        "name": "Eddie Antonio Santos"
                    },
                    {
                        "name": "Brett A. Becker"
                    }
                ],
                "author_detail": {
                    "name": "Brett A. Becker"
                },
                "author": "Brett A. Becker",
                "arxiv_doi": "10.1145/3689535.3689554",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689535.3689554",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of the 2024 UK and Ireland Computing\n  Education Research conference (UKICER '24)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06349v2",
                "updated": "2024-09-27T11:45:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    45,
                    9,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-09T14:40:08Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    14,
                    40,
                    8,
                    1,
                    100,
                    0
                ],
                "title": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of\n  LLMs"
                },
                "summary": "The ability to understand causality significantly impacts the competence of\nlarge language models (LLMs) in output explanation and counterfactual\nreasoning, as causality reveals the underlying data distribution. However, the\nlack of a comprehensive benchmark currently limits the evaluation of LLMs'\ncausal learning capabilities. To fill this gap, this paper develops CausalBench\nbased on data from the causal research community, enabling comparative\nevaluations of LLMs against traditional causal learning algorithms. To provide\na comprehensive investigation, we offer three tasks of varying difficulties,\nincluding correlation, causal skeleton, and causality identification.\nEvaluations of 19 leading LLMs reveal that, while closed-source LLMs show\npotential for simple causal relationships, they significantly lag behind\ntraditional algorithms on larger-scale networks ($>50$ nodes). Specifically,\nLLMs struggle with collider structures but excel at chain structures,\nespecially at long-chain causality analogous to Chains-of-Thought techniques.\nThis supports the current prompt approaches while suggesting directions to\nenhance LLMs' causal reasoning capability. Furthermore, CausalBench\nincorporates background knowledge and training data into prompts to thoroughly\nunlock LLMs' text-comprehension ability during evaluation, whose findings\nindicate that, LLM understand causality through semantic associations with\ndistinct entities, rather than directly from contextual information or\nnumerical distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to understand causality significantly impacts the competence of\nlarge language models (LLMs) in output explanation and counterfactual\nreasoning, as causality reveals the underlying data distribution. However, the\nlack of a comprehensive benchmark currently limits the evaluation of LLMs'\ncausal learning capabilities. To fill this gap, this paper develops CausalBench\nbased on data from the causal research community, enabling comparative\nevaluations of LLMs against traditional causal learning algorithms. To provide\na comprehensive investigation, we offer three tasks of varying difficulties,\nincluding correlation, causal skeleton, and causality identification.\nEvaluations of 19 leading LLMs reveal that, while closed-source LLMs show\npotential for simple causal relationships, they significantly lag behind\ntraditional algorithms on larger-scale networks ($>50$ nodes). Specifically,\nLLMs struggle with collider structures but excel at chain structures,\nespecially at long-chain causality analogous to Chains-of-Thought techniques.\nThis supports the current prompt approaches while suggesting directions to\nenhance LLMs' causal reasoning capability. Furthermore, CausalBench\nincorporates background knowledge and training data into prompts to thoroughly\nunlock LLMs' text-comprehension ability during evaluation, whose findings\nindicate that, LLM understand causality through semantic associations with\ndistinct entities, rather than directly from contextual information or\nnumerical distributions."
                },
                "authors": [
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Beicheng Huang"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15861v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15861v5",
                "updated": "2024-09-27T11:28:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    11,
                    28,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-24T17:08:45Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    17,
                    8,
                    45,
                    5,
                    55,
                    0
                ],
                "title": "MATHWELL: Generating Educational Math Word Problems Using Teacher\n  Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATHWELL: Generating Educational Math Word Problems Using Teacher\n  Annotations"
                },
                "summary": "Math word problems are critical K-8 educational tools, but writing them is\ntime consuming and requires extensive expertise. To be educational, problems\nmust be solvable, have accurate answers, and, most importantly, be\neducationally appropriate. We propose that language models have potential to\nsupport K-8 math education by automatically generating word problems. However,\nevaluating educational appropriateness is hard to quantify. We fill this gap by\nhaving teachers evaluate problems generated by LLMs, who find existing models\nand data often fail to be educationally appropriate. We then explore\nautomatically generating educational word problems, ultimately using our expert\nannotations to finetune a 70B language model. Our model, MATHWELL, is the first\nK-8 word problem generator targeted at educational appropriateness. Further\nexpert studies find MATHWELL generates problems far more solvable, accurate,\nand appropriate than public models. MATHWELL also matches GPT-4's problem\nquality while attaining more appropriate reading levels for K-8 students and\navoiding generating harmful questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math word problems are critical K-8 educational tools, but writing them is\ntime consuming and requires extensive expertise. To be educational, problems\nmust be solvable, have accurate answers, and, most importantly, be\neducationally appropriate. We propose that language models have potential to\nsupport K-8 math education by automatically generating word problems. However,\nevaluating educational appropriateness is hard to quantify. We fill this gap by\nhaving teachers evaluate problems generated by LLMs, who find existing models\nand data often fail to be educationally appropriate. We then explore\nautomatically generating educational word problems, ultimately using our expert\nannotations to finetune a 70B language model. Our model, MATHWELL, is the first\nK-8 word problem generator targeted at educational appropriateness. Further\nexpert studies find MATHWELL generates problems far more solvable, accurate,\nand appropriate than public models. MATHWELL also matches GPT-4's problem\nquality while attaining more appropriate reading levels for K-8 students and\navoiding generating harmful questions."
                },
                "authors": [
                    {
                        "name": "Bryan R Christ"
                    },
                    {
                        "name": "Jonathan Kropko"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "arxiv_comment": "24 pages, 10 figures Accepted to EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15861v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15861v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18618v1",
                "updated": "2024-09-27T10:35:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T10:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    35,
                    45,
                    4,
                    271,
                    0
                ],
                "title": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback"
                },
                "summary": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback."
                },
                "authors": [
                    {
                        "name": "Jaepill Choi"
                    },
                    {
                        "name": "Kyubyung Chae"
                    },
                    {
                        "name": "Jiwoo Song"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03882v2",
                "updated": "2024-09-27T10:19:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    19,
                    55,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-06T21:57:35Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    21,
                    57,
                    35,
                    0,
                    127,
                    0
                ],
                "title": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free\n  Efficient Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free\n  Efficient Vision Transformer"
                },
                "summary": "Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\nTrio-ViT accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and\n$\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over\nstate-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$,\n$\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP\nefficiency.} Codes are available at\n\\url{https://github.com/shihuihong214/Trio-ViT}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\nTrio-ViT accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and\n$\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over\nstate-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$,\n$\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP\nefficiency.} Codes are available at\n\\url{https://github.com/shihuihong214/Trio-ViT}."
                },
                "authors": [
                    {
                        "name": "Huihong Shi"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Wendong Mao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17699v2",
                "updated": "2024-09-27T10:16:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    16,
                    37,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T10:12:19Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hamed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Mark Purcell"
                    }
                ],
                "author_detail": {
                    "name": "Mark Purcell"
                },
                "author": "Mark Purcell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18602v1",
                "updated": "2024-09-27T10:07:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    7,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T10:07:33Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    7,
                    33,
                    4,
                    271,
                    0
                ],
                "title": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to\n  Addressee Recognition and Response Selection in Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to\n  Addressee Recognition and Response Selection in Conversations"
                },
                "summary": "Assessing the performance of systems to classify Multi-Party Conversations\n(MPC) is challenging due to the interconnection between linguistic and\nstructural characteristics of conversations. Conventional evaluation methods\noften overlook variances in model behavior across different levels of\nstructural complexity on interaction graphs. In this work, we propose a\nmethodological pipeline to investigate model performance across specific\nstructural attributes of conversations. As a proof of concept we focus on\nResponse Selection and Addressee Recognition tasks, to diagnose model\nweaknesses. To this end, we extract representative diagnostic subdatasets with\na fixed number of users and a good structural variety from a large and open\ncorpus of online MPCs. We further frame our work in terms of data minimization,\navoiding the use of original usernames to preserve privacy, and propose\nalternatives to using original text messages. Results show that response\nselection relies more on the textual content of conversations, while addressee\nrecognition requires capturing their structural dimension. Using an LLM in a\nzero-shot setting, we further highlight how sensitivity to prompt variations is\ntask-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the performance of systems to classify Multi-Party Conversations\n(MPC) is challenging due to the interconnection between linguistic and\nstructural characteristics of conversations. Conventional evaluation methods\noften overlook variances in model behavior across different levels of\nstructural complexity on interaction graphs. In this work, we propose a\nmethodological pipeline to investigate model performance across specific\nstructural attributes of conversations. As a proof of concept we focus on\nResponse Selection and Addressee Recognition tasks, to diagnose model\nweaknesses. To this end, we extract representative diagnostic subdatasets with\na fixed number of users and a good structural variety from a large and open\ncorpus of online MPCs. We further frame our work in terms of data minimization,\navoiding the use of original usernames to preserve privacy, and propose\nalternatives to using original text messages. Results show that response\nselection relies more on the textual content of conversations, while addressee\nrecognition requires capturing their structural dimension. Using an LLM in a\nzero-shot setting, we further highlight how sensitivity to prompt variations is\ntask-dependent."
                },
                "authors": [
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Maryam Sajedinia"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Sara Tonelli"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Accepted to EMNLP 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08424v2",
                "updated": "2024-09-27T10:05:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    10,
                    5,
                    56,
                    4,
                    271,
                    0
                ],
                "published": "2024-04-12T12:15:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    12,
                    15,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task"
                },
                "summary": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot."
                },
                "authors": [
                    {
                        "name": "Hassan Ali"
                    },
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Accepted at ICSR 2024,14 pages,5 figures,2 tables; work was co-funded\n  by Horizon Europe project TERAIS under Grant agreement number 101079338",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18596v1",
                "updated": "2024-09-27T09:56:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    56,
                    2,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:56:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    56,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "ASAG2024: A Combined Benchmark for Short Answer Grading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAG2024: A Combined Benchmark for Short Answer Grading"
                },
                "summary": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems."
                },
                "authors": [
                    {
                        "name": "Gérôme Meyer"
                    },
                    {
                        "name": "Philip Breuer"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "arxiv_doi": "10.1145/3649409.3691083",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649409.3691083",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGCSE-Virtual 2024",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18594v1",
                "updated": "2024-09-27T09:53:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    53,
                    48,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:53:48Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    53,
                    48,
                    4,
                    271,
                    0
                ],
                "title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot\n  Decision Tree Induction and Embedding with Large Language Models"
                },
                "summary": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime."
                },
                "authors": [
                    {
                        "name": "Ricardo Knauer"
                    },
                    {
                        "name": "Mario Koddenbrock"
                    },
                    {
                        "name": "Raphael Wallsberger"
                    },
                    {
                        "name": "Nicholas M. Brisson"
                    },
                    {
                        "name": "Georg N. Duda"
                    },
                    {
                        "name": "Deborah Falla"
                    },
                    {
                        "name": "David W. Evans"
                    },
                    {
                        "name": "Erik Rodner"
                    }
                ],
                "author_detail": {
                    "name": "Erik Rodner"
                },
                "author": "Erik Rodner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14842v2",
                "updated": "2024-09-27T09:52:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    52,
                    57,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T09:20:19Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    20,
                    19,
                    0,
                    267,
                    0
                ],
                "title": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks"
                },
                "summary": "This paper presents the submission of Huawei Translation Services Center\n(HW-TSC) to machine translation tasks of the 20th China Conference on Machine\nTranslation (CCMT 2024). We participate in the bilingual machine translation\ntask and multi-domain machine translation task. For these two translation\ntasks, we use training strategies such as regularized dropout, bidirectional\ntraining, data diversification, forward translation, back translation,\nalternated training, curriculum learning, and transductive ensemble learning to\ntrain neural machine translation (NMT) models based on the deep Transformer-big\narchitecture. Furthermore, to explore whether large language model (LLM) can\nhelp improve the translation quality of NMT systems, we use supervised\nfine-tuning to train llama2-13b as an Automatic post-editing (APE) model to\nimprove the translation results of the NMT model on the multi-domain machine\ntranslation task. By using these plyometric strategies, our submission achieves\na competitive result in the final evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the submission of Huawei Translation Services Center\n(HW-TSC) to machine translation tasks of the 20th China Conference on Machine\nTranslation (CCMT 2024). We participate in the bilingual machine translation\ntask and multi-domain machine translation task. For these two translation\ntasks, we use training strategies such as regularized dropout, bidirectional\ntraining, data diversification, forward translation, back translation,\nalternated training, curriculum learning, and transductive ensemble learning to\ntrain neural machine translation (NMT) models based on the deep Transformer-big\narchitecture. Furthermore, to explore whether large language model (LLM) can\nhelp improve the translation quality of NMT systems, we use supervised\nfine-tuning to train llama2-13b as an Automatic post-editing (APE) model to\nimprove the translation results of the NMT model on the multi-domain machine\ntranslation task. By using these plyometric strategies, our submission achieves\na competitive result in the final evaluation."
                },
                "authors": [
                    {
                        "name": "Zhanglin Wu"
                    },
                    {
                        "name": "Yuanchang Luo"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Jiawei Zheng"
                    },
                    {
                        "name": "Bin Wei"
                    },
                    {
                        "name": "Zongyao Li"
                    },
                    {
                        "name": "Hengchao Shang"
                    },
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Shaojun Li"
                    },
                    {
                        "name": "Weidong Zhang"
                    },
                    {
                        "name": "Ning Xie"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_comment": "14 pages, 2 figures, 6 Tables, CCMT2024. arXiv admin note:\n  substantial text overlap with arXiv:2409.14800",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18583v1",
                "updated": "2024-09-27T09:41:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    41,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T09:41:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    41,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hit the Sweet Spot! Span-Level Ensemble for Large Language Models"
                },
                "summary": "Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods."
                },
                "authors": [
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14364v2",
                "updated": "2024-09-27T09:13:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    9,
                    13,
                    19,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    51,
                    18,
                    6,
                    266,
                    0
                ],
                "title": "More Effective LLM Compressed Tokens with Uniformly Spread Position\n  Identifiers and Compression Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Effective LLM Compressed Tokens with Uniformly Spread Position\n  Identifiers and Compression Loss"
                },
                "summary": "Compressing Transformer inputs into compressd tokens allows running LLMs with\nimproved speed and cost efficiency. Based on the compression method ICAE, we\ncarefully examine the position identifier choices for compressed tokens and\nalso propose a new compression loss. We demonstrate empirically that our\nproposed methods achieve significantly higher compression ratios (15x compared\nto 4x for ICAE), while being able to attain comparable reconstruction\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Transformer inputs into compressd tokens allows running LLMs with\nimproved speed and cost efficiency. Based on the compression method ICAE, we\ncarefully examine the position identifier choices for compressed tokens and\nalso propose a new compression loss. We demonstrate empirically that our\nproposed methods achieve significantly higher compression ratios (15x compared\nto 4x for ICAE), while being able to attain comparable reconstruction\nperformance."
                },
                "authors": [
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19181v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19181v3",
                "updated": "2024-09-27T08:57:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    57,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-28T07:22:16Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    7,
                    22,
                    16,
                    3,
                    88,
                    0
                ],
                "title": "Make Large Language Model a Better Ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Large Language Model a Better Ranker"
                },
                "summary": "Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines."
                },
                "authors": [
                    {
                        "name": "Wen-Shuo Chao"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Hengshu Zhu"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19181v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19181v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00343v2",
                "updated": "2024-09-27T08:29:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    29,
                    50,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-30T12:46:15Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    12,
                    46,
                    15,
                    5,
                    90,
                    0
                ],
                "title": "Commonsense Scene Graph-based Target Localization for Object Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Scene Graph-based Target Localization for Object Search"
                },
                "summary": "Object search is a fundamental skill for household robots, yet the core\nproblem lies in the robot's ability to locate the target object accurately. The\ndynamic nature of household environments, characterized by the arbitrary\nplacement of daily objects by users, makes it challenging to perform target\nlocalization. To efficiently locate the target object, the robot needs to be\nequipped with knowledge at both the object and room level. However, existing\napproaches rely solely on one type of knowledge, leading to unsatisfactory\nobject localization performance and, consequently, inefficient object search\nprocesses. To address this problem, we propose a commonsense scene graph-based\ntarget localization, CSG-TL, to enhance target object search in the household\nenvironment. Given the pre-built map with stationary items, the robot models\nthe room-level knowledge with object-level commonsense knowledge generated by a\nlarge language model (LLM) to a commonsense scene graph (CSG), supporting both\ntypes of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on\ntarget localization, extensive experiments are performed on the real-world\nScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to\nan object search framework, CSG-OS, validated in both simulated and real-world\nenvironments. Code and videos are available at\nhttps://sites.google.com/view/csg-os.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object search is a fundamental skill for household robots, yet the core\nproblem lies in the robot's ability to locate the target object accurately. The\ndynamic nature of household environments, characterized by the arbitrary\nplacement of daily objects by users, makes it challenging to perform target\nlocalization. To efficiently locate the target object, the robot needs to be\nequipped with knowledge at both the object and room level. However, existing\napproaches rely solely on one type of knowledge, leading to unsatisfactory\nobject localization performance and, consequently, inefficient object search\nprocesses. To address this problem, we propose a commonsense scene graph-based\ntarget localization, CSG-TL, to enhance target object search in the household\nenvironment. Given the pre-built map with stationary items, the robot models\nthe room-level knowledge with object-level commonsense knowledge generated by a\nlarge language model (LLM) to a commonsense scene graph (CSG), supporting both\ntypes of knowledge for CSG-TL. To demonstrate the superiority of CSG-TL on\ntarget localization, extensive experiments are performed on the real-world\nScanNet dataset and the AI2THOR simulator. Moreover, we have extended CSG-TL to\nan object search framework, CSG-OS, validated in both simulated and real-world\nenvironments. Code and videos are available at\nhttps://sites.google.com/view/csg-os."
                },
                "authors": [
                    {
                        "name": "Wenqi Ge"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09972v3",
                "updated": "2024-09-27T08:22:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    22,
                    21,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-15T02:38:26Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    2,
                    38,
                    26,
                    4,
                    75,
                    0
                ],
                "title": "Think Twice Before Trusting: Self-Detection for Large Language Models\n  through Comprehensive Answer Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Twice Before Trusting: Self-Detection for Large Language Models\n  through Comprehensive Answer Reflection"
                },
                "summary": "Self-detection for Large Language Models (LLMs) seeks to evaluate the\ntrustworthiness of the LLM's output by leveraging its own capabilities, thereby\nalleviating the issue of output hallucination. However, existing self-detection\napproaches only retrospectively evaluate answers generated by LLM, typically\nleading to the over-trust in incorrectly generated answers. To tackle this\nlimitation, we propose a novel self-detection paradigm that considers the\ncomprehensive answer space beyond LLM-generated answers. It thoroughly compares\nthe trustworthiness of multiple candidate answers to mitigate the over-trust in\nLLM-generated incorrect answers. Building upon this paradigm, we introduce a\ntwo-step framework, which firstly instructs LLM to reflect and provide\njustifications for each candidate answer, and then aggregates the\njustifications for comprehensive target answer evaluation. This framework can\nbe seamlessly integrated with existing approaches for superior self-detection.\nExtensive experiments on six datasets spanning three tasks demonstrate the\neffectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-detection for Large Language Models (LLMs) seeks to evaluate the\ntrustworthiness of the LLM's output by leveraging its own capabilities, thereby\nalleviating the issue of output hallucination. However, existing self-detection\napproaches only retrospectively evaluate answers generated by LLM, typically\nleading to the over-trust in incorrectly generated answers. To tackle this\nlimitation, we propose a novel self-detection paradigm that considers the\ncomprehensive answer space beyond LLM-generated answers. It thoroughly compares\nthe trustworthiness of multiple candidate answers to mitigate the over-trust in\nLLM-generated incorrect answers. Building upon this paradigm, we introduce a\ntwo-step framework, which firstly instructs LLM to reflect and provide\njustifications for each candidate answer, and then aggregates the\njustifications for comprehensive target answer evaluation. This framework can\nbe seamlessly integrated with existing approaches for superior self-detection.\nExtensive experiments on six datasets spanning three tasks demonstrate the\neffectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "EMNLP findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18541v1",
                "updated": "2024-09-27T08:20:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    20,
                    59,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:20:59Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    20,
                    59,
                    4,
                    271,
                    0
                ],
                "title": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation"
                },
                "summary": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the total training sample size from 158k\nto 14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the total training sample size from 158k\nto 14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA."
                },
                "authors": [
                    {
                        "name": "Hongzhe Huang"
                    },
                    {
                        "name": "Zhewen Yu"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Li Cai"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17745v2",
                "updated": "2024-09-27T08:19:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    19,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-26T11:19:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model"
                },
                "summary": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline."
                },
                "authors": [
                    {
                        "name": "Nilanjan Sinhababu"
                    },
                    {
                        "name": "Andrew Parry"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Debasis Samanta"
                    },
                    {
                        "name": "Pabitra Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Pabitra Mitra"
                },
                "author": "Pabitra Mitra",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18538v1",
                "updated": "2024-09-27T08:17:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    17,
                    53,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:17:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    17,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "A Survey on Complex Tasks for Goal-Directed Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Complex Tasks for Goal-Directed Interactive Agents"
                },
                "summary": "Goal-directed interactive agents, which autonomously complete tasks through\ninteractions with their environment, can assist humans in various domains of\ntheir daily lives. Recent advances in large language models (LLMs) led to a\nsurge of new, more and more challenging tasks to evaluate such agents. To\nproperly contextualize performance across these tasks, it is imperative to\nunderstand the different challenges they pose to agents. To this end, this\nsurvey compiles relevant tasks and environments for evaluating goal-directed\ninteractive agents, structuring them along dimensions relevant for\nunderstanding current obstacles. An up-to-date compilation of relevant\nresources can be found on our project website:\nhttps://coli-saar.github.io/interactive-agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-directed interactive agents, which autonomously complete tasks through\ninteractions with their environment, can assist humans in various domains of\ntheir daily lives. Recent advances in large language models (LLMs) led to a\nsurge of new, more and more challenging tasks to evaluate such agents. To\nproperly contextualize performance across these tasks, it is imperative to\nunderstand the different challenges they pose to agents. To this end, this\nsurvey compiles relevant tasks and environments for evaluating goal-directed\ninteractive agents, structuring them along dimensions relevant for\nunderstanding current obstacles. An up-to-date compilation of relevant\nresources can be found on our project website:\nhttps://coli-saar.github.io/interactive-agents."
                },
                "authors": [
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03553v3",
                "updated": "2024-09-27T08:16:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    16,
                    28,
                    4,
                    271,
                    0
                ],
                "published": "2024-05-06T15:20:30Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    20,
                    30,
                    0,
                    127,
                    0
                ],
                "title": "AlphaMath Almost Zero: Process Supervision without Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMath Almost Zero: Process Supervision without Process"
                },
                "summary": "Although recent advancements in large language models (LLMs) have\nsignificantly improved their performance on various tasks, they still face\nchallenges with complex and symbolic multi-step reasoning, particularly in\nmathematical reasoning. To bolster the mathematical reasoning capabilities of\nLLMs, most existing efforts concentrate on seeking assistance from either\ndomain experts or GPT-4 for high-quality process-supervised data, which is not\nonly expensive but also labor-intensive. In our study, we propose an innovative\nframework, AlphaMath, that bypasses the need for process annotations (from\nhumans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework\nfocuses on unleashing the potential of a well-pretrained LLM to autonomously\nenhance its mathematical reasoning. Specifically, we integrate a value model\nwith the LLM, automatically generating both process supervision and step-level\nevaluation signals in MCTS. Furthermore, we propose an efficient inference\nstrategy, step-level beam search, where the value model is crafted to assist\nthe policy model (i.e., LLM) in navigating more effective reasoning paths,\nrather than solely relying on prior probabilities. The experimental results on\nboth in-domain and out-of-domain datasets demonstrate that even without GPT-4\nor human-annotated process supervision, our AlphaMath framework achieves\ncomparable or superior results to previous state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent advancements in large language models (LLMs) have\nsignificantly improved their performance on various tasks, they still face\nchallenges with complex and symbolic multi-step reasoning, particularly in\nmathematical reasoning. To bolster the mathematical reasoning capabilities of\nLLMs, most existing efforts concentrate on seeking assistance from either\ndomain experts or GPT-4 for high-quality process-supervised data, which is not\nonly expensive but also labor-intensive. In our study, we propose an innovative\nframework, AlphaMath, that bypasses the need for process annotations (from\nhumans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework\nfocuses on unleashing the potential of a well-pretrained LLM to autonomously\nenhance its mathematical reasoning. Specifically, we integrate a value model\nwith the LLM, automatically generating both process supervision and step-level\nevaluation signals in MCTS. Furthermore, we propose an efficient inference\nstrategy, step-level beam search, where the value model is crafted to assist\nthe policy model (i.e., LLM) in navigating more effective reasoning paths,\nrather than solely relying on prior probabilities. The experimental results on\nboth in-domain and out-of-domain datasets demonstrate that even without GPT-4\nor human-annotated process supervision, our AlphaMath framework achieves\ncomparable or superior results to previous state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15049v2",
                "updated": "2024-09-27T08:03:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    3,
                    44,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-23T14:22:53Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    14,
                    22,
                    53,
                    0,
                    267,
                    0
                ],
                "title": "PackageIntel: Leveraging Large Language Models for Automated\n  Intelligence Extraction in Package Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackageIntel: Leveraging Large Language Models for Automated\n  Intelligence Extraction in Package Ecosystems"
                },
                "summary": "The rise of malicious packages in public registries poses a significant\nthreat to software supply chain (SSC) security. Although academia and industry\nemploy methods like software composition analysis (SCA) to address this issue,\nexisting approaches often lack timely and comprehensive intelligence updates.\nThis paper introduces PackageIntel, a novel platform that revolutionizes the\ncollection, processing, and retrieval of malicious package intelligence. By\nutilizing exhaustive search techniques, snowball sampling from diverse sources,\nand large language models (LLMs) with specialized prompts, PackageIntel ensures\nenhanced coverage, timeliness, and accuracy. We have developed a comprehensive\ndatabase containing 20,692 malicious NPM and PyPI packages sourced from 21\ndistinct intelligence repositories. Empirical evaluations demonstrate that\nPackageIntel achieves a precision of 98.6% and an F1 score of 92.0 in\nintelligence extraction. Additionally, it detects threats on average 70%\nearlier than leading databases like Snyk and OSV, and operates cost-effectively\nat $0.094 per intelligence piece. The platform has successfully identified and\nreported over 1,000 malicious packages in downstream package manager mirror\nregistries. This research provides a robust, efficient, and timely solution for\nidentifying and mitigating threats within the software supply chain ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of malicious packages in public registries poses a significant\nthreat to software supply chain (SSC) security. Although academia and industry\nemploy methods like software composition analysis (SCA) to address this issue,\nexisting approaches often lack timely and comprehensive intelligence updates.\nThis paper introduces PackageIntel, a novel platform that revolutionizes the\ncollection, processing, and retrieval of malicious package intelligence. By\nutilizing exhaustive search techniques, snowball sampling from diverse sources,\nand large language models (LLMs) with specialized prompts, PackageIntel ensures\nenhanced coverage, timeliness, and accuracy. We have developed a comprehensive\ndatabase containing 20,692 malicious NPM and PyPI packages sourced from 21\ndistinct intelligence repositories. Empirical evaluations demonstrate that\nPackageIntel achieves a precision of 98.6% and an F1 score of 92.0 in\nintelligence extraction. Additionally, it detects threats on average 70%\nearlier than leading databases like Snyk and OSV, and operates cost-effectively\nat $0.094 per intelligence piece. The platform has successfully identified and\nreported over 1,000 malicious packages in downstream package manager mirror\nregistries. This research provides a robust, efficient, and timely solution for\nidentifying and mitigating threats within the software supply chain ecosystem."
                },
                "authors": [
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Chengwei Liu"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Zhengzi Xu"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Yong Fang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10858v2",
                "updated": "2024-09-27T08:03:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    3,
                    7,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-16T09:06:17Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    9,
                    6,
                    17,
                    6,
                    168,
                    0
                ],
                "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-level Value Preference Optimization for Mathematical Reasoning"
                },
                "summary": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for EMNLP2024-Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18511v1",
                "updated": "2024-09-27T07:46:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    46,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T07:46:06Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    46,
                    6,
                    4,
                    271,
                    0
                ],
                "title": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation"
                },
                "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns, even when trained on large\ngeneral-purpose corpora. This study sheds light on the necessity of developing\ndomain-specific embedding models in the LLM era, offering valuable insights for\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns, even when trained on large\ngeneral-purpose corpora. This study sheds light on the necessity of developing\ndomain-specific embedding models in the LLM era, offering valuable insights for\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/FinMTEB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04222v2",
                "updated": "2024-09-27T07:08:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    7,
                    8,
                    10,
                    4,
                    271,
                    0
                ],
                "published": "2024-03-07T04:50:38Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    4,
                    50,
                    38,
                    3,
                    67,
                    0
                ],
                "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evaluation of Large Language Model based on Glass-box Features"
                },
                "summary": "The proliferation of open-source Large Language Models (LLMs) underscores the\npressing need for evaluation methods. Existing works primarily rely on external\nevaluators, focusing on training and prompting strategies. However, a crucial\naspect, model-aware glass-box features, is overlooked. In this study, we\nexplore the utility of glass-box features under the scenario of\nself-evaluation, namely applying an LLM to evaluate its own output. We\ninvestigate various glass-box feature groups and discovered that the softmax\ndistribution serves as a reliable quality indicator for self-evaluation.\nExperimental results on public benchmarks validate the feasibility of\nself-evaluation of LLMs using glass-box features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-source Large Language Models (LLMs) underscores the\npressing need for evaluation methods. Existing works primarily rely on external\nevaluators, focusing on training and prompting strategies. However, a crucial\naspect, model-aware glass-box features, is overlooked. In this study, we\nexplore the utility of glass-box features under the scenario of\nself-evaluation, namely applying an LLM to evaluate its own output. We\ninvestigate various glass-box feature groups and discovered that the softmax\ndistribution serves as a reliable quality indicator for self-evaluation.\nExperimental results on public benchmarks validate the feasibility of\nself-evaluation of LLMs using glass-box features."
                },
                "authors": [
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yingqi Qu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Wenpeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Lu"
                },
                "author": "Wenpeng Lu",
                "arxiv_comment": "accepted as Findings of EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12842v3",
                "updated": "2024-09-27T06:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    6,
                    25,
                    33,
                    4,
                    271,
                    0
                ],
                "published": "2024-02-20T09:10:08Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    9,
                    10,
                    8,
                    1,
                    51,
                    0
                ],
                "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning"
                },
                "summary": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements."
                },
                "authors": [
                    {
                        "name": "Gyeongman Kim"
                    },
                    {
                        "name": "Doohyuk Jang"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "arxiv_comment": "EMNLP 2024 Findings. Our project page: https://promptkd.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.09298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.09298v2",
                "updated": "2024-09-27T05:55:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    55,
                    1,
                    4,
                    271,
                    0
                ],
                "published": "2023-09-17T15:19:29Z",
                "published_parsed": [
                    2023,
                    9,
                    17,
                    15,
                    19,
                    29,
                    6,
                    260,
                    0
                ],
                "title": "OWL: A Large Language Model for IT Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OWL: A Large Language Model for IT Operations"
                },
                "summary": "With the rapid development of IT operations, it has become increasingly\ncrucial to efficiently manage and analyze large volumes of data for practical\napplications. The techniques of Natural Language Processing (NLP) have shown\nremarkable capabilities for various tasks, including named entity recognition,\nmachine translation and dialogue systems. Recently, Large Language Models\n(LLMs) have achieved significant improvements across various NLP downstream\ntasks. However, there is a lack of specialized LLMs for IT operations. In this\npaper, we introduce the OWL, a large language model trained on our collected\nOWL-Instruct dataset with a wide range of IT-related information, where the\nmixture-of-adapter strategy is proposed to improve the parameter-efficient\ntuning across different domains or tasks. Furthermore, we evaluate the\nperformance of our OWL on the OWL-Bench established by us and open IT-related\nbenchmarks. OWL demonstrates superior performance results on IT tasks, which\noutperforms existing models by significant margins. Moreover, we hope that the\nfindings of our work will provide more insights to revolutionize the techniques\nof IT operations with specialized LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of IT operations, it has become increasingly\ncrucial to efficiently manage and analyze large volumes of data for practical\napplications. The techniques of Natural Language Processing (NLP) have shown\nremarkable capabilities for various tasks, including named entity recognition,\nmachine translation and dialogue systems. Recently, Large Language Models\n(LLMs) have achieved significant improvements across various NLP downstream\ntasks. However, there is a lack of specialized LLMs for IT operations. In this\npaper, we introduce the OWL, a large language model trained on our collected\nOWL-Instruct dataset with a wide range of IT-related information, where the\nmixture-of-adapter strategy is proposed to improve the parameter-efficient\ntuning across different domains or tasks. Furthermore, we evaluate the\nperformance of our OWL on the OWL-Bench established by us and open IT-related\nbenchmarks. OWL demonstrates superior performance results on IT tasks, which\noutperforms existing models by significant margins. Moreover, we hope that the\nfindings of our work will provide more insights to revolutionize the techniques\nof IT operations with specialized LLMs."
                },
                "authors": [
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Jiaqi Bai"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Xiaorong Hu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Dongfeng Zhang"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Tieqiao Zheng"
                    },
                    {
                        "name": "Liangfan Zheng"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.09298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.09298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18455v1",
                "updated": "2024-09-27T05:31:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    31,
                    4,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T05:31:04Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    31,
                    4,
                    4,
                    271,
                    0
                ],
                "title": "Review of Digital Asset Development with Graph Neural Network Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Digital Asset Development with Graph Neural Network Unlearning"
                },
                "summary": "In the rapidly evolving landscape of digital assets, the imperative for\nrobust data privacy and compliance with regulatory frameworks has intensified.\nThis paper investigates the critical role of Graph Neural Networks (GNNs) in\nthe management of digital assets and introduces innovative unlearning\ntechniques specifically tailored to GNN architectures. We categorize unlearning\nstrategies into two primary classes: data-driven approximation, which\nmanipulates the graph structure to isolate and remove the influence of specific\nnodes, and model-driven approximation, which modifies the internal parameters\nand architecture of the GNN itself. By examining recent advancements in these\nunlearning methodologies, we highlight their applicability in various use\ncases, including fraud detection, risk assessment, token relationship\nprediction, and decentralized governance. We discuss the challenges inherent in\nbalancing model performance with the requirements for data unlearning,\nparticularly in the context of real-time financial applications. Furthermore,\nwe propose a hybrid approach that combines the strengths of both unlearning\nstrategies to enhance the efficiency and effectiveness of GNNs in digital asset\necosystems. Ultimately, this paper aims to provide a comprehensive framework\nfor understanding and implementing GNN unlearning techniques, paving the way\nfor secure and compliant deployment of machine learning in the digital asset\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of digital assets, the imperative for\nrobust data privacy and compliance with regulatory frameworks has intensified.\nThis paper investigates the critical role of Graph Neural Networks (GNNs) in\nthe management of digital assets and introduces innovative unlearning\ntechniques specifically tailored to GNN architectures. We categorize unlearning\nstrategies into two primary classes: data-driven approximation, which\nmanipulates the graph structure to isolate and remove the influence of specific\nnodes, and model-driven approximation, which modifies the internal parameters\nand architecture of the GNN itself. By examining recent advancements in these\nunlearning methodologies, we highlight their applicability in various use\ncases, including fraud detection, risk assessment, token relationship\nprediction, and decentralized governance. We discuss the challenges inherent in\nbalancing model performance with the requirements for data unlearning,\nparticularly in the context of real-time financial applications. Furthermore,\nwe propose a hybrid approach that combines the strengths of both unlearning\nstrategies to enhance the efficiency and effectiveness of GNNs in digital asset\necosystems. Ultimately, this paper aims to provide a comprehensive framework\nfor understanding and implementing GNN unlearning techniques, paving the way\nfor secure and compliant deployment of machine learning in the digital asset\ndomain."
                },
                "authors": [
                    {
                        "name": "Zara Lisbon"
                    }
                ],
                "author_detail": {
                    "name": "Zara Lisbon"
                },
                "author": "Zara Lisbon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18454v1",
                "updated": "2024-09-27T05:29:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    29,
                    31,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T05:29:31Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    29,
                    31,
                    4,
                    271,
                    0
                ],
                "title": "Leveraging Long-Context Large Language Models for Multi-Document\n  Understanding and Summarization in Enterprise Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Long-Context Large Language Models for Multi-Document\n  Understanding and Summarization in Enterprise Applications"
                },
                "summary": "The rapid increase in unstructured data across various fields has made\nmulti-document comprehension and summarization a critical task. Traditional\napproaches often fail to capture relevant context, maintain logical\nconsistency, and extract essential information from lengthy documents. This\npaper explores the use of Long-context Large Language Models (LLMs) for\nmulti-document summarization, demonstrating their exceptional capacity to grasp\nextensive connections, provide cohesive summaries, and adapt to various\nindustry domains and integration with enterprise applications/systems. The\npaper discusses the workflow of multi-document summarization for effectively\ndeploying long-context LLMs, supported by case studies in legal applications,\nenterprise functions such as HR, finance, and sourcing, as well as in the\nmedical and news domains. These case studies show notable enhancements in both\nefficiency and accuracy. Technical obstacles, such as dataset diversity, model\nscalability, and ethical considerations like bias mitigation and factual\naccuracy, are carefully analyzed. Prospective research avenues are suggested to\naugment the functionalities and applications of long-context LLMs, establishing\nthem as pivotal tools for transforming information processing across diverse\nsectors and enterprise applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in unstructured data across various fields has made\nmulti-document comprehension and summarization a critical task. Traditional\napproaches often fail to capture relevant context, maintain logical\nconsistency, and extract essential information from lengthy documents. This\npaper explores the use of Long-context Large Language Models (LLMs) for\nmulti-document summarization, demonstrating their exceptional capacity to grasp\nextensive connections, provide cohesive summaries, and adapt to various\nindustry domains and integration with enterprise applications/systems. The\npaper discusses the workflow of multi-document summarization for effectively\ndeploying long-context LLMs, supported by case studies in legal applications,\nenterprise functions such as HR, finance, and sourcing, as well as in the\nmedical and news domains. These case studies show notable enhancements in both\nefficiency and accuracy. Technical obstacles, such as dataset diversity, model\nscalability, and ethical considerations like bias mitigation and factual\naccuracy, are carefully analyzed. Prospective research avenues are suggested to\naugment the functionalities and applications of long-context LLMs, establishing\nthem as pivotal tools for transforming information processing across diverse\nsectors and enterprise applications."
                },
                "authors": [
                    {
                        "name": "Aditi Godbole"
                    },
                    {
                        "name": "Jabin Geevarghese George"
                    },
                    {
                        "name": "Smita Shandilya"
                    }
                ],
                "author_detail": {
                    "name": "Smita Shandilya"
                },
                "author": "Smita Shandilya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13997v2",
                "updated": "2024-09-27T05:20:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    20,
                    37,
                    4,
                    271,
                    0
                ],
                "published": "2024-06-20T04:52:19Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    4,
                    52,
                    19,
                    3,
                    172,
                    0
                ],
                "title": "\"Global is Good, Local is Bad?\": Understanding Brand Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Global is Good, Local is Bad?\": Understanding Brand Bias in LLMs"
                },
                "summary": "Many recent studies have investigated social biases in LLMs but brand bias\nhas received little attention. This research examines the biases exhibited by\nLLMs towards different brands, a significant concern given the widespread use\nof LLMs in affected use cases such as product recommendation and market\nanalysis. Biased models may perpetuate societal inequalities, unfairly favoring\nestablished global brands while marginalizing local ones. Using a curated\ndataset across four brand categories, we probe the behavior of LLMs in this\nspace. We find a consistent pattern of bias in this space -- both in terms of\ndisproportionately associating global brands with positive attributes and\ndisproportionately recommending luxury gifts for individuals in high-income\ncountries. We also find LLMs are subject to country-of-origin effects which may\nboost local brand preference in LLM outputs in specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent studies have investigated social biases in LLMs but brand bias\nhas received little attention. This research examines the biases exhibited by\nLLMs towards different brands, a significant concern given the widespread use\nof LLMs in affected use cases such as product recommendation and market\nanalysis. Biased models may perpetuate societal inequalities, unfairly favoring\nestablished global brands while marginalizing local ones. Using a curated\ndataset across four brand categories, we probe the behavior of LLMs in this\nspace. We find a consistent pattern of bias in this space -- both in terms of\ndisproportionately associating global brands with positive attributes and\ndisproportionately recommending luxury gifts for individuals in high-income\ncountries. We also find LLMs are subject to country-of-origin effects which may\nboost local brand preference in LLM outputs in specific contexts."
                },
                "authors": [
                    {
                        "name": "Mahammed Kamruzzaman"
                    },
                    {
                        "name": "Hieu Minh Nguyen"
                    },
                    {
                        "name": "Gene Louis Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gene Louis Kim"
                },
                "author": "Gene Louis Kim",
                "arxiv_comment": "Accepted at EMNLP-2024 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18446v1",
                "updated": "2024-09-27T05:06:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    6,
                    43,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T05:06:43Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    5,
                    6,
                    43,
                    4,
                    271,
                    0
                ],
                "title": "Exploring Language Model Generalization in Low-Resource Extractive QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Language Model Generalization in Low-Resource Extractive QA"
                },
                "summary": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize well to\nclosed-domains that require specific knowledge such as medicine and law in a\nzero-shot fashion without additional in-domain training? To this end, we devise\na series of experiments to empirically explain the performance gap. Our\nfindings suggest that: a) LLMs struggle with dataset demands of closed-domains\nsuch as retrieving long answer-spans; b) Certain LLMs, despite showing strong\noverall performance, display weaknesses in meeting basic requirements as\ndiscriminating between domain-specific senses of words which we link to\npre-processing decisions; c) Scaling model parameters is not always effective\nfor cross-domain generalization; and d) Closed-domain datasets are\nquantitatively much different than open-domain EQA datasets and current LLMs\nstruggle to deal with them. Our findings point out important directions for\nimproving existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate Extractive Question Answering (EQA) with Large\nLanguage Models (LLMs) under domain drift, i.e., can LLMs generalize well to\nclosed-domains that require specific knowledge such as medicine and law in a\nzero-shot fashion without additional in-domain training? To this end, we devise\na series of experiments to empirically explain the performance gap. Our\nfindings suggest that: a) LLMs struggle with dataset demands of closed-domains\nsuch as retrieving long answer-spans; b) Certain LLMs, despite showing strong\noverall performance, display weaknesses in meeting basic requirements as\ndiscriminating between domain-specific senses of words which we link to\npre-processing decisions; c) Scaling model parameters is not always effective\nfor cross-domain generalization; and d) Closed-domain datasets are\nquantitatively much different than open-domain EQA datasets and current LLMs\nstruggle to deal with them. Our findings point out important directions for\nimproving existing LLMs."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18433v1",
                "updated": "2024-09-27T03:49:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    49,
                    56,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T03:49:56Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    49,
                    56,
                    4,
                    271,
                    0
                ],
                "title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM\n  Performance and Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM\n  Performance and Generalization"
                },
                "summary": "While generalization over tasks from easy to hard is crucial to profile\nlanguage models (LLMs), the datasets with fine-grained difficulty annotations\nfor each problem across a broad range of complexity are still blank. Aiming to\naddress this limitation, we present Easy2Hard-Bench, a consistently formatted\ncollection of 6 benchmark datasets spanning various domains, such as\nmathematics and programming problems, chess puzzles, and reasoning questions.\nEach problem within these datasets is annotated with numerical difficulty\nscores. To systematically estimate problem difficulties, we collect abundant\nperformance data on attempts to each problem by humans in the real world or\nLLMs on the prominent leaderboard. Leveraging the rich performance data, we\napply well-established difficulty ranking systems, such as Item Response Theory\n(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to\nproblems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from\nprevious collections by a higher proportion of challenging problems. Through\nextensive experiments with six state-of-the-art LLMs, we provide a\ncomprehensive analysis of their performance and generalization capabilities\nacross varying levels of difficulty, with the aim of inspiring future research\nin LLM generalization. The datasets are available at\nhttps://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generalization over tasks from easy to hard is crucial to profile\nlanguage models (LLMs), the datasets with fine-grained difficulty annotations\nfor each problem across a broad range of complexity are still blank. Aiming to\naddress this limitation, we present Easy2Hard-Bench, a consistently formatted\ncollection of 6 benchmark datasets spanning various domains, such as\nmathematics and programming problems, chess puzzles, and reasoning questions.\nEach problem within these datasets is annotated with numerical difficulty\nscores. To systematically estimate problem difficulties, we collect abundant\nperformance data on attempts to each problem by humans in the real world or\nLLMs on the prominent leaderboard. Leveraging the rich performance data, we\napply well-established difficulty ranking systems, such as Item Response Theory\n(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to\nproblems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from\nprevious collections by a higher proportion of challenging problems. Through\nextensive experiments with six state-of-the-art LLMs, we provide a\ncomprehensive analysis of their performance and generalization capabilities\nacross varying levels of difficulty, with the aim of inspiring future research\nin LLM generalization. The datasets are available at\nhttps://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench."
                },
                "authors": [
                    {
                        "name": "Mucong Ding"
                    },
                    {
                        "name": "Chenghao Deng"
                    },
                    {
                        "name": "Jocelyn Choo"
                    },
                    {
                        "name": "Zichu Wu"
                    },
                    {
                        "name": "Aakriti Agrawal"
                    },
                    {
                        "name": "Avi Schwarzschild"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "John Langford"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15460v3",
                "updated": "2024-09-27T03:20:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    20,
                    4,
                    4,
                    271,
                    0
                ],
                "published": "2024-08-28T00:52:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    0,
                    52,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "Lagrangian approach to origami vertex analysis: Kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lagrangian approach to origami vertex analysis: Kinematics"
                },
                "summary": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design."
                },
                "authors": [
                    {
                        "name": "Matthew Grasinger"
                    },
                    {
                        "name": "Andrew Gillman"
                    },
                    {
                        "name": "Philip Buskohl"
                    }
                ],
                "author_detail": {
                    "name": "Philip Buskohl"
                },
                "author": "Philip Buskohl",
                "arxiv_doi": "10.1098/rsta.2024.0203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsta.2024.0203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Electronic supplementary information can be found at the published\n  article, https://doi.org/10.1098/rsta.2024.0203",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18417v1",
                "updated": "2024-09-27T03:15:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    15,
                    7,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T03:15:07Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    15,
                    7,
                    4,
                    271,
                    0
                ],
                "title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement\n  Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement\n  Learning from Human Feedback"
                },
                "summary": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM) to instill human expectations into LLMs.\nWhile preference annotation comes with a monetized cost, the economic utility\nof a preference dataset has not been considered by far. What exacerbates this\nsituation is that given complex intransitive or cyclic relationships in\npreference datasets, existing algorithms for fine-tuning LLMs are still far\nfrom capturing comprehensive preferences. This raises severe cost-efficiency\nconcerns in production environments, where preference data accumulate over\ntime. In this paper, we see the fine-tuning of LLMs as a monetized economy and\nintroduce an auction mechanism to improve the efficiency of the preference data\ncollection in dollar terms. We show that introducing an auction mechanism can\nplay an essential role in enhancing the cost-efficiency of RLHF while\nmaintaining satisfactory model performance. Experimental results demonstrate\nthat our proposed auction-based protocol is cost-efficient for fine-tuning LLMs\nby concentrating on high-quality feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM) to instill human expectations into LLMs.\nWhile preference annotation comes with a monetized cost, the economic utility\nof a preference dataset has not been considered by far. What exacerbates this\nsituation is that given complex intransitive or cyclic relationships in\npreference datasets, existing algorithms for fine-tuning LLMs are still far\nfrom capturing comprehensive preferences. This raises severe cost-efficiency\nconcerns in production environments, where preference data accumulate over\ntime. In this paper, we see the fine-tuning of LLMs as a monetized economy and\nintroduce an auction mechanism to improve the efficiency of the preference data\ncollection in dollar terms. We show that introducing an auction mechanism can\nplay an essential role in enhancing the cost-efficiency of RLHF while\nmaintaining satisfactory model performance. Experimental results demonstrate\nthat our proposed auction-based protocol is cost-efficient for fine-tuning LLMs\nby concentrating on high-quality feedback."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhang"
                    },
                    {
                        "name": "Jiuding Duan"
                    }
                ],
                "author_detail": {
                    "name": "Jiuding Duan"
                },
                "author": "Jiuding Duan",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18412v1",
                "updated": "2024-09-27T03:00:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    0,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T03:00:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    0,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciDFM: A Large Language Model with Mixture-of-Experts for Science"
                },
                "summary": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0."
                },
                "authors": [
                    {
                        "name": "Liangtai Sun"
                    },
                    {
                        "name": "Danyu Luo"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Baocai Chen"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "12 pages, 1 figure, 9 tables. Technical Report, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00234v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00234v5",
                "updated": "2024-09-27T02:55:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    55,
                    6,
                    4,
                    271,
                    0
                ],
                "published": "2022-12-31T15:57:09Z",
                "published_parsed": [
                    2022,
                    12,
                    31,
                    15,
                    57,
                    9,
                    5,
                    365,
                    0
                ],
                "title": "A Survey on In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on In-context Learning"
                },
                "summary": "With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL."
                },
                "authors": [
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Xu Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "Update",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.00234v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00234v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18405v1",
                "updated": "2024-09-27T02:42:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    42,
                    55,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T02:42:55Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    42,
                    55,
                    4,
                    271,
                    0
                ],
                "title": "Word2Wave: Language Driven Mission Programming for Efficient Subsea\n  Deployments of Marine Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word2Wave: Language Driven Mission Programming for Efficient Subsea\n  Deployments of Marine Robots"
                },
                "summary": "This paper explores the design and development of a language-based interface\nfor dynamic mission programming of autonomous underwater vehicles (AUVs). The\nproposed 'Word2Wave' (W2W) framework enables interactive programming and\nparameter configuration of AUVs for remote subsea missions. The W2W framework\nincludes: (i) a set of novel language rules and command structures for\nefficient language-to-mission mapping; (ii) a GPT-based prompt engineering\nmodule for training data generation; (iii) a small language model (SLM)-based\nsequence-to-sequence learning pipeline for mission command generation from\nhuman speech or text; and (iv) a novel user interface for 2D mission map\nvisualization and human-machine interfacing. The proposed learning pipeline\nadapts an SLM named T5-Small that can learn language-to-mission mapping from\nprocessed language data effectively, providing robust and efficient\nperformance. In addition to a benchmark evaluation with state-of-the-art, we\nconduct a user interaction study to demonstrate the effectiveness of W2W over\ncommercial AUV programming interfaces. Across participants, W2W-based\nprogramming required less than 10% time for mission programming compared to\ntraditional interfaces; it is deemed to be a simpler and more natural paradigm\nfor subsea mission programming with a usability score of 76.25. W2W opens up\npromising future research opportunities on hands-free AUV mission programming\nfor efficient subsea deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design and development of a language-based interface\nfor dynamic mission programming of autonomous underwater vehicles (AUVs). The\nproposed 'Word2Wave' (W2W) framework enables interactive programming and\nparameter configuration of AUVs for remote subsea missions. The W2W framework\nincludes: (i) a set of novel language rules and command structures for\nefficient language-to-mission mapping; (ii) a GPT-based prompt engineering\nmodule for training data generation; (iii) a small language model (SLM)-based\nsequence-to-sequence learning pipeline for mission command generation from\nhuman speech or text; and (iv) a novel user interface for 2D mission map\nvisualization and human-machine interfacing. The proposed learning pipeline\nadapts an SLM named T5-Small that can learn language-to-mission mapping from\nprocessed language data effectively, providing robust and efficient\nperformance. In addition to a benchmark evaluation with state-of-the-art, we\nconduct a user interaction study to demonstrate the effectiveness of W2W over\ncommercial AUV programming interfaces. Across participants, W2W-based\nprogramming required less than 10% time for mission programming compared to\ntraditional interfaces; it is deemed to be a simpler and more natural paradigm\nfor subsea mission programming with a usability score of 76.25. W2W opens up\npromising future research opportunities on hands-free AUV mission programming\nfor efficient subsea deployments."
                },
                "authors": [
                    {
                        "name": "Ruo Chen"
                    },
                    {
                        "name": "David Blow"
                    },
                    {
                        "name": "Adnan Abdullah"
                    },
                    {
                        "name": "Md Jahidul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md Jahidul Islam"
                },
                "author": "Md Jahidul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18395v1",
                "updated": "2024-09-27T02:25:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    25,
                    29,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T02:25:29Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    25,
                    29,
                    4,
                    271,
                    0
                ],
                "title": "Code Vulnerability Repair with Large Language Model using Context-Aware\n  Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Vulnerability Repair with Large Language Model using Context-Aware\n  Prompt Tuning"
                },
                "summary": "Large Language Models (LLMs) have shown significant challenges in detecting\nand repairing vulnerable code, particularly when dealing with vulnerabilities\ninvolving multiple aspects, such as variables, code flows, and code structures.\nIn this study, we utilize GitHub Copilot as the LLM and focus on buffer\noverflow vulnerabilities. Our experiments reveal a notable gap in Copilot's\nabilities when dealing with buffer overflow vulnerabilities, with a 76%\nvulnerability detection rate but only a 15% vulnerability repair rate. To\naddress this issue, we propose context-aware prompt tuning techniques designed\nto enhance LLM performance in repairing buffer overflow. By injecting a\nsequence of domain knowledge about the vulnerability, including various\nsecurity and code contexts, we demonstrate that Copilot's successful repair\nrate increases to 63%, representing more than four times the improvement\ncompared to repairs without domain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant challenges in detecting\nand repairing vulnerable code, particularly when dealing with vulnerabilities\ninvolving multiple aspects, such as variables, code flows, and code structures.\nIn this study, we utilize GitHub Copilot as the LLM and focus on buffer\noverflow vulnerabilities. Our experiments reveal a notable gap in Copilot's\nabilities when dealing with buffer overflow vulnerabilities, with a 76%\nvulnerability detection rate but only a 15% vulnerability repair rate. To\naddress this issue, we propose context-aware prompt tuning techniques designed\nto enhance LLM performance in repairing buffer overflow. By injecting a\nsequence of domain knowledge about the vulnerability, including various\nsecurity and code contexts, we demonstrate that Copilot's successful repair\nrate increases to 63%, representing more than four times the improvement\ncompared to repairs without domain knowledge."
                },
                "authors": [
                    {
                        "name": "Arshiya Khan"
                    },
                    {
                        "name": "Guannan Liu"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17391v2",
                "updated": "2024-09-27T02:18:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    18,
                    22,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-25T22:08:31Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    22,
                    8,
                    31,
                    2,
                    269,
                    0
                ],
                "title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An\n  Example using Pythia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Behavior for Large Language Models regarding Numeral Systems: An\n  Example using Pythia"
                },
                "summary": "Though Large Language Models (LLMs) have shown remarkable abilities in\nmathematics reasoning, they are still struggling with performing numeric\noperations accurately, such as addition and multiplication. Numbers can be\ntokenized into tokens in various ways by different LLMs and affect the numeric\noperations performance. Currently, there are two representatives: 1) Tokenize\ninto $1$-digit, and 2) Tokenize into $1\\sim 3$ digit. The difference is roughly\nequivalent to using different numeral systems (namely base $10$ or base\n$10^{3}$). In light of this, we study the scaling behavior of different numeral\nsystems in the context of transformer-based large language models. We\nempirically show that a base $10$ system is consistently more data-efficient\nthan a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes\nunder from-scratch training settings, while different number systems have very\nsimilar fine-tuning performances. We attribute this to higher token frequencies\nof a base $10$ system. Additionally, we reveal extrapolation behavior patterns\non addition and multiplication. We identify that base $100$ and base $1000$\nsystems struggle on token-level discernment and token-level operations. We also\nsheds light on the mechanism learnt by the models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though Large Language Models (LLMs) have shown remarkable abilities in\nmathematics reasoning, they are still struggling with performing numeric\noperations accurately, such as addition and multiplication. Numbers can be\ntokenized into tokens in various ways by different LLMs and affect the numeric\noperations performance. Currently, there are two representatives: 1) Tokenize\ninto $1$-digit, and 2) Tokenize into $1\\sim 3$ digit. The difference is roughly\nequivalent to using different numeral systems (namely base $10$ or base\n$10^{3}$). In light of this, we study the scaling behavior of different numeral\nsystems in the context of transformer-based large language models. We\nempirically show that a base $10$ system is consistently more data-efficient\nthan a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes\nunder from-scratch training settings, while different number systems have very\nsimilar fine-tuning performances. We attribute this to higher token frequencies\nof a base $10$ system. Additionally, we reveal extrapolation behavior patterns\non addition and multiplication. We identify that base $100$ and base $1000$\nsystems struggle on token-level discernment and token-level operations. We also\nsheds light on the mechanism learnt by the models."
                },
                "authors": [
                    {
                        "name": "Zhejian Zhou"
                    },
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18382v1",
                "updated": "2024-09-27T01:48:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    1,
                    48,
                    16,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T01:48:16Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    1,
                    48,
                    16,
                    4,
                    271,
                    0
                ],
                "title": "CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot\n  Skills using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot\n  Skills using Large Language Models"
                },
                "summary": "Curriculum learning is a training mechanism in reinforcement learning (RL)\nthat facilitates the achievement of complex policies by progressively\nincreasing the task difficulty during training. However, designing effective\ncurricula for a specific task often requires extensive domain knowledge and\nhuman intervention, which limits its applicability across various domains. Our\ncore idea is that large language models (LLMs), with their extensive training\non diverse language data and ability to encapsulate world knowledge, present\nsignificant potential for efficiently breaking down tasks and decomposing\nskills across various robotics environments. Additionally, the demonstrated\nsuccess of LLMs in translating natural language into executable code for RL\nagents strengthens their role in generating task curricula. In this work, we\npropose CurricuLLM, which leverages the high-level planning and programming\ncapabilities of LLMs for curriculum design, thereby enhancing the efficient\nlearning of complex target tasks. CurricuLLM consists of: (Step 1) Generating\nsequence of subtasks that aid target task learning in natural language form,\n(Step 2) Translating natural language description of subtasks in executable\ntask code, including the reward code and goal distribution code, and (Step 3)\nEvaluating trained policies based on trajectory rollout and subtask\ndescription. We evaluate CurricuLLM in various robotics simulation\nenvironments, ranging from manipulation, navigation, and locomotion, to show\nthat CurricuLLM can aid learning complex robot control tasks. In addition, we\nvalidate humanoid locomotion policy learned through CurricuLLM in real-world.\nThe code is provided in https://github.com/labicon/CurricuLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum learning is a training mechanism in reinforcement learning (RL)\nthat facilitates the achievement of complex policies by progressively\nincreasing the task difficulty during training. However, designing effective\ncurricula for a specific task often requires extensive domain knowledge and\nhuman intervention, which limits its applicability across various domains. Our\ncore idea is that large language models (LLMs), with their extensive training\non diverse language data and ability to encapsulate world knowledge, present\nsignificant potential for efficiently breaking down tasks and decomposing\nskills across various robotics environments. Additionally, the demonstrated\nsuccess of LLMs in translating natural language into executable code for RL\nagents strengthens their role in generating task curricula. In this work, we\npropose CurricuLLM, which leverages the high-level planning and programming\ncapabilities of LLMs for curriculum design, thereby enhancing the efficient\nlearning of complex target tasks. CurricuLLM consists of: (Step 1) Generating\nsequence of subtasks that aid target task learning in natural language form,\n(Step 2) Translating natural language description of subtasks in executable\ntask code, including the reward code and goal distribution code, and (Step 3)\nEvaluating trained policies based on trajectory rollout and subtask\ndescription. We evaluate CurricuLLM in various robotics simulation\nenvironments, ranging from manipulation, navigation, and locomotion, to show\nthat CurricuLLM can aid learning complex robot control tasks. In addition, we\nvalidate humanoid locomotion policy learned through CurricuLLM in real-world.\nThe code is provided in https://github.com/labicon/CurricuLLM"
                },
                "authors": [
                    {
                        "name": "Kanghyun Ryu"
                    },
                    {
                        "name": "Qiayuan Liao"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Negar Mehr"
                    }
                ],
                "author_detail": {
                    "name": "Negar Mehr"
                },
                "author": "Negar Mehr",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17353v2",
                "updated": "2024-09-27T01:42:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    1,
                    42,
                    54,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-25T20:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    20,
                    59,
                    12,
                    2,
                    269,
                    0
                ],
                "title": "Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM"
                },
                "summary": "Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Robin Shing-Hei Yuen"
                    },
                    {
                        "name": "Timothy Tin-Long Tse"
                    },
                    {
                        "name": "Jian Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhu"
                },
                "author": "Jian Zhu",
                "arxiv_comment": "Corrected style from final to preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09890v2",
                "updated": "2024-09-27T00:01:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    0,
                    1,
                    3,
                    4,
                    271,
                    0
                ],
                "published": "2024-07-13T13:43:39Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    43,
                    39,
                    5,
                    195,
                    0
                ],
                "title": "Speech-Guided Sequential Planning for Autonomous Navigation using Large\n  Language Model Meta AI 3 (Llama3)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-Guided Sequential Planning for Autonomous Navigation using Large\n  Language Model Meta AI 3 (Llama3)"
                },
                "summary": "In social robotics, a pivotal focus is enabling robots to engage with humans\nin a more natural and seamless manner. The emergence of advanced large language\nmodels (LLMs) such as Generative Pre-trained Transformers (GPTs) and\nautoregressive models like Large Language Model Meta AI (Llamas) has driven\nsignificant advancements in integrating natural language understanding\ncapabilities into social robots. This paper presents a system for speech-guided\nsequential planning in autonomous navigation, utilizing Llama3 and the Robot\nOperating System~(ROS). The proposed system involves using Llama3 to interpret\nvoice commands, extracting essential details through parsing, and decoding\nthese commands into sequential actions for tasks. Such sequential planning is\nessential in various domains, particularly in the pickup and delivery of an\nobject. Once a sequential navigation task is evaluated, we employ DRL-VO, a\nlearning-based control policy that allows a robot to autonomously navigate\nthrough social spaces with static infrastructure and (crowds of) people. We\ndemonstrate the effectiveness of the system in simulation experiment using\nTurtlebot 2 in ROS1 and Turtlebot 3 in ROS2. We conduct hardware trials using a\nClearpath Robotics Jackal UGV, highlighting its potential for real-world\ndeployment in scenarios requiring flexible and interactive robotic behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In social robotics, a pivotal focus is enabling robots to engage with humans\nin a more natural and seamless manner. The emergence of advanced large language\nmodels (LLMs) such as Generative Pre-trained Transformers (GPTs) and\nautoregressive models like Large Language Model Meta AI (Llamas) has driven\nsignificant advancements in integrating natural language understanding\ncapabilities into social robots. This paper presents a system for speech-guided\nsequential planning in autonomous navigation, utilizing Llama3 and the Robot\nOperating System~(ROS). The proposed system involves using Llama3 to interpret\nvoice commands, extracting essential details through parsing, and decoding\nthese commands into sequential actions for tasks. Such sequential planning is\nessential in various domains, particularly in the pickup and delivery of an\nobject. Once a sequential navigation task is evaluated, we employ DRL-VO, a\nlearning-based control policy that allows a robot to autonomously navigate\nthrough social spaces with static infrastructure and (crowds of) people. We\ndemonstrate the effectiveness of the system in simulation experiment using\nTurtlebot 2 in ROS1 and Turtlebot 3 in ROS2. We conduct hardware trials using a\nClearpath Robotics Jackal UGV, highlighting its potential for real-world\ndeployment in scenarios requiring flexible and interactive robotic behaviors."
                },
                "authors": [
                    {
                        "name": "Alkesh K. Srivastava"
                    },
                    {
                        "name": "Philip Dames"
                    }
                ],
                "author_detail": {
                    "name": "Philip Dames"
                },
                "author": "Philip Dames",
                "arxiv_comment": "Accepted at the 16th International Conference on Social Robotics + AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08044v2",
                "updated": "2024-09-26T23:47:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    47,
                    3,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-10T20:52:18Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    20,
                    52,
                    18,
                    2,
                    192,
                    0
                ],
                "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective\n  Weight-Activation Quantization"
                },
                "summary": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA"
                },
                "authors": [
                    {
                        "name": "Xijie Huang"
                    },
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "EMNLP 2024 Findings, Codes: https://github.com/HuangOwen/RoLoRA,\n  Models:\n  https://huggingface.co/collections/ScarletAce/rolora-66f5f228a90681c7c4512b28",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18345v1",
                "updated": "2024-09-26T23:46:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    46,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T23:46:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    46,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "A Generalized LLM-Augmented BIM Framework: Application to a\n  Speech-to-BIM system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generalized LLM-Augmented BIM Framework: Application to a\n  Speech-to-BIM system"
                },
                "summary": "Performing building information modeling (BIM) tasks is a complex process\nthat imposes a steep learning curve and a heavy cognitive load due to the\nnecessity of remembering sequences of numerous commands. With the rapid\nadvancement of large language models (LLMs), it is foreseeable that BIM tasks,\nincluding querying and managing BIM data, 4D and 5D BIM, design compliance\nchecking, or authoring a design, using written or spoken natural language\n(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical\nuser interfaces. This paper proposes a generalized LLM-augmented BIM framework\nto expedite the development of LLM-enhanced BIM applications by providing a\nstep-by-step development process. The proposed framework consists of six steps:\ninterpret-fill-match-structure-execute-check. The paper demonstrates the\napplicability of the proposed framework through implementing a speech-to-BIM\napplication, NADIA-S (Natural-language-based Architectural Detailing through\nInteraction with Artificial Intelligence via Speech), using exterior wall\ndetailing as an example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing building information modeling (BIM) tasks is a complex process\nthat imposes a steep learning curve and a heavy cognitive load due to the\nnecessity of remembering sequences of numerous commands. With the rapid\nadvancement of large language models (LLMs), it is foreseeable that BIM tasks,\nincluding querying and managing BIM data, 4D and 5D BIM, design compliance\nchecking, or authoring a design, using written or spoken natural language\n(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical\nuser interfaces. This paper proposes a generalized LLM-augmented BIM framework\nto expedite the development of LLM-enhanced BIM applications by providing a\nstep-by-step development process. The proposed framework consists of six steps:\ninterpret-fill-match-structure-execute-check. The paper demonstrates the\napplicability of the proposed framework through implementing a speech-to-BIM\napplication, NADIA-S (Natural-language-based Architectural Detailing through\nInteraction with Artificial Intelligence via Speech), using exterior wall\ndetailing as an example."
                },
                "authors": [
                    {
                        "name": "Ghang Lee"
                    },
                    {
                        "name": "Suhyung Jang"
                    },
                    {
                        "name": "Seokho Hyun"
                    }
                ],
                "author_detail": {
                    "name": "Seokho Hyun"
                },
                "author": "Seokho Hyun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18341v1",
                "updated": "2024-09-26T23:30:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    30,
                    48,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T23:30:48Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    30,
                    48,
                    3,
                    270,
                    0
                ],
                "title": "Does End-to-End Autonomous Driving Really Need Perception Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does End-to-End Autonomous Driving Really Need Perception Tasks?"
                },
                "summary": "End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised\nperception tasks to extract explicit scene information (e.g., objects, maps).\nThis reliance necessitates expensive annotations and constrains deployment and\ndata scalability in real-time applications. In this paper, we introduce SSR, a\nnovel framework that utilizes only 16 navigation-guided tokens as Sparse Scene\nRepresentation, efficiently extracting crucial scene information for E2EAD. Our\nmethod eliminates the need for supervised sub-tasks, allowing computational\nresources to concentrate on essential elements directly related to navigation\nintent. We further introduce a temporal enhancement module that employs a\nBird's-Eye View (BEV) world model, aligning predicted future scenes with actual\nfuture scenes through self-supervision. SSR achieves state-of-the-art planning\nperformance on the nuScenes dataset, demonstrating a 27.2\\% relative reduction\nin L2 error and a 51.6\\% decrease in collision rate to the leading E2EAD\nmethod, UniAD. Moreover, SSR offers a 10.9$\\times$ faster inference speed and\n13$\\times$ faster training time. This framework represents a significant leap\nin real-time autonomous driving systems and paves the way for future scalable\ndeployment. Code will be released at \\url{https://github.com/PeidongLi/SSR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised\nperception tasks to extract explicit scene information (e.g., objects, maps).\nThis reliance necessitates expensive annotations and constrains deployment and\ndata scalability in real-time applications. In this paper, we introduce SSR, a\nnovel framework that utilizes only 16 navigation-guided tokens as Sparse Scene\nRepresentation, efficiently extracting crucial scene information for E2EAD. Our\nmethod eliminates the need for supervised sub-tasks, allowing computational\nresources to concentrate on essential elements directly related to navigation\nintent. We further introduce a temporal enhancement module that employs a\nBird's-Eye View (BEV) world model, aligning predicted future scenes with actual\nfuture scenes through self-supervision. SSR achieves state-of-the-art planning\nperformance on the nuScenes dataset, demonstrating a 27.2\\% relative reduction\nin L2 error and a 51.6\\% decrease in collision rate to the leading E2EAD\nmethod, UniAD. Moreover, SSR offers a 10.9$\\times$ faster inference speed and\n13$\\times$ faster training time. This framework represents a significant leap\nin real-time autonomous driving systems and paves the way for future scalable\ndeployment. Code will be released at \\url{https://github.com/PeidongLi/SSR}."
                },
                "authors": [
                    {
                        "name": "Peidong Li"
                    },
                    {
                        "name": "Dixiao Cui"
                    }
                ],
                "author_detail": {
                    "name": "Dixiao Cui"
                },
                "author": "Dixiao Cui",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18339v1",
                "updated": "2024-09-26T23:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    25,
                    21,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T23:25:21Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    25,
                    21,
                    3,
                    270,
                    0
                ],
                "title": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language\n  Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Xin Hong"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Vidhyasaharan Sethu"
                    },
                    {
                        "name": "Ting Dang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Dang"
                },
                "author": "Ting Dang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18336v1",
                "updated": "2024-09-26T23:18:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    18,
                    25,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T23:18:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    23,
                    18,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "DeBaRA: Denoising-Based 3D Room Arrangement Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeBaRA: Denoising-Based 3D Room Arrangement Generation"
                },
                "summary": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic and diverse layouts of furnished indoor 3D scenes\nunlocks multiple interactive applications impacting a wide range of industries.\nThe inherent complexity of object interactions, the limited amount of available\ndata and the requirement to fulfill spatial constraints all make generative\nmodeling for 3D scene synthesis and arrangement challenging. Current methods\naddress these challenges autoregressively or by using off-the-shelf diffusion\nobjectives by simultaneously predicting all attributes without 3D reasoning\nconsiderations. In this paper, we introduce DeBaRA, a score-based model\nspecifically tailored for precise, controllable and flexible arrangement\ngeneration in a bounded environment. We argue that the most critical component\nof a scene synthesis system is to accurately establish the size and position of\nvarious objects within a restricted area. Based on this insight, we propose a\nlightweight conditional score-based model designed with 3D spatial awareness at\nits core. We demonstrate that by focusing on spatial attributes of objects, a\nsingle trained DeBaRA model can be leveraged at test time to perform several\ndownstream applications such as scene synthesis, completion and re-arrangement.\nFurther, we introduce a novel Self Score Evaluation procedure so it can be\noptimally employed alongside external LLM models. We evaluate our approach\nthrough extensive experiments and demonstrate significant improvement upon\nstate-of-the-art approaches in a range of scenarios."
                },
                "authors": [
                    {
                        "name": "Léopold Maillard"
                    },
                    {
                        "name": "Nicolas Sereyjol-Garros"
                    },
                    {
                        "name": "Tom Durand"
                    },
                    {
                        "name": "Maks Ovsjanikov"
                    }
                ],
                "author_detail": {
                    "name": "Maks Ovsjanikov"
                },
                "author": "Maks Ovsjanikov",
                "arxiv_comment": "Accepted at NeurIPS 2024. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18319v1",
                "updated": "2024-09-26T21:59:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    59,
                    11,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T21:59:11Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    59,
                    11,
                    3,
                    270,
                    0
                ],
                "title": "Cross-Institutional Structured Radiology Reporting for Lung Cancer\n  Screening Using a Dynamic Template-Constrained Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Institutional Structured Radiology Reporting for Lung Cancer\n  Screening Using a Dynamic Template-Constrained Large Language Model"
                },
                "summary": "Structured radiology reporting is advantageous for optimizing clinical\nworkflows and patient outcomes. Current LLMs in creating structured reports\nface the challenges of formatting errors, content hallucinations, and privacy\nleakage concerns when uploaded to external servers. We aim to develop an\nenhanced open-source LLM for creating structured and standardized LCS reports\nfrom free-text descriptions. After institutional IRB approvals, 5,442\nde-identified LCS reports from two institutions were retrospectively analyzed.\n500 reports were randomly selected from the two institutions evenly and then\nmanually labeled for evaluation. Two radiologists from the two institutions\ndeveloped a standardized template including 29 features for lung nodule\nreporting. We proposed template-constrained decoding to enhance\nstate-of-the-art open-source LLMs, including LLAMA, Qwen, and Mistral. The LLM\nperformance was extensively evaluated in terms of F1 score, confidence\ninterval, McNemar test, and z-test. Based on the structured reports created\nfrom the large-scale dataset, a nodule-level retrieval system was prototyped\nand an automatic statistical analysis was performed. Our software,\nvLLM-structure, is publicly available for local deployment with enhanced LLMs.\nOur template-constrained decoding approach consistently enhanced the LLM\nperformance on multi-institutional datasets, with neither formatting errors nor\ncontent hallucinations. Our method improved the best open-source LLAMA-3.1 405B\nby up to 10.42%, and outperformed GPT-4o by 17.19%. A novel nodule retrieval\nsystem was successfully prototyped and demonstrated on a large-scale multimodal\ndatabase using our enhanced LLM technologies. The automatically derived\nstatistical distributions were closely consistent with the prior findings in\nterms of nodule type, location, size, status, and Lung-RADS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured radiology reporting is advantageous for optimizing clinical\nworkflows and patient outcomes. Current LLMs in creating structured reports\nface the challenges of formatting errors, content hallucinations, and privacy\nleakage concerns when uploaded to external servers. We aim to develop an\nenhanced open-source LLM for creating structured and standardized LCS reports\nfrom free-text descriptions. After institutional IRB approvals, 5,442\nde-identified LCS reports from two institutions were retrospectively analyzed.\n500 reports were randomly selected from the two institutions evenly and then\nmanually labeled for evaluation. Two radiologists from the two institutions\ndeveloped a standardized template including 29 features for lung nodule\nreporting. We proposed template-constrained decoding to enhance\nstate-of-the-art open-source LLMs, including LLAMA, Qwen, and Mistral. The LLM\nperformance was extensively evaluated in terms of F1 score, confidence\ninterval, McNemar test, and z-test. Based on the structured reports created\nfrom the large-scale dataset, a nodule-level retrieval system was prototyped\nand an automatic statistical analysis was performed. Our software,\nvLLM-structure, is publicly available for local deployment with enhanced LLMs.\nOur template-constrained decoding approach consistently enhanced the LLM\nperformance on multi-institutional datasets, with neither formatting errors nor\ncontent hallucinations. Our method improved the best open-source LLAMA-3.1 405B\nby up to 10.42%, and outperformed GPT-4o by 17.19%. A novel nodule retrieval\nsystem was successfully prototyped and demonstrated on a large-scale multimodal\ndatabase using our enhanced LLM technologies. The automatically derived\nstatistical distributions were closely consistent with the prior findings in\nterms of nodule type, location, size, status, and Lung-RADS."
                },
                "authors": [
                    {
                        "name": "Chuang Niu"
                    },
                    {
                        "name": "Parisa Kaviani"
                    },
                    {
                        "name": "Qing Lyu"
                    },
                    {
                        "name": "Mannudeep K. Kalra"
                    },
                    {
                        "name": "Christopher T. Whitlow"
                    },
                    {
                        "name": "Ge Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Wang"
                },
                "author": "Ge Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v3",
                "updated": "2024-09-26T21:28:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    28,
                    42,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18290v1",
                "updated": "2024-09-26T21:00:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    0,
                    51,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T21:00:51Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    0,
                    51,
                    3,
                    270,
                    0
                ],
                "title": "Retrospective Comparative Analysis of Prostate Cancer In-Basket\n  Messages: Responses from Closed-Domain LLM vs. Clinical Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Comparative Analysis of Prostate Cancer In-Basket\n  Messages: Responses from Closed-Domain LLM vs. Clinical Teams"
                },
                "summary": "In-basket message interactions play a crucial role in physician-patient\ncommunication, occurring during all phases (pre-, during, and post) of a\npatient's care journey. However, responding to these patients' inquiries has\nbecome a significant burden on healthcare workflows, consuming considerable\ntime for clinical care teams. To address this, we introduce RadOnc-GPT, a\nspecialized Large Language Model (LLM) powered by GPT-4 that has been designed\nwith a focus on radiotherapeutic treatment of prostate cancer with advanced\nprompt engineering, and specifically designed to assist in generating\nresponses. We integrated RadOnc-GPT with patient electronic health records\n(EHR) from both the hospital-wide EHR database and an internal,\nradiation-oncology-specific database. RadOnc-GPT was evaluated on 158\npreviously recorded in-basket message interactions. Quantitative natural\nlanguage processing (NLP) analysis and two grading studies with clinicians and\nnurses were used to assess RadOnc-GPT's responses. Our findings indicate that\nRadOnc-GPT slightly outperformed the clinical care team in \"Clarity\" and\n\"Empathy,\" while achieving comparable scores in \"Completeness\" and\n\"Correctness.\" RadOnc-GPT is estimated to save 5.2 minutes per message for\nnurses and 2.4 minutes for clinicians, from reading the inquiry to sending the\nresponse. Employing RadOnc-GPT for in-basket message draft generation has the\npotential to alleviate the workload of clinical care teams and reduce\nhealthcare costs by producing high-quality, timely responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-basket message interactions play a crucial role in physician-patient\ncommunication, occurring during all phases (pre-, during, and post) of a\npatient's care journey. However, responding to these patients' inquiries has\nbecome a significant burden on healthcare workflows, consuming considerable\ntime for clinical care teams. To address this, we introduce RadOnc-GPT, a\nspecialized Large Language Model (LLM) powered by GPT-4 that has been designed\nwith a focus on radiotherapeutic treatment of prostate cancer with advanced\nprompt engineering, and specifically designed to assist in generating\nresponses. We integrated RadOnc-GPT with patient electronic health records\n(EHR) from both the hospital-wide EHR database and an internal,\nradiation-oncology-specific database. RadOnc-GPT was evaluated on 158\npreviously recorded in-basket message interactions. Quantitative natural\nlanguage processing (NLP) analysis and two grading studies with clinicians and\nnurses were used to assess RadOnc-GPT's responses. Our findings indicate that\nRadOnc-GPT slightly outperformed the clinical care team in \"Clarity\" and\n\"Empathy,\" while achieving comparable scores in \"Completeness\" and\n\"Correctness.\" RadOnc-GPT is estimated to save 5.2 minutes per message for\nnurses and 2.4 minutes for clinicians, from reading the inquiry to sending the\nresponse. Employing RadOnc-GPT for in-basket message draft generation has the\npotential to alleviate the workload of clinical care teams and reduce\nhealthcare costs by producing high-quality, timely responses."
                },
                "authors": [
                    {
                        "name": "Yuexing Hao"
                    },
                    {
                        "name": "Jason M. Holmes"
                    },
                    {
                        "name": "Jared Hobson"
                    },
                    {
                        "name": "Alexandra Bennett"
                    },
                    {
                        "name": "Daniel K. Ebner"
                    },
                    {
                        "name": "David M. Routman"
                    },
                    {
                        "name": "Satomi Shiraishi"
                    },
                    {
                        "name": "Samir H. Patel"
                    },
                    {
                        "name": "Nathan Y. Yu"
                    },
                    {
                        "name": "Chris L. Hallemeier"
                    },
                    {
                        "name": "Brooke E. Ball"
                    },
                    {
                        "name": "Mark R. Waddle"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18289v1",
                "updated": "2024-09-26T21:00:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    0,
                    45,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T21:00:45Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    0,
                    45,
                    3,
                    270,
                    0
                ],
                "title": "Criticality and Safety Margins for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criticality and Safety Margins for Reinforcement Learning"
                },
                "summary": "State of the art reinforcement learning methods sometimes encounter unsafe\nsituations. Identifying when these situations occur is of interest both for\npost-hoc analysis and during deployment, where it might be advantageous to call\nout to a human overseer for help. Efforts to gauge the criticality of different\npoints in time have been developed, but their accuracy is not well established\ndue to a lack of ground truth, and they are not designed to be easily\ninterpretable by end users. Therefore, we seek to define a criticality\nframework with both a quantifiable ground truth and a clear significance to\nusers. We introduce true criticality as the expected drop in reward when an\nagent deviates from its policy for n consecutive random actions. We also\nintroduce the concept of proxy criticality, a low-overhead metric that has a\nstatistically monotonic relationship to true criticality. Safety margins make\nthese interpretable, when defined as the number of random actions for which\nperformance loss will not exceed some tolerance with high confidence. We\ndemonstrate this approach in several environment-agent combinations; for an A3C\nagent in an Atari Beamrider environment, the lowest 5% of safety margins\ncontain 47% of agent losses; i.e., supervising only 5% of decisions could\npotentially prevent roughly half of an agent's errors. This criticality\nframework measures the potential impacts of bad decisions, even before those\ndecisions are made, allowing for more effective debugging and oversight of\nautonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State of the art reinforcement learning methods sometimes encounter unsafe\nsituations. Identifying when these situations occur is of interest both for\npost-hoc analysis and during deployment, where it might be advantageous to call\nout to a human overseer for help. Efforts to gauge the criticality of different\npoints in time have been developed, but their accuracy is not well established\ndue to a lack of ground truth, and they are not designed to be easily\ninterpretable by end users. Therefore, we seek to define a criticality\nframework with both a quantifiable ground truth and a clear significance to\nusers. We introduce true criticality as the expected drop in reward when an\nagent deviates from its policy for n consecutive random actions. We also\nintroduce the concept of proxy criticality, a low-overhead metric that has a\nstatistically monotonic relationship to true criticality. Safety margins make\nthese interpretable, when defined as the number of random actions for which\nperformance loss will not exceed some tolerance with high confidence. We\ndemonstrate this approach in several environment-agent combinations; for an A3C\nagent in an Atari Beamrider environment, the lowest 5% of safety margins\ncontain 47% of agent losses; i.e., supervising only 5% of decisions could\npotentially prevent roughly half of an agent's errors. This criticality\nframework measures the potential impacts of bad decisions, even before those\ndecisions are made, allowing for more effective debugging and oversight of\nautonomous agents."
                },
                "authors": [
                    {
                        "name": "Alexander Grushin"
                    },
                    {
                        "name": "Walt Woods"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    },
                    {
                        "name": "Simon Khan"
                    }
                ],
                "author_detail": {
                    "name": "Simon Khan"
                },
                "author": "Simon Khan",
                "arxiv_comment": "17 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17073v2",
                "updated": "2024-09-26T20:40:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    20,
                    40,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T16:32:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    32,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition"
                },
                "summary": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nexactly should be attributed? This involves identifying the specific\ninformation units within an answer that require grounding. In this paper, we\npropose and investigate a novel approach to the factual decomposition of\ngenerated answers for attribution, employing template-based in-context\nlearning. To accomplish this, we utilize the question and integrate negative\nsampling during few-shot in-context learning for decomposition. This approach\nenhances the semantic understanding of both abstractive and extractive answers.\nWe examine the impact of answer decomposition by providing a thorough\nexamination of various attribution approaches, ranging from retrieval-based\ntechniques to LLM-based attributors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nexactly should be attributed? This involves identifying the specific\ninformation units within an answer that require grounding. In this paper, we\npropose and investigate a novel approach to the factual decomposition of\ngenerated answers for attribution, employing template-based in-context\nlearning. To accomplish this, we utilize the question and integrate negative\nsampling during few-shot in-context learning for decomposition. This approach\nenhances the semantic understanding of both abstractive and extractive answers.\nWe examine the impact of answer decomposition by providing a thorough\nexamination of various attribution approaches, ranging from retrieval-based\ntechniques to LLM-based attributors."
                },
                "authors": [
                    {
                        "name": "Pritika Ramu"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Balaji Vasan Srinivavsan"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Vasan Srinivavsan"
                },
                "author": "Balaji Vasan Srinivavsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16005v2",
                "updated": "2024-09-26T20:37:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    20,
                    37,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-05-25T02:02:08Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    2,
                    2,
                    8,
                    5,
                    146,
                    0
                ],
                "title": "PTQ4DiT: Post-training Quantization for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTQ4DiT: Post-training Quantization for Diffusion Transformers"
                },
                "summary": "The recent introduction of Diffusion Transformers (DiTs) has demonstrated\nexceptional capabilities in image generation by using a different backbone\narchitecture, departing from traditional U-Nets and embracing the scalable\nnature of transformers. Despite their advanced capabilities, the wide\ndeployment of DiTs, particularly for real-time applications, is currently\nhampered by considerable computational demands at the inference stage.\nPost-training Quantization (PTQ) has emerged as a fast and data-efficient\nsolution that can significantly reduce computation and memory footprint by\nusing low-bit weights and activations. However, its applicability to DiTs has\nnot yet been explored and faces non-trivial difficulties due to the unique\ndesign of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ\nmethod for DiTs. We discover two primary quantization challenges inherent in\nDiTs, notably the presence of salient channels with extreme magnitudes and the\ntemporal variability in distributions of salient activation over multiple\ntimesteps. To tackle these challenges, we propose Channel-wise Salience\nBalancing (CSB) and Spearmen's $\\rho$-guided Salience Calibration (SSC). CSB\nleverages the complementarity property of channel magnitudes to redistribute\nthe extremes, alleviating quantization errors for both activations and weights.\nSSC extends this approach by dynamically adjusting the balanced salience to\ncapture the temporal variations in activation. Additionally, to eliminate extra\ncomputational costs caused by PTQ4DiT during inference, we design an offline\nre-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT\nsuccessfully quantizes DiTs to 8-bit precision (W8A8) while preserving\ncomparable generation ability and further enables effective quantization to\n4-bit weight precision (W4A8) for the first time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent introduction of Diffusion Transformers (DiTs) has demonstrated\nexceptional capabilities in image generation by using a different backbone\narchitecture, departing from traditional U-Nets and embracing the scalable\nnature of transformers. Despite their advanced capabilities, the wide\ndeployment of DiTs, particularly for real-time applications, is currently\nhampered by considerable computational demands at the inference stage.\nPost-training Quantization (PTQ) has emerged as a fast and data-efficient\nsolution that can significantly reduce computation and memory footprint by\nusing low-bit weights and activations. However, its applicability to DiTs has\nnot yet been explored and faces non-trivial difficulties due to the unique\ndesign of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ\nmethod for DiTs. We discover two primary quantization challenges inherent in\nDiTs, notably the presence of salient channels with extreme magnitudes and the\ntemporal variability in distributions of salient activation over multiple\ntimesteps. To tackle these challenges, we propose Channel-wise Salience\nBalancing (CSB) and Spearmen's $\\rho$-guided Salience Calibration (SSC). CSB\nleverages the complementarity property of channel magnitudes to redistribute\nthe extremes, alleviating quantization errors for both activations and weights.\nSSC extends this approach by dynamically adjusting the balanced salience to\ncapture the temporal variations in activation. Additionally, to eliminate extra\ncomputational costs caused by PTQ4DiT during inference, we design an offline\nre-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT\nsuccessfully quantizes DiTs to 8-bit precision (W8A8) while preserving\ncomparable generation ability and further enables effective quantization to\n4-bit weight precision (W4A8) for the first time."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Haoxuan Wang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Yan Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yan Yan"
                },
                "author": "Yan Yan",
                "arxiv_comment": "NeurIPS 2024. Code is available at\n  https://github.com/adreamwu/PTQ4DiT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16383v2",
                "updated": "2024-09-26T20:26:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    20,
                    26,
                    24,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T18:35:09Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    35,
                    9,
                    1,
                    268,
                    0
                ],
                "title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation"
                },
                "summary": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings."
                },
                "authors": [
                    {
                        "name": "Ioannis Panagiotopoulos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18244v1",
                "updated": "2024-09-26T19:37:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    19,
                    37,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T19:37:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    19,
                    37,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Development of an Edge Resilient ML Ensemble to Tolerate ICS Adversarial\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of an Edge Resilient ML Ensemble to Tolerate ICS Adversarial\n  Attacks"
                },
                "summary": "Deploying machine learning (ML) in dynamic data-driven applications systems\n(DDDAS) can improve the security of industrial control systems (ICS). However,\nML-based DDDAS are vulnerable to adversarial attacks because adversaries can\nalter the input data slightly so that the ML models predict a different result.\nIn this paper, our goal is to build a resilient edge machine learning (reML)\narchitecture that is designed to withstand adversarial attacks by performing\nData Air Gap Transformation (DAGT) to anonymize data feature spaces using deep\nneural networks and randomize the ML models used for predictions. The reML is\nbased on the Resilient DDDAS paradigm, Moving Target Defense (MTD) theory, and\nTinyML and is applied to combat adversarial attacks on ICS. Furthermore, the\nproposed approach is power-efficient and privacy-preserving and, therefore, can\nbe deployed on power-constrained devices to enhance ICS security. This approach\nenables resilient ML inference at the edge by shifting the computation from the\ncomputing-intensive platforms to the resource-constrained edge devices. The\nincorporation of TinyML with TensorFlow Lite ensures efficient resource\nutilization and, consequently, makes reML suitable for deployment in various\nindustrial control environments. Furthermore, the dynamic nature of reML,\nfacilitated by the resilient DDDAS development environment, allows for\ncontinuous adaptation and improvement in response to emerging threats. Lastly,\nwe evaluate our approach on an ICS dataset and demonstrate that reML provides a\nviable and effective solution for resilient ML inference at the edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying machine learning (ML) in dynamic data-driven applications systems\n(DDDAS) can improve the security of industrial control systems (ICS). However,\nML-based DDDAS are vulnerable to adversarial attacks because adversaries can\nalter the input data slightly so that the ML models predict a different result.\nIn this paper, our goal is to build a resilient edge machine learning (reML)\narchitecture that is designed to withstand adversarial attacks by performing\nData Air Gap Transformation (DAGT) to anonymize data feature spaces using deep\nneural networks and randomize the ML models used for predictions. The reML is\nbased on the Resilient DDDAS paradigm, Moving Target Defense (MTD) theory, and\nTinyML and is applied to combat adversarial attacks on ICS. Furthermore, the\nproposed approach is power-efficient and privacy-preserving and, therefore, can\nbe deployed on power-constrained devices to enhance ICS security. This approach\nenables resilient ML inference at the edge by shifting the computation from the\ncomputing-intensive platforms to the resource-constrained edge devices. The\nincorporation of TinyML with TensorFlow Lite ensures efficient resource\nutilization and, consequently, makes reML suitable for deployment in various\nindustrial control environments. Furthermore, the dynamic nature of reML,\nfacilitated by the resilient DDDAS development environment, allows for\ncontinuous adaptation and improvement in response to emerging threats. Lastly,\nwe evaluate our approach on an ICS dataset and demonstrate that reML provides a\nviable and effective solution for resilient ML inference at the edge devices."
                },
                "authors": [
                    {
                        "name": "Likai Yao"
                    },
                    {
                        "name": "Qinxuan Shi"
                    },
                    {
                        "name": "Zhanglong Yang"
                    },
                    {
                        "name": "Sicong Shao"
                    },
                    {
                        "name": "Salim Hariri"
                    }
                ],
                "author_detail": {
                    "name": "Salim Hariri"
                },
                "author": "Salim Hariri",
                "arxiv_comment": "Accepted by Dynamic Data Driven Applications Systems: International\n  Conference, DDDAS, Springer. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02369v2",
                "updated": "2024-09-26T19:07:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    19,
                    7,
                    30,
                    3,
                    270,
                    0
                ],
                "published": "2024-01-04T17:23:44Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    17,
                    23,
                    44,
                    3,
                    4,
                    0
                ],
                "title": "SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded\n  Entity Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded\n  Entity Retrieval"
                },
                "summary": "Clinician must write a lengthy summary each time a patient is discharged from\nthe hospital. This task is time-consuming due to the sheer number of unique\nclinical concepts covered in the admission. Identifying and covering salient\nentities is vital for the summary to be clinically useful. We fine-tune\nopen-source LLMs (Mistral-7B-Instruct and Zephyr-7B-beta) on the task and find\nthat they generate incomplete and unfaithful summaries. To increase entity\ncoverage, we train a smaller, encoder-only model to predict salient entities,\nwhich are treated as content-plans to guide the LLM. To encourage the LLM to\nfocus on specific mentions in the source notes, we propose SPEER:\nSentence-level Planning via Embedded Entity Retrieval. Specifically, we mark\neach salient entity span with special \"{{ }}\" boundary tags and instruct the\nLLM to retrieve marked spans before generating each sentence. Sentence-level\nplanning acts as a form of state tracking in that the model is explicitly\nrecording the entities it uses. We fine-tune Mistral and Zephyr variants on a\nlarge-scale, diverse dataset of ~167k in-patient hospital admissions and\nevaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness\nmetrics over non-guided and guided baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinician must write a lengthy summary each time a patient is discharged from\nthe hospital. This task is time-consuming due to the sheer number of unique\nclinical concepts covered in the admission. Identifying and covering salient\nentities is vital for the summary to be clinically useful. We fine-tune\nopen-source LLMs (Mistral-7B-Instruct and Zephyr-7B-beta) on the task and find\nthat they generate incomplete and unfaithful summaries. To increase entity\ncoverage, we train a smaller, encoder-only model to predict salient entities,\nwhich are treated as content-plans to guide the LLM. To encourage the LLM to\nfocus on specific mentions in the source notes, we propose SPEER:\nSentence-level Planning via Embedded Entity Retrieval. Specifically, we mark\neach salient entity span with special \"{{ }}\" boundary tags and instruct the\nLLM to retrieve marked spans before generating each sentence. Sentence-level\nplanning acts as a form of state tracking in that the model is explicitly\nrecording the entities it uses. We fine-tune Mistral and Zephyr variants on a\nlarge-scale, diverse dataset of ~167k in-patient hospital admissions and\nevaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness\nmetrics over non-guided and guided baselines."
                },
                "authors": [
                    {
                        "name": "Griffin Adams"
                    },
                    {
                        "name": "Jason Zucker"
                    },
                    {
                        "name": "Noémie Elhadad"
                    }
                ],
                "author_detail": {
                    "name": "Noémie Elhadad"
                },
                "author": "Noémie Elhadad",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18222v1",
                "updated": "2024-09-26T19:02:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    19,
                    2,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T19:02:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    19,
                    2,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Trustworthy AI: Securing Sensitive Data in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI: Securing Sensitive Data in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) by enabling robust text generation and understanding. However, their\ndeployment in sensitive domains like healthcare, finance, and legal services\nraises critical concerns about privacy and data security. This paper proposes a\ncomprehensive framework for embedding trust mechanisms into LLMs to dynamically\ncontrol the disclosure of sensitive information. The framework integrates three\ncore components: User Trust Profiling, Information Sensitivity Detection, and\nAdaptive Output Control. By leveraging techniques such as Role-Based Access\nControl (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition\n(NER), contextual analysis, and privacy-preserving methods like differential\nprivacy, the system ensures that sensitive information is disclosed\nappropriately based on the user's trust level. By focusing on balancing data\nutility and privacy, the proposed solution offers a novel approach to securely\ndeploying LLMs in high-risk environments. Future work will focus on testing\nthis framework across various domains to evaluate its effectiveness in managing\nsensitive data while maintaining system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) by enabling robust text generation and understanding. However, their\ndeployment in sensitive domains like healthcare, finance, and legal services\nraises critical concerns about privacy and data security. This paper proposes a\ncomprehensive framework for embedding trust mechanisms into LLMs to dynamically\ncontrol the disclosure of sensitive information. The framework integrates three\ncore components: User Trust Profiling, Information Sensitivity Detection, and\nAdaptive Output Control. By leveraging techniques such as Role-Based Access\nControl (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition\n(NER), contextual analysis, and privacy-preserving methods like differential\nprivacy, the system ensures that sensitive information is disclosed\nappropriately based on the user's trust level. By focusing on balancing data\nutility and privacy, the proposed solution offers a novel approach to securely\ndeploying LLMs in high-risk environments. Future work will focus on testing\nthis framework across various domains to evaluate its effectiveness in managing\nsensitive data while maintaining system efficiency."
                },
                "authors": [
                    {
                        "name": "Georgios Feretzakis"
                    },
                    {
                        "name": "Vassilios S. Verykios"
                    }
                ],
                "author_detail": {
                    "name": "Vassilios S. Verykios"
                },
                "author": "Vassilios S. Verykios",
                "arxiv_comment": "40 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18216v1",
                "updated": "2024-09-26T18:51:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    51,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T18:51:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    51,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following\n  Benchmark"
                },
                "summary": "Evaluating instruction following capabilities for multimodal, multi-turn\ndialogue is challenging. With potentially multiple instructions in the input\nmodel context, the task is time-consuming for human raters and we show LLM\nbased judges are biased towards answers from the same model. We propose\nMMMT-IF, an image based multi-turn Q$\\&$A evaluation set with added global\ninstructions between questions, constraining the answer format. This challenges\nmodels to retrieve instructions dispersed across long dialogues and reason\nunder instruction constraints. All instructions are objectively verifiable\nthrough code execution. We introduce the Programmatic Instruction Following\n($\\operatorname{PIF}$) metric to measure the fraction of the instructions that\nare correctly followed while performing a reasoning task. The\n$\\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by\nmeasuring the fraction of samples in a corpus where, for each sample, at least\nK out of N generated model responses achieve a $\\operatorname{PIF}$ score of\none. The $\\operatorname{PIF}$ metric aligns with human instruction following\nratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,\nGPT-4o, and Claude 3.5 Sonnet, have a $\\operatorname{PIF}$ metric that drops\nfrom 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across\nall turns, when each response is repeated 4 times ($\\operatorname{PIF-4-4}$),\nGPT-4o and Gemini successfully follow all instructions only $11\\%$ of the time.\nWhen all the instructions are also appended to the end of the model input\ncontext, the $\\operatorname{PIF}$ metric improves by 22.3 points on average,\nshowing that the challenge with the task lies not only in following the\ninstructions, but also in retrieving the instructions spread out in the model\ncontext. We plan to open source the MMMT-IF dataset and metric computation\ncode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating instruction following capabilities for multimodal, multi-turn\ndialogue is challenging. With potentially multiple instructions in the input\nmodel context, the task is time-consuming for human raters and we show LLM\nbased judges are biased towards answers from the same model. We propose\nMMMT-IF, an image based multi-turn Q$\\&$A evaluation set with added global\ninstructions between questions, constraining the answer format. This challenges\nmodels to retrieve instructions dispersed across long dialogues and reason\nunder instruction constraints. All instructions are objectively verifiable\nthrough code execution. We introduce the Programmatic Instruction Following\n($\\operatorname{PIF}$) metric to measure the fraction of the instructions that\nare correctly followed while performing a reasoning task. The\n$\\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by\nmeasuring the fraction of samples in a corpus where, for each sample, at least\nK out of N generated model responses achieve a $\\operatorname{PIF}$ score of\none. The $\\operatorname{PIF}$ metric aligns with human instruction following\nratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,\nGPT-4o, and Claude 3.5 Sonnet, have a $\\operatorname{PIF}$ metric that drops\nfrom 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across\nall turns, when each response is repeated 4 times ($\\operatorname{PIF-4-4}$),\nGPT-4o and Gemini successfully follow all instructions only $11\\%$ of the time.\nWhen all the instructions are also appended to the end of the model input\ncontext, the $\\operatorname{PIF}$ metric improves by 22.3 points on average,\nshowing that the challenge with the task lies not only in following the\ninstructions, but also in retrieving the instructions spread out in the model\ncontext. We plan to open source the MMMT-IF dataset and metric computation\ncode."
                },
                "authors": [
                    {
                        "name": "Elliot L. Epstein"
                    },
                    {
                        "name": "Kaisheng Yao"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xinyi Bai"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "arxiv_comment": "24 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18203v1",
                "updated": "2024-09-26T18:34:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    34,
                    16,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T18:34:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    34,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking"
                },
                "summary": "Whether a large language model policy is an explicit constitution or an\nimplicit reward model, it is challenging to assess coverage over the unbounded\nset of real-world situations that a policy must contend with. We introduce an\nAI policy design process inspired by mapmaking, which has developed tactics for\nvisualizing and iterating on maps even when full coverage is not possible. With\nPolicy Projector, policy designers can survey the landscape of model\ninput-output pairs, define custom regions (e.g., \"violence\"), and navigate\nthese regions with rules that can be applied to LLM outputs (e.g., if output\ncontains \"violence\" and \"graphic details,\" then rewrite without \"graphic\ndetails\"). Policy Projector supports interactive policy authoring using LLM\nclassification and steering and a map visualization reflecting the policy\ndesigner's work. In an evaluation with 12 AI safety experts, our system helps\npolicy designers to address problematic model behaviors extending beyond an\nexisting, comprehensive harm taxonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether a large language model policy is an explicit constitution or an\nimplicit reward model, it is challenging to assess coverage over the unbounded\nset of real-world situations that a policy must contend with. We introduce an\nAI policy design process inspired by mapmaking, which has developed tactics for\nvisualizing and iterating on maps even when full coverage is not possible. With\nPolicy Projector, policy designers can survey the landscape of model\ninput-output pairs, define custom regions (e.g., \"violence\"), and navigate\nthese regions with rules that can be applied to LLM outputs (e.g., if output\ncontains \"violence\" and \"graphic details,\" then rewrite without \"graphic\ndetails\"). Policy Projector supports interactive policy authoring using LLM\nclassification and steering and a map visualization reflecting the policy\ndesigner's work. In an evaluation with 12 AI safety experts, our system helps\npolicy designers to address problematic model behaviors extending beyond an\nexisting, comprehensive harm taxonomy."
                },
                "authors": [
                    {
                        "name": "Michelle S. Lam"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Jeffrey P. Bigham"
                    },
                    {
                        "name": "Kenneth Holstein"
                    },
                    {
                        "name": "Mary Beth Kery"
                    }
                ],
                "author_detail": {
                    "name": "Mary Beth Kery"
                },
                "author": "Mary Beth Kery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18193v1",
                "updated": "2024-09-26T18:10:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T18:10:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "LowREm: A Repository of Word Embeddings for 87 Low-Resource Languages\n  Enhanced with Multilingual Graph Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LowREm: A Repository of Word Embeddings for 87 Low-Resource Languages\n  Enhanced with Multilingual Graph Knowledge"
                },
                "summary": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Training LLMs for such languages is often difficult due to\ninsufficient data and high computational cost. Especially for very low resource\nlanguages, static word embeddings thus still offer a viable alternative. There\nis, however, a notable lack of comprehensive repositories with such embeddings\nfor diverse languages. To address this, we present LowREm, a centralized\nrepository of static embeddings for 87 low-resource languages. We also propose\na novel method to enhance GloVe-based embeddings by integrating multilingual\ngraph knowledge, utilizing another source of knowledge. We demonstrate the\nsuperior performance of our enhanced embeddings as compared to contextualized\nembeddings extracted from XLM-R on sentiment analysis. Our code and data are\npublicly available under https://huggingface.co/DFKI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Training LLMs for such languages is often difficult due to\ninsufficient data and high computational cost. Especially for very low resource\nlanguages, static word embeddings thus still offer a viable alternative. There\nis, however, a notable lack of comprehensive repositories with such embeddings\nfor diverse languages. To address this, we present LowREm, a centralized\nrepository of static embeddings for 87 low-resource languages. We also propose\na novel method to enhance GloVe-based embeddings by integrating multilingual\ngraph knowledge, utilizing another source of knowledge. We demonstrate the\nsuperior performance of our enhanced embeddings as compared to contextualized\nembeddings extracted from XLM-R on sentiment analysis. Our code and data are\npublicly available under https://huggingface.co/DFKI."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Rishu Kumar"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Short paper, preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18127v1",
                "updated": "2024-09-26T17:59:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLM: Multi-Modal Language Model of Egocentric Motions"
                },
                "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."
                },
                "authors": [
                    {
                        "name": "Fangzhou Hong"
                    },
                    {
                        "name": "Vladimir Guzov"
                    },
                    {
                        "name": "Hyo Jin Kim"
                    },
                    {
                        "name": "Yuting Ye"
                    },
                    {
                        "name": "Richard Newcombe"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Lingni Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lingni Ma"
                },
                "author": "Lingni Ma",
                "arxiv_comment": "Project Page: https://hongfz16.github.io/projects/EgoLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18111v1",
                "updated": "2024-09-26T17:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18164v1",
                "updated": "2024-09-26T17:30:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    30,
                    28,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:30:28Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    30,
                    28,
                    3,
                    270,
                    0
                ],
                "title": "Data-Prep-Kit: getting your data ready for LLM application development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Prep-Kit: getting your data ready for LLM application development"
                },
                "summary": "Data preparation is the first and a very important step towards any Large\nLanguage Model (LLM) development. This paper introduces an easy-to-use,\nextensible, and scale-flexible open-source data preparation toolkit called Data\nPrep Kit (DPK). DPK is architected and designed to enable users to scale their\ndata preparation to their needs. With DPK they can prepare data on a local\nmachine or effortlessly scale to run on a cluster with thousands of CPU Cores.\nDPK comes with a highly scalable, yet extensible set of modules that transform\nnatural language and code data. If the user needs additional transforms, they\ncan be easily developed using extensive DPK support for transform creation.\nThese modules can be used independently or pipelined to perform a series of\noperations. In this paper, we describe DPK architecture and show its\nperformance from a small scale to a very large number of CPUs. The modules from\nDPK have been used for the preparation of Granite Models [1] [2]. We believe\nDPK is a valuable contribution to the AI community to easily prepare data to\nenhance the performance of their LLM models or to fine-tune models with\nRetrieval-Augmented Generation (RAG).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data preparation is the first and a very important step towards any Large\nLanguage Model (LLM) development. This paper introduces an easy-to-use,\nextensible, and scale-flexible open-source data preparation toolkit called Data\nPrep Kit (DPK). DPK is architected and designed to enable users to scale their\ndata preparation to their needs. With DPK they can prepare data on a local\nmachine or effortlessly scale to run on a cluster with thousands of CPU Cores.\nDPK comes with a highly scalable, yet extensible set of modules that transform\nnatural language and code data. If the user needs additional transforms, they\ncan be easily developed using extensive DPK support for transform creation.\nThese modules can be used independently or pipelined to perform a series of\noperations. In this paper, we describe DPK architecture and show its\nperformance from a small scale to a very large number of CPUs. The modules from\nDPK have been used for the preparation of Granite Models [1] [2]. We believe\nDPK is a valuable contribution to the AI community to easily prepare data to\nenhance the performance of their LLM models or to fine-tune models with\nRetrieval-Augmented Generation (RAG)."
                },
                "authors": [
                    {
                        "name": "David Wood"
                    },
                    {
                        "name": "Boris Lublinsky"
                    },
                    {
                        "name": "Alexy Roytman"
                    },
                    {
                        "name": "Shivdeep Singh"
                    },
                    {
                        "name": "Abdulhamid Adebayo"
                    },
                    {
                        "name": "Revital Eres"
                    },
                    {
                        "name": "Mohammad Nassar"
                    },
                    {
                        "name": "Hima Patel"
                    },
                    {
                        "name": "Yousaf Shah"
                    },
                    {
                        "name": "Constantin Adam"
                    },
                    {
                        "name": "Petros Zerfos"
                    },
                    {
                        "name": "Nirmit Desai"
                    },
                    {
                        "name": "Daiki Tsuzuku"
                    },
                    {
                        "name": "Takuya Goto"
                    },
                    {
                        "name": "Michele Dolfi"
                    },
                    {
                        "name": "Saptha Surendran"
                    },
                    {
                        "name": "Paramesvaran Selvam"
                    },
                    {
                        "name": "Sungeun An"
                    },
                    {
                        "name": "Yuan Chi Chang"
                    },
                    {
                        "name": "Dhiraj Joshi"
                    },
                    {
                        "name": "Hajar Emami-Gohari"
                    },
                    {
                        "name": "Xuan-Hong Dang"
                    },
                    {
                        "name": "Yan Koyfman"
                    },
                    {
                        "name": "Shahrokh Daijavad"
                    }
                ],
                "author_detail": {
                    "name": "Shahrokh Daijavad"
                },
                "author": "Shahrokh Daijavad",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18162v1",
                "updated": "2024-09-26T17:19:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    25,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:19:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "The Nexus of AR/VR, Large Language Models, UI/UX, and Robotics\n  Technologies in Enhancing Learning and Social Interaction for Children: A\n  Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nexus of AR/VR, Large Language Models, UI/UX, and Robotics\n  Technologies in Enhancing Learning and Social Interaction for Children: A\n  Systematic Review"
                },
                "summary": "The combination of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is examined in\nthis review study. 150 publications were found by a thorough literature search\nthroughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 42 of them\nwere chosen for in-depth study due to their methodological rigor and relevance.\nThree primary areas are covered in this review: how AR can improve social and\nlearning results; how LLMs can help with communication; and how UI/UX design\naffects how effective these technologies are. Results reveal that while LLMs\ncan provide individualized learning and communication support, AR has\ndemonstrated promise in enhancing social skills, motivation, and attention. For\nchildren with ASD, accessible and interesting interventions depend heavily on\neffective UI/UX design. To optimize the benefits of these technologies in ASD\ntherapies, the study emphasizes the need for additional research to address\ndifficulties related to customization, accessibility, and integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is examined in\nthis review study. 150 publications were found by a thorough literature search\nthroughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 42 of them\nwere chosen for in-depth study due to their methodological rigor and relevance.\nThree primary areas are covered in this review: how AR can improve social and\nlearning results; how LLMs can help with communication; and how UI/UX design\naffects how effective these technologies are. Results reveal that while LLMs\ncan provide individualized learning and communication support, AR has\ndemonstrated promise in enhancing social skills, motivation, and attention. For\nchildren with ASD, accessible and interesting interventions depend heavily on\neffective UI/UX design. To optimize the benefits of these technologies in ASD\ntherapies, the study emphasizes the need for additional research to address\ndifficulties related to customization, accessibility, and integration."
                },
                "authors": [
                    {
                        "name": "Biplov Paneru"
                    },
                    {
                        "name": "Bishwash Paneru"
                    }
                ],
                "author_detail": {
                    "name": "Bishwash Paneru"
                },
                "author": "Bishwash Paneru",
                "arxiv_comment": "none",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]